[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v1",
                "updated": "2025-03-30T08:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v1",
                "updated": "2025-03-29T01:06:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v1",
                "updated": "2025-03-28T21:02:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Gra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Gra"
                },
                "author": "Fabian Gra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "Jos-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio Gonzlez"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Gonzlez"
                },
                "author": "Antonio Gonzlez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Morgane Rivire"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gal Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Andr Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Pluciska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Pder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Lonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Lonard Hussenot"
                },
                "author": "Lonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.24391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24391v1",
                "updated": "2025-03-31T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    58,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training"
                },
                "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/"
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Yuliang Xiu"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Anpei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Anpei Chen"
                },
                "author": "Anpei Chen",
                "arxiv_comment": "Page: https://easi3r.github.io/ Code:\n  https://github.com/Inception3D/Easi3R",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24388v1",
                "updated": "2025-03-31T17:59:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:59:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    59,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy"
                },
                "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhonghan Zhao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Haian Huang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Jianfei Gao"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24377v1",
                "updated": "2025-03-31T17:58:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:58:07Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Shudong Liu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Derek Fai Wong"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v3",
                "updated": "2025-03-31T17:55:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues"
                },
                "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yunbo Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Long"
                },
                "author": "Yunbo Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24376v1",
                "updated": "2025-03-31T17:55:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:55:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1"
                },
                "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."
                },
                "authors": [
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Lu Qiu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24374v1",
                "updated": "2025-03-31T17:53:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    53,
                    5,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:53:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    53,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERUPT: Efficient Rendering with Unposed Patch Transformer"
                },
                "summary": "This work addresses the problem of novel view synthesis in diverse scenes\nfrom small collections of RGB images. We propose ERUPT (Efficient Rendering\nwith Unposed Patch Transformer) a state-of-the-art scene reconstruction model\ncapable of efficient scene rendering using unposed imagery. We introduce\npatch-based querying, in contrast to existing pixel-based queries, to reduce\nthe compute required to render a target view. This makes our model highly\nefficient both during training and at inference, capable of rendering at 600\nfps on commercial hardware. Notably, our model is designed to use a learned\nlatent camera pose which allows for training using unposed targets in datasets\nwith sparse or inaccurate ground truth camera pose. We show that our approach\ncan generalize on large real-world data and introduce a new benchmark dataset\n(MSVS-1M) for latent view synthesis using street-view imagery collected from\nMapillary. In contrast to NeRF and Gaussian Splatting, which require dense\nimagery and precise metadata, ERUPT can render novel views of arbitrary scenes\nwith as few as five unposed input images. ERUPT achieves better rendered image\nquality than current state-of-the-art methods for unposed image synthesis\ntasks, reduces labeled data requirements by ~95\\% and decreases computational\nrequirements by an order of magnitude, providing efficient novel view synthesis\nfor diverse real-world scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the problem of novel view synthesis in diverse scenes\nfrom small collections of RGB images. We propose ERUPT (Efficient Rendering\nwith Unposed Patch Transformer) a state-of-the-art scene reconstruction model\ncapable of efficient scene rendering using unposed imagery. We introduce\npatch-based querying, in contrast to existing pixel-based queries, to reduce\nthe compute required to render a target view. This makes our model highly\nefficient both during training and at inference, capable of rendering at 600\nfps on commercial hardware. Notably, our model is designed to use a learned\nlatent camera pose which allows for training using unposed targets in datasets\nwith sparse or inaccurate ground truth camera pose. We show that our approach\ncan generalize on large real-world data and introduce a new benchmark dataset\n(MSVS-1M) for latent view synthesis using street-view imagery collected from\nMapillary. In contrast to NeRF and Gaussian Splatting, which require dense\nimagery and precise metadata, ERUPT can render novel views of arbitrary scenes\nwith as few as five unposed input images. ERUPT achieves better rendered image\nquality than current state-of-the-art methods for unposed image synthesis\ntasks, reduces labeled data requirements by ~95\\% and decreases computational\nrequirements by an order of magnitude, providing efficient novel view synthesis\nfor diverse real-world scenes."
                },
                "authors": [
                    {
                        "name": "Maxim V. Shugaev"
                    },
                    {
                        "name": "Vincent Chen"
                    },
                    {
                        "name": "Maxim Karrenbach"
                    },
                    {
                        "name": "Kyle Ashley"
                    },
                    {
                        "name": "Bridget Kennedy"
                    },
                    {
                        "name": "Naresh P. Cuntoor"
                    }
                ],
                "author_detail": {
                    "name": "Naresh P. Cuntoor"
                },
                "author": "Naresh P. Cuntoor",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24370v1",
                "updated": "2025-03-31T17:50:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:50:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively Controlling Reasoning Models through Thinking Intervention"
                },
                "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Chong Xiang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24368v1",
                "updated": "2025-03-31T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    47,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:47:42Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    47,
                    42,
                    0,
                    90,
                    0
                ],
                "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Vision Foundation Models for Real-time Ultrasound Image\n  Segmentation"
                },
                "summary": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications."
                },
                "authors": [
                    {
                        "name": "Xiaoran Zhang"
                    },
                    {
                        "name": "Eric Z. Chen"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Yikang Liu"
                    },
                    {
                        "name": "Boris Maihe"
                    },
                    {
                        "name": "James S. Duncan"
                    },
                    {
                        "name": "Terrence Chen"
                    },
                    {
                        "name": "Shanhui Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shanhui Sun"
                },
                "author": "Shanhui Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24367v1",
                "updated": "2025-03-31T17:46:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    46,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:46:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    46,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "The structure and topology of an amorphous metal-organic framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structure and topology of an amorphous metal-organic framework"
                },
                "summary": "Amorphous metal-organic frameworks are an important emerging materials class\nthat combine the attractive physical properties of the amorphous state with the\nversatility of metal-organic framework (MOF) chemistry. The structures of\namorphous MOFs have largely been inferred by drawing analogies to crystalline\npolymorphs and inorganic glasses, but ultimately the validity of such\nstructural models has been challenging to establish either experimentally or\ncomputationally. Here we use a unified data-driven approach, combining\nexperimental scattering data and active machine learning for interatomic\npotentials, to determine the structure of an amorphous zeolitic imidazolate\nframework (a-ZIF) -- the canonical amorphous MOF. Our results reveal clear\ndifferences between the structure of a-ZIF and that of other amorphous\ntetrahedral networks, allowing us to invalidate the long-standing assumption\nthat these inorganic and hybrid glasses are topologically equivalent. To this\nend, we introduce a systematic notation for the network topology of amorphous\nsolids, building a bridge to the successful use of topology analysis in\ncrystalline MOFs and to materials informatics. Our work provides insights into\nthe structure and topology of the archetypal amorphous MOF and opens up new\navenues for modelling and understanding amorphous framework materials more\ngenerally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amorphous metal-organic frameworks are an important emerging materials class\nthat combine the attractive physical properties of the amorphous state with the\nversatility of metal-organic framework (MOF) chemistry. The structures of\namorphous MOFs have largely been inferred by drawing analogies to crystalline\npolymorphs and inorganic glasses, but ultimately the validity of such\nstructural models has been challenging to establish either experimentally or\ncomputationally. Here we use a unified data-driven approach, combining\nexperimental scattering data and active machine learning for interatomic\npotentials, to determine the structure of an amorphous zeolitic imidazolate\nframework (a-ZIF) -- the canonical amorphous MOF. Our results reveal clear\ndifferences between the structure of a-ZIF and that of other amorphous\ntetrahedral networks, allowing us to invalidate the long-standing assumption\nthat these inorganic and hybrid glasses are topologically equivalent. To this\nend, we introduce a systematic notation for the network topology of amorphous\nsolids, building a bridge to the successful use of topology analysis in\ncrystalline MOFs and to materials informatics. Our work provides insights into\nthe structure and topology of the archetypal amorphous MOF and opens up new\navenues for modelling and understanding amorphous framework materials more\ngenerally."
                },
                "authors": [
                    {
                        "name": "Thomas C. Nicholas"
                    },
                    {
                        "name": "Daniel F. Thomas du Toit"
                    },
                    {
                        "name": "Louise A. M. Rosset"
                    },
                    {
                        "name": "Davide M. Proserpio"
                    },
                    {
                        "name": "Andrew L. Goodwin"
                    },
                    {
                        "name": "Volker L. Deringer"
                    }
                ],
                "author_detail": {
                    "name": "Volker L. Deringer"
                },
                "author": "Volker L. Deringer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24364v1",
                "updated": "2025-03-31T17:43:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    43,
                    36,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:43:36Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    43,
                    36,
                    0,
                    90,
                    0
                ],
                "title": "Query and Conquer: Execution-Guided SQL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query and Conquer: Execution-Guided SQL Generation"
                },
                "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation."
                },
                "authors": [
                    {
                        "name": "ukasz Borchmann"
                    },
                    {
                        "name": "Marek Wydmuch"
                    }
                ],
                "author_detail": {
                    "name": "Marek Wydmuch"
                },
                "author": "Marek Wydmuch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24354v1",
                "updated": "2025-03-31T17:34:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:34:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion"
                },
                "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts."
                },
                "authors": [
                    {
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "name": "Dongwen Tang"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24322v1",
                "updated": "2025-03-31T17:08:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    8,
                    57,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:08:57Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    8,
                    57,
                    0,
                    90,
                    0
                ],
                "title": "NoProp: Training Neural Networks without Back-propagation or\n  Forward-propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoProp: Training Neural Networks without Back-propagation or\n  Forward-propagation"
                },
                "summary": "The canonical deep learning approach for learning requires computing a\ngradient term at each layer by back-propagating the error signal from the\noutput towards each learnable parameter. Given the stacked structure of neural\nnetworks, where each layer builds on the representation of the layer below,\nthis approach leads to hierarchical representations. More abstract features\nlive on the top layers of the model, while features on lower layers are\nexpected to be less abstract. In contrast to this, we introduce a new learning\nmethod named NoProp, which does not rely on either forward or backwards\npropagation. Instead, NoProp takes inspiration from diffusion and flow matching\nmethods, where each layer independently learns to denoise a noisy target. We\nbelieve this work takes a first step towards introducing a new family of\ngradient-free learning methods, that does not learn hierarchical\nrepresentations -- at least not in the usual sense. NoProp needs to fix the\nrepresentation at each layer beforehand to a noised version of the target,\nlearning a local denoising process that can then be exploited at inference. We\ndemonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100\nimage classification benchmarks. Our results show that NoProp is a viable\nlearning algorithm which achieves superior accuracy, is easier to use and\ncomputationally more efficient compared to other existing back-propagation-free\nmethods. By departing from the traditional gradient based learning paradigm,\nNoProp alters how credit assignment is done within the network, enabling more\nefficient distributed learning as well as potentially impacting other\ncharacteristics of the learning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The canonical deep learning approach for learning requires computing a\ngradient term at each layer by back-propagating the error signal from the\noutput towards each learnable parameter. Given the stacked structure of neural\nnetworks, where each layer builds on the representation of the layer below,\nthis approach leads to hierarchical representations. More abstract features\nlive on the top layers of the model, while features on lower layers are\nexpected to be less abstract. In contrast to this, we introduce a new learning\nmethod named NoProp, which does not rely on either forward or backwards\npropagation. Instead, NoProp takes inspiration from diffusion and flow matching\nmethods, where each layer independently learns to denoise a noisy target. We\nbelieve this work takes a first step towards introducing a new family of\ngradient-free learning methods, that does not learn hierarchical\nrepresentations -- at least not in the usual sense. NoProp needs to fix the\nrepresentation at each layer beforehand to a noised version of the target,\nlearning a local denoising process that can then be exploited at inference. We\ndemonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100\nimage classification benchmarks. Our results show that NoProp is a viable\nlearning algorithm which achieves superior accuracy, is easier to use and\ncomputationally more efficient compared to other existing back-propagation-free\nmethods. By departing from the traditional gradient based learning paradigm,\nNoProp alters how credit assignment is done within the network, enabling more\nefficient distributed learning as well as potentially impacting other\ncharacteristics of the learning process."
                },
                "authors": [
                    {
                        "name": "Qinyu Li"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24320v1",
                "updated": "2025-03-31T17:07:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    7,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:07:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    7,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Can Test-Time Scaling Improve World Foundation Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Test-Time Scaling Improve World Foundation Model?"
                },
                "summary": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. The code is available at\nhttps://github.com/Mia-Cong/SWIFT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. The code is available at\nhttps://github.com/Mia-Cong/SWIFT.git."
                },
                "authors": [
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Bangya Liu"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24311v1",
                "updated": "2025-03-31T16:57:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    57,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:57:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    57,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "Selective Inference in Graphical Models via Maximum Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Inference in Graphical Models via Maximum Likelihood"
                },
                "summary": "The graphical lasso is a widely used algorithm for fitting undirected\nGaussian graphical models. However, for inference on functionals of edge values\nin the learned graph, standard tools lack formal statistical guarantees, such\nas control of the type I error rate. In this paper, we introduce a selective\ninference method for asymptotically valid inference after graphical lasso\nselection with added randomization. We obtain a selective likelihood,\nconditional on the event of selection, through a change of variable on the\nknown density of the randomization variables. Our method enables interval\nestimation and hypothesis testing for a wide range of functionals of edge\nvalues in the learned graph using the conditional maximum likelihood estimate.\nOur numerical studies show that introducing a small amount of randomization:\n(i) greatly increases power and yields substantially shorter intervals compared\nto other conditional inference methods, including data splitting; (ii) ensures\nintervals of bounded length in high-dimensional settings where data splitting\nis infeasible due to insufficient samples for inference; (iii) enables\ninference for a wide range of inferential targets in the learned graph,\nincluding measures of node influence and connectivity between nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The graphical lasso is a widely used algorithm for fitting undirected\nGaussian graphical models. However, for inference on functionals of edge values\nin the learned graph, standard tools lack formal statistical guarantees, such\nas control of the type I error rate. In this paper, we introduce a selective\ninference method for asymptotically valid inference after graphical lasso\nselection with added randomization. We obtain a selective likelihood,\nconditional on the event of selection, through a change of variable on the\nknown density of the randomization variables. Our method enables interval\nestimation and hypothesis testing for a wide range of functionals of edge\nvalues in the learned graph using the conditional maximum likelihood estimate.\nOur numerical studies show that introducing a small amount of randomization:\n(i) greatly increases power and yields substantially shorter intervals compared\nto other conditional inference methods, including data splitting; (ii) ensures\nintervals of bounded length in high-dimensional settings where data splitting\nis infeasible due to insufficient samples for inference; (iii) enables\ninference for a wide range of inferential targets in the learned graph,\nincluding measures of node influence and connectivity between nodes."
                },
                "authors": [
                    {
                        "name": "Sofia Guglielmini"
                    },
                    {
                        "name": "Gerda Claeskens"
                    },
                    {
                        "name": "Snigdha Panigrahi"
                    }
                ],
                "author_detail": {
                    "name": "Snigdha Panigrahi"
                },
                "author": "Snigdha Panigrahi",
                "arxiv_comment": "46 pages, 2 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24310v1",
                "updated": "2025-03-31T16:56:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:56:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models"
                },
                "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models."
                },
                "authors": [
                    {
                        "name": "Alok Abhishek"
                    },
                    {
                        "name": "Lisa Erickson"
                    },
                    {
                        "name": "Tushar Bandopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Bandopadhyay"
                },
                "author": "Tushar Bandopadhyay",
                "arxiv_comment": "32 pages, 33 figures, preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24307v1",
                "updated": "2025-03-31T16:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG"
                },
                "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility."
                },
                "authors": [
                    {
                        "name": "Arshia Kermani"
                    },
                    {
                        "name": "Veronica Perez-Rosas"
                    },
                    {
                        "name": "Vangelis Metsis"
                    }
                ],
                "author_detail": {
                    "name": "Vangelis Metsis"
                },
                "author": "Vangelis Metsis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24306v1",
                "updated": "2025-03-31T16:53:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    53,
                    9,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:53:09Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    53,
                    9,
                    0,
                    90,
                    0
                ],
                "title": "Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR)\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR)\n  Challenge"
                },
                "summary": "Understanding tissue motion in surgery is crucial to enable applications in\ndownstream tasks such as segmentation, 3D reconstruction, virtual tissue\nlandmarking, autonomous probe-based scanning, and subtask autonomy. Labeled\ndata are essential to enabling algorithms in these downstream tasks since they\nallow us to quantify and train algorithms. This paper introduces a point\ntracking challenge to address this, wherein participants can submit their\nalgorithms for quantification. The submitted algorithms are evaluated using a\ndataset named surgical tattoos in infrared (STIR), with the challenge aptly\nnamed the STIR Challenge 2024. The STIR Challenge 2024 comprises two\nquantitative components: accuracy and efficiency. The accuracy component tests\nthe accuracy of algorithms on in vivo and ex vivo sequences. The efficiency\ncomponent tests the latency of algorithm inference. The challenge was conducted\nas a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with\n4 teams submitting before and 4 submitting after challenge day. This paper\ndetails the STIR Challenge 2024, which serves to move the field towards more\naccurate and efficient algorithms for spatial understanding in surgery. In this\npaper we summarize the design, submissions, and results from the challenge. The\nchallenge dataset is available here: https://zenodo.org/records/14803158 , and\nthe code for baseline models and metric calculation is available here:\nhttps://github.com/athaddius/STIRMetrics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding tissue motion in surgery is crucial to enable applications in\ndownstream tasks such as segmentation, 3D reconstruction, virtual tissue\nlandmarking, autonomous probe-based scanning, and subtask autonomy. Labeled\ndata are essential to enabling algorithms in these downstream tasks since they\nallow us to quantify and train algorithms. This paper introduces a point\ntracking challenge to address this, wherein participants can submit their\nalgorithms for quantification. The submitted algorithms are evaluated using a\ndataset named surgical tattoos in infrared (STIR), with the challenge aptly\nnamed the STIR Challenge 2024. The STIR Challenge 2024 comprises two\nquantitative components: accuracy and efficiency. The accuracy component tests\nthe accuracy of algorithms on in vivo and ex vivo sequences. The efficiency\ncomponent tests the latency of algorithm inference. The challenge was conducted\nas a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with\n4 teams submitting before and 4 submitting after challenge day. This paper\ndetails the STIR Challenge 2024, which serves to move the field towards more\naccurate and efficient algorithms for spatial understanding in surgery. In this\npaper we summarize the design, submissions, and results from the challenge. The\nchallenge dataset is available here: https://zenodo.org/records/14803158 , and\nthe code for baseline models and metric calculation is available here:\nhttps://github.com/athaddius/STIRMetrics"
                },
                "authors": [
                    {
                        "name": "Adam Schmidt"
                    },
                    {
                        "name": "Mert Asim Karaoglu"
                    },
                    {
                        "name": "Soham Sinha"
                    },
                    {
                        "name": "Mingang Jang"
                    },
                    {
                        "name": "Ho-Gun Ha"
                    },
                    {
                        "name": "Kyungmin Jung"
                    },
                    {
                        "name": "Kyeongmo Gu"
                    },
                    {
                        "name": "Ihsan Ullah"
                    },
                    {
                        "name": "Hyunki Lee"
                    },
                    {
                        "name": "Jon erch"
                    },
                    {
                        "name": "Michal Neoral"
                    },
                    {
                        "name": "Ji Matas"
                    },
                    {
                        "name": "Rulin Zhou"
                    },
                    {
                        "name": "Wenlong He"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Hongliang Ren"
                    },
                    {
                        "name": "Bruno Silva"
                    },
                    {
                        "name": "Sandro Queirs"
                    },
                    {
                        "name": "Estvo Lima"
                    },
                    {
                        "name": "Joo L. Vilaa"
                    },
                    {
                        "name": "Shunsuke Kikuchi"
                    },
                    {
                        "name": "Atsushi Kouno"
                    },
                    {
                        "name": "Hiroki Matsuzaki"
                    },
                    {
                        "name": "Tongtong Li"
                    },
                    {
                        "name": "Yulu Chen"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Xiang Ma"
                    },
                    {
                        "name": "Xiaojian Li"
                    },
                    {
                        "name": "Mona Sheikh Zeinoddin"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Zafer Tandogdu"
                    },
                    {
                        "name": "Greg Shaw"
                    },
                    {
                        "name": "Evangelos Mazomenos"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Alexander Ladikos"
                    },
                    {
                        "name": "Simon DiMaio"
                    },
                    {
                        "name": "Septimiu E. Salcudean"
                    },
                    {
                        "name": "Omid Mohareri"
                    }
                ],
                "author_detail": {
                    "name": "Omid Mohareri"
                },
                "author": "Omid Mohareri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24293v1",
                "updated": "2025-03-31T16:41:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Is analogy enough to draw novel adjective-noun inferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is analogy enough to draw novel adjective-noun inferences?"
                },
                "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."
                },
                "authors": [
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Kathryn Davidson"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "arxiv_comment": "8 pages (16 pages with appendix). Submitted to SCiL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24289v1",
                "updated": "2025-03-31T16:36:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:36:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning"
                },
                "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Kun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Kun Qian"
                },
                "author": "Kun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v5",
                "updated": "2025-03-31T16:35:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    35,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "12 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22164v2",
                "updated": "2025-03-31T16:26:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    26,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T06:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents"
                },
                "summary": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management."
                },
                "authors": [
                    {
                        "name": "Bowen Gao"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yiqiao Liu"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yanyan Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Lan"
                },
                "author": "Yanyan Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24280v1",
                "updated": "2025-03-31T16:25:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    25,
                    50,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:25:50Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    25,
                    50,
                    0,
                    90,
                    0
                ],
                "title": "BOWIE-ALIGN: Sub-stellar metallicity and carbon depletion in the aligned\n  TrES-4b with JWST NIRSpec transmission spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOWIE-ALIGN: Sub-stellar metallicity and carbon depletion in the aligned\n  TrES-4b with JWST NIRSpec transmission spectroscopy"
                },
                "summary": "The formation and migration history of a planet is expected to be imprinted\nin its atmosphere, in particular its carbon-to-oxygen (C/O) ratio and\nmetallicity. The BOWIE-ALIGN programme is performing a comparative study of\nJWST spectra of four aligned and four misaligned hot Jupiters, with the aim of\ncharacterising their atmospheres and corroborating the link between the\nobservables and the formation history. In this work, we present the $2.8-5.2$\nmicron transmission spectrum of TrES-4b, a hot Jupiter with an orbit aligned\nwith the rotation axis of its F-type host star. Using free chemistry\natmospheric retrievals, we report a confident detection of H$_2$O at an\nabundance of $\\log X_\\mathrm{H_2O}=-2.98^{+0.68}_{-0.73}$ at a significance of\n$8.4\\sigma$. We also find evidence for CO and small amounts of CO$_2$,\nretrieving abundances $\\log X_\\mathrm{CO}= -3.76^{+0.89}_{-1.01}$ and $\\log\nX_\\mathrm{CO_2}= -6.86^{+0.62}_{-0.65}$ ($3.1\\sigma$ and $4.0\\sigma$\nrespectively). The observations are consistent with the the atmosphere being in\nchemical equilibrium; our retrievals yield $\\mathrm{C/O}$ between $0.30-0.42$\nand constrain the atmospheric metallicity to the range $0.4-0.7\\times$ solar.\nThe inferred sub-stellar properties (C/O and metallicity) challenge traditional\nmodels, and could have arisen from an oxygen-rich gas accretion scenario, or a\ncombination of low-metallicity gas and carbon-poor solid accretion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The formation and migration history of a planet is expected to be imprinted\nin its atmosphere, in particular its carbon-to-oxygen (C/O) ratio and\nmetallicity. The BOWIE-ALIGN programme is performing a comparative study of\nJWST spectra of four aligned and four misaligned hot Jupiters, with the aim of\ncharacterising their atmospheres and corroborating the link between the\nobservables and the formation history. In this work, we present the $2.8-5.2$\nmicron transmission spectrum of TrES-4b, a hot Jupiter with an orbit aligned\nwith the rotation axis of its F-type host star. Using free chemistry\natmospheric retrievals, we report a confident detection of H$_2$O at an\nabundance of $\\log X_\\mathrm{H_2O}=-2.98^{+0.68}_{-0.73}$ at a significance of\n$8.4\\sigma$. We also find evidence for CO and small amounts of CO$_2$,\nretrieving abundances $\\log X_\\mathrm{CO}= -3.76^{+0.89}_{-1.01}$ and $\\log\nX_\\mathrm{CO_2}= -6.86^{+0.62}_{-0.65}$ ($3.1\\sigma$ and $4.0\\sigma$\nrespectively). The observations are consistent with the the atmosphere being in\nchemical equilibrium; our retrievals yield $\\mathrm{C/O}$ between $0.30-0.42$\nand constrain the atmospheric metallicity to the range $0.4-0.7\\times$ solar.\nThe inferred sub-stellar properties (C/O and metallicity) challenge traditional\nmodels, and could have arisen from an oxygen-rich gas accretion scenario, or a\ncombination of low-metallicity gas and carbon-poor solid accretion."
                },
                "authors": [
                    {
                        "name": "Annabella Meech"
                    },
                    {
                        "name": "Alastair B. Claringbold"
                    },
                    {
                        "name": "Eva-Maria Ahrer"
                    },
                    {
                        "name": "James Kirk"
                    },
                    {
                        "name": "Mercedes Lpez-Morales"
                    },
                    {
                        "name": "Jake Taylor"
                    },
                    {
                        "name": "Richard A. Booth"
                    },
                    {
                        "name": "Anna B. T. Penzlin"
                    },
                    {
                        "name": "Lili Alderson"
                    },
                    {
                        "name": "Duncan A. Christie"
                    },
                    {
                        "name": "Emma Esparza-Borges"
                    },
                    {
                        "name": "Charlotte Fairman"
                    },
                    {
                        "name": "Nathan J. Mayne"
                    },
                    {
                        "name": "Mason McCormack"
                    },
                    {
                        "name": "James E. Owen"
                    },
                    {
                        "name": "Vatsal Panwar"
                    },
                    {
                        "name": "Diana Powell"
                    },
                    {
                        "name": "Denis E. Sergeev"
                    },
                    {
                        "name": "Daniel Valentine"
                    },
                    {
                        "name": "Hannah R. Wakeford"
                    },
                    {
                        "name": "Peter J. Wheatley"
                    },
                    {
                        "name": "Maria Zamyatina"
                    }
                ],
                "author_detail": {
                    "name": "Maria Zamyatina"
                },
                "author": "Maria Zamyatina",
                "arxiv_comment": "23 pages, 20 figures, 7 tables. Accepted to MNRAS on 26 March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24277v1",
                "updated": "2025-03-31T16:22:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:22:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality"
                },
                "summary": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae."
                },
                "authors": [
                    {
                        "name": "Sewoong Lee"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Marc E. Canby"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24270v2",
                "updated": "2025-04-01T03:16:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    3,
                    16,
                    38,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T16:16:10Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    16,
                    10,
                    0,
                    90,
                    0
                ],
                "title": "Visual Acoustic Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Acoustic Fields"
                },
                "summary": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/."
                },
                "authors": [
                    {
                        "name": "Yuelei Li"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "Fangneng Zhan"
                    },
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Mazeyu Ji"
                    },
                    {
                        "name": "Xiaojun Shan"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Paul Liang"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24266v1",
                "updated": "2025-03-31T16:12:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    12,
                    46,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:12:46Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    12,
                    46,
                    0,
                    90,
                    0
                ],
                "title": "EP240414a: Off-axis View of a Jet-Cocoon System from an Expanded\n  Progenitor Star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EP240414a: Off-axis View of a Jet-Cocoon System from an Expanded\n  Progenitor Star"
                },
                "summary": "When a relativistic jet is launched following the core-collapse of a star,\nits interaction with the stellar envelope leads to the formation of a hot\ncocoon, which produces various viewing-angle-dependent observational phenomena\nfollowing the breakout from the surface. We study the observational signatures\nof fast X-ray transient (FXT) EP240414a, which may originate from a jet-cocoon\nsystem viewed slightly off-axis. In our model, (1) the prompt X-ray emission\nlasting $\\sim\\! 100\\,{\\rm{s}}$ is attributed to the cooling emission from the\ninner cocoon (shocked jet material); (2) the $\\sim\\! 0.1\\,{\\rm{d}}$ X-ray\nemission comes from the inner cocoon's afterglow; (3) the $\\sim\\!\n0.4\\,{\\rm{d}}$ thermal-dominated optical emission arises from the cooling of\nthe outer cocoon (shocked stellar material); (4) the $\\sim\\! 3\\,{\\rm{d}}$\nnon-thermal optical component and subsequent radio emission can be explained by\nthe afterglow from a jet with a viewing angle of $10^{\\circ}\\lesssim\n\\theta_{\\rm{v}}\\lesssim15^\\circ$; and (5) the associated broad-lined Type Ic\nsupernova only dominates the optical emission after $\\sim\\! 7\\rm\\, d$. Both the\njet inferred from the off-axis afterglow and the inner cocoon constrained by\nthe cooling emission are found to have similar kinetic energies, on the order\nof $10^{51}\\,{\\rm{erg}}$. We find that the progenitor's radius is\n$\\sim3\\,R_\\odot$ as constrained by the { inner cocoon's} cooling emissions,\nindicating that the pre-explosion star may be a massive helium star that is\nslightly inflated. More FXTs associated with off-axis jets and supernovae will\nbe further examined by the Einstein Probe, leading to a deeper understanding of\njet-cocoon systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a relativistic jet is launched following the core-collapse of a star,\nits interaction with the stellar envelope leads to the formation of a hot\ncocoon, which produces various viewing-angle-dependent observational phenomena\nfollowing the breakout from the surface. We study the observational signatures\nof fast X-ray transient (FXT) EP240414a, which may originate from a jet-cocoon\nsystem viewed slightly off-axis. In our model, (1) the prompt X-ray emission\nlasting $\\sim\\! 100\\,{\\rm{s}}$ is attributed to the cooling emission from the\ninner cocoon (shocked jet material); (2) the $\\sim\\! 0.1\\,{\\rm{d}}$ X-ray\nemission comes from the inner cocoon's afterglow; (3) the $\\sim\\!\n0.4\\,{\\rm{d}}$ thermal-dominated optical emission arises from the cooling of\nthe outer cocoon (shocked stellar material); (4) the $\\sim\\! 3\\,{\\rm{d}}$\nnon-thermal optical component and subsequent radio emission can be explained by\nthe afterglow from a jet with a viewing angle of $10^{\\circ}\\lesssim\n\\theta_{\\rm{v}}\\lesssim15^\\circ$; and (5) the associated broad-lined Type Ic\nsupernova only dominates the optical emission after $\\sim\\! 7\\rm\\, d$. Both the\njet inferred from the off-axis afterglow and the inner cocoon constrained by\nthe cooling emission are found to have similar kinetic energies, on the order\nof $10^{51}\\,{\\rm{erg}}$. We find that the progenitor's radius is\n$\\sim3\\,R_\\odot$ as constrained by the { inner cocoon's} cooling emissions,\nindicating that the pre-explosion star may be a massive helium star that is\nslightly inflated. More FXTs associated with off-axis jets and supernovae will\nbe further examined by the Einstein Probe, leading to a deeper understanding of\njet-cocoon systems."
                },
                "authors": [
                    {
                        "name": "Jian-He Zheng"
                    },
                    {
                        "name": "Jin-Ping Zhu"
                    },
                    {
                        "name": "Wenbin Lu"
                    },
                    {
                        "name": "Bing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bing Zhang"
                },
                "author": "Bing Zhang",
                "arxiv_comment": "16 pages, 5 figures, 1 table. Revised version, first submitted on\n  March 1st",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09433v2",
                "updated": "2025-03-31T16:07:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    7,
                    10,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-12T14:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection"
                },
                "summary": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark."
                },
                "authors": [
                    {
                        "name": "Richard A. Dubniczky"
                    },
                    {
                        "name": "Krisztofer Zoltn Horvt"
                    },
                    {
                        "name": "Tams Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Tihanyi"
                },
                "author": "Norbert Tihanyi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07565v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07565v6",
                "updated": "2025-03-31T16:02:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    2,
                    38,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-10T17:37:39Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    37,
                    39,
                    0,
                    69,
                    0
                ],
                "title": "Inductive Moment Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Moment Matching"
                },
                "summary": "Diffusion models and Flow Matching generate high-quality samples but are slow\nat inference, and distilling them into few-step models often leads to\ninstability and extensive tuning. To resolve these trade-offs, we propose\nInductive Moment Matching (IMM), a new class of generative models for one- or\nfew-step sampling with a single-stage training procedure. Unlike distillation,\nIMM does not require pre-training initialization and optimization of two\nnetworks; and unlike Consistency Models, IMM guarantees distribution-level\nconvergence and remains stable under various hyperparameters and standard model\narchitectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID\nusing only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98\non CIFAR-10 for a model trained from scratch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models and Flow Matching generate high-quality samples but are slow\nat inference, and distilling them into few-step models often leads to\ninstability and extensive tuning. To resolve these trade-offs, we propose\nInductive Moment Matching (IMM), a new class of generative models for one- or\nfew-step sampling with a single-stage training procedure. Unlike distillation,\nIMM does not require pre-training initialization and optimization of two\nnetworks; and unlike Consistency Models, IMM guarantees distribution-level\nconvergence and remains stable under various hyperparameters and standard model\narchitectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID\nusing only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98\non CIFAR-10 for a model trained from scratch."
                },
                "authors": [
                    {
                        "name": "Linqi Zhou"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Jiaming Song"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Song"
                },
                "author": "Jiaming Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07565v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07565v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24245v1",
                "updated": "2025-03-31T15:58:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:58:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."
                },
                "authors": [
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yan Xin"
                    },
                    {
                        "name": "Jianzhong"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Charlie",
                "author": "Zhang",
                "arxiv_comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24242v1",
                "updated": "2025-03-31T15:54:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    54,
                    30,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:54:30Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    54,
                    30,
                    0,
                    90,
                    0
                ],
                "title": "Orlando's flask: detection of a lost-and-found valley on the Moon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orlando's flask: detection of a lost-and-found valley on the Moon"
                },
                "summary": "High angular resolution holds the key to extending our knowledge in several\ndomains of astronomical research. In addition to the development of new\ninstruments, advancements in post-processing algorithms can enhance the\nperformances attainable in an observation, turning archival observations into a\ntreasure. We developed a machine-learning tool, named zoom-in, that is able to\nimprove the angular resolution of an astronomical image by a factor of $\\sim\n100$ by optimally recombining short-cadence sequences of images. After training\nour model on real-life photographs, we tested our method on archival images of\nthe Moon taken through ESO instruments. We were able to achieve a remarkable\nspatial resolution of $\\sim 1$ m of the lunar surface. While analyzing one of\nthe fields from the sample, we discovered structures of clear anthropic origin\ninside the Aristarchus crater. The features appear to be consistent with\nancient ruins of cities and castles. A thorough analysis of the relevant\nliterature allowed us to conclude that this valley corresponds to the one\ndescribed in Ludovico Ariosto's \"Orlando Furioso\": a place where all the items\nlost by humans gather and pile up. Analyses of the surface brightness from our\nimages, indicating an abnormally high albedo of $\\sim 0.25$, further\ncorroborate this idea suggesting a conspicuous presence of glass. We infer the\npresence of >1 billion flasks of human wits on the lunar surface, whose origin\nwe investigate in detail. We urge for a dedicated mission, astolfo, to be\ncarried out by Artemis astronauts in order to recover human wits and bring them\nback to the Earth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High angular resolution holds the key to extending our knowledge in several\ndomains of astronomical research. In addition to the development of new\ninstruments, advancements in post-processing algorithms can enhance the\nperformances attainable in an observation, turning archival observations into a\ntreasure. We developed a machine-learning tool, named zoom-in, that is able to\nimprove the angular resolution of an astronomical image by a factor of $\\sim\n100$ by optimally recombining short-cadence sequences of images. After training\nour model on real-life photographs, we tested our method on archival images of\nthe Moon taken through ESO instruments. We were able to achieve a remarkable\nspatial resolution of $\\sim 1$ m of the lunar surface. While analyzing one of\nthe fields from the sample, we discovered structures of clear anthropic origin\ninside the Aristarchus crater. The features appear to be consistent with\nancient ruins of cities and castles. A thorough analysis of the relevant\nliterature allowed us to conclude that this valley corresponds to the one\ndescribed in Ludovico Ariosto's \"Orlando Furioso\": a place where all the items\nlost by humans gather and pile up. Analyses of the surface brightness from our\nimages, indicating an abnormally high albedo of $\\sim 0.25$, further\ncorroborate this idea suggesting a conspicuous presence of glass. We infer the\npresence of >1 billion flasks of human wits on the lunar surface, whose origin\nwe investigate in detail. We urge for a dedicated mission, astolfo, to be\ncarried out by Artemis astronauts in order to recover human wits and bring them\nback to the Earth."
                },
                "authors": [
                    {
                        "name": "Vito Squicciarini"
                    },
                    {
                        "name": "Irina Mirova"
                    },
                    {
                        "name": "Francis D. Anderson"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Wahman al-Khwarizmi"
                    }
                ],
                "author_detail": {
                    "name": "Wahman al-Khwarizmi"
                },
                "author": "Wahman al-Khwarizmi",
                "arxiv_comment": "19 pages, 7 figures, 1 table. Submitted for publication on 1st April\n  2025 to the prestigious journal Acta Prima Aprilia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24235v1",
                "updated": "2025-03-31T15:46:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:46:15Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models"
                },
                "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Zhihan Guo"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24228v1",
                "updated": "2025-03-31T15:41:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:41:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAARS: Persona Aligned Agentic Retail Shoppers"
                },
                "summary": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work."
                },
                "authors": [
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Leonardo Perelli"
                    },
                    {
                        "name": "Lorenzo Mainetti"
                    },
                    {
                        "name": "George Davidson"
                    },
                    {
                        "name": "Stefano D'Amato"
                    }
                ],
                "author_detail": {
                    "name": "Stefano D'Amato"
                },
                "author": "Stefano D'Amato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05804v3",
                "updated": "2025-03-31T15:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    30,
                    45,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-08T08:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    36,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection"
                },
                "summary": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method."
                },
                "authors": [
                    {
                        "name": "Mingyi Guo"
                    },
                    {
                        "name": "Yuyang Liu"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Zongying Lin"
                    },
                    {
                        "name": "Peixi Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18296v2",
                "updated": "2025-03-31T15:29:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    29,
                    24,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T03:02:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    2,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Surgical Action Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical Action Planning with Large Language Models"
                },
                "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy."
                },
                "authors": [
                    {
                        "name": "Mengya Xu"
                    },
                    {
                        "name": "Zhongzhen Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Qi Dou"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dou"
                },
                "author": "Qi Dou",
                "arxiv_comment": "10 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24206v1",
                "updated": "2025-03-31T15:24:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "Synthetic News Generation for Fake News Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic News Generation for Fake News Classification"
                },
                "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."
                },
                "authors": [
                    {
                        "name": "Abdul Sittar"
                    },
                    {
                        "name": "Luka Golob"
                    },
                    {
                        "name": "Mateja Smiljanic"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Smiljanic"
                },
                "author": "Mateja Smiljanic",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24198v1",
                "updated": "2025-03-31T15:16:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:16:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Jingxian Xu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Weichang Liu"
                    },
                    {
                        "name": "Hanbing Liu"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24193v1",
                "updated": "2025-03-31T15:09:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:09:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval"
                },
                "summary": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts."
                },
                "authors": [
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Timothy Christopher Heath"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Hugues Bouchard"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24191v1",
                "updated": "2025-03-31T15:08:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:08:06Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "title": "Output Constraints as Attack Surface: Exploiting Structured Generation\n  to Bypass LLM Safety Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Constraints as Attack Surface: Exploiting Structured Generation\n  to Bypass LLM Safety Mechanisms"
                },
                "summary": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed."
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui",
                "arxiv_comment": "15 pages, 13 figures, 4 tables Work In Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16306v2",
                "updated": "2025-03-31T15:07:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T04:08:35Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    4,
                    8,
                    35,
                    0,
                    176,
                    0
                ],
                "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Anamika Lochab"
                    },
                    {
                        "name": "Ananth Grama"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24190v1",
                "updated": "2025-03-31T15:07:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:07:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Implicit In-Context Learning: Evidence from Artificial Language\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit In-Context Learning: Evidence from Artificial Language\n  Experiments"
                },
                "summary": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Ma"
                    },
                    {
                        "name": "Qihui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Xu"
                },
                "author": "Qihui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11824v2",
                "updated": "2025-03-31T14:50:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    50,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-18T18:44:00Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    44,
                    0,
                    0,
                    323,
                    0
                ],
                "title": "Theoretical Foundations of Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Foundations of Conformal Prediction"
                },
                "summary": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book is about conformal prediction and related inferential techniques\nthat build on permutation tests and exchangeability. These techniques are\nuseful in a diverse array of tasks, including hypothesis testing and providing\nuncertainty quantification guarantees for machine learning systems. Much of the\ncurrent interest in conformal prediction is due to its ability to integrate\ninto complex machine learning workflows, solving the problem of forming\nprediction sets without any assumptions on the form of the data generating\ndistribution. Since contemporary machine learning algorithms have generally\nproven difficult to analyze directly, conformal prediction's main appeal is its\nability to provide formal, finite-sample guarantees when paired with such\nmethods.\n  The goal of this book is to teach the reader about the fundamental technical\narguments that arise when researching conformal prediction and related\nquestions in distribution-free inference. Many of these proof strategies,\nespecially the more recent ones, are scattered among research papers, making it\ndifficult for researchers to understand where to look, which results are\nimportant, and how exactly the proofs work. We hope to bridge this gap by\ncurating what we believe to be some of the most important results in the\nliterature and presenting their proofs in a unified language, with\nillustrations, and with an eye towards pedagogy."
                },
                "authors": [
                    {
                        "name": "Anastasios N. Angelopoulos"
                    },
                    {
                        "name": "Rina Foygel Barber"
                    },
                    {
                        "name": "Stephen Bates"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Bates"
                },
                "author": "Stephen Bates",
                "arxiv_comment": "This material will be published by Cambridge University Press as\n  Theoretical Foundations of Conformal Prediction by Anastasios N.\n  Angelopoulos, Rina Foygel Barber and Stephen Bates. This prepublication\n  version is free to view/download for personal use only. Not for\n  redistribution/resale/use in derivative works. Copyright Anastasios N.\n  Angelopoulos, Rina Foygel Barber and Stephen Bates, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v1",
                "updated": "2025-03-31T14:40:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05080v3",
                "updated": "2025-03-31T14:39:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    39,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-07T14:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery"
                },
                "summary": "The advancements of large language models (LLMs) have piqued growing interest\nin developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about their true\ncapabilities. In this work, we call for rigorous assessment of agents on\nindividual tasks in a scientific workflow before making bold claims on\nend-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using ScienceAgentBench, we evaluate\nfive open-weight and proprietary LLMs, each with three frameworks: direct\nprompting, OpenHands CodeAct, and self-debug. Given three attempts for each\ntask, the best-performing agent can only solve 32.4% of the tasks independently\nand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI\no1-preview with direct prompting and self-debug, which can boost the\nperformance to 42.2%, demonstrating the effectiveness of increasing\ninference-time compute but with more than 10 times the cost of other LLMs.\nStill, our results underscore the limitations of current language agents in\ngenerating code for data-driven discovery, let alone end-to-end automation for\nscientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements of large language models (LLMs) have piqued growing interest\nin developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about their true\ncapabilities. In this work, we call for rigorous assessment of agents on\nindividual tasks in a scientific workflow before making bold claims on\nend-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using ScienceAgentBench, we evaluate\nfive open-weight and proprietary LLMs, each with three frameworks: direct\nprompting, OpenHands CodeAct, and self-debug. Given three attempts for each\ntask, the best-performing agent can only solve 32.4% of the tasks independently\nand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI\no1-preview with direct prompting and self-debug, which can boost the\nperformance to 42.2%, demonstrating the effectiveness of increasing\ninference-time compute but with more than 10 times the cost of other LLMs.\nStill, our results underscore the limitations of current language agents in\ngenerating code for data-driven discovery, let alone end-to-end automation for\nscientific research."
                },
                "authors": [
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Qianheng Zhang"
                    },
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zitong Lu"
                    },
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Mingyi Xue"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Benjamin Burns"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xuhui Huang"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "ICLR 2025. 60 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04756v2",
                "updated": "2025-03-31T14:37:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    37,
                    40,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-07T08:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "Concept Navigation and Classification via Open-Source Large Language\n  Model Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Navigation and Classification via Open-Source Large Language\n  Model Processing"
                },
                "summary": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks."
                },
                "authors": [
                    {
                        "name": "Mal Kubli"
                    }
                ],
                "author_detail": {
                    "name": "Mal Kubli"
                },
                "author": "Mal Kubli",
                "arxiv_comment": "36 pages, 1 figure, 5 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01584v3",
                "updated": "2025-03-31T14:21:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    21,
                    49,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-03T18:10:38Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    10,
                    38,
                    0,
                    34,
                    0
                ],
                "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models"
                },
                "summary": "Existing benchmarks for frontier models often test specialized, \"PhD-level\"\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark with 594 problems based on the NPR Sunday Puzzle Challenge that\nrequires only general knowledge. Our benchmark is challenging for both humans\nand models; however correct solutions are easy to verify, and models' mistakes\nare easy to spot. As LLMs are more widely deployed in society, we believe it is\nuseful to develop benchmarks for frontier models that humans can understand\nwithout the need for deep domain expertise.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models on our benchmark,\ndespite being on par with other models when tested on benchmarks that test\nspecialized knowledge. Furthermore, our analysis of reasoning outputs uncovers\nnew kinds of failures. DeepSeek R1, for instance, often concedes with \"I give\nup\" before providing an answer that it knows is wrong. R1 can also be\nremarkably \"uncertain\" in its output and in rare cases, it does not \"finish\nthinking,\" which suggests the need for techniques to \"wrap up\" before the\ncontext window limit is reached. We also quantify the effectiveness of\nreasoning longer to identify the point beyond which more reasoning is unlikely\nto improve accuracy on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for frontier models often test specialized, \"PhD-level\"\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark with 594 problems based on the NPR Sunday Puzzle Challenge that\nrequires only general knowledge. Our benchmark is challenging for both humans\nand models; however correct solutions are easy to verify, and models' mistakes\nare easy to spot. As LLMs are more widely deployed in society, we believe it is\nuseful to develop benchmarks for frontier models that humans can understand\nwithout the need for deep domain expertise.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models on our benchmark,\ndespite being on par with other models when tested on benchmarks that test\nspecialized knowledge. Furthermore, our analysis of reasoning outputs uncovers\nnew kinds of failures. DeepSeek R1, for instance, often concedes with \"I give\nup\" before providing an answer that it knows is wrong. R1 can also be\nremarkably \"uncertain\" in its output and in rare cases, it does not \"finish\nthinking,\" which suggests the need for techniques to \"wrap up\" before the\ncontext window limit is reached. We also quantify the effectiveness of\nreasoning longer to identify the point beyond which more reasoning is unlikely\nto improve accuracy on our benchmark."
                },
                "authors": [
                    {
                        "name": "Zixuan Wu"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Aleksander Boruch-Gruszecki"
                    },
                    {
                        "name": "Jingmiao Zhao"
                    },
                    {
                        "name": "Carolyn Jane Anderson"
                    },
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Molly Q Feldman"
                    },
                    {
                        "name": "Arjun Guha"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Guha"
                },
                "author": "Arjun Guha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24127v1",
                "updated": "2025-03-31T14:12:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    12,
                    2,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:12:02Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    12,
                    2,
                    0,
                    90,
                    0
                ],
                "title": "Compression Metadata-assisted RoI Extraction and Adaptive Inference for\n  Efficient Video Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Metadata-assisted RoI Extraction and Adaptive Inference for\n  Efficient Video Analytics"
                },
                "summary": "Video analytics demand substantial computing resources, posing significant\nchallenges in computing resource-constrained environment. In this paper, to\nachieve high accuracy with acceptable computational workload, we propose a\ncost-effective regions of interest (RoIs) extraction and adaptive inference\nscheme based on the informative encoding metadata. Specifically, to achieve\nefficient RoI-based analytics, we explore motion vectors from encoding metadata\nto identify RoIs in non-reference frames through morphological opening\noperation. Furthermore, considering the content variation of RoIs, which calls\nfor inference by models with distinct size, we measure RoI complexity based on\nthe bitrate allocation information from encoding metadata. Finally, we design\nan algorithm that prioritizes scheduling RoIs to models of the appropriate\ncomplexity, balancing accuracy and latency. Extensive experimental results show\nthat our proposed scheme reduces latency by nearly 40% and improves 2.2% on\naverage in accuracy, outperforming the latest benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video analytics demand substantial computing resources, posing significant\nchallenges in computing resource-constrained environment. In this paper, to\nachieve high accuracy with acceptable computational workload, we propose a\ncost-effective regions of interest (RoIs) extraction and adaptive inference\nscheme based on the informative encoding metadata. Specifically, to achieve\nefficient RoI-based analytics, we explore motion vectors from encoding metadata\nto identify RoIs in non-reference frames through morphological opening\noperation. Furthermore, considering the content variation of RoIs, which calls\nfor inference by models with distinct size, we measure RoI complexity based on\nthe bitrate allocation information from encoding metadata. Finally, we design\nan algorithm that prioritizes scheduling RoIs to models of the appropriate\ncomplexity, balancing accuracy and latency. Extensive experimental results show\nthat our proposed scheme reduces latency by nearly 40% and improves 2.2% on\naverage in accuracy, outperforming the latest benchmarks."
                },
                "authors": [
                    {
                        "name": "Chengzhi Wang"
                    },
                    {
                        "name": "Peng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yang"
                },
                "author": "Peng Yang",
                "arxiv_comment": "Accepted by the IEEE ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24123v1",
                "updated": "2025-03-31T14:08:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    8,
                    58,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:08:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    8,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic\n  Learning"
                },
                "summary": "Many computational tasks benefit from being formulated as the composition of\nneural networks followed by a discrete symbolic program. The goal of\nneurosymbolic learning is to train the neural networks using only end-to-end\ninput-output labels of the composite. We introduce CTSketch, a novel, scalable\nneurosymbolic learning algorithm. CTSketch uses two techniques to improve the\nscalability of neurosymbolic inference: decompose the symbolic program into\nsub-programs and summarize each sub-program with a sketched tensor. This\nstrategy allows us to approximate the output distribution of the program with\nsimple tensor operations over the input distributions and summaries. We provide\ntheoretical insight into the maximum error of the approximation. Furthermore,\nwe evaluate CTSketch on many benchmarks from the neurosymbolic literature,\nincluding some designed for evaluating scalability. Our results show that\nCTSketch pushes neurosymbolic learning to new scales that have previously been\nunattainable by obtaining high accuracy on tasks involving over one thousand\ninputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many computational tasks benefit from being formulated as the composition of\nneural networks followed by a discrete symbolic program. The goal of\nneurosymbolic learning is to train the neural networks using only end-to-end\ninput-output labels of the composite. We introduce CTSketch, a novel, scalable\nneurosymbolic learning algorithm. CTSketch uses two techniques to improve the\nscalability of neurosymbolic inference: decompose the symbolic program into\nsub-programs and summarize each sub-program with a sketched tensor. This\nstrategy allows us to approximate the output distribution of the program with\nsimple tensor operations over the input distributions and summaries. We provide\ntheoretical insight into the maximum error of the approximation. Furthermore,\nwe evaluate CTSketch on many benchmarks from the neurosymbolic literature,\nincluding some designed for evaluating scalability. Our results show that\nCTSketch pushes neurosymbolic learning to new scales that have previously been\nunattainable by obtaining high accuracy on tasks involving over one thousand\ninputs."
                },
                "authors": [
                    {
                        "name": "Seewon Choi"
                    },
                    {
                        "name": "Alaia Solko-Breslin"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v1",
                "updated": "2025-03-31T14:06:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19004v2",
                "updated": "2025-03-31T14:03:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    3,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T14:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    41,
                    16,
                    0,
                    83,
                    0
                ],
                "title": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts"
                },
                "summary": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap."
                },
                "authors": [
                    {
                        "name": "Simon Goorney"
                    },
                    {
                        "name": "Eleni Karydi"
                    },
                    {
                        "name": "Borja Muoz"
                    },
                    {
                        "name": "Otto Santesson"
                    },
                    {
                        "name": "Zeki Can Seskir"
                    },
                    {
                        "name": "Ana Alina Tudoran"
                    },
                    {
                        "name": "Jacob Sherson"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Sherson"
                },
                "author": "Jacob Sherson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24110v1",
                "updated": "2025-03-31T14:01:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    1,
                    39,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:01:39Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    1,
                    39,
                    0,
                    90,
                    0
                ],
                "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to\n  Embodied Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to\n  Embodied Cognition"
                },
                "summary": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding."
                },
                "authors": [
                    {
                        "name": "Franois Olivier"
                    },
                    {
                        "name": "Zied Bouraoui"
                    }
                ],
                "author_detail": {
                    "name": "Zied Bouraoui"
                },
                "author": "Zied Bouraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v1",
                "updated": "2025-03-31T13:56:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?"
                },
                "summary": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advancements in Large\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates the\nlimitations of current LLMs across 200 languages using benchmarks such as\nFLORES-200. We also explore alternative data sources, including news articles\nand bilingual dictionaries, and demonstrate how knowledge distillation from\nlarge pre-trained models can significantly improve smaller LRL translations.\nAdditionally, we investigate various fine-tuning strategies, revealing that\nincremental enhancements markedly reduce performance gaps on smaller LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advancements in Large\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates the\nlimitations of current LLMs across 200 languages using benchmarks such as\nFLORES-200. We also explore alternative data sources, including news articles\nand bilingual dictionaries, and demonstrate how knowledge distillation from\nlarge pre-trained models can significantly improve smaller LRL translations.\nAdditionally, we investigate various fine-tuning strategies, revealing that\nincremental enhancements markedly reduce performance gaps on smaller LLMs."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v3",
                "updated": "2025-03-31T13:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    55,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro Scir"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24096v1",
                "updated": "2025-03-31T13:49:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    49,
                    43,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:49:43Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    49,
                    43,
                    0,
                    90,
                    0
                ],
                "title": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description"
                },
                "summary": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations."
                },
                "authors": [
                    {
                        "name": "Adrienne Deganutti"
                    },
                    {
                        "name": "Simon Hadfield"
                    },
                    {
                        "name": "Andrew Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gilbert"
                },
                "author": "Andrew Gilbert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00326v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00326v3",
                "updated": "2025-03-31T13:33:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    33,
                    54,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-29T05:59:53Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    5,
                    59,
                    53,
                    5,
                    181,
                    0
                ],
                "title": "Teola: Towards End-to-End Optimization of LLM-based Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teola: Towards End-to-End Optimization of LLM-based Applications"
                },
                "summary": "Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications. The code is available at\nhttps://github.com/NetX-lab/Ayo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications. The code is available at\nhttps://github.com/NetX-lab/Ayo."
                },
                "authors": [
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yitao Yang"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00326v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00326v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24078v1",
                "updated": "2025-03-31T13:33:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    33,
                    30,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    33,
                    30,
                    0,
                    90,
                    0
                ],
                "title": "A Complete Epistemic Temporal Logic for Intelligent Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Epistemic Temporal Logic for Intelligent Agent"
                },
                "summary": "In this paper, we present a complete epistemic temporal logic, called BPICTL,\nwhich generalizes CTL by introducing epistemic modalities. A sound and complete\ninference system of BPICTL is given. We prove the finite model property of\nBPICTL. Furthermore, we present a model checking algorithm for BPICTL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a complete epistemic temporal logic, called BPICTL,\nwhich generalizes CTL by introducing epistemic modalities. A sound and complete\ninference system of BPICTL is given. We prove the finite model property of\nBPICTL. Furthermore, we present a model checking algorithm for BPICTL."
                },
                "authors": [
                    {
                        "name": "Zining Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zining Cao"
                },
                "author": "Zining Cao",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08247v2",
                "updated": "2025-03-31T13:31:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    31,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-11T09:55:09Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    55,
                    9,
                    2,
                    346,
                    0
                ],
                "title": "MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues"
                },
                "summary": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kong Aik Lee"
                    },
                    {
                        "name": "Man-Wai Mak"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24074v1",
                "updated": "2025-03-31T13:30:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    30,
                    46,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:30:46Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    30,
                    46,
                    0,
                    90,
                    0
                ],
                "title": "Physics-informed neural networks for hidden boundary detection and flow\n  field reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed neural networks for hidden boundary detection and flow\n  field reconstruction"
                },
                "summary": "Simultaneously detecting hidden solid boundaries and reconstructing flow\nfields from sparse observations poses a significant inverse challenge in fluid\nmechanics. This study presents a physics-informed neural network (PINN)\nframework designed to infer the presence, shape, and motion of static or moving\nsolid boundaries within a flow field. By integrating a body fraction parameter\ninto the governing equations, the model enforces no-slip/no-penetration\nboundary conditions in solid regions while preserving conservation laws of\nfluid dynamics. Using partial flow field data, the method simultaneously\nreconstructs the unknown flow field and infers the body fraction distribution,\nthereby revealing solid boundaries. The framework is validated across diverse\nscenarios, including incompressible Navier-Stokes and compressible Euler flows,\nsuch as steady flow past a fixed cylinder, an inline oscillating cylinder, and\nsubsonic flow over an airfoil. The results demonstrate accurate detection of\nhidden boundaries, reconstruction of missing flow data, and estimation of\ntrajectories and velocities of a moving body. Further analysis examines the\neffects of data sparsity, velocity-only measurements, and noise on inference\naccuracy. The proposed method exhibits robustness and versatility, highlighting\nits potential for applications when only limited experimental or numerical data\nare available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously detecting hidden solid boundaries and reconstructing flow\nfields from sparse observations poses a significant inverse challenge in fluid\nmechanics. This study presents a physics-informed neural network (PINN)\nframework designed to infer the presence, shape, and motion of static or moving\nsolid boundaries within a flow field. By integrating a body fraction parameter\ninto the governing equations, the model enforces no-slip/no-penetration\nboundary conditions in solid regions while preserving conservation laws of\nfluid dynamics. Using partial flow field data, the method simultaneously\nreconstructs the unknown flow field and infers the body fraction distribution,\nthereby revealing solid boundaries. The framework is validated across diverse\nscenarios, including incompressible Navier-Stokes and compressible Euler flows,\nsuch as steady flow past a fixed cylinder, an inline oscillating cylinder, and\nsubsonic flow over an airfoil. The results demonstrate accurate detection of\nhidden boundaries, reconstruction of missing flow data, and estimation of\ntrajectories and velocities of a moving body. Further analysis examines the\neffects of data sparsity, velocity-only measurements, and noise on inference\naccuracy. The proposed method exhibits robustness and versatility, highlighting\nits potential for applications when only limited experimental or numerical data\nare available."
                },
                "authors": [
                    {
                        "name": "Yongzheng Zhu"
                    },
                    {
                        "name": "Weizheng Chen"
                    },
                    {
                        "name": "Jian Deng"
                    },
                    {
                        "name": "Xin Bian"
                    }
                ],
                "author_detail": {
                    "name": "Xin Bian"
                },
                "author": "Xin Bian",
                "arxiv_comment": "21 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24062v1",
                "updated": "2025-03-31T13:22:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    22,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:22:34Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    22,
                    34,
                    0,
                    90,
                    0
                ],
                "title": "Artificial Conversations, Real Results: Fostering Language Detection\n  with Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Conversations, Real Results: Fostering Language Detection\n  with Synthetic Data"
                },
                "summary": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs."
                },
                "authors": [
                    {
                        "name": "Fatemeh Mohammadi"
                    },
                    {
                        "name": "Tommaso Romano"
                    },
                    {
                        "name": "Samira Maghool"
                    },
                    {
                        "name": "Paolo Ceravolo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Ceravolo"
                },
                "author": "Paolo Ceravolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24053v1",
                "updated": "2025-03-31T13:15:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    15,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:15:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    15,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "ReaLM: Reliable and Efficient Large Language Model Inference with\n  Statistical Algorithm-Based Fault Tolerance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaLM: Reliable and Efficient Large Language Model Inference with\n  Statistical Algorithm-Based Fault Tolerance"
                },
                "summary": "The demand for efficient large language model (LLM) inference has propelled\nthe development of dedicated accelerators. As accelerators are vulnerable to\nhardware faults due to aging, variation, etc, existing accelerator designs\noften reserve a large voltage margin or leverage algorithm-based fault\ntolerance (ABFT) techniques to ensure LLM inference correctness. However,\nprevious methods often overlook the inherent fault tolerance of LLMs, leading\nto high computation and energy overhead. To enable reliable yet efficient LLM\ninference, in this paper, we propose a novel algorithm/circuit co-design\nframework, dubbed ReaLM. For the first time, we systematically characterize the\nfault tolerance of LLMs by performing a large-scale error injection study of\nrepresentative LLMs and natural language understanding tasks. Then, we propose\na statistical ABFT algorithm that fully leverages the error robustness to\nminimize error recovery as much as possible. We also customize the error\ndetection circuits to enable a low-cost online collection of error statistics.\nExtensive experiments show that with only 1.42% circuit area and 1.79% power\noverhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29.\nCompared to existing methods, ReaLM consistently reduces recovery costs across\ndifferent operating voltages and improves energy efficiency by up to 35.83%\nwithout compromising LLM performance. Our error injection code is available at\nhttps://github.com/2000012835xt/ReaLM-DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for efficient large language model (LLM) inference has propelled\nthe development of dedicated accelerators. As accelerators are vulnerable to\nhardware faults due to aging, variation, etc, existing accelerator designs\noften reserve a large voltage margin or leverage algorithm-based fault\ntolerance (ABFT) techniques to ensure LLM inference correctness. However,\nprevious methods often overlook the inherent fault tolerance of LLMs, leading\nto high computation and energy overhead. To enable reliable yet efficient LLM\ninference, in this paper, we propose a novel algorithm/circuit co-design\nframework, dubbed ReaLM. For the first time, we systematically characterize the\nfault tolerance of LLMs by performing a large-scale error injection study of\nrepresentative LLMs and natural language understanding tasks. Then, we propose\na statistical ABFT algorithm that fully leverages the error robustness to\nminimize error recovery as much as possible. We also customize the error\ndetection circuits to enable a low-cost online collection of error statistics.\nExtensive experiments show that with only 1.42% circuit area and 1.79% power\noverhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29.\nCompared to existing methods, ReaLM consistently reduces recovery costs across\ndifferent operating voltages and improves energy efficiency by up to 35.83%\nwithout compromising LLM performance. Our error injection code is available at\nhttps://github.com/2000012835xt/ReaLM-DAC."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Jiawang Zhao"
                    },
                    {
                        "name": "Zishen Wan"
                    },
                    {
                        "name": "Zuodong Zhang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "6 pages, 10 figures. Accepted by Design Automation Conference (DAC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v6",
                "updated": "2025-03-31T13:13:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    13,
                    27,
                    0,
                    90,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "TestART: Improving LLM-based Unit Testing via Co-evolution of Automated\n  Generation and Repair Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestART: Improving LLM-based Unit Testing via Co-evolution of Automated\n  Generation and Repair Iteration"
                },
                "summary": "Unit testing is crucial for detecting bugs in individual program units but\nconsumes time and effort. Recently, large language models (LLMs) have\ndemonstrated remarkable capabilities in generating unit test cases. However,\nseveral problems limit their ability to generate high-quality unit test cases:\n(1) compilation and runtime errors caused by the hallucination of LLMs; (2)\nlack of testing and coverage feedback information restricting the increase of\ncode coverage;(3) the repetitive suppression problem causing invalid LLM-based\nrepair and generation attempts. To address these limitations, we propose\nTestART, a novel unit test generation method. TestART improves LLM-based unit\ntesting via co-evolution of automated generation and repair iteration,\nrepresenting a significant advancement in automated unit test generation.\nTestART leverages the template-based repair strategy to effectively fix bugs in\nLLM-generated test cases for the first time. Meanwhile, TestART extracts\ncoverage information from successful test cases and uses it as coverage-guided\ntesting feedback. It also incorporates positive prompt injection to prevent\nrepetition suppression, thereby enhancing the sufficiency of the final test\ncase. This synergy between generation and repair elevates the correctness and\nsufficiency of the produced test cases significantly beyond previous methods.\nIn comparative experiments, TestART demonstrates an 18% improvement in pass\nrate and a 20% enhancement in coverage across three types of datasets compared\nto baseline models. Additionally, it achieves better coverage rates than\nEvoSuite with only half the number of test cases. These results demonstrate\nTestART's superior ability to produce high-quality unit test cases by\nharnessing the power of LLMs while overcoming their inherent flaws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing is crucial for detecting bugs in individual program units but\nconsumes time and effort. Recently, large language models (LLMs) have\ndemonstrated remarkable capabilities in generating unit test cases. However,\nseveral problems limit their ability to generate high-quality unit test cases:\n(1) compilation and runtime errors caused by the hallucination of LLMs; (2)\nlack of testing and coverage feedback information restricting the increase of\ncode coverage;(3) the repetitive suppression problem causing invalid LLM-based\nrepair and generation attempts. To address these limitations, we propose\nTestART, a novel unit test generation method. TestART improves LLM-based unit\ntesting via co-evolution of automated generation and repair iteration,\nrepresenting a significant advancement in automated unit test generation.\nTestART leverages the template-based repair strategy to effectively fix bugs in\nLLM-generated test cases for the first time. Meanwhile, TestART extracts\ncoverage information from successful test cases and uses it as coverage-guided\ntesting feedback. It also incorporates positive prompt injection to prevent\nrepetition suppression, thereby enhancing the sufficiency of the final test\ncase. This synergy between generation and repair elevates the correctness and\nsufficiency of the produced test cases significantly beyond previous methods.\nIn comparative experiments, TestART demonstrates an 18% improvement in pass\nrate and a 20% enhancement in coverage across three types of datasets compared\nto baseline models. Additionally, it achieves better coverage rates than\nEvoSuite with only half the number of test cases. These results demonstrate\nTestART's superior ability to produce high-quality unit test cases by\nharnessing the power of LLMs while overcoming their inherent flaws."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Kecheng Li"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Liuchuan Zhu"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24047v1",
                "updated": "2025-03-31T13:11:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:11:28Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents"
                },
                "summary": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery."
                },
                "authors": [
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Can Xie"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "34 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24044v1",
                "updated": "2025-03-31T13:08:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    8,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:08:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    8,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "Bi-Level Route Optimization and Path Planning with Hazard Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Level Route Optimization and Path Planning with Hazard Exploration"
                },
                "summary": "Effective risk monitoring in dynamic environments such as disaster zones\nrequires an adaptive exploration strategy to detect hidden threats. We propose\na bi-level unmanned aerial vehicle (UAV) monitoring strategy that efficiently\nintegrates high-level route optimization with low-level path planning for known\nand unknown hazards. At the high level, we formulate the route optimization as\na vehicle routing problem (VRP) to determine the optimal sequence for visiting\nknown hazard locations. To strategically incorporate exploration efficiency, we\nintroduce an edge-based centroidal Voronoi tessellation (CVT), which refines\nbaseline routes using pseudo-nodes and allocates path budgets based on the\nUAV's battery capacity using a line segment Voronoi diagram. At the low level,\npath planning maximizes information gain within the allocated path budget by\ngenerating kinematically feasible B-spline trajectories. Bayesian inference is\napplied to dynamically update hazard probabilities, enabling the UAVs to\nprioritize unexplored regions. Simulation results demonstrate that edge-based\nCVT improves spatial coverage and route uniformity compared to the node-based\nmethod. Additionally, our optimized path planning consistently outperforms\nbaselines in hazard discovery rates across a diverse set of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk monitoring in dynamic environments such as disaster zones\nrequires an adaptive exploration strategy to detect hidden threats. We propose\na bi-level unmanned aerial vehicle (UAV) monitoring strategy that efficiently\nintegrates high-level route optimization with low-level path planning for known\nand unknown hazards. At the high level, we formulate the route optimization as\na vehicle routing problem (VRP) to determine the optimal sequence for visiting\nknown hazard locations. To strategically incorporate exploration efficiency, we\nintroduce an edge-based centroidal Voronoi tessellation (CVT), which refines\nbaseline routes using pseudo-nodes and allocates path budgets based on the\nUAV's battery capacity using a line segment Voronoi diagram. At the low level,\npath planning maximizes information gain within the allocated path budget by\ngenerating kinematically feasible B-spline trajectories. Bayesian inference is\napplied to dynamically update hazard probabilities, enabling the UAVs to\nprioritize unexplored regions. Simulation results demonstrate that edge-based\nCVT improves spatial coverage and route uniformity compared to the node-based\nmethod. Additionally, our optimized path planning consistently outperforms\nbaselines in hazard discovery rates across a diverse set of scenarios."
                },
                "authors": [
                    {
                        "name": "Jimin Choi"
                    },
                    {
                        "name": "Grant Stagg"
                    },
                    {
                        "name": "Cameron K. Peterson"
                    },
                    {
                        "name": "Max Z. Li"
                    }
                ],
                "author_detail": {
                    "name": "Max Z. Li"
                },
                "author": "Max Z. Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06816v2",
                "updated": "2025-03-31T13:03:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    3,
                    14,
                    0,
                    90,
                    0
                ],
                "published": "2024-08-13T11:17:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty"
                },
                "summary": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Hwaran Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwaran Lee"
                },
                "author": "Hwaran Lee",
                "arxiv_comment": "Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13323v3",
                "updated": "2025-03-31T13:02:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    2,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-20T13:46:04Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "title": "Are Large Language Models Memorizing Bug Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Memorizing Bug Benchmarks?"
                },
                "summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities."
                },
                "authors": [
                    {
                        "name": "Daniel Ramos"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Kush Jain"
                    },
                    {
                        "name": "Paulo Canelas"
                    },
                    {
                        "name": "Catarina Gamboa"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24028v1",
                "updated": "2025-03-31T12:53:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    53,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:53:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    53,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Pay More Attention to the Robustness of Prompt for Instruction Data\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay More Attention to the Robustness of Prompt for Instruction Data\n  Mining"
                },
                "summary": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness."
                },
                "authors": [
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Dawei Feng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Bo Ding"
                    },
                    {
                        "name": "Huaimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huaimin Wang"
                },
                "author": "Huaimin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24021v1",
                "updated": "2025-03-31T12:48:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    48,
                    39,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:48:39Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    48,
                    39,
                    0,
                    90,
                    0
                ],
                "title": "IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos\n  Plots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos\n  Plots"
                },
                "summary": "Genomics data is essential in biological and medical domains, and\nbioinformatics analysts often manually create circos plots to analyze the data\nand extract valuable insights. However, creating circos plots is complex, as it\nrequires careful design for multiple track attributes and positional\nrelationships between them. Typically, analysts often seek inspiration from\nexisting circos plots, and they have to iteratively adjust and refine the plot\nto achieve a satisfactory final design, making the process both tedious and\ntime-intensive. To address these challenges, we propose IntelliCircos, an\nAI-powered interactive authoring tool that streamlines the process from initial\nvisual design to the final implementation of circos plots. Specifically, we\nbuild a new dataset containing 4396 circos plots with corresponding annotations\nand configurations, which are extracted and labeled from published papers. With\nthe dataset, we further identify track combination patterns, and utilize Large\nLanguage Model (LLM) to provide domain-specific design recommendations and\nconfiguration references to navigate the design of circos plots. We conduct a\nuser study with 8 bioinformatics analysts to evaluate IntelliCircos, and the\nresults demonstrate its usability and effectiveness in authoring circos plots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genomics data is essential in biological and medical domains, and\nbioinformatics analysts often manually create circos plots to analyze the data\nand extract valuable insights. However, creating circos plots is complex, as it\nrequires careful design for multiple track attributes and positional\nrelationships between them. Typically, analysts often seek inspiration from\nexisting circos plots, and they have to iteratively adjust and refine the plot\nto achieve a satisfactory final design, making the process both tedious and\ntime-intensive. To address these challenges, we propose IntelliCircos, an\nAI-powered interactive authoring tool that streamlines the process from initial\nvisual design to the final implementation of circos plots. Specifically, we\nbuild a new dataset containing 4396 circos plots with corresponding annotations\nand configurations, which are extracted and labeled from published papers. With\nthe dataset, we further identify track combination patterns, and utilize Large\nLanguage Model (LLM) to provide domain-specific design recommendations and\nconfiguration references to navigate the design of circos plots. We conduct a\nuser study with 8 bioinformatics analysts to evaluate IntelliCircos, and the\nresults demonstrate its usability and effectiveness in authoring circos plots."
                },
                "authors": [
                    {
                        "name": "Mingyang Gu"
                    },
                    {
                        "name": "Jiamin Zhu"
                    },
                    {
                        "name": "Qipeng Wang"
                    },
                    {
                        "name": "Fengjie Wang"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Min Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhu"
                },
                "author": "Min Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24016v1",
                "updated": "2025-03-31T12:40:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    40,
                    50,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:40:50Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    40,
                    50,
                    0,
                    90,
                    0
                ],
                "title": "Bayesian Predictive Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Predictive Coding"
                },
                "summary": "Predictive coding (PC) is an influential theory of information processing in\nthe brain, providing a biologically plausible alternative to backpropagation.\nIt is motivated in terms of Bayesian inference, as hidden states and parameters\nare optimised via gradient descent on variational free energy. However,\nimplementations of PC rely on maximum \\textit{a posteriori} (MAP) estimates of\nhidden states and maximum likelihood (ML) estimates of parameters, limiting\ntheir ability to quantify epistemic uncertainty. In this work, we investigate a\nBayesian extension to PC that estimates a posterior distribution over network\nparameters. This approach, termed Bayesian Predictive coding (BPC), preserves\nthe locality of PC and results in closed-form Hebbian weight updates. Compared\nto PC, our BPC algorithm converges in fewer epochs in the full-batch setting\nand remains competitive in the mini-batch setting. Additionally, we demonstrate\nthat BPC offers uncertainty quantification comparable to existing methods in\nBayesian deep learning, while also improving convergence properties. Together,\nthese results suggest that BPC provides a biologically plausible method for\nBayesian learning in the brain, as well as an attractive approach to\nuncertainty quantification in deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive coding (PC) is an influential theory of information processing in\nthe brain, providing a biologically plausible alternative to backpropagation.\nIt is motivated in terms of Bayesian inference, as hidden states and parameters\nare optimised via gradient descent on variational free energy. However,\nimplementations of PC rely on maximum \\textit{a posteriori} (MAP) estimates of\nhidden states and maximum likelihood (ML) estimates of parameters, limiting\ntheir ability to quantify epistemic uncertainty. In this work, we investigate a\nBayesian extension to PC that estimates a posterior distribution over network\nparameters. This approach, termed Bayesian Predictive coding (BPC), preserves\nthe locality of PC and results in closed-form Hebbian weight updates. Compared\nto PC, our BPC algorithm converges in fewer epochs in the full-batch setting\nand remains competitive in the mini-batch setting. Additionally, we demonstrate\nthat BPC offers uncertainty quantification comparable to existing methods in\nBayesian deep learning, while also improving convergence properties. Together,\nthese results suggest that BPC provides a biologically plausible method for\nBayesian learning in the brain, as well as an attractive approach to\nuncertainty quantification in deep learning."
                },
                "authors": [
                    {
                        "name": "Alexander Tschantz"
                    },
                    {
                        "name": "Magnus Koudahl"
                    },
                    {
                        "name": "Hampus Linander"
                    },
                    {
                        "name": "Lancelot Da Costa"
                    },
                    {
                        "name": "Conor Heins"
                    },
                    {
                        "name": "Jeff Beck"
                    },
                    {
                        "name": "Christopher Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Buckley"
                },
                "author": "Christopher Buckley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19475v2",
                "updated": "2025-03-31T12:39:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    39,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-27T06:00:30Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    0,
                    30,
                    4,
                    362,
                    0
                ],
                "title": "Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary\n  XL-MIMO Channel Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary\n  XL-MIMO Channel Tracking"
                },
                "summary": "This work considers a spatial non-stationary channel tracking problem in\nbroadband extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. In the case of spatial non-stationary, each scatterer has a certain\nvisibility region (VR) over antennas and power change may occur among visible\nantennas. Concentrating on the temporal correlation of XL-MIMO channels, we\ndesign a three-layer Markov prior model and hierarchical two-dimensional (2D)\nMarkov model to exploit the dynamic sparsity of sparse channel vectors and VRs,\nrespectively. Then, we formulate the channel tracking problem as a bilinear\nmeasurement process, and a novel dynamic alternating maximum a posteriori\n(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four\nbasic modules: channel estimation module, VR detection module, grid update\nmodule, and temporal correlated module. Specifically, the first module is an\ninverse-free variational Bayesian inference (IF-VBI) estimator that avoids\ncomputational intensive matrix inverse each iteration; the second module is a\nturbo compressive sensing (Turbo-CS) algorithm that only needs small-scale\nmatrix operations in a parallel fashion; the third module refines the\npolar-delay domain grid; and the fourth module can process the temporal prior\ninformation to ensure high-efficiency channel tracking. Simulations show that\nthe proposed method can achieve a significant channel tracking performance\nwhile achieving low computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work considers a spatial non-stationary channel tracking problem in\nbroadband extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. In the case of spatial non-stationary, each scatterer has a certain\nvisibility region (VR) over antennas and power change may occur among visible\nantennas. Concentrating on the temporal correlation of XL-MIMO channels, we\ndesign a three-layer Markov prior model and hierarchical two-dimensional (2D)\nMarkov model to exploit the dynamic sparsity of sparse channel vectors and VRs,\nrespectively. Then, we formulate the channel tracking problem as a bilinear\nmeasurement process, and a novel dynamic alternating maximum a posteriori\n(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four\nbasic modules: channel estimation module, VR detection module, grid update\nmodule, and temporal correlated module. Specifically, the first module is an\ninverse-free variational Bayesian inference (IF-VBI) estimator that avoids\ncomputational intensive matrix inverse each iteration; the second module is a\nturbo compressive sensing (Turbo-CS) algorithm that only needs small-scale\nmatrix operations in a parallel fashion; the third module refines the\npolar-delay domain grid; and the fourth module can process the temporal prior\ninformation to ensure high-efficiency channel tracking. Simulations show that\nthe proposed method can achieve a significant channel tracking performance\nwhile achieving low computational overhead."
                },
                "authors": [
                    {
                        "name": "Wenkang Xu"
                    },
                    {
                        "name": "An Liu"
                    },
                    {
                        "name": "Min-jian Zhao"
                    },
                    {
                        "name": "Giuseppe Caire"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "arxiv_comment": "13 pages, 11 figures,Submitted to IEEE TSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24011v1",
                "updated": "2025-03-31T12:38:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    38,
                    21,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    38,
                    21,
                    0,
                    90,
                    0
                ],
                "title": "Simulations in Statistical Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations in Statistical Workflows"
                },
                "summary": "Simulations play important and diverse roles in statistical workflows, for\nexample, in model specification, checking, validation, and even directly in\nmodel inference. Over the past decades, the application areas and overall\npotential of simulations in statistical workflows have expanded significantly,\ndriven by the development of new simulation-based algorithms and exponentially\nincreasing computational resources. In this paper, we examine past and current\ntrends in the field and offer perspectives on how simulations may shape the\nfuture of statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations play important and diverse roles in statistical workflows, for\nexample, in model specification, checking, validation, and even directly in\nmodel inference. Over the past decades, the application areas and overall\npotential of simulations in statistical workflows have expanded significantly,\ndriven by the development of new simulation-based algorithms and exponentially\nincreasing computational resources. In this paper, we examine past and current\ntrends in the field and offer perspectives on how simulations may shape the\nfuture of statistical practice."
                },
                "authors": [
                    {
                        "name": "Paul-Christian Brkner"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19250v2",
                "updated": "2025-03-31T12:24:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    24,
                    30,
                    0,
                    90,
                    0
                ],
                "published": "2024-09-28T05:48:51Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    5,
                    48,
                    51,
                    5,
                    272,
                    0
                ],
                "title": "Fast and Accurate Task Planning using Neuro-Symbolic Language Models and\n  Multi-level Goal Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Task Planning using Neuro-Symbolic Language Models and\n  Multi-level Goal Decomposition"
                },
                "summary": "In robotic task planning, symbolic planners using rule-based representations\nlike PDDL are effective but struggle with long-sequential tasks in complicated\nenvironments due to exponentially increasing search space. Meanwhile, LLM-based\napproaches, which are grounded in artificial neural networks, offer faster\ninference and commonsense reasoning but suffer from lower success rates. To\naddress the limitations of the current symbolic (slow speed) or LLM-based\napproaches (low accuracy), we propose a novel neuro-symbolic task planner that\ndecomposes complex tasks into subgoals using LLM and carries out task planning\nfor each subgoal using either symbolic or MCTS-based LLM planners, depending on\nthe subgoal complexity. This decomposition reduces planning time and improves\nsuccess rates by narrowing the search space and enabling LLMs to focus on more\nmanageable tasks. Our method significantly reduces planning time while\nmaintaining high success rates across task planning domains, as well as\nreal-world and simulated robotics environments. More details are available at\nhttp://graphics.ewha.ac.kr/LLMTAMP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotic task planning, symbolic planners using rule-based representations\nlike PDDL are effective but struggle with long-sequential tasks in complicated\nenvironments due to exponentially increasing search space. Meanwhile, LLM-based\napproaches, which are grounded in artificial neural networks, offer faster\ninference and commonsense reasoning but suffer from lower success rates. To\naddress the limitations of the current symbolic (slow speed) or LLM-based\napproaches (low accuracy), we propose a novel neuro-symbolic task planner that\ndecomposes complex tasks into subgoals using LLM and carries out task planning\nfor each subgoal using either symbolic or MCTS-based LLM planners, depending on\nthe subgoal complexity. This decomposition reduces planning time and improves\nsuccess rates by narrowing the search space and enabling LLMs to focus on more\nmanageable tasks. Our method significantly reduces planning time while\nmaintaining high success rates across task planning domains, as well as\nreal-world and simulated robotics environments. More details are available at\nhttp://graphics.ewha.ac.kr/LLMTAMP/."
                },
                "authors": [
                    {
                        "name": "Minseo Kwon"
                    },
                    {
                        "name": "Yaesol Kim"
                    },
                    {
                        "name": "Young J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young J. Kim"
                },
                "author": "Young J. Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23997v1",
                "updated": "2025-03-31T12:20:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    20,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:20:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    20,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "On the origin of Jupiter's fuzzy core: constraints from N-body, impact\n  and evolution simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the origin of Jupiter's fuzzy core: constraints from N-body, impact\n  and evolution simulations"
                },
                "summary": "It has been suggested that Jupiter's fuzzy core could be a result of a giant\nimpact. Here, we investigate the expected impact conditions from N-body\nsimulations. We then use state-of-the-art SPH simulations to investigate the\nresults of impacts with different conditions including various impactor masses\nand composition, different formation stages in Jupiter's growth, and different\nresolutions. We next simulate the long-term thermal evolution of Jupiter\npost-impact. We find that N-body simulations predict rather oblique impacts,\nand that head-on collisions are rare. Moreover, our results show that even\nunder a head-on collision, Jupiter's fuzzy core cannot be formed. We next\nsimulated Jupiter's thermal evolution and showed that unless post-impact\ntemperatures are extremely low, a giant impact would not lead to an extended\ndilute core as inferred by interior models. We conclude that Jupiter's fuzzy\ncore is not caused by an impact and is likely to be an outcome of its formation\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been suggested that Jupiter's fuzzy core could be a result of a giant\nimpact. Here, we investigate the expected impact conditions from N-body\nsimulations. We then use state-of-the-art SPH simulations to investigate the\nresults of impacts with different conditions including various impactor masses\nand composition, different formation stages in Jupiter's growth, and different\nresolutions. We next simulate the long-term thermal evolution of Jupiter\npost-impact. We find that N-body simulations predict rather oblique impacts,\nand that head-on collisions are rare. Moreover, our results show that even\nunder a head-on collision, Jupiter's fuzzy core cannot be formed. We next\nsimulated Jupiter's thermal evolution and showed that unless post-impact\ntemperatures are extremely low, a giant impact would not lead to an extended\ndilute core as inferred by interior models. We conclude that Jupiter's fuzzy\ncore is not caused by an impact and is likely to be an outcome of its formation\nprocess."
                },
                "authors": [
                    {
                        "name": "Thomas Meier"
                    },
                    {
                        "name": "Christian Reinhardt"
                    },
                    {
                        "name": "Sho Shibata"
                    },
                    {
                        "name": "Simon Mller"
                    },
                    {
                        "name": "Joachim Stadel"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "31 pages, 17 figures, 7 tables, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23992v1",
                "updated": "2025-03-31T12:09:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    9,
                    21,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:09:21Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    9,
                    21,
                    0,
                    90,
                    0
                ],
                "title": "A cost of capital approach to determining the LGD discount rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cost of capital approach to determining the LGD discount rate"
                },
                "summary": "Loss Given Default (LGD) is a key risk parameter in determining a bank's\nregulatory capital. During LGD-estimation, realised recovery cash flows are to\nbe discounted at an appropriate rate. Regulatory guidance mandates that this\nrate should allow for the time value of money, as well as include a risk\npremium that reflects the \"undiversifiable risk\" within these recoveries.\nHaving extensively reviewed earlier methods of determining this rate, we\npropose a new approach that is inspired by the cost of capital approach from\nthe Solvency II regulatory regime. Our method involves estimating a\nmarket-consistent price for a portfolio of defaulted loans, from which an\nassociated discount rate may be inferred. We apply this method to mortgage and\npersonal loans data from a large South African bank. The results reveal the\nmain drivers of the discount rate to be the mean and variance of these\nrecoveries, as well as the bank's cost of capital in excess of the risk-free\nrate. Our method therefore produces a discount rate that reflects both the\nundiversifiable risk of recovery recoveries and the time value of money,\nthereby satisfying regulatory requirements. This work can subsequently enhance\nthe LGD-component within the modelling of both regulatory and economic capital.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loss Given Default (LGD) is a key risk parameter in determining a bank's\nregulatory capital. During LGD-estimation, realised recovery cash flows are to\nbe discounted at an appropriate rate. Regulatory guidance mandates that this\nrate should allow for the time value of money, as well as include a risk\npremium that reflects the \"undiversifiable risk\" within these recoveries.\nHaving extensively reviewed earlier methods of determining this rate, we\npropose a new approach that is inspired by the cost of capital approach from\nthe Solvency II regulatory regime. Our method involves estimating a\nmarket-consistent price for a portfolio of defaulted loans, from which an\nassociated discount rate may be inferred. We apply this method to mortgage and\npersonal loans data from a large South African bank. The results reveal the\nmain drivers of the discount rate to be the mean and variance of these\nrecoveries, as well as the bank's cost of capital in excess of the risk-free\nrate. Our method therefore produces a discount rate that reflects both the\nundiversifiable risk of recovery recoveries and the time value of money,\nthereby satisfying regulatory requirements. This work can subsequently enhance\nthe LGD-component within the modelling of both regulatory and economic capital."
                },
                "authors": [
                    {
                        "name": "Janette Larney"
                    },
                    {
                        "name": "Arno Botha"
                    },
                    {
                        "name": "Gerrit Lodewicus Grobler"
                    },
                    {
                        "name": "Helgard Raubenheimer"
                    }
                ],
                "author_detail": {
                    "name": "Helgard Raubenheimer"
                },
                "author": "Helgard Raubenheimer",
                "arxiv_comment": "7374 words, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05886v2",
                "updated": "2025-03-31T12:01:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    1,
                    55,
                    0,
                    90,
                    0
                ],
                "published": "2025-01-10T11:39:25Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    39,
                    25,
                    4,
                    10,
                    0
                ],
                "title": "Modelling variability power spectra of active galaxies from irregular\n  time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling variability power spectra of active galaxies from irregular\n  time series"
                },
                "summary": "A common feature of Active Galactic Nuclei (AGN) is their random variations\nin brightness across the whole emission spectrum, from radio to $\\gamma$-rays.\nStudying the nature and origin of these fluctuations is critical to\ncharacterising the underlying variability process of the accretion flow that\npowers AGN. Random timing fluctuations are often studied with the power\nspectrum; this quantifies how the amplitude of variations is distributed over\ntemporal frequencies. Red noise variability -- when the power spectrum\nincreases smoothly towards low frequencies -- is ubiquitous in AGN. The\ncommonly used Fourier analysis methods, have significant challenges when\napplied to arbitrarily sampled light curves of red noise variability. Several\ntime-domain methods exist to infer the power spectral shape in the case of\nirregular sampling but they suffer from biases which can be difficult to\nmitigate, or are computationally expensive. In this paper, we demonstrate a\nmethod infer the shape of broad-band power spectra for irregular time series,\nusing a Gaussian process regression method scalable to large datasets. The\npower spectrum is modelled as a power-law model with one or two bends with\nflexible slopes. The method is fully Bayesian and we demonstrate its utility\nusing simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1\ngalaxy, is used as a test case and we find consistent results with the\nliterature using independent X-ray data from XMM-Newton and Swift. We provide\npublicly available, documented and tested implementations in Python and Julia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common feature of Active Galactic Nuclei (AGN) is their random variations\nin brightness across the whole emission spectrum, from radio to $\\gamma$-rays.\nStudying the nature and origin of these fluctuations is critical to\ncharacterising the underlying variability process of the accretion flow that\npowers AGN. Random timing fluctuations are often studied with the power\nspectrum; this quantifies how the amplitude of variations is distributed over\ntemporal frequencies. Red noise variability -- when the power spectrum\nincreases smoothly towards low frequencies -- is ubiquitous in AGN. The\ncommonly used Fourier analysis methods, have significant challenges when\napplied to arbitrarily sampled light curves of red noise variability. Several\ntime-domain methods exist to infer the power spectral shape in the case of\nirregular sampling but they suffer from biases which can be difficult to\nmitigate, or are computationally expensive. In this paper, we demonstrate a\nmethod infer the shape of broad-band power spectra for irregular time series,\nusing a Gaussian process regression method scalable to large datasets. The\npower spectrum is modelled as a power-law model with one or two bends with\nflexible slopes. The method is fully Bayesian and we demonstrate its utility\nusing simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1\ngalaxy, is used as a test case and we find consistent results with the\nliterature using independent X-ray data from XMM-Newton and Swift. We provide\npublicly available, documented and tested implementations in Python and Julia."
                },
                "authors": [
                    {
                        "name": "Mehdy Lefkir"
                    },
                    {
                        "name": "Simon Vaughan"
                    },
                    {
                        "name": "Daniela Huppenkothen"
                    },
                    {
                        "name": "Phil Uttley"
                    },
                    {
                        "name": "Vysakh Anilkumar"
                    }
                ],
                "author_detail": {
                    "name": "Vysakh Anilkumar"
                },
                "author": "Vysakh Anilkumar",
                "arxiv_comment": "21 pages, 16 figures, accepted for publication in MNRAS, code\n  available https://github.com/mlefkir/Pioran.jl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15687v3",
                "updated": "2025-03-31T12:00:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    0,
                    43,
                    0,
                    90,
                    0
                ],
                "published": "2024-07-22T14:54:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    54,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "SoftCVI: Contrastive variational inference with self-generated soft\n  labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftCVI: Contrastive variational inference with self-generated soft\n  labels"
                },
                "summary": "Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both are often\nchallenging to apply reliably, particularly when the posterior has complex\ngeometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI),\nwhich allows a family of variational objectives to be derived through a\ncontrastive estimation framework. The approach parameterizes a classifier in\nterms of a variational distribution, reframing the inference task as a\ncontrastive estimation problem aiming to identify a single true posterior\nsample among a set of samples. Despite this framing, we do not require positive\nor negative samples, but rather learn by sampling the variational distribution\nand computing ground truth soft classification labels from the unnormalized\nposterior itself. The objectives have zero variance gradient when the\nvariational approximation is exact, without the need for specialized gradient\nestimators. We empirically investigate the performance on a variety of Bayesian\ninference tasks, using both simple (e.g. normal) and expressive (normalizing\nflow) variational distributions. We find that SoftCVI can be used to form\nobjectives which are stable to train and mass-covering, frequently\noutperforming inference with other variational approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both are often\nchallenging to apply reliably, particularly when the posterior has complex\ngeometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI),\nwhich allows a family of variational objectives to be derived through a\ncontrastive estimation framework. The approach parameterizes a classifier in\nterms of a variational distribution, reframing the inference task as a\ncontrastive estimation problem aiming to identify a single true posterior\nsample among a set of samples. Despite this framing, we do not require positive\nor negative samples, but rather learn by sampling the variational distribution\nand computing ground truth soft classification labels from the unnormalized\nposterior itself. The objectives have zero variance gradient when the\nvariational approximation is exact, without the need for specialized gradient\nestimators. We empirically investigate the performance on a variety of Bayesian\ninference tasks, using both simple (e.g. normal) and expressive (normalizing\nflow) variational distributions. We find that SoftCVI can be used to form\nobjectives which are stable to train and mass-covering, frequently\noutperforming inference with other variational approaches."
                },
                "authors": [
                    {
                        "name": "Daniel Ward"
                    },
                    {
                        "name": "Mark Beaumont"
                    },
                    {
                        "name": "Matteo Fasiolo"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Fasiolo"
                },
                "author": "Matteo Fasiolo",
                "arxiv_comment": "Updated to match version accepted at ICLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23989v1",
                "updated": "2025-03-31T11:59:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    59,
                    43,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:59:43Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    59,
                    43,
                    0,
                    90,
                    0
                ],
                "title": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With\n  Question-Specific Rubrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With\n  Question-Specific Rubrics"
                },
                "summary": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness."
                },
                "authors": [
                    {
                        "name": "Aditya Pathak"
                    },
                    {
                        "name": "Rachit Gandhi"
                    },
                    {
                        "name": "Vaibhav Uttam"
                    },
                    {
                        "name": "Devansh"
                    },
                    {
                        "name": "Yashwanth Nakka"
                    },
                    {
                        "name": "Aaryan Raj Jindal"
                    },
                    {
                        "name": "Pratyush Ghosh"
                    },
                    {
                        "name": "Arnav Ramamoorthy"
                    },
                    {
                        "name": "Shreyash Verma"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Aashna Ased"
                    },
                    {
                        "name": "Chirag Khatri"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19777v2",
                "updated": "2025-03-31T11:44:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    44,
                    28,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-27T05:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    5,
                    33,
                    18,
                    3,
                    58,
                    0
                ],
                "title": "InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models"
                },
                "summary": "Prompt tuning has become a popular strategy for adapting Vision-Language\nModels (VLMs) to zero/few-shot visual recognition tasks. Some prompting\ntechniques introduce prior knowledge due to its richness, but when learnable\ntokens are randomly initialized and disconnected from prior knowledge, they\ntend to overfit on seen classes and struggle with domain shifts for unseen\nones. To address this issue, we propose the InPK model, which infuses\nclass-specific prior knowledge into the learnable tokens during initialization,\nthus enabling the model to explicitly focus on class-relevant information.\nFurthermore, to mitigate the weakening of class information by multi-layer\nencoders, we continuously reinforce the interaction between learnable tokens\nand prior knowledge across multiple feature levels. This progressive\ninteraction allows the learnable tokens to better capture the fine-grained\ndifferences and universal visual concepts within prior knowledge, enabling the\nmodel to extract more discriminative and generalized text features. Even for\nunseen classes, the learned interaction allows the model to capture their\ncommon representations and infer their appropriate positions within the\nexisting semantic structure. Moreover, we introduce a learnable text-to-vision\nprojection layer to accommodate the text adjustments, ensuring better alignment\nof visual-text semantics. Extensive experiments on 11 recognition datasets show\nthat InPK significantly outperforms state-of-the-art methods in multiple\nzero/few-shot image classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning has become a popular strategy for adapting Vision-Language\nModels (VLMs) to zero/few-shot visual recognition tasks. Some prompting\ntechniques introduce prior knowledge due to its richness, but when learnable\ntokens are randomly initialized and disconnected from prior knowledge, they\ntend to overfit on seen classes and struggle with domain shifts for unseen\nones. To address this issue, we propose the InPK model, which infuses\nclass-specific prior knowledge into the learnable tokens during initialization,\nthus enabling the model to explicitly focus on class-relevant information.\nFurthermore, to mitigate the weakening of class information by multi-layer\nencoders, we continuously reinforce the interaction between learnable tokens\nand prior knowledge across multiple feature levels. This progressive\ninteraction allows the learnable tokens to better capture the fine-grained\ndifferences and universal visual concepts within prior knowledge, enabling the\nmodel to extract more discriminative and generalized text features. Even for\nunseen classes, the learned interaction allows the model to capture their\ncommon representations and infer their appropriate positions within the\nexisting semantic structure. Moreover, we introduce a learnable text-to-vision\nprojection layer to accommodate the text adjustments, ensuring better alignment\nof visual-text semantics. Extensive experiments on 11 recognition datasets show\nthat InPK significantly outperforms state-of-the-art methods in multiple\nzero/few-shot image classification tasks."
                },
                "authors": [
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Shiyuan He"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Jie Zou"
                    },
                    {
                        "name": "Ning Xie"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03916v2",
                "updated": "2025-03-31T11:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    38,
                    20,
                    0,
                    90,
                    0
                ],
                "published": "2024-01-08T14:24:04Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    14,
                    24,
                    4,
                    0,
                    8,
                    0
                ],
                "title": "Estimation of nuclear polarization via discrete measurement of NV center\n  spin evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of nuclear polarization via discrete measurement of NV center\n  spin evolution"
                },
                "summary": "We propose a method for the estimation of the initial polarization of spinful\nnuclei of the 13 C isotope in diamond via a measurement of the evolution of the\ncoherence of an NV center spin qubit. Existing polarization measurement methods\nare difficult to implement experimentally, because they require direct\ninterference in the environment of the qubit. Here, in order to obtain the\ninformation, it is necessary to measure the qubit coherence at certain points\nof time, which are unambiguously determined by the applied magnetic field. For\nsufficiently high magnetic fields, the minimum value of the measured coherence\nconstitutes an upper bound on the product of the initial polarizations of each\nenvironmental spin. The most significant advantage of the method, which allows\nto infer initial values of nuclear polarizations without any direct access to\nthe environment, lies in its simplicity and the small amount of experimental\nresources that it requires. We exemplify the operation of the scheme on a\nrealistic, randomly generated environment of eight nuclear spins, obtaining a\nreasonably accurate estimation of the initial polarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for the estimation of the initial polarization of spinful\nnuclei of the 13 C isotope in diamond via a measurement of the evolution of the\ncoherence of an NV center spin qubit. Existing polarization measurement methods\nare difficult to implement experimentally, because they require direct\ninterference in the environment of the qubit. Here, in order to obtain the\ninformation, it is necessary to measure the qubit coherence at certain points\nof time, which are unambiguously determined by the applied magnetic field. For\nsufficiently high magnetic fields, the minimum value of the measured coherence\nconstitutes an upper bound on the product of the initial polarizations of each\nenvironmental spin. The most significant advantage of the method, which allows\nto infer initial values of nuclear polarizations without any direct access to\nthe environment, lies in its simplicity and the small amount of experimental\nresources that it requires. We exemplify the operation of the scheme on a\nrealistic, randomly generated environment of eight nuclear spins, obtaining a\nreasonably accurate estimation of the initial polarization."
                },
                "authors": [
                    {
                        "name": "Mateusz Kuniej"
                    },
                    {
                        "name": "Katarzyna Roszak"
                    }
                ],
                "author_detail": {
                    "name": "Katarzyna Roszak"
                },
                "author": "Katarzyna Roszak",
                "arxiv_doi": "10.1103/PhysRevA.111.032620",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevA.111.032620",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.03916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures",
                "arxiv_journal_ref": "Phys. Rev. A 111, 032620 (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23969v1",
                "updated": "2025-03-31T11:34:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    34,
                    9,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:34:09Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    34,
                    9,
                    0,
                    90,
                    0
                ],
                "title": "Electronic structure of UGe$_2$ at ambient pressure: comparison with\n  X-ray photoemission spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic structure of UGe$_2$ at ambient pressure: comparison with\n  X-ray photoemission spectra"
                },
                "summary": "Based on experimental crystallographic data, electronic structure of UGe$_2$\nhave been calculated and compared with our results of X-ray photoelectron\nspectroscopy (XPS) measurements. We employed two different advanced full\npotential (FP) methods: FP-local-orbital (FPLO) and FP-linear augmented plane\nwaves (Wien2k) codes for non-magnetic and ferromagnetic states. Starting from\nthe local spin-density approximation (LSDA) or generalised gradient\napproximation (GGA), we verified either the orbital polarisation (OP)\ncorrection or the GGA+U approach for the U 5f-electrons, changing\nCoulomb-repulsion energies U in the range 0-4 eV. Satisfying agreement was\nachieved between experimental and our calculated magnetic moments using\nab-initio LSDA+OP and non-ab-initio GGA+U approaches, the latter for realistic\nU values of 2-3 eV. We proved by the LSDA+OP approach an existence of the Fermi\nsurface nesting vector along the a axis, possibly responsible for the triplet\nsuperconducting pairing. The calculated data reveal predominantly an itinerant\nU 5f-electron character of bands near the Fermi level, EF, with only small\ncontributions from the U 6d and Ge 4p states. The experimental XPS spectrum of\nvalence bands (VB) also contains the sharp main 5f-electron peak at EF, a wide\nhump (around -2 eV), and broad small peaks at higher energies. In the\ncalculated XPS spectrum, the width of the main 5f-electron peak varies between\n0.8 and 1.4 eV, depending on a method used in computations, but the hump\nremains unresolved. A newly observed asymmetric 1-eV satellite in the\nexperimental 4f-core XPS spectrum together with known 3-eV and 7-eV satellites\nsuggest dual behaviour of U-5f-electrons in UGe$_2$, the feature is inferred\nalso from the VB studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on experimental crystallographic data, electronic structure of UGe$_2$\nhave been calculated and compared with our results of X-ray photoelectron\nspectroscopy (XPS) measurements. We employed two different advanced full\npotential (FP) methods: FP-local-orbital (FPLO) and FP-linear augmented plane\nwaves (Wien2k) codes for non-magnetic and ferromagnetic states. Starting from\nthe local spin-density approximation (LSDA) or generalised gradient\napproximation (GGA), we verified either the orbital polarisation (OP)\ncorrection or the GGA+U approach for the U 5f-electrons, changing\nCoulomb-repulsion energies U in the range 0-4 eV. Satisfying agreement was\nachieved between experimental and our calculated magnetic moments using\nab-initio LSDA+OP and non-ab-initio GGA+U approaches, the latter for realistic\nU values of 2-3 eV. We proved by the LSDA+OP approach an existence of the Fermi\nsurface nesting vector along the a axis, possibly responsible for the triplet\nsuperconducting pairing. The calculated data reveal predominantly an itinerant\nU 5f-electron character of bands near the Fermi level, EF, with only small\ncontributions from the U 6d and Ge 4p states. The experimental XPS spectrum of\nvalence bands (VB) also contains the sharp main 5f-electron peak at EF, a wide\nhump (around -2 eV), and broad small peaks at higher energies. In the\ncalculated XPS spectrum, the width of the main 5f-electron peak varies between\n0.8 and 1.4 eV, depending on a method used in computations, but the hump\nremains unresolved. A newly observed asymmetric 1-eV satellite in the\nexperimental 4f-core XPS spectrum together with known 3-eV and 7-eV satellites\nsuggest dual behaviour of U-5f-electrons in UGe$_2$, the feature is inferred\nalso from the VB studies."
                },
                "authors": [
                    {
                        "name": "M. Samsel-Czekaa"
                    },
                    {
                        "name": "M. Werwiski"
                    },
                    {
                        "name": "A. Szajek"
                    },
                    {
                        "name": "G. Chekowska"
                    },
                    {
                        "name": "R. Tro"
                    }
                ],
                "author_detail": {
                    "name": "R. Tro"
                },
                "arxiv_affiliation": "W. Trzebiatowski Institute of Low Temperature and Structure Research, Polish Academy of Sciences, Wrocaw, Poland",
                "author": "R. Tro",
                "arxiv_doi": "10.1016/j.intermet.2011.05.008",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.intermet.2011.05.008",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Intermetallics 19 (2011) 1411-1419",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23963v1",
                "updated": "2025-03-31T11:24:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    24,
                    53,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:24:53Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    24,
                    53,
                    0,
                    90,
                    0
                ],
                "title": "A Benchmark for Vision-Centric HD Mapping by V2I Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark for Vision-Centric HD Mapping by V2I Systems"
                },
                "summary": "Autonomous driving faces safety challenges due to a lack of global\nperspective and the semantic information of vectorized high-definition (HD)\nmaps. Information from roadside cameras can greatly expand the map perception\nrange through vehicle-to-infrastructure (V2I) communications. However, there is\nstill no dataset from the real world available for the study on map\nvectorization onboard under the scenario of vehicle-infrastructure cooperation.\nTo prosper the research on online HD mapping for Vehicle-Infrastructure\nCooperative Autonomous Driving (VICAD), we release a real-world dataset, which\ncontains collaborative camera frames from both vehicles and roadside\ninfrastructures, and provides human annotations of HD map elements. We also\npresent an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric\nV2I systems to construct vectorized maps. To reduce computation costs and\nfurther deploy V2I-HD on autonomous vehicles, we introduce a directionally\ndecoupled self-attention mechanism to V2I-HD. Extensive experiments show that\nV2I-HD has superior performance in real-time inference speed, as tested by our\nreal-world dataset. Abundant qualitative results also demonstrate stable and\nrobust map construction quality with low cost in complex and various driving\nscenes. As a benchmark, both source codes and the dataset have been released at\nOneDrive for the purpose of further study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving faces safety challenges due to a lack of global\nperspective and the semantic information of vectorized high-definition (HD)\nmaps. Information from roadside cameras can greatly expand the map perception\nrange through vehicle-to-infrastructure (V2I) communications. However, there is\nstill no dataset from the real world available for the study on map\nvectorization onboard under the scenario of vehicle-infrastructure cooperation.\nTo prosper the research on online HD mapping for Vehicle-Infrastructure\nCooperative Autonomous Driving (VICAD), we release a real-world dataset, which\ncontains collaborative camera frames from both vehicles and roadside\ninfrastructures, and provides human annotations of HD map elements. We also\npresent an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric\nV2I systems to construct vectorized maps. To reduce computation costs and\nfurther deploy V2I-HD on autonomous vehicles, we introduce a directionally\ndecoupled self-attention mechanism to V2I-HD. Extensive experiments show that\nV2I-HD has superior performance in real-time inference speed, as tested by our\nreal-world dataset. Abundant qualitative results also demonstrate stable and\nrobust map construction quality with low cost in complex and various driving\nscenes. As a benchmark, both source codes and the dataset have been released at\nOneDrive for the purpose of further study."
                },
                "authors": [
                    {
                        "name": "Miao Fan"
                    },
                    {
                        "name": "Shanshan Yu"
                    },
                    {
                        "name": "Shengtong Xu"
                    },
                    {
                        "name": "Kun Jiang"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Xiangzeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzeng Liu"
                },
                "author": "Xiangzeng Liu",
                "arxiv_comment": "Accepted by IEEE IV'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23959v2",
                "updated": "2025-04-01T08:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    34,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T11:18:27Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    18,
                    27,
                    0,
                    90,
                    0
                ],
                "title": "Local Information Matters: Inference Acceleration For Grounded\n  Conversation Generation Models Through Adaptive Local-Aware Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Information Matters: Inference Acceleration For Grounded\n  Conversation Generation Models Through Adaptive Local-Aware Token Pruning"
                },
                "summary": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop."
                },
                "authors": [
                    {
                        "name": "Bizhe Bai"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yadan Luo"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23957v1",
                "updated": "2025-03-31T11:15:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    15,
                    30,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:15:30Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    15,
                    30,
                    0,
                    90,
                    0
                ],
                "title": "Written in the Stars: How your (pens and) papers decide the fate of the\n  arXiverse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Written in the Stars: How your (pens and) papers decide the fate of the\n  arXiverse"
                },
                "summary": "We all love the ecstasy that comes with submitting papers to journals or\narXiv. Some have described it as yeeting their back-breaking products of labor\ninto the void, wishing they could never deal with them ever again. The very act\nof yeeting papers onto arXiv contributes to the expansion of the arXiverse;\nhowever, we have yet to quantify our contribution to the cause. In this work, I\ninvestigate the expansion of the arXiverse using the arXiv astro-ph submission\ndata from 1992 to date. I coin the term \"the arXiverse constant\", $a_0$, to\nquantify the rate of expansion of the arXiverse. I find that astro-ph as a\nwhole has a positive $a_0$, but this does not always hold true for the six\nsubcategories of astro-ph. I then investigate the temporal changes in $a_0$ for\nthe astro-ph subcategories and astro-ph as a whole, from which I infer the fate\nof the arXiverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We all love the ecstasy that comes with submitting papers to journals or\narXiv. Some have described it as yeeting their back-breaking products of labor\ninto the void, wishing they could never deal with them ever again. The very act\nof yeeting papers onto arXiv contributes to the expansion of the arXiverse;\nhowever, we have yet to quantify our contribution to the cause. In this work, I\ninvestigate the expansion of the arXiverse using the arXiv astro-ph submission\ndata from 1992 to date. I coin the term \"the arXiverse constant\", $a_0$, to\nquantify the rate of expansion of the arXiverse. I find that astro-ph as a\nwhole has a positive $a_0$, but this does not always hold true for the six\nsubcategories of astro-ph. I then investigate the temporal changes in $a_0$ for\nthe astro-ph subcategories and astro-ph as a whole, from which I infer the fate\nof the arXiverse."
                },
                "authors": [
                    {
                        "name": "Joanne Tan"
                    }
                ],
                "author_detail": {
                    "name": "Joanne Tan"
                },
                "author": "Joanne Tan",
                "arxiv_comment": "9 pages, 6 figures, 3 memes. Published in the 2024 issue of Acta\n  Prima Aprila. An arXiv resubmission after a year",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.pop-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21788v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21788v3",
                "updated": "2025-04-01T02:12:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    12,
                    44,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-12T12:53:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    53,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "PharMolixFM: All-Atom Foundation Models for Molecular Modeling and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharMolixFM: All-Atom Foundation Models for Molecular Modeling and\n  Generation"
                },
                "summary": "Structural biology relies on accurate three-dimensional biomolecular\nstructures to advance our understanding of biological functions, disease\nmechanisms, and therapeutics. While recent advances in deep learning have\nenabled the development of all-atom foundation models for molecular modeling\nand generation, existing approaches face challenges in generalization due to\nthe multi-modal nature of atomic data and the lack of comprehensive analysis of\ntraining and sampling strategies. To address these limitations, we propose\nPharMolixFM, a unified framework for constructing all-atom foundation models\nbased on multi-modal generative techniques. Our framework includes three\nvariants using state-of-the-art multi-modal generative models. By formulating\nmolecular tasks as a generalized denoising process with task-specific priors,\nPharMolixFM achieves robust performance across various structural biology\napplications. Experimental results demonstrate that PharMolixFM-Diff achieves\ncompetitive prediction accuracy in protein-small-molecule docking (83.9% vs.\n90.2% RMSD < 2{\\AA}, given pocket) with significantly improved inference speed.\nMoreover, we explore the empirical inference scaling law by introducing more\nsampling repeats or steps. Our code and model are available at\nhttps://github.com/PharMolix/OpenBioMed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural biology relies on accurate three-dimensional biomolecular\nstructures to advance our understanding of biological functions, disease\nmechanisms, and therapeutics. While recent advances in deep learning have\nenabled the development of all-atom foundation models for molecular modeling\nand generation, existing approaches face challenges in generalization due to\nthe multi-modal nature of atomic data and the lack of comprehensive analysis of\ntraining and sampling strategies. To address these limitations, we propose\nPharMolixFM, a unified framework for constructing all-atom foundation models\nbased on multi-modal generative techniques. Our framework includes three\nvariants using state-of-the-art multi-modal generative models. By formulating\nmolecular tasks as a generalized denoising process with task-specific priors,\nPharMolixFM achieves robust performance across various structural biology\napplications. Experimental results demonstrate that PharMolixFM-Diff achieves\ncompetitive prediction accuracy in protein-small-molecule docking (83.9% vs.\n90.2% RMSD < 2{\\AA}, given pocket) with significantly improved inference speed.\nMoreover, we explore the empirical inference scaling law by introducing more\nsampling repeats or steps. Our code and model are available at\nhttps://github.com/PharMolix/OpenBioMed."
                },
                "authors": [
                    {
                        "name": "Yizhen Luo"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Zaiqing Nie"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqing Nie"
                },
                "author": "Zaiqing Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21788v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21788v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13319v2",
                "updated": "2025-03-31T11:03:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    3,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-17T15:58:27Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    58,
                    27,
                    0,
                    76,
                    0
                ],
                "title": "MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale\n  Few-Step Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale\n  Few-Step Synthesis"
                },
                "summary": "Recently, open-source video diffusion models (VDMs), such as WanX, Magic141\nand HunyuanVideo, have been scaled to over 10 billion parameters. These\nlarge-scale VDMs have demonstrated significant improvements over smaller-scale\nVDMs across multiple dimensions, including enhanced visual quality and more\nnatural motion dynamics. However, these models face two major limitations: (1)\nHigh inference overhead: Large-scale VDMs require approximately 10 minutes to\nsynthesize a 28-step video on a single H100 GPU. (2) Limited in portrait video\nsynthesis: Models like WanX-I2V and HunyuanVideo-I2V often produce unnatural\nfacial expressions and movements in portrait videos. To address these\nchallenges, we propose MagicDistillation, a novel framework designed to reduce\ninference overhead while ensuring the generalization of VDMs for portrait video\nsynthesis. Specifically, we primarily use sufficiently high-quality talking\nvideo to fine-tune Magic141, which is dedicated to portrait video synthesis. We\nthen employ LoRA to effectively and efficiently fine-tune the fake DiT within\nthe step distillation framework known as distribution matching distillation\n(DMD). Following this, we apply weak-to-strong (W2S) distribution matching and\nminimize the discrepancy between the fake data distribution and the ground\ntruth distribution, thereby improving the visual fidelity and motion dynamics\nof the synthesized videos. Experimental results on portrait video synthesis\ndemonstrate the effectiveness of MagicDistillation, as our method surpasses\nEuler, LCM, and DMD baselines in both FID/FVD metrics and VBench. Moreover,\nMagicDistillation, requiring only 4 steps, also outperforms WanX-I2V (14B) and\nHunyuanVideo-I2V (13B) on visualization and VBench. Our project page is\nhttps://magicdistillation.github.io/MagicDistillation/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, open-source video diffusion models (VDMs), such as WanX, Magic141\nand HunyuanVideo, have been scaled to over 10 billion parameters. These\nlarge-scale VDMs have demonstrated significant improvements over smaller-scale\nVDMs across multiple dimensions, including enhanced visual quality and more\nnatural motion dynamics. However, these models face two major limitations: (1)\nHigh inference overhead: Large-scale VDMs require approximately 10 minutes to\nsynthesize a 28-step video on a single H100 GPU. (2) Limited in portrait video\nsynthesis: Models like WanX-I2V and HunyuanVideo-I2V often produce unnatural\nfacial expressions and movements in portrait videos. To address these\nchallenges, we propose MagicDistillation, a novel framework designed to reduce\ninference overhead while ensuring the generalization of VDMs for portrait video\nsynthesis. Specifically, we primarily use sufficiently high-quality talking\nvideo to fine-tune Magic141, which is dedicated to portrait video synthesis. We\nthen employ LoRA to effectively and efficiently fine-tune the fake DiT within\nthe step distillation framework known as distribution matching distillation\n(DMD). Following this, we apply weak-to-strong (W2S) distribution matching and\nminimize the discrepancy between the fake data distribution and the ground\ntruth distribution, thereby improving the visual fidelity and motion dynamics\nof the synthesized videos. Experimental results on portrait video synthesis\ndemonstrate the effectiveness of MagicDistillation, as our method surpasses\nEuler, LCM, and DMD baselines in both FID/FVD metrics and VBench. Moreover,\nMagicDistillation, requiring only 4 steps, also outperforms WanX-I2V (14B) and\nHunyuanVideo-I2V (13B) on visualization and VBench. Our project page is\nhttps://magicdistillation.github.io/MagicDistillation/."
                },
                "authors": [
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hanzhong Guo"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Michael Lingelbach"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23934v1",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in\n  Discriminative and Generative AI Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in\n  Discriminative and Generative AI Operations"
                },
                "summary": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models."
                },
                "authors": [
                    {
                        "name": "Adrin Snchez-Momp"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Konstantinos Katsaros"
                    },
                    {
                        "name": "Aftab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Aftab Khan"
                },
                "author": "Aftab Khan",
                "arxiv_doi": "10.3390/info16040281",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/info16040281",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published to MDPI Information - Artificial Intelligence Section",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06720v2",
                "updated": "2025-03-31T10:21:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    21,
                    54,
                    0,
                    90,
                    0
                ],
                "published": "2023-10-10T15:47:22Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    15,
                    47,
                    22,
                    1,
                    283,
                    0
                ],
                "title": "Asymptotic theory for Bayesian inference and prediction: from the\n  ordinary to a conditional Peaks-Over-Threshold method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic theory for Bayesian inference and prediction: from the\n  ordinary to a conditional Peaks-Over-Threshold method"
                },
                "summary": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT, the asymptotic theory for such\nproposals is missing. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper predictive\nstatistical approach has received no attention to date. In this paper we fill\nthis gap by developing the asymptotic theory of posterior distributions\n(consistency, contraction rates, asymptotic normality and asymptotic coverage\nof credible intervals) and prediction within the Bayesian framework in the POT\ncontext. We extend this asymptotic theory to account for cases where the focus\nis on the tail properties of the conditional distribution of a response\nvariable given a vector of random covariates. To enable accurate predictions of\nextreme events more severe than those previously observed, we derive the\nposterior predictive distribution as an estimator of the conditional\ndistribution of an out-of-sample random variable, given that it exceeds a\nsufficiently high threshold. We establish Wasserstein consistency of the\nposterior predictive distribution under both the unconditional and\ncovariate-conditional approaches and derive its contraction rates. Simulations\nshow the good performances of the proposed Bayesian inferential methods. The\nanalysis of the change in the frequency of financial crises over time shows the\nutility of our methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT, the asymptotic theory for such\nproposals is missing. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper predictive\nstatistical approach has received no attention to date. In this paper we fill\nthis gap by developing the asymptotic theory of posterior distributions\n(consistency, contraction rates, asymptotic normality and asymptotic coverage\nof credible intervals) and prediction within the Bayesian framework in the POT\ncontext. We extend this asymptotic theory to account for cases where the focus\nis on the tail properties of the conditional distribution of a response\nvariable given a vector of random covariates. To enable accurate predictions of\nextreme events more severe than those previously observed, we derive the\nposterior predictive distribution as an estimator of the conditional\ndistribution of an out-of-sample random variable, given that it exceeds a\nsufficiently high threshold. We establish Wasserstein consistency of the\nposterior predictive distribution under both the unconditional and\ncovariate-conditional approaches and derive its contraction rates. Simulations\nshow the good performances of the proposed Bayesian inferential methods. The\nanalysis of the change in the frequency of financial crises over time shows the\nutility of our methodology."
                },
                "authors": [
                    {
                        "name": "Clment Dombry"
                    },
                    {
                        "name": "Simone A. Padoan"
                    },
                    {
                        "name": "Stefano Rizzelli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Rizzelli"
                },
                "author": "Stefano Rizzelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G32, 62F15, 62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23924v1",
                "updated": "2025-03-31T10:16:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    16,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:16:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    16,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Hemorrhage and the Robustness Limits of Large Language Models"
                },
                "summary": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Gui-Song Xia"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Liangpei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "33 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22456v2",
                "updated": "2025-03-31T10:13:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    13,
                    48,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T14:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning"
                },
                "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Abdullah Vanlioglu"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Vanlioglu"
                },
                "author": "Abdullah Vanlioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14292v2",
                "updated": "2025-03-31T10:11:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    11,
                    20,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-21T16:39:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    39,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "Hypothesis testing of symmetry in quantum dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis testing of symmetry in quantum dynamics"
                },
                "summary": "Symmetry plays a crucial role in quantum physics, dictating the behavior and\ndynamics of physical systems. In this paper, we develop a hypothesis-testing\nframework for quantum dynamics symmetry using a limited number of queries to\nthe unknown unitary operation and establish the quantum max-relative entropy\nlower bound for the type-II error. We construct optimal ancilla-free protocols\nthat achieve optimal type-II error probability for testing time-reversal\nsymmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries.\nContrasting with the advantages of indefinite causal order strategies in\nvarious quantum information processing tasks, we show that parallel, adaptive,\nand indefinite causal order strategies have equal power for our tasks. We\nestablish optimal protocols for T-symmetry testing and Z-symmetry testing for 6\nand 5 queries, respectively, from which we infer that the type-II error\nexhibits a decay rate of $\\mathcal{O}(m^{-2})$ with respect to the number of\nqueries $m$. This represents a significant improvement over the basic\nrepetition protocols without using global entanglement, where the error decays\nat a slower rate of $\\mathcal{O}(m^{-1})$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetry plays a crucial role in quantum physics, dictating the behavior and\ndynamics of physical systems. In this paper, we develop a hypothesis-testing\nframework for quantum dynamics symmetry using a limited number of queries to\nthe unknown unitary operation and establish the quantum max-relative entropy\nlower bound for the type-II error. We construct optimal ancilla-free protocols\nthat achieve optimal type-II error probability for testing time-reversal\nsymmetry (T-symmetry) and diagonal symmetry (Z-symmetry) with limited queries.\nContrasting with the advantages of indefinite causal order strategies in\nvarious quantum information processing tasks, we show that parallel, adaptive,\nand indefinite causal order strategies have equal power for our tasks. We\nestablish optimal protocols for T-symmetry testing and Z-symmetry testing for 6\nand 5 queries, respectively, from which we infer that the type-II error\nexhibits a decay rate of $\\mathcal{O}(m^{-2})$ with respect to the number of\nqueries $m$. This represents a significant improvement over the basic\nrepetition protocols without using global entanglement, where the error decays\nat a slower rate of $\\mathcal{O}(m^{-1})$."
                },
                "authors": [
                    {
                        "name": "Yu-Ao Chen"
                    },
                    {
                        "name": "Chenghong Zhu"
                    },
                    {
                        "name": "Keming He"
                    },
                    {
                        "name": "Yingjian Liu"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "v2, 15 pages including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23899v1",
                "updated": "2025-03-31T09:48:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:48:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset"
                },
                "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance."
                },
                "authors": [
                    {
                        "name": "Diana Galvan-Sosa"
                    },
                    {
                        "name": "Gabrielle Gaudeau"
                    },
                    {
                        "name": "Pride Kavumba"
                    },
                    {
                        "name": "Yunmeng Li"
                    },
                    {
                        "name": "Hongyi gu"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "9 main pages (21 appendix pages), 7 figures, submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23895v1",
                "updated": "2025-03-31T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    35,
                    0,
                    90,
                    0
                ],
                "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented\n  Generation for Test-time Knowledge Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better wit than wealth: Dynamic Parametric Retrieval Augmented\n  Generation for Test-time Knowledge Enhancement"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23889v1",
                "updated": "2025-03-31T09:41:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Robust Predictive Routing for Internet of Vehicles Leveraging Both V2I\n  and V2V Links",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Predictive Routing for Internet of Vehicles Leveraging Both V2I\n  and V2V Links"
                },
                "summary": "With the developments of the Internet of Vehicles (IoV) from 4G to 5G,\nvehicle-to-infrastructure (V2I) communications are becoming attractive for\nvehicle users (VUEs) to obtain diverse cloud service through base stations\n(BSs). To tackle V2I link deterioration caused by blockage and out-of-coverage\ncases, multi-hop V2X routing with both vehicle-to-vehicle (V2V) and V2I links\nneeds to be investigated. However, traditional routing reacts to statistical or\nreal-time information, which may suffer link degradation during path switchover\nin fast-changing vehicular networks. Predictive routing protocols take timely\nactions by forecasting link connectivity, but they fail to satisfy specific QoS\nrequirements. Low robustness to link failures is also incurred without\nconsidering imperfect prediction. To build continual paths between VUEs and BSs\nfor QoS provision of cloud service, a robust predictive routing framework\n(ROPE) is proposed with three major components: 1) an early warning scheme\ndetects V2I link deterioration in advance via predicting vehicle mobility and\nlink signal strength to facilitate seamless path switchover; 2) a virtual\nrouting mechanism finds top3 paths that have the highest path strength and\nsatisfy the connectivity and hop count constraints based on the prediction\nresults to fulfill QoS requirements of cloud service; 3) a path verification\nprotocol checks availability and quality of the top3 paths shortly before\nswitchover and activates one qualified path for switchover to ensure routing\nrobustness. We implement ROPE in a simulation framework incorporating\nreal-world urban maps, microscopic traffic generation, geometry-based channel\nmodeling, and offline data analysis as well as online inference. Extensive\nsimulations demonstrate the superiority of ROPE over direct V2I communications\nand a connectivity-based predictive routing protocol under various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the developments of the Internet of Vehicles (IoV) from 4G to 5G,\nvehicle-to-infrastructure (V2I) communications are becoming attractive for\nvehicle users (VUEs) to obtain diverse cloud service through base stations\n(BSs). To tackle V2I link deterioration caused by blockage and out-of-coverage\ncases, multi-hop V2X routing with both vehicle-to-vehicle (V2V) and V2I links\nneeds to be investigated. However, traditional routing reacts to statistical or\nreal-time information, which may suffer link degradation during path switchover\nin fast-changing vehicular networks. Predictive routing protocols take timely\nactions by forecasting link connectivity, but they fail to satisfy specific QoS\nrequirements. Low robustness to link failures is also incurred without\nconsidering imperfect prediction. To build continual paths between VUEs and BSs\nfor QoS provision of cloud service, a robust predictive routing framework\n(ROPE) is proposed with three major components: 1) an early warning scheme\ndetects V2I link deterioration in advance via predicting vehicle mobility and\nlink signal strength to facilitate seamless path switchover; 2) a virtual\nrouting mechanism finds top3 paths that have the highest path strength and\nsatisfy the connectivity and hop count constraints based on the prediction\nresults to fulfill QoS requirements of cloud service; 3) a path verification\nprotocol checks availability and quality of the top3 paths shortly before\nswitchover and activates one qualified path for switchover to ensure routing\nrobustness. We implement ROPE in a simulation framework incorporating\nreal-world urban maps, microscopic traffic generation, geometry-based channel\nmodeling, and offline data analysis as well as online inference. Extensive\nsimulations demonstrate the superiority of ROPE over direct V2I communications\nand a connectivity-based predictive routing protocol under various scenarios."
                },
                "authors": [
                    {
                        "name": "Yawen Chang"
                    },
                    {
                        "name": "Xudong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Wang"
                },
                "author": "Xudong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23886v1",
                "updated": "2025-03-31T09:39:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    39,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:39:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    39,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database\n  Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database\n  Schema"
                },
                "summary": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation."
                },
                "authors": [
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Youhuan Li"
                    },
                    {
                        "name": "Yansong Feng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Yuequn Dou"
                    },
                    {
                        "name": "chuchu Gao"
                    },
                    {
                        "name": "Zebin Huang"
                    },
                    {
                        "name": "Zihui Si"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Zhaohai Sun"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Wenqiang Jin"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Jin"
                },
                "author": "Wenqiang Jin",
                "arxiv_comment": "19 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23873v1",
                "updated": "2025-03-31T09:23:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    23,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:23:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    23,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "Exploring In-Context Learning Capabilities of ChatGPT for Pathological\n  Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring In-Context Learning Capabilities of ChatGPT for Pathological\n  Speech Detection"
                },
                "summary": "Automatic pathological speech detection approaches have shown promising\nresults, gaining attention as potential diagnostic tools alongside costly\ntraditional methods. While these approaches can achieve high accuracy, their\nlack of interpretability limits their applicability in clinical practice. In\nthis paper, we investigate the use of multimodal Large Language Models (LLMs),\nspecifically ChatGPT-4o, for automatic pathological speech detection in a\nfew-shot in-context learning setting. Experimental results show that this\napproach not only delivers promising performance but also provides explanations\nfor its decisions, enhancing model interpretability. To further understand its\neffectiveness, we conduct an ablation study to analyze the impact of different\nfactors, such as input type and system prompts, on the final results. Our\nfindings highlight the potential of multimodal LLMs for further exploration and\nadvancement in automatic pathological speech detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic pathological speech detection approaches have shown promising\nresults, gaining attention as potential diagnostic tools alongside costly\ntraditional methods. While these approaches can achieve high accuracy, their\nlack of interpretability limits their applicability in clinical practice. In\nthis paper, we investigate the use of multimodal Large Language Models (LLMs),\nspecifically ChatGPT-4o, for automatic pathological speech detection in a\nfew-shot in-context learning setting. Experimental results show that this\napproach not only delivers promising performance but also provides explanations\nfor its decisions, enhancing model interpretability. To further understand its\neffectiveness, we conduct an ablation study to analyze the impact of different\nfactors, such as input type and system prompts, on the final results. Our\nfindings highlight the potential of multimodal LLMs for further exploration and\nadvancement in automatic pathological speech detection."
                },
                "authors": [
                    {
                        "name": "Mahdi Amiri"
                    },
                    {
                        "name": "Hatef Otroshi Shahreza"
                    },
                    {
                        "name": "Ina Kodrasi"
                    }
                ],
                "author_detail": {
                    "name": "Ina Kodrasi"
                },
                "author": "Ina Kodrasi",
                "arxiv_comment": "submitted to EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23869v1",
                "updated": "2025-03-31T09:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:18:42Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "title": "Communication-Efficient and Personalized Federated Foundation Model\n  Fine-Tuning via Tri-Matrix Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-Efficient and Personalized Federated Foundation Model\n  Fine-Tuning via Tri-Matrix Adaptation"
                },
                "summary": "In federated learning, fine-tuning pre-trained foundation models poses\nsignificant challenges, particularly regarding high communication cost and\nsuboptimal model performance due to data heterogeneity between the clients. To\naddress these issues, this paper introduces communication-efficient federated\nLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank\nadaptation approach with personalized model parameter aggregation. We first\npresents a novel LoRA parameter factorization by introducing a small-size dense\nmatrix, which can significantly reduce the communication cost and achieve\ncomparable empirical performance than transferring the low-rank parameter\nmatrix used by existing methods. Without violating data privacy, the server\nconsiders the client similarity in both training dataset and model parameter\nspace, and learns personalized weights for model aggregation. Our experiments\non various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only\nsignificantly reduces communication overhead but also improves performance\nunder not independently and identically distributed data conditions. In\naddition, CE-LoRA improves data privacy protection, effectively mitigating\ngradient-based data reconstruction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In federated learning, fine-tuning pre-trained foundation models poses\nsignificant challenges, particularly regarding high communication cost and\nsuboptimal model performance due to data heterogeneity between the clients. To\naddress these issues, this paper introduces communication-efficient federated\nLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank\nadaptation approach with personalized model parameter aggregation. We first\npresents a novel LoRA parameter factorization by introducing a small-size dense\nmatrix, which can significantly reduce the communication cost and achieve\ncomparable empirical performance than transferring the low-rank parameter\nmatrix used by existing methods. Without violating data privacy, the server\nconsiders the client similarity in both training dataset and model parameter\nspace, and learns personalized weights for model aggregation. Our experiments\non various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only\nsignificantly reduces communication overhead but also improves performance\nunder not independently and identically distributed data conditions. In\naddition, CE-LoRA improves data privacy protection, effectively mitigating\ngradient-based data reconstruction attacks."
                },
                "authors": [
                    {
                        "name": "Yongle Li"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Sheng Huang"
                    },
                    {
                        "name": "ZHeng ZHang"
                    },
                    {
                        "name": "Xiaotong Yuan"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17610v2",
                "updated": "2025-03-31T09:12:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    12,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-01-29T12:33:16Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    12,
                    33,
                    16,
                    2,
                    29,
                    0
                ],
                "title": "FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models\n  with Extremely Low Communication Overhead of One Bit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeedSign: Robust Full-parameter Federated Fine-tuning of Large Models\n  with Extremely Low Communication Overhead of One Bit"
                },
                "summary": "Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with\nprivate data from distributed clients by exchanging models rather than data\nunder the orchestration of a parameter server (PS). To overcome the bottleneck\nforged by the growing communication and memory overhead for clients in such\nsystems due to the growing model sizes, we propose \\textit{FeedSign}, an FFT\nalgorithm in which the upload and download payload for an aggregation step is\nexactly $1$ bit per step, while the memory overhead is squeezed to the amount\nneeded for inference. This is realized by utilizing zeroth-order (ZO)\noptimizers on large models and shared pseudo-random number generators (PRNG)\nacross devices to represent the gradient estimates as seed-sign pairs. We\nconduct theoretical analysis on FeedSign and show that it converges at an\nexponential rate $\\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed\nsteps under widely used assumptions. Moreover, FeedSign is found to be robust\nagainst data heterogeneity and Byzantine attacks. We conducted extensive\nexperiments on models across different structures and sizes (11M to 13B) and\nfound that the proposed method performs better or closely, depending on\nscenarios, compared to its ZO and FO counterparts, albeit with an\norders-of-magnitude lower communication overhead. We also discuss some\ninteresting advantages as byproducts guaranteed by the minimalistic design of\n\\textit{FeedSign}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning (FFT) attempts to fine-tune a pre-trained model with\nprivate data from distributed clients by exchanging models rather than data\nunder the orchestration of a parameter server (PS). To overcome the bottleneck\nforged by the growing communication and memory overhead for clients in such\nsystems due to the growing model sizes, we propose \\textit{FeedSign}, an FFT\nalgorithm in which the upload and download payload for an aggregation step is\nexactly $1$ bit per step, while the memory overhead is squeezed to the amount\nneeded for inference. This is realized by utilizing zeroth-order (ZO)\noptimizers on large models and shared pseudo-random number generators (PRNG)\nacross devices to represent the gradient estimates as seed-sign pairs. We\nconduct theoretical analysis on FeedSign and show that it converges at an\nexponential rate $\\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed\nsteps under widely used assumptions. Moreover, FeedSign is found to be robust\nagainst data heterogeneity and Byzantine attacks. We conducted extensive\nexperiments on models across different structures and sizes (11M to 13B) and\nfound that the proposed method performs better or closely, depending on\nscenarios, compared to its ZO and FO counterparts, albeit with an\norders-of-magnitude lower communication overhead. We also discuss some\ninteresting advantages as byproducts guaranteed by the minimalistic design of\n\\textit{FeedSign}."
                },
                "authors": [
                    {
                        "name": "Zhijie Cai"
                    },
                    {
                        "name": "Haolong Chen"
                    },
                    {
                        "name": "Guangxu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Guangxu Zhu"
                },
                "author": "Guangxu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23859v1",
                "updated": "2025-03-31T09:06:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:06:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks"
                },
                "summary": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting."
                },
                "authors": [
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "T. Cecconello"
                    },
                    {
                        "name": "A. Pilzer"
                    },
                    {
                        "name": "S. Palazzo"
                    },
                    {
                        "name": "N. Gupta"
                    },
                    {
                        "name": "A. M. Hopkins"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "G. Umana"
                    }
                ],
                "author_detail": {
                    "name": "G. Umana"
                },
                "author": "G. Umana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.24377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24377v1",
                "updated": "2025-03-31T17:58:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:58:07Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    58,
                    7,
                    0,
                    90,
                    0
                ],
                "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Shudong Liu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Derek Fai Wong"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22232v2",
                "updated": "2025-03-31T17:56:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    56,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T08:27:47Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    27,
                    47,
                    4,
                    87,
                    0
                ],
                "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks"
                },
                "summary": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
                },
                "authors": [
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_comment": "10 pages, 6 figures. Author's version; accepted and presented at the\n  IEEE 23rd International Conference on Trust, Security and Privacy in\n  Computing and Communications (TrustCom) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21080v3",
                "updated": "2025-03-31T17:55:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-27T01:41:34Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    1,
                    41,
                    34,
                    3,
                    86,
                    0
                ],
                "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues"
                },
                "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yunbo Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunbo Long"
                },
                "author": "Yunbo Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24376v1",
                "updated": "2025-03-31T17:55:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:55:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    55,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1"
                },
                "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."
                },
                "authors": [
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yuying Ge"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Lu Qiu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24370v1",
                "updated": "2025-03-31T17:50:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:50:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    50,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively Controlling Reasoning Models through Thinking Intervention"
                },
                "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Chong Xiang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12952v2",
                "updated": "2025-03-31T17:36:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    36,
                    36,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-17T09:06:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    6,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms"
                },
                "summary": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure."
                },
                "authors": [
                    {
                        "name": "Elif Dicle Demir"
                    },
                    {
                        "name": "Buse Bilgin"
                    },
                    {
                        "name": "Mehmet Cengiz Onbasli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Cengiz Onbasli"
                },
                "author": "Mehmet Cengiz Onbasli",
                "arxiv_comment": "6 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24354v1",
                "updated": "2025-03-31T17:34:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:34:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    34,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent\n  Diffusion"
                },
                "summary": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts."
                },
                "authors": [
                    {
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "name": "Dongwen Tang"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24347v1",
                "updated": "2025-03-31T17:32:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    32,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:32:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    32,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "Entanglement Distribution in Lossy Quantum Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement Distribution in Lossy Quantum Networks"
                },
                "summary": "Entanglement distribution is essential for unlocking the potential of\ndistributed quantum information processing. We consider an $N$-partite network\nwhere entanglement is distributed via a central source over lossy channels, and\nnetwork participants cooperate to establish entanglement between any two chosen\nparties under local operations and classical communication (LOCC) constraints.\nWe develop a general mathematical framework to assess the optimal average\nbipartite entanglement shared in a lossy distribution, and introduce a\ntractable lower bound by optimizing over a subset of single-parameter LOCC\ntransformations. Our results show that probabilistically extracting Bell pairs\nfrom W states is more advantageous than deterministically extracting them from\nGHZ-like states in lossy networks, with this advantage increasing with network\nsize. We further extend our analysis analytically, proving that W states remain\nmore effective in large-scale networks. These findings offer valuable insights\ninto the practical deployment of near-term networks, revealing a fundamental\ntrade-off between deterministic entanglement distribution protocols and\nloss-sensitive resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement distribution is essential for unlocking the potential of\ndistributed quantum information processing. We consider an $N$-partite network\nwhere entanglement is distributed via a central source over lossy channels, and\nnetwork participants cooperate to establish entanglement between any two chosen\nparties under local operations and classical communication (LOCC) constraints.\nWe develop a general mathematical framework to assess the optimal average\nbipartite entanglement shared in a lossy distribution, and introduce a\ntractable lower bound by optimizing over a subset of single-parameter LOCC\ntransformations. Our results show that probabilistically extracting Bell pairs\nfrom W states is more advantageous than deterministically extracting them from\nGHZ-like states in lossy networks, with this advantage increasing with network\nsize. We further extend our analysis analytically, proving that W states remain\nmore effective in large-scale networks. These findings offer valuable insights\ninto the practical deployment of near-term networks, revealing a fundamental\ntrade-off between deterministic entanglement distribution protocols and\nloss-sensitive resources."
                },
                "authors": [
                    {
                        "name": "Leonardo Oleynik"
                    },
                    {
                        "name": "Junaid ur Rehman"
                    },
                    {
                        "name": "Seid Koudia"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24310v1",
                "updated": "2025-03-31T16:56:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:56:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    56,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models"
                },
                "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models."
                },
                "authors": [
                    {
                        "name": "Alok Abhishek"
                    },
                    {
                        "name": "Lisa Erickson"
                    },
                    {
                        "name": "Tushar Bandopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Bandopadhyay"
                },
                "author": "Tushar Bandopadhyay",
                "arxiv_comment": "32 pages, 33 figures, preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24307v1",
                "updated": "2025-03-31T16:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    54,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Evaluation of LLM Strategies for Mental Health Text\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG"
                },
                "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility."
                },
                "authors": [
                    {
                        "name": "Arshia Kermani"
                    },
                    {
                        "name": "Veronica Perez-Rosas"
                    },
                    {
                        "name": "Vangelis Metsis"
                    }
                ],
                "author_detail": {
                    "name": "Vangelis Metsis"
                },
                "author": "Vangelis Metsis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24293v1",
                "updated": "2025-03-31T16:41:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    41,
                    16,
                    0,
                    90,
                    0
                ],
                "title": "Is analogy enough to draw novel adjective-noun inferences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is analogy enough to draw novel adjective-noun inferences?"
                },
                "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition."
                },
                "authors": [
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Kathryn Davidson"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "arxiv_comment": "8 pages (16 pages with appendix). Submitted to SCiL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24289v1",
                "updated": "2025-03-31T16:36:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:36:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning"
                },
                "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Kun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Kun Qian"
                },
                "author": "Kun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v5",
                "updated": "2025-03-31T16:35:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    35,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "12 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22164v2",
                "updated": "2025-03-31T16:26:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    26,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T06:02:53Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    2,
                    53,
                    4,
                    87,
                    0
                ],
                "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents"
                },
                "summary": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel small molecule drugs remains a critical scientific\nchallenge with far-reaching implications for treating diseases and advancing\nhuman health. Traditional drug development--especially for small molecule\ntherapeutics--is a highly complex, resource-intensive, and time-consuming\nprocess that requires multidisciplinary collaboration. Recent breakthroughs in\nartificial intelligence (AI), particularly the rise of large language models\n(LLMs), present a transformative opportunity to streamline and accelerate this\nprocess. In this paper, we introduce PharmAgents, a virtual pharmaceutical\necosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates\nthe full drug discovery workflow--from target discovery to preclinical\nevaluation--by integrating explainable, LLM-driven agents equipped with\nspecialized machine learning models and computational tools. Through structured\nknowledge exchange and automated optimization, PharmAgents identifies potential\ntherapeutic targets, discovers promising lead compounds, enhances binding\naffinity and key molecular properties, and performs in silico analyses of\ntoxicity and synthetic feasibility. Additionally, the system supports\ninterpretability, agent interaction, and self-evolvement, enabling it to refine\nfuture drug designs based on prior experience. By showcasing the potential of\nLLM-powered multi-agent systems in drug discovery, this work establishes a new\nparadigm for autonomous, explainable, and scalable pharmaceutical research,\nwith future extensions toward comprehensive drug lifecycle management."
                },
                "authors": [
                    {
                        "name": "Bowen Gao"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yiqiao Liu"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yanyan Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Lan"
                },
                "author": "Yanyan Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24277v1",
                "updated": "2025-03-31T16:22:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:22:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    22,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Designing Sparse Autoencoders by Approximating\n  Quasi-Orthogonality"
                },
                "summary": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae."
                },
                "authors": [
                    {
                        "name": "Sewoong Lee"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Marc E. Canby"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24262v1",
                "updated": "2025-03-31T16:08:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    8,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T16:08:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    8,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "New Statistical Framework for Extreme Error Probability in High-Stakes\n  Domains for Reliable Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Statistical Framework for Extreme Error Probability in High-Stakes\n  Domains for Reliable Machine Learning"
                },
                "summary": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis."
                },
                "authors": [
                    {
                        "name": "Umberto Michelucci"
                    },
                    {
                        "name": "Francesca Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Venturini"
                },
                "author": "Francesca Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09433v2",
                "updated": "2025-03-31T16:07:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    7,
                    10,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-12T14:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection"
                },
                "summary": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark."
                },
                "authors": [
                    {
                        "name": "Richard A. Dubniczky"
                    },
                    {
                        "name": "Krisztofer Zoltn Horvt"
                    },
                    {
                        "name": "Tams Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Tihanyi"
                },
                "author": "Norbert Tihanyi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24245v1",
                "updated": "2025-03-31T15:58:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:58:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    58,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models (LLMs) for Telecommunications using\n  Knowledge Graphs and Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches."
                },
                "authors": [
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yan Xin"
                    },
                    {
                        "name": "Jianzhong"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Charlie",
                "author": "Zhang",
                "arxiv_comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24235v1",
                "updated": "2025-03-31T15:46:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:46:15Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    46,
                    15,
                    0,
                    90,
                    0
                ],
                "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models"
                },
                "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions."
                },
                "authors": [
                    {
                        "name": "Qiyuan Zhang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Weixu Zhang"
                    },
                    {
                        "name": "Zhihan Guo"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24228v1",
                "updated": "2025-03-31T15:41:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:41:51Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    41,
                    51,
                    0,
                    90,
                    0
                ],
                "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAARS: Persona Aligned Agentic Retail Shoppers"
                },
                "summary": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work."
                },
                "authors": [
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Leonardo Perelli"
                    },
                    {
                        "name": "Lorenzo Mainetti"
                    },
                    {
                        "name": "George Davidson"
                    },
                    {
                        "name": "Stefano D'Amato"
                    }
                ],
                "author_detail": {
                    "name": "Stefano D'Amato"
                },
                "author": "Stefano D'Amato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05804v3",
                "updated": "2025-03-31T15:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    30,
                    45,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-08T08:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    36,
                    12,
                    1,
                    282,
                    0
                ],
                "title": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for\n  Efficient Incremental Object Detection"
                },
                "summary": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental object detection is fundamentally challenged by catastrophic\nforgetting. A major factor contributing to this issue is background shift,\nwhere background categories in sequential tasks may overlap with either\npreviously learned or future unseen classes. To address this, we propose a\nnovel method called Class-Agnostic Shared Attribute Base (CASA) that encourages\nthe model to learn category-agnostic attributes shared across incremental\nclasses. Our approach leverages an LLM to generate candidate textual\nattributes, selects the most relevant ones based on the current training data,\nand records their importance in an assignment matrix. For subsequent tasks, the\nretained attributes are frozen, and new attributes are selected from the\nremaining candidates, ensuring both knowledge retention and adaptability.\nExtensive experiments on the COCO dataset demonstrate the state-of-the-art\nperformance of our method."
                },
                "authors": [
                    {
                        "name": "Mingyi Guo"
                    },
                    {
                        "name": "Yuyang Liu"
                    },
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Zongying Lin"
                    },
                    {
                        "name": "Peixi Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18296v2",
                "updated": "2025-03-31T15:29:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    29,
                    24,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T03:02:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    2,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Surgical Action Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical Action Planning with Large Language Models"
                },
                "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, and\nsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments show\nthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy."
                },
                "authors": [
                    {
                        "name": "Mengya Xu"
                    },
                    {
                        "name": "Zhongzhen Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Qi Dou"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dou"
                },
                "author": "Qi Dou",
                "arxiv_comment": "10 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24206v1",
                "updated": "2025-03-31T15:24:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "Synthetic News Generation for Fake News Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic News Generation for Fake News Classification"
                },
                "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."
                },
                "authors": [
                    {
                        "name": "Abdul Sittar"
                    },
                    {
                        "name": "Luka Golob"
                    },
                    {
                        "name": "Mateja Smiljanic"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Smiljanic"
                },
                "author": "Mateja Smiljanic",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24198v1",
                "updated": "2025-03-31T15:16:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:16:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    16,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with\n  Multi-Teachers' Guidance"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Jingxian Xu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Weichang Liu"
                    },
                    {
                        "name": "Hanbing Liu"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24193v1",
                "updated": "2025-03-31T15:09:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:09:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    9,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval"
                },
                "summary": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have enabled users to provide\nhighly specific music recommendation requests using natural language prompts\n(e.g. \"Can you recommend some old classics for slow dancing?\"). In this setup,\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\nthe LLM generates the track titles one token at a time. While intuitive, this\napproach has several limitation. First, it is based on a general purpose\ntokenization that is optimized for words rather than for track titles. Second,\nit necessitates an additional entity resolution layer that matches the track\ntitle to the actual track identifier. Third, the number of decoding steps\nscales linearly with the length of the track title, slowing down inference. In\nthis paper, we propose to address the task of prompt-based music recommendation\nas a generative retrieval task. Within this setting, we introduce novel,\neffective, and efficient representations of track identifiers that\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\ngenerative retrieval model that learns a mapping from a user's music\nrecommendation prompt to the relevant track IDs directly. Through an offline\nevaluation on a dataset of playlists with language inputs, we find that (1) the\nstrategy to create IDs for music tracks is the most important factor for the\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\nused strategies that rely on song titles as identifiers (2) provided with the\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\nretrieval solutions trained to retrieve tracks from language prompts."
                },
                "authors": [
                    {
                        "name": "Enrico Palumbo"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Timothy Christopher Heath"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Hugues Bouchard"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24191v1",
                "updated": "2025-03-31T15:08:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:08:06Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    8,
                    6,
                    0,
                    90,
                    0
                ],
                "title": "Output Constraints as Attack Surface: Exploiting Structured Generation\n  to Bypass LLM Safety Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output Constraints as Attack Surface: Exploiting Structured Generation\n  to Bypass LLM Safety Mechanisms"
                },
                "summary": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed."
                },
                "authors": [
                    {
                        "name": "Shuoming Zhang"
                    },
                    {
                        "name": "Jiacheng Zhao"
                    },
                    {
                        "name": "Ruiyuan Xu"
                    },
                    {
                        "name": "Xiaobing Feng"
                    },
                    {
                        "name": "Huimin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Cui"
                },
                "author": "Huimin Cui",
                "arxiv_comment": "15 pages, 13 figures, 4 tables Work In Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16306v2",
                "updated": "2025-03-31T15:07:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T04:08:35Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    4,
                    8,
                    35,
                    0,
                    176,
                    0
                ],
                "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade Reward Sampling for Efficient Decoding-Time Alignment"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Anamika Lochab"
                    },
                    {
                        "name": "Ananth Grama"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24190v1",
                "updated": "2025-03-31T15:07:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T15:07:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    7,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Implicit In-Context Learning: Evidence from Artificial Language\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit In-Context Learning: Evidence from Artificial Language\n  Experiments"
                },
                "summary": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Ma"
                    },
                    {
                        "name": "Qihui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Xu"
                },
                "author": "Qihui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v1",
                "updated": "2025-03-31T14:40:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05080v3",
                "updated": "2025-03-31T14:39:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    39,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-07T14:33:50Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    33,
                    50,
                    0,
                    281,
                    0
                ],
                "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery"
                },
                "summary": "The advancements of large language models (LLMs) have piqued growing interest\nin developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about their true\ncapabilities. In this work, we call for rigorous assessment of agents on\nindividual tasks in a scientific workflow before making bold claims on\nend-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using ScienceAgentBench, we evaluate\nfive open-weight and proprietary LLMs, each with three frameworks: direct\nprompting, OpenHands CodeAct, and self-debug. Given three attempts for each\ntask, the best-performing agent can only solve 32.4% of the tasks independently\nand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI\no1-preview with direct prompting and self-debug, which can boost the\nperformance to 42.2%, demonstrating the effectiveness of increasing\ninference-time compute but with more than 10 times the cost of other LLMs.\nStill, our results underscore the limitations of current language agents in\ngenerating code for data-driven discovery, let alone end-to-end automation for\nscientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements of large language models (LLMs) have piqued growing interest\nin developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about their true\ncapabilities. In this work, we call for rigorous assessment of agents on\nindividual tasks in a scientific workflow before making bold claims on\nend-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using ScienceAgentBench, we evaluate\nfive open-weight and proprietary LLMs, each with three frameworks: direct\nprompting, OpenHands CodeAct, and self-debug. Given three attempts for each\ntask, the best-performing agent can only solve 32.4% of the tasks independently\nand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI\no1-preview with direct prompting and self-debug, which can boost the\nperformance to 42.2%, demonstrating the effectiveness of increasing\ninference-time compute but with more than 10 times the cost of other LLMs.\nStill, our results underscore the limitations of current language agents in\ngenerating code for data-driven discovery, let alone end-to-end automation for\nscientific research."
                },
                "authors": [
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Yuting Ning"
                    },
                    {
                        "name": "Qianheng Zhang"
                    },
                    {
                        "name": "Boshi Wang"
                    },
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zitong Lu"
                    },
                    {
                        "name": "Vishal Dey"
                    },
                    {
                        "name": "Mingyi Xue"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Benjamin Burns"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xuhui Huang"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "ICLR 2025. 60 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04756v2",
                "updated": "2025-03-31T14:37:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    37,
                    40,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-07T08:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "Concept Navigation and Classification via Open-Source Large Language\n  Model Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Navigation and Classification via Open-Source Large Language\n  Model Processing"
                },
                "summary": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks."
                },
                "authors": [
                    {
                        "name": "Mal Kubli"
                    }
                ],
                "author_detail": {
                    "name": "Mal Kubli"
                },
                "author": "Mal Kubli",
                "arxiv_comment": "36 pages, 1 figure, 5 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24148v1",
                "updated": "2025-03-31T14:33:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    33,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:33:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    33,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "Trident: Interference Avoidance in Multi-reader Backscatter Network via\n  Frequency-space Division",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trident: Interference Avoidance in Multi-reader Backscatter Network via\n  Frequency-space Division"
                },
                "summary": "Backscatter is a key technology for battery-free sensing in industrial IoT\napplications. To fully cover numerous tags in the deployment area, one often\nneeds to deploy multiple readers, each of which communicates with tags within\nits communication range. However, the actual backscattered signals from a tag\nare likely to reach a reader outside its communication range and cause\ninterference. Conventional TDMA or CSMA based approaches for interference\navoidance separate readers' media access in time, leading to limited network\nthroughput. In this paper, we propose TRIDENT, a novel backscatter design that\nenables interference avoidance via frequency-space division. By incorporating a\ntunable bandpass filter and multiple terminal loads, a TRIDENT tag can detect\nits channel condition and adaptively adjust the frequency and the power of its\nbackscattered signals. We further propose a frequency assignment algorithm for\nthe readers. With these designs, all the readers in the network can operate\nconcurrently without being interfered. We implement TRIDENT and evaluate its\nperformance under various settings. The results demonstrate that TRIDENT\nenhances the network throughput by 3.18x, compared to the TDMA-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscatter is a key technology for battery-free sensing in industrial IoT\napplications. To fully cover numerous tags in the deployment area, one often\nneeds to deploy multiple readers, each of which communicates with tags within\nits communication range. However, the actual backscattered signals from a tag\nare likely to reach a reader outside its communication range and cause\ninterference. Conventional TDMA or CSMA based approaches for interference\navoidance separate readers' media access in time, leading to limited network\nthroughput. In this paper, we propose TRIDENT, a novel backscatter design that\nenables interference avoidance via frequency-space division. By incorporating a\ntunable bandpass filter and multiple terminal loads, a TRIDENT tag can detect\nits channel condition and adaptively adjust the frequency and the power of its\nbackscattered signals. We further propose a frequency assignment algorithm for\nthe readers. With these designs, all the readers in the network can operate\nconcurrently without being interfered. We implement TRIDENT and evaluate its\nperformance under various settings. The results demonstrate that TRIDENT\nenhances the network throughput by 3.18x, compared to the TDMA-based scheme."
                },
                "authors": [
                    {
                        "name": "Yang Zou"
                    },
                    {
                        "name": "Xin Na"
                    },
                    {
                        "name": "Yimiao Sun"
                    },
                    {
                        "name": "Yuan He"
                    }
                ],
                "author_detail": {
                    "name": "Yuan He"
                },
                "author": "Yuan He",
                "arxiv_doi": "10.1109/TNET.2024.3495660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNET.2024.3495660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.24148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13268v2",
                "updated": "2025-03-31T14:30:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    30,
                    41,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-17T15:20:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    20,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "Channel Estimation for Pinching-Antenna Systems (PASS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Estimation for Pinching-Antenna Systems (PASS)"
                },
                "summary": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability."
                },
                "authors": [
                    {
                        "name": "Jian Xiao"
                    },
                    {
                        "name": "Ji Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01584v3",
                "updated": "2025-03-31T14:21:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    21,
                    49,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-03T18:10:38Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    10,
                    38,
                    0,
                    34,
                    0
                ],
                "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models"
                },
                "summary": "Existing benchmarks for frontier models often test specialized, \"PhD-level\"\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark with 594 problems based on the NPR Sunday Puzzle Challenge that\nrequires only general knowledge. Our benchmark is challenging for both humans\nand models; however correct solutions are easy to verify, and models' mistakes\nare easy to spot. As LLMs are more widely deployed in society, we believe it is\nuseful to develop benchmarks for frontier models that humans can understand\nwithout the need for deep domain expertise.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models on our benchmark,\ndespite being on par with other models when tested on benchmarks that test\nspecialized knowledge. Furthermore, our analysis of reasoning outputs uncovers\nnew kinds of failures. DeepSeek R1, for instance, often concedes with \"I give\nup\" before providing an answer that it knows is wrong. R1 can also be\nremarkably \"uncertain\" in its output and in rare cases, it does not \"finish\nthinking,\" which suggests the need for techniques to \"wrap up\" before the\ncontext window limit is reached. We also quantify the effectiveness of\nreasoning longer to identify the point beyond which more reasoning is unlikely\nto improve accuracy on our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for frontier models often test specialized, \"PhD-level\"\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark with 594 problems based on the NPR Sunday Puzzle Challenge that\nrequires only general knowledge. Our benchmark is challenging for both humans\nand models; however correct solutions are easy to verify, and models' mistakes\nare easy to spot. As LLMs are more widely deployed in society, we believe it is\nuseful to develop benchmarks for frontier models that humans can understand\nwithout the need for deep domain expertise.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models on our benchmark,\ndespite being on par with other models when tested on benchmarks that test\nspecialized knowledge. Furthermore, our analysis of reasoning outputs uncovers\nnew kinds of failures. DeepSeek R1, for instance, often concedes with \"I give\nup\" before providing an answer that it knows is wrong. R1 can also be\nremarkably \"uncertain\" in its output and in rare cases, it does not \"finish\nthinking,\" which suggests the need for techniques to \"wrap up\" before the\ncontext window limit is reached. We also quantify the effectiveness of\nreasoning longer to identify the point beyond which more reasoning is unlikely\nto improve accuracy on our benchmark."
                },
                "authors": [
                    {
                        "name": "Zixuan Wu"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Aleksander Boruch-Gruszecki"
                    },
                    {
                        "name": "Jingmiao Zhao"
                    },
                    {
                        "name": "Carolyn Jane Anderson"
                    },
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Molly Q Feldman"
                    },
                    {
                        "name": "Arjun Guha"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Guha"
                },
                "author": "Arjun Guha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24115v1",
                "updated": "2025-03-31T14:06:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:06:17Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    6,
                    17,
                    0,
                    90,
                    0
                ],
                "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection"
                },
                "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud."
                },
                "authors": [
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Minhua Huang"
                    },
                    {
                        "name": "Jingpeng Wang"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Xiangzhao Lv"
                    },
                    {
                        "name": "Yachun Pang"
                    },
                    {
                        "name": "Yin Yang"
                    },
                    {
                        "name": "Wenjie Tang"
                    },
                    {
                        "name": "Yuchen Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Kang"
                },
                "author": "Yuchen Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19004v2",
                "updated": "2025-03-31T14:03:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    3,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T14:41:16Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    14,
                    41,
                    16,
                    0,
                    83,
                    0
                ],
                "title": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Quantum Technology Job Market: Data Driven Analysis of 3641 Job\n  Posts"
                },
                "summary": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Quantum Technology (QT) has created a growing demand\nfor a specialized workforce, spanning across academia and industry. This study\npresents a quantitative analysis of the QT job market by systematically\nextracting and classifying thousands of job postings worldwide. The\nclassification pipeline leverages large language models (LLMs) whilst\nincorporating a \"human-in-the-loop\" validation process to ensure reliability,\nachieving an F1-score of 89%: a high level of accuracy. The research identifies\nkey trends in regional job distribution, degree and skill requirements, and the\nevolving demand for QT-related roles. Findings reveal a strong presence of the\nQT job market in the United States and Europe, with increasing corporate demand\nfor engineers, software developers, and PhD-level researchers. Despite growing\nindustry applications, the sector remains in its early stages, dominated by\nlarge technology firms and requiring significant investment in education and\nworkforce development. The study highlights the need for targeted educational\nprograms, interdisciplinary collaboration, and industry-academic partnerships\nto bridge the QT workforce gap."
                },
                "authors": [
                    {
                        "name": "Simon Goorney"
                    },
                    {
                        "name": "Eleni Karydi"
                    },
                    {
                        "name": "Borja Muoz"
                    },
                    {
                        "name": "Otto Santesson"
                    },
                    {
                        "name": "Zeki Can Seskir"
                    },
                    {
                        "name": "Ana Alina Tudoran"
                    },
                    {
                        "name": "Jacob Sherson"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Sherson"
                },
                "author": "Jacob Sherson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24110v1",
                "updated": "2025-03-31T14:01:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    1,
                    39,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T14:01:39Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    1,
                    39,
                    0,
                    90,
                    0
                ],
                "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to\n  Embodied Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to\n  Embodied Cognition"
                },
                "summary": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding."
                },
                "authors": [
                    {
                        "name": "Franois Olivier"
                    },
                    {
                        "name": "Zied Bouraoui"
                    }
                ],
                "author_detail": {
                    "name": "Zied Bouraoui"
                },
                "author": "Zied Bouraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12491v2",
                "updated": "2025-03-31T13:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    58,
                    36,
                    0,
                    90,
                    0
                ],
                "published": "2024-08-22T15:31:48Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    15,
                    31,
                    48,
                    3,
                    235,
                    0
                ],
                "title": "AI in radiological imaging of soft-tissue and bone tumours: a systematic\n  review evaluating against CLAIM and FUTURE-AI guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in radiological imaging of soft-tissue and bone tumours: a systematic\n  review evaluating against CLAIM and FUTURE-AI guidelines"
                },
                "summary": "Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging\nlesions with variable clinical behaviours and treatment approaches. This\nsystematic review provides an overview of Artificial Intelligence (AI) methods\nusing radiological imaging for diagnosis and prognosis of these tumours,\nhighlighting challenges in clinical translation, and evaluating study alignment\nwith the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI\ninternational consensus guidelines for trustworthy and deployable AI to promote\nthe clinical translation of AI methods. The review covered literature from\nseveral bibliographic databases, including papers published before 17/07/2024.\nOriginal research in peer-reviewed journals focused on radiology-based AI for\ndiagnosing or prognosing primary STBT was included. Exclusion criteria were\nanimal, cadaveric, or laboratory studies, and non-English papers. Abstracts\nwere screened by two of three independent reviewers for eligibility. Eligible\npapers were assessed against guidelines by one of three independent reviewers.\nThe search identified 15,015 abstracts, from which 325 articles were included\nfor evaluation. Most studies performed moderately on CLAIM, averaging a score\nof 28.9$\\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\\pm$2.1 out\nof 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,\nindicating significant room for improvement. Future efforts by AI developers\nshould focus on design (e.g. define unmet clinical need, intended clinical\nsetting and how AI would be integrated in clinical workflow), development (e.g.\nbuild on previous work, explainability), evaluation (e.g. evaluating and\naddressing biases, evaluating AI against best practices), and data\nreproducibility and availability (making documented code and data publicly\navailable). Following these recommendations could improve clinical translation\nof AI methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging\nlesions with variable clinical behaviours and treatment approaches. This\nsystematic review provides an overview of Artificial Intelligence (AI) methods\nusing radiological imaging for diagnosis and prognosis of these tumours,\nhighlighting challenges in clinical translation, and evaluating study alignment\nwith the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI\ninternational consensus guidelines for trustworthy and deployable AI to promote\nthe clinical translation of AI methods. The review covered literature from\nseveral bibliographic databases, including papers published before 17/07/2024.\nOriginal research in peer-reviewed journals focused on radiology-based AI for\ndiagnosing or prognosing primary STBT was included. Exclusion criteria were\nanimal, cadaveric, or laboratory studies, and non-English papers. Abstracts\nwere screened by two of three independent reviewers for eligibility. Eligible\npapers were assessed against guidelines by one of three independent reviewers.\nThe search identified 15,015 abstracts, from which 325 articles were included\nfor evaluation. Most studies performed moderately on CLAIM, averaging a score\nof 28.9$\\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\\pm$2.1 out\nof 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,\nindicating significant room for improvement. Future efforts by AI developers\nshould focus on design (e.g. define unmet clinical need, intended clinical\nsetting and how AI would be integrated in clinical workflow), development (e.g.\nbuild on previous work, explainability), evaluation (e.g. evaluating and\naddressing biases, evaluating AI against best practices), and data\nreproducibility and availability (making documented code and data publicly\navailable). Following these recommendations could improve clinical translation\nof AI methods."
                },
                "authors": [
                    {
                        "name": "Douwe J. Spaanderman"
                    },
                    {
                        "name": "Matthew Marzetti"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Andrew F. Scarsbrook"
                    },
                    {
                        "name": "Philip Robinson"
                    },
                    {
                        "name": "Edwin H. G. Oei"
                    },
                    {
                        "name": "Jacob J. Visser"
                    },
                    {
                        "name": "Robert Hemke"
                    },
                    {
                        "name": "Kirsten van Langevelde"
                    },
                    {
                        "name": "David F. Hanff"
                    },
                    {
                        "name": "Geert J. L. H. van Leenders"
                    },
                    {
                        "name": "Cornelis Verhoef"
                    },
                    {
                        "name": "Dirk J. Gruhagen"
                    },
                    {
                        "name": "Wiro J. Niessen"
                    },
                    {
                        "name": "Stefan Klein"
                    },
                    {
                        "name": "Martijn P. A. Starmans"
                    }
                ],
                "author_detail": {
                    "name": "Martijn P. A. Starmans"
                },
                "arxiv_affiliation": "Department of Radiology and Nuclear Medicine, Erasmus MC Cancer Institute, University Medical Center Rotterdam, Rotterdam, the Netherlands",
                "author": "Martijn P. A. Starmans",
                "arxiv_doi": "10.1016/j.ebiom.2025.105642",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ebiom.2025.105642",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 6 figures, 8 supplementary figures",
                "arxiv_journal_ref": "eBioMedicine(2025), Volume 114, 105642",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24102v1",
                "updated": "2025-03-31T13:56:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    56,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?"
                },
                "summary": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advancements in Large\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates the\nlimitations of current LLMs across 200 languages using benchmarks such as\nFLORES-200. We also explore alternative data sources, including news articles\nand bilingual dictionaries, and demonstrate how knowledge distillation from\nlarge pre-trained models can significantly improve smaller LRL translations.\nAdditionally, we investigate various fine-tuning strategies, revealing that\nincremental enhancements markedly reduce performance gaps on smaller LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Resource Languages (LRLs) present significant challenges in natural\nlanguage processing due to their limited linguistic resources and\nunderrepresentation in standard datasets. While recent advancements in Large\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\nimproved translation capabilities for high-resource languages, performance\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\nresource-constrained scenarios. This paper systematically evaluates the\nlimitations of current LLMs across 200 languages using benchmarks such as\nFLORES-200. We also explore alternative data sources, including news articles\nand bilingual dictionaries, and demonstrate how knowledge distillation from\nlarge pre-trained models can significantly improve smaller LRL translations.\nAdditionally, we investigate various fine-tuning strategies, revealing that\nincremental enhancements markedly reduce performance gaps on smaller LLMs."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Lama Sleem"
                    },
                    {
                        "name": "Niccolo Gentile"
                    },
                    {
                        "name": "Radu State"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Jacques Klein"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Klein"
                },
                "author": "Jacques Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19655v3",
                "updated": "2025-03-31T13:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    55,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-29T12:21:15Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    12,
                    21,
                    15,
                    4,
                    334,
                    0
                ],
                "title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis"
                },
                "summary": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field."
                },
                "authors": [
                    {
                        "name": "Alessandro Scir"
                    },
                    {
                        "name": "Andrei Stefan Bejgu"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Karim Ghonim"
                    },
                    {
                        "name": "Federico Martelli"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "15 pages. To be submitted to CL journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24096v1",
                "updated": "2025-03-31T13:49:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    49,
                    43,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:49:43Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    49,
                    43,
                    0,
                    90,
                    0
                ],
                "title": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description"
                },
                "summary": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations."
                },
                "authors": [
                    {
                        "name": "Adrienne Deganutti"
                    },
                    {
                        "name": "Simon Hadfield"
                    },
                    {
                        "name": "Andrew Gilbert"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gilbert"
                },
                "author": "Andrew Gilbert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00326v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00326v3",
                "updated": "2025-03-31T13:33:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    33,
                    54,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-29T05:59:53Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    5,
                    59,
                    53,
                    5,
                    181,
                    0
                ],
                "title": "Teola: Towards End-to-End Optimization of LLM-based Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teola: Towards End-to-End Optimization of LLM-based Applications"
                },
                "summary": "Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications. The code is available at\nhttps://github.com/NetX-lab/Ayo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications. The code is available at\nhttps://github.com/NetX-lab/Ayo."
                },
                "authors": [
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yitao Yang"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00326v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00326v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24062v1",
                "updated": "2025-03-31T13:22:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    22,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:22:34Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    22,
                    34,
                    0,
                    90,
                    0
                ],
                "title": "Artificial Conversations, Real Results: Fostering Language Detection\n  with Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Conversations, Real Results: Fostering Language Detection\n  with Synthetic Data"
                },
                "summary": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs."
                },
                "authors": [
                    {
                        "name": "Fatemeh Mohammadi"
                    },
                    {
                        "name": "Tommaso Romano"
                    },
                    {
                        "name": "Samira Maghool"
                    },
                    {
                        "name": "Paolo Ceravolo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Ceravolo"
                },
                "author": "Paolo Ceravolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24053v1",
                "updated": "2025-03-31T13:15:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    15,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:15:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    15,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "ReaLM: Reliable and Efficient Large Language Model Inference with\n  Statistical Algorithm-Based Fault Tolerance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaLM: Reliable and Efficient Large Language Model Inference with\n  Statistical Algorithm-Based Fault Tolerance"
                },
                "summary": "The demand for efficient large language model (LLM) inference has propelled\nthe development of dedicated accelerators. As accelerators are vulnerable to\nhardware faults due to aging, variation, etc, existing accelerator designs\noften reserve a large voltage margin or leverage algorithm-based fault\ntolerance (ABFT) techniques to ensure LLM inference correctness. However,\nprevious methods often overlook the inherent fault tolerance of LLMs, leading\nto high computation and energy overhead. To enable reliable yet efficient LLM\ninference, in this paper, we propose a novel algorithm/circuit co-design\nframework, dubbed ReaLM. For the first time, we systematically characterize the\nfault tolerance of LLMs by performing a large-scale error injection study of\nrepresentative LLMs and natural language understanding tasks. Then, we propose\na statistical ABFT algorithm that fully leverages the error robustness to\nminimize error recovery as much as possible. We also customize the error\ndetection circuits to enable a low-cost online collection of error statistics.\nExtensive experiments show that with only 1.42% circuit area and 1.79% power\noverhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29.\nCompared to existing methods, ReaLM consistently reduces recovery costs across\ndifferent operating voltages and improves energy efficiency by up to 35.83%\nwithout compromising LLM performance. Our error injection code is available at\nhttps://github.com/2000012835xt/ReaLM-DAC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for efficient large language model (LLM) inference has propelled\nthe development of dedicated accelerators. As accelerators are vulnerable to\nhardware faults due to aging, variation, etc, existing accelerator designs\noften reserve a large voltage margin or leverage algorithm-based fault\ntolerance (ABFT) techniques to ensure LLM inference correctness. However,\nprevious methods often overlook the inherent fault tolerance of LLMs, leading\nto high computation and energy overhead. To enable reliable yet efficient LLM\ninference, in this paper, we propose a novel algorithm/circuit co-design\nframework, dubbed ReaLM. For the first time, we systematically characterize the\nfault tolerance of LLMs by performing a large-scale error injection study of\nrepresentative LLMs and natural language understanding tasks. Then, we propose\na statistical ABFT algorithm that fully leverages the error robustness to\nminimize error recovery as much as possible. We also customize the error\ndetection circuits to enable a low-cost online collection of error statistics.\nExtensive experiments show that with only 1.42% circuit area and 1.79% power\noverhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29.\nCompared to existing methods, ReaLM consistently reduces recovery costs across\ndifferent operating voltages and improves energy efficiency by up to 35.83%\nwithout compromising LLM performance. Our error injection code is available at\nhttps://github.com/2000012835xt/ReaLM-DAC."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Jiawang Zhao"
                    },
                    {
                        "name": "Zishen Wan"
                    },
                    {
                        "name": "Zuodong Zhang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "6 pages, 10 figures. Accepted by Design Automation Conference (DAC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v6",
                "updated": "2025-03-31T13:13:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    13,
                    27,
                    0,
                    90,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "TestART: Improving LLM-based Unit Testing via Co-evolution of Automated\n  Generation and Repair Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestART: Improving LLM-based Unit Testing via Co-evolution of Automated\n  Generation and Repair Iteration"
                },
                "summary": "Unit testing is crucial for detecting bugs in individual program units but\nconsumes time and effort. Recently, large language models (LLMs) have\ndemonstrated remarkable capabilities in generating unit test cases. However,\nseveral problems limit their ability to generate high-quality unit test cases:\n(1) compilation and runtime errors caused by the hallucination of LLMs; (2)\nlack of testing and coverage feedback information restricting the increase of\ncode coverage;(3) the repetitive suppression problem causing invalid LLM-based\nrepair and generation attempts. To address these limitations, we propose\nTestART, a novel unit test generation method. TestART improves LLM-based unit\ntesting via co-evolution of automated generation and repair iteration,\nrepresenting a significant advancement in automated unit test generation.\nTestART leverages the template-based repair strategy to effectively fix bugs in\nLLM-generated test cases for the first time. Meanwhile, TestART extracts\ncoverage information from successful test cases and uses it as coverage-guided\ntesting feedback. It also incorporates positive prompt injection to prevent\nrepetition suppression, thereby enhancing the sufficiency of the final test\ncase. This synergy between generation and repair elevates the correctness and\nsufficiency of the produced test cases significantly beyond previous methods.\nIn comparative experiments, TestART demonstrates an 18% improvement in pass\nrate and a 20% enhancement in coverage across three types of datasets compared\nto baseline models. Additionally, it achieves better coverage rates than\nEvoSuite with only half the number of test cases. These results demonstrate\nTestART's superior ability to produce high-quality unit test cases by\nharnessing the power of LLMs while overcoming their inherent flaws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing is crucial for detecting bugs in individual program units but\nconsumes time and effort. Recently, large language models (LLMs) have\ndemonstrated remarkable capabilities in generating unit test cases. However,\nseveral problems limit their ability to generate high-quality unit test cases:\n(1) compilation and runtime errors caused by the hallucination of LLMs; (2)\nlack of testing and coverage feedback information restricting the increase of\ncode coverage;(3) the repetitive suppression problem causing invalid LLM-based\nrepair and generation attempts. To address these limitations, we propose\nTestART, a novel unit test generation method. TestART improves LLM-based unit\ntesting via co-evolution of automated generation and repair iteration,\nrepresenting a significant advancement in automated unit test generation.\nTestART leverages the template-based repair strategy to effectively fix bugs in\nLLM-generated test cases for the first time. Meanwhile, TestART extracts\ncoverage information from successful test cases and uses it as coverage-guided\ntesting feedback. It also incorporates positive prompt injection to prevent\nrepetition suppression, thereby enhancing the sufficiency of the final test\ncase. This synergy between generation and repair elevates the correctness and\nsufficiency of the produced test cases significantly beyond previous methods.\nIn comparative experiments, TestART demonstrates an 18% improvement in pass\nrate and a 20% enhancement in coverage across three types of datasets compared\nto baseline models. Additionally, it achieves better coverage rates than\nEvoSuite with only half the number of test cases. These results demonstrate\nTestART's superior ability to produce high-quality unit test cases by\nharnessing the power of LLMs while overcoming their inherent flaws."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Kecheng Li"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Liuchuan Zhu"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24047v1",
                "updated": "2025-03-31T13:11:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T13:11:28Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    11,
                    28,
                    0,
                    90,
                    0
                ],
                "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents"
                },
                "summary": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery."
                },
                "authors": [
                    {
                        "name": "Shuo Ren"
                    },
                    {
                        "name": "Pu Jian"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Chunlin Leng"
                    },
                    {
                        "name": "Can Xie"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "34 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06816v2",
                "updated": "2025-03-31T13:03:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    3,
                    14,
                    0,
                    90,
                    0
                ],
                "published": "2024-08-13T11:17:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty"
                },
                "summary": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Hwaran Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwaran Lee"
                },
                "author": "Hwaran Lee",
                "arxiv_comment": "Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13323v3",
                "updated": "2025-03-31T13:02:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    13,
                    2,
                    51,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-20T13:46:04Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "title": "Are Large Language Models Memorizing Bug Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Memorizing Bug Benchmarks?"
                },
                "summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities."
                },
                "authors": [
                    {
                        "name": "Daniel Ramos"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Kush Jain"
                    },
                    {
                        "name": "Paulo Canelas"
                    },
                    {
                        "name": "Catarina Gamboa"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24028v1",
                "updated": "2025-03-31T12:53:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    53,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:53:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    53,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "Pay More Attention to the Robustness of Prompt for Instruction Data\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay More Attention to the Robustness of Prompt for Instruction Data\n  Mining"
                },
                "summary": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness."
                },
                "authors": [
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Dawei Feng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Bo Ding"
                    },
                    {
                        "name": "Huaimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huaimin Wang"
                },
                "author": "Huaimin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24021v1",
                "updated": "2025-03-31T12:48:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    48,
                    39,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:48:39Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    48,
                    39,
                    0,
                    90,
                    0
                ],
                "title": "IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos\n  Plots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos\n  Plots"
                },
                "summary": "Genomics data is essential in biological and medical domains, and\nbioinformatics analysts often manually create circos plots to analyze the data\nand extract valuable insights. However, creating circos plots is complex, as it\nrequires careful design for multiple track attributes and positional\nrelationships between them. Typically, analysts often seek inspiration from\nexisting circos plots, and they have to iteratively adjust and refine the plot\nto achieve a satisfactory final design, making the process both tedious and\ntime-intensive. To address these challenges, we propose IntelliCircos, an\nAI-powered interactive authoring tool that streamlines the process from initial\nvisual design to the final implementation of circos plots. Specifically, we\nbuild a new dataset containing 4396 circos plots with corresponding annotations\nand configurations, which are extracted and labeled from published papers. With\nthe dataset, we further identify track combination patterns, and utilize Large\nLanguage Model (LLM) to provide domain-specific design recommendations and\nconfiguration references to navigate the design of circos plots. We conduct a\nuser study with 8 bioinformatics analysts to evaluate IntelliCircos, and the\nresults demonstrate its usability and effectiveness in authoring circos plots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genomics data is essential in biological and medical domains, and\nbioinformatics analysts often manually create circos plots to analyze the data\nand extract valuable insights. However, creating circos plots is complex, as it\nrequires careful design for multiple track attributes and positional\nrelationships between them. Typically, analysts often seek inspiration from\nexisting circos plots, and they have to iteratively adjust and refine the plot\nto achieve a satisfactory final design, making the process both tedious and\ntime-intensive. To address these challenges, we propose IntelliCircos, an\nAI-powered interactive authoring tool that streamlines the process from initial\nvisual design to the final implementation of circos plots. Specifically, we\nbuild a new dataset containing 4396 circos plots with corresponding annotations\nand configurations, which are extracted and labeled from published papers. With\nthe dataset, we further identify track combination patterns, and utilize Large\nLanguage Model (LLM) to provide domain-specific design recommendations and\nconfiguration references to navigate the design of circos plots. We conduct a\nuser study with 8 bioinformatics analysts to evaluate IntelliCircos, and the\nresults demonstrate its usability and effectiveness in authoring circos plots."
                },
                "authors": [
                    {
                        "name": "Mingyang Gu"
                    },
                    {
                        "name": "Jiamin Zhu"
                    },
                    {
                        "name": "Qipeng Wang"
                    },
                    {
                        "name": "Fengjie Wang"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Min Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhu"
                },
                "author": "Min Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19250v2",
                "updated": "2025-03-31T12:24:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    24,
                    30,
                    0,
                    90,
                    0
                ],
                "published": "2024-09-28T05:48:51Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    5,
                    48,
                    51,
                    5,
                    272,
                    0
                ],
                "title": "Fast and Accurate Task Planning using Neuro-Symbolic Language Models and\n  Multi-level Goal Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Task Planning using Neuro-Symbolic Language Models and\n  Multi-level Goal Decomposition"
                },
                "summary": "In robotic task planning, symbolic planners using rule-based representations\nlike PDDL are effective but struggle with long-sequential tasks in complicated\nenvironments due to exponentially increasing search space. Meanwhile, LLM-based\napproaches, which are grounded in artificial neural networks, offer faster\ninference and commonsense reasoning but suffer from lower success rates. To\naddress the limitations of the current symbolic (slow speed) or LLM-based\napproaches (low accuracy), we propose a novel neuro-symbolic task planner that\ndecomposes complex tasks into subgoals using LLM and carries out task planning\nfor each subgoal using either symbolic or MCTS-based LLM planners, depending on\nthe subgoal complexity. This decomposition reduces planning time and improves\nsuccess rates by narrowing the search space and enabling LLMs to focus on more\nmanageable tasks. Our method significantly reduces planning time while\nmaintaining high success rates across task planning domains, as well as\nreal-world and simulated robotics environments. More details are available at\nhttp://graphics.ewha.ac.kr/LLMTAMP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotic task planning, symbolic planners using rule-based representations\nlike PDDL are effective but struggle with long-sequential tasks in complicated\nenvironments due to exponentially increasing search space. Meanwhile, LLM-based\napproaches, which are grounded in artificial neural networks, offer faster\ninference and commonsense reasoning but suffer from lower success rates. To\naddress the limitations of the current symbolic (slow speed) or LLM-based\napproaches (low accuracy), we propose a novel neuro-symbolic task planner that\ndecomposes complex tasks into subgoals using LLM and carries out task planning\nfor each subgoal using either symbolic or MCTS-based LLM planners, depending on\nthe subgoal complexity. This decomposition reduces planning time and improves\nsuccess rates by narrowing the search space and enabling LLMs to focus on more\nmanageable tasks. Our method significantly reduces planning time while\nmaintaining high success rates across task planning domains, as well as\nreal-world and simulated robotics environments. More details are available at\nhttp://graphics.ewha.ac.kr/LLMTAMP/."
                },
                "authors": [
                    {
                        "name": "Minseo Kwon"
                    },
                    {
                        "name": "Yaesol Kim"
                    },
                    {
                        "name": "Young J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young J. Kim"
                },
                "author": "Young J. Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23989v1",
                "updated": "2025-03-31T11:59:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    59,
                    43,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:59:43Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    59,
                    43,
                    0,
                    90,
                    0
                ],
                "title": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With\n  Question-Specific Rubrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With\n  Question-Specific Rubrics"
                },
                "summary": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness."
                },
                "authors": [
                    {
                        "name": "Aditya Pathak"
                    },
                    {
                        "name": "Rachit Gandhi"
                    },
                    {
                        "name": "Vaibhav Uttam"
                    },
                    {
                        "name": "Devansh"
                    },
                    {
                        "name": "Yashwanth Nakka"
                    },
                    {
                        "name": "Aaryan Raj Jindal"
                    },
                    {
                        "name": "Pratyush Ghosh"
                    },
                    {
                        "name": "Arnav Ramamoorthy"
                    },
                    {
                        "name": "Shreyash Verma"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Aashna Ased"
                    },
                    {
                        "name": "Chirag Khatri"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23965v1",
                "updated": "2025-03-31T11:27:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    27,
                    48,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:27:48Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    27,
                    48,
                    0,
                    90,
                    0
                ],
                "title": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous\n  Driving"
                },
                "summary": "Real-time traffic light recognition is fundamental for autonomous driving\nsafety and navigation in urban environments. While existing approaches rely on\nsingle-frame analysis from onboard cameras, they struggle with complex\nscenarios involving occlusions and adverse lighting conditions. We present\n\\textit{ViTLR}, a novel video-based end-to-end neural network that processes\nmultiple consecutive frames to achieve robust traffic light detection and state\nclassification. The architecture leverages a transformer-like design with\nconvolutional self-attention modules, which is optimized specifically for\ndeployment on the Rockchip RV1126 embedded platform. Extensive evaluations on\ntwo real-world datasets demonstrate that \\textit{ViTLR} achieves\nstate-of-the-art performance while maintaining real-time processing\ncapabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness\nacross temporal stability, varying target distances, and challenging\nenvironmental conditions compared to existing single-frame approaches. We have\nsuccessfully integrated \\textit{ViTLR} into an ego-lane traffic light\nrecognition system using HD maps for autonomous driving applications. The\ncomplete implementation, including source code and datasets, is made publicly\navailable to facilitate further research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time traffic light recognition is fundamental for autonomous driving\nsafety and navigation in urban environments. While existing approaches rely on\nsingle-frame analysis from onboard cameras, they struggle with complex\nscenarios involving occlusions and adverse lighting conditions. We present\n\\textit{ViTLR}, a novel video-based end-to-end neural network that processes\nmultiple consecutive frames to achieve robust traffic light detection and state\nclassification. The architecture leverages a transformer-like design with\nconvolutional self-attention modules, which is optimized specifically for\ndeployment on the Rockchip RV1126 embedded platform. Extensive evaluations on\ntwo real-world datasets demonstrate that \\textit{ViTLR} achieves\nstate-of-the-art performance while maintaining real-time processing\ncapabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness\nacross temporal stability, varying target distances, and challenging\nenvironmental conditions compared to existing single-frame approaches. We have\nsuccessfully integrated \\textit{ViTLR} into an ego-lane traffic light\nrecognition system using HD maps for autonomous driving applications. The\ncomplete implementation, including source code and datasets, is made publicly\navailable to facilitate further research in this domain."
                },
                "authors": [
                    {
                        "name": "Miao Fan"
                    },
                    {
                        "name": "Xuxu Kong"
                    },
                    {
                        "name": "Shengtong Xu"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Xiangzeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzeng Liu"
                },
                "author": "Xiangzeng Liu",
                "arxiv_comment": "Accepted by IEEE IV'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23948v1",
                "updated": "2025-03-31T10:58:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    58,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:58:34Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    58,
                    34,
                    0,
                    90,
                    0
                ],
                "title": "AI2Agent: An End-to-End Framework for Deploying AI Projects as\n  Autonomous Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI2Agent: An End-to-End Framework for Deploying AI Projects as\n  Autonomous Agents"
                },
                "summary": "As AI technology advances, it is driving innovation across industries,\nincreasing the demand for scalable AI project deployment. However, deployment\nremains a critical challenge due to complex environment configurations,\ndependency conflicts, cross-platform adaptation, and debugging difficulties,\nwhich hinder automation and adoption. This paper introduces AI2Agent, an\nend-to-end framework that automates AI project deployment through\nguideline-driven execution, self-adaptive debugging, and case \\& solution\naccumulation. AI2Agent dynamically analyzes deployment challenges, learns from\npast cases, and iteratively refines its approach, significantly reducing human\nintervention. To evaluate its effectiveness, we conducted experiments on 30 AI\ndeployment cases, covering TTS, text-to-image generation, image editing, and\nother AI applications. Results show that AI2Agent significantly reduces\ndeployment time and improves success rates. The code and demo video are now\npublicly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI technology advances, it is driving innovation across industries,\nincreasing the demand for scalable AI project deployment. However, deployment\nremains a critical challenge due to complex environment configurations,\ndependency conflicts, cross-platform adaptation, and debugging difficulties,\nwhich hinder automation and adoption. This paper introduces AI2Agent, an\nend-to-end framework that automates AI project deployment through\nguideline-driven execution, self-adaptive debugging, and case \\& solution\naccumulation. AI2Agent dynamically analyzes deployment challenges, learns from\npast cases, and iteratively refines its approach, significantly reducing human\nintervention. To evaluate its effectiveness, we conducted experiments on 30 AI\ndeployment cases, covering TTS, text-to-image generation, image editing, and\nother AI applications. Results show that AI2Agent significantly reduces\ndeployment time and improves success rates. The code and demo video are now\npublicly accessible."
                },
                "authors": [
                    {
                        "name": "Jiaxiang Chen"
                    },
                    {
                        "name": "Jingwei Shi"
                    },
                    {
                        "name": "Lei Gan"
                    },
                    {
                        "name": "Jiale Zhang"
                    },
                    {
                        "name": "Qingyu Zhang"
                    },
                    {
                        "name": "Dongqian Zhang"
                    },
                    {
                        "name": "Xin Pang"
                    },
                    {
                        "name": "Zhucong Li"
                    },
                    {
                        "name": "Yinghui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Xu"
                },
                "author": "Yinghui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23941v1",
                "updated": "2025-03-31T10:47:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    47,
                    20,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:47:20Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    47,
                    20,
                    0,
                    90,
                    0
                ],
                "title": "Choco-Q: Commute Hamiltonian-based QAOA for Constrained Binary\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choco-Q: Commute Hamiltonian-based QAOA for Constrained Binary\n  Optimization"
                },
                "summary": "Constrained binary optimization aims to find an optimal assignment to\nminimize or maximize the objective meanwhile satisfying the constraints, which\nis a representative NP problem in various domains, including transportation,\nscheduling, and economy. Quantum approximate optimization algorithms (QAOA)\nprovide a promising methodology for solving this problem by exploiting the\nparallelism of quantum entanglement. However, existing QAOA approaches based on\npenalty-term or Hamiltonian simulation fail to thoroughly encode the\nconstraints, leading to extremely low success rate and long searching latency.\n  This paper proposes Choco-Q, a formal and universal framework for constrained\nbinary optimization problems, which comprehensively covers all constraints and\nexhibits high deployability for current quantum devices. The main innovation of\nChoco-Q is to embed the commute Hamiltonian as the driver Hamiltonian,\nresulting in a much more general encoding formulation that can deal with\narbitrary linear constraints. Leveraging the arithmetic features of commute\nHamiltonian, we propose three optimization techniques to squeeze the overall\ncircuit complexity, including Hamiltonian serialization, equivalent\ndecomposition, and variable elimination. The serialization mechanism transforms\nthe original Hamiltonian into smaller ones. Our decomposition methods only take\nlinear time complexity, achieving end-to-end acceleration. Experiments\ndemonstrate that Choco-Q shows more than 235$\\times$ algorithmic improvement in\nsuccessfully finding the optimal solution, and achieves 4.69$\\times$ end-to-end\nacceleration, compared to prior QAOA designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained binary optimization aims to find an optimal assignment to\nminimize or maximize the objective meanwhile satisfying the constraints, which\nis a representative NP problem in various domains, including transportation,\nscheduling, and economy. Quantum approximate optimization algorithms (QAOA)\nprovide a promising methodology for solving this problem by exploiting the\nparallelism of quantum entanglement. However, existing QAOA approaches based on\npenalty-term or Hamiltonian simulation fail to thoroughly encode the\nconstraints, leading to extremely low success rate and long searching latency.\n  This paper proposes Choco-Q, a formal and universal framework for constrained\nbinary optimization problems, which comprehensively covers all constraints and\nexhibits high deployability for current quantum devices. The main innovation of\nChoco-Q is to embed the commute Hamiltonian as the driver Hamiltonian,\nresulting in a much more general encoding formulation that can deal with\narbitrary linear constraints. Leveraging the arithmetic features of commute\nHamiltonian, we propose three optimization techniques to squeeze the overall\ncircuit complexity, including Hamiltonian serialization, equivalent\ndecomposition, and variable elimination. The serialization mechanism transforms\nthe original Hamiltonian into smaller ones. Our decomposition methods only take\nlinear time complexity, achieving end-to-end acceleration. Experiments\ndemonstrate that Choco-Q shows more than 235$\\times$ algorithmic improvement in\nsuccessfully finding the optimal solution, and achieves 4.69$\\times$ end-to-end\nacceleration, compared to prior QAOA designs."
                },
                "authors": [
                    {
                        "name": "Debin Xiang"
                    },
                    {
                        "name": "Qifan Jiang"
                    },
                    {
                        "name": "Liqiang Lu"
                    },
                    {
                        "name": "Siwei Tan"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "14 pages, 14 figures, conference",
                "arxiv_journal_ref": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23934v1",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "title": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in\n  Discriminative and Generative AI Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in\n  Discriminative and Generative AI Operations"
                },
                "summary": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models."
                },
                "authors": [
                    {
                        "name": "Adrin Snchez-Momp"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Konstantinos Katsaros"
                    },
                    {
                        "name": "Aftab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Aftab Khan"
                },
                "author": "Aftab Khan",
                "arxiv_doi": "10.3390/info16040281",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/info16040281",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published to MDPI Information - Artificial Intelligence Section",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23933v1",
                "updated": "2025-03-31T10:27:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    27,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:27:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    27,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "PupiNet: Seamless OCT-OCTA Interconversion Through Wavelet-Driven and\n  Multi-Scale Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PupiNet: Seamless OCT-OCTA Interconversion Through Wavelet-Driven and\n  Multi-Scale Attention Mechanisms"
                },
                "summary": "Optical Coherence Tomography (OCT) and Optical Coherence Tomography\nAngiography (OCTA) are key diagnostic tools for clinical evaluation and\nmanagement of retinal diseases. Compared to traditional OCT, OCTA provides\nricher microvascular information, but its acquisition requires specialized\nsensors and high-cost equipment, creating significant challenges for the\nclinical deployment of hardware-dependent OCTA imaging methods. Given the\ntechnical complexity of OCTA image acquisition and potential mechanical\nartifacts, this study proposes a bidirectional image conversion framework\ncalled PupiNet, which accurately achieves bidirectional transformation between\n3D OCT and 3D OCTA. The generator module of this framework innovatively\nintegrates wavelet transformation and multi-scale attention mechanisms,\nsignificantly enhancing image conversion quality. Meanwhile, an Adaptive\nDiscriminator Augmentation (ADA) module has been incorporated into the\ndiscriminator to optimize model training stability and convergence efficiency.\nTo ensure clinical accuracy of vascular structures in the converted images, we\ndesigned a Vessel Structure Matcher (VSM) supervision module, achieving precise\nmatching of vascular morphology between generated images and target images.\nAdditionally, the Hierarchical Feature Calibration (HFC) module further\nguarantees high consistency of texture details between generated images and\ntarget images across different depth levels. To rigorously validate the\nclinical effectiveness of the proposed method, we conducted a comprehensive\nevaluation on a paired OCT-OCTA image dataset containing 300 eyes with various\nretinal pathologies. Experimental results demonstrate that PupiNet not only\nreliably achieves high-quality bidirectional transformation between the two\nmodalities but also shows significant advantages in image fidelity, vessel\nstructure preservation, and clinical usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Coherence Tomography (OCT) and Optical Coherence Tomography\nAngiography (OCTA) are key diagnostic tools for clinical evaluation and\nmanagement of retinal diseases. Compared to traditional OCT, OCTA provides\nricher microvascular information, but its acquisition requires specialized\nsensors and high-cost equipment, creating significant challenges for the\nclinical deployment of hardware-dependent OCTA imaging methods. Given the\ntechnical complexity of OCTA image acquisition and potential mechanical\nartifacts, this study proposes a bidirectional image conversion framework\ncalled PupiNet, which accurately achieves bidirectional transformation between\n3D OCT and 3D OCTA. The generator module of this framework innovatively\nintegrates wavelet transformation and multi-scale attention mechanisms,\nsignificantly enhancing image conversion quality. Meanwhile, an Adaptive\nDiscriminator Augmentation (ADA) module has been incorporated into the\ndiscriminator to optimize model training stability and convergence efficiency.\nTo ensure clinical accuracy of vascular structures in the converted images, we\ndesigned a Vessel Structure Matcher (VSM) supervision module, achieving precise\nmatching of vascular morphology between generated images and target images.\nAdditionally, the Hierarchical Feature Calibration (HFC) module further\nguarantees high consistency of texture details between generated images and\ntarget images across different depth levels. To rigorously validate the\nclinical effectiveness of the proposed method, we conducted a comprehensive\nevaluation on a paired OCT-OCTA image dataset containing 300 eyes with various\nretinal pathologies. Experimental results demonstrate that PupiNet not only\nreliably achieves high-quality bidirectional transformation between the two\nmodalities but also shows significant advantages in image fidelity, vessel\nstructure preservation, and clinical usability."
                },
                "authors": [
                    {
                        "name": "Renzhi Tian"
                    },
                    {
                        "name": "Jinjie Wang"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Weizhen Li"
                    },
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Yiran Zhu"
                    },
                    {
                        "name": "Chengchang Pan"
                    },
                    {
                        "name": "Honggang Qi"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Qi"
                },
                "author": "Honggang Qi",
                "arxiv_comment": "8 pages,4 figures,5 tables,submitted to the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23926v1",
                "updated": "2025-03-31T10:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:18:42Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "title": "Reliable Traffic Monitoring Using Low-Cost Doppler Radar Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Traffic Monitoring Using Low-Cost Doppler Radar Units"
                },
                "summary": "Road traffic monitoring typically involves the counting and recording of\nvehicles on public roads over extended periods. The data gathered from such\nmonitoring provides useful information to municipal authorities in urban areas.\nThis paper presents a low-cost, widely deployable sensing subsystem based on\nContinuous Wave Doppler radar. The proposed system can perform vehicle\ndetection and speed estimation with a total cost of less than 100 USD. The\nsensing system (including the hardware subsystem and the algorithms) is\ndesigned to be placed on the side of the road, allowing for easy deployment and\nserviceability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road traffic monitoring typically involves the counting and recording of\nvehicles on public roads over extended periods. The data gathered from such\nmonitoring provides useful information to municipal authorities in urban areas.\nThis paper presents a low-cost, widely deployable sensing subsystem based on\nContinuous Wave Doppler radar. The proposed system can perform vehicle\ndetection and speed estimation with a total cost of less than 100 USD. The\nsensing system (including the hardware subsystem and the algorithms) is\ndesigned to be placed on the side of the road, allowing for easy deployment and\nserviceability."
                },
                "authors": [
                    {
                        "name": "Mishay Naidoo"
                    },
                    {
                        "name": "Stephen Paine"
                    },
                    {
                        "name": "Amit Kumar Mishra"
                    },
                    {
                        "name": "Mohammed Yunus Abdul Gaffar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Yunus Abdul Gaffar"
                },
                "author": "Mohammed Yunus Abdul Gaffar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23924v1",
                "updated": "2025-03-31T10:16:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    16,
                    3,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:16:03Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    16,
                    3,
                    0,
                    90,
                    0
                ],
                "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Hemorrhage and the Robustness Limits of Large Language Models"
                },
                "summary": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Gui-Song Xia"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Liangpei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "33 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22456v2",
                "updated": "2025-03-31T10:13:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    13,
                    48,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-28T14:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    7,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-guided sequence weighting for efficient exploration in RL-based\n  LLM fine-tuning"
                },
                "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that\nenhances the exploration-exploitation tradeoff by dynamically assigning weights\nto generated outputs based on their advantage and entropy for Reinforcement\nLearning-based Large Language Model fine-tuning. EGSW integrates entropy\nregularization with advantage-based weighting to balance policy updates,\nenabling efficient exploration in high-dimensional state spaces. By employing\ntemperature-scaled softmax weighting over sequences, EGSW prioritizing\nhigh-reward, high-uncertainty steps while maintaining training stability.\nAlthough originally developed to improve Group Relative Policy Optimization\n(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to\nother reinforcement learning (RL) algorithms and can be implemented in both\nstep-wise and trajectory-wise settings. Empirical evaluations demonstrate that\nEGSW enhances GRPO reasoning ability, yielding improvements in sample\nefficiency. Future work will explore the application of EGSW to advanced RL\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Abdullah Vanlioglu"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Vanlioglu"
                },
                "author": "Abdullah Vanlioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23914v1",
                "updated": "2025-03-31T10:06:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    6,
                    27,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T10:06:27Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    6,
                    27,
                    0,
                    90,
                    0
                ],
                "title": "Scenarios for the Deployment of Automated Vehicles in Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenarios for the Deployment of Automated Vehicles in Europe"
                },
                "summary": "The deployment of Automated Vehicles (AVs) is expected to address road\ntransport externalities (e.g., safety, traffic, environmental impact, etc.).\nFor this reason, a legal framework for their large-scale market introduction\nand deployment is currently being developed in the European Union. Despite the\nfirst steps towards road transport automation, the timeline for full automation\nand its potential economic benefits remains uncertain. The aim of this paper is\ntwofold. First, it presents a methodological framework to determine deployment\npathways of the five different levels of automation in EU27+UK to 2050 under\nthree scenarios (i.e., slow, medium baseline and fast) focusing on passenger\nvehicles. Second, it proposes an assessment of the economic impact of AVs\nthrough the calculation of the value-added. The method to define assumptions\nand uptake trajectories involves a comprehensive literature review, expert\ninterviews, and a model to forecast the new registrations of different levels\nof automation. In this way, the interviews provided insights that complemented\nthe literature and informed the design of assumptions and deployment\ntrajectories. The added-value assessment shows additional economic activity due\nto the introduction of automated technologies in all uptake scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Automated Vehicles (AVs) is expected to address road\ntransport externalities (e.g., safety, traffic, environmental impact, etc.).\nFor this reason, a legal framework for their large-scale market introduction\nand deployment is currently being developed in the European Union. Despite the\nfirst steps towards road transport automation, the timeline for full automation\nand its potential economic benefits remains uncertain. The aim of this paper is\ntwofold. First, it presents a methodological framework to determine deployment\npathways of the five different levels of automation in EU27+UK to 2050 under\nthree scenarios (i.e., slow, medium baseline and fast) focusing on passenger\nvehicles. Second, it proposes an assessment of the economic impact of AVs\nthrough the calculation of the value-added. The method to define assumptions\nand uptake trajectories involves a comprehensive literature review, expert\ninterviews, and a model to forecast the new registrations of different levels\nof automation. In this way, the interviews provided insights that complemented\nthe literature and informed the design of assumptions and deployment\ntrajectories. The added-value assessment shows additional economic activity due\nto the introduction of automated technologies in all uptake scenarios."
                },
                "authors": [
                    {
                        "name": "Louison Duboz"
                    },
                    {
                        "name": "Ioan Cristinel Raileanu"
                    },
                    {
                        "name": "Jette Krause"
                    },
                    {
                        "name": "Ana Norman-Lpez"
                    },
                    {
                        "name": "Matthias Weitzel"
                    },
                    {
                        "name": "Biagio Ciuffo"
                    }
                ],
                "author_detail": {
                    "name": "Biagio Ciuffo"
                },
                "author": "Biagio Ciuffo",
                "arxiv_comment": "This article is submitted to the journal Transportation Research\n  Interdisciplinary Perspectives (TRIPS), from the family of journals Transport\n  Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23899v1",
                "updated": "2025-03-31T09:48:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:48:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset"
                },
                "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance."
                },
                "authors": [
                    {
                        "name": "Diana Galvan-Sosa"
                    },
                    {
                        "name": "Gabrielle Gaudeau"
                    },
                    {
                        "name": "Pride Kavumba"
                    },
                    {
                        "name": "Yunmeng Li"
                    },
                    {
                        "name": "Hongyi gu"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "9 main pages (21 appendix pages), 7 figures, submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23895v1",
                "updated": "2025-03-31T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    35,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    35,
                    0,
                    90,
                    0
                ],
                "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented\n  Generation for Test-time Knowledge Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better wit than wealth: Dynamic Parametric Retrieval Augmented\n  Generation for Test-time Knowledge Enhancement"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."
                },
                "authors": [
                    {
                        "name": "Yuqiao Tan"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23886v1",
                "updated": "2025-03-31T09:39:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    39,
                    19,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:39:19Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    39,
                    19,
                    0,
                    90,
                    0
                ],
                "title": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database\n  Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database\n  Schema"
                },
                "summary": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation."
                },
                "authors": [
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Youhuan Li"
                    },
                    {
                        "name": "Yansong Feng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Yuequn Dou"
                    },
                    {
                        "name": "chuchu Gao"
                    },
                    {
                        "name": "Zebin Huang"
                    },
                    {
                        "name": "Zihui Si"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Zhaohai Sun"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Wenqiang Jin"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Jin"
                },
                "author": "Wenqiang Jin",
                "arxiv_comment": "19 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23877v1",
                "updated": "2025-03-31T09:27:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    27,
                    0,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:27:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    27,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos"
                },
                "summary": "Many recent advances in robotic manipulation have come through imitation\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\nform of demonstrations: those collected on the same robot in the same room with\nthe same objects as the trained policy must handle at test time. In contrast,\nlarge pre-recorded human video datasets demonstrating manipulation skills\nin-the-wild already exist, which contain valuable information for robots. Is it\npossible to distill a repository of useful robotic skill policies out of such\ndata without any additional requirements on robot-specific demonstrations or\nexploration? We present the first such system ZeroMimic, that generates\nimmediately deployable image goal-conditioned skill policies for several common\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\ncutting, and stirring) each capable of acting upon diverse objects and across\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\nadvances in semantic and geometric visual understanding of human videos,\ntogether with modern grasp affordance detectors and imitation policy classes.\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\nand simulated kitchen settings with two different robot embodiments,\ndemonstrating its impressive abilities to handle these varied tasks. To enable\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\nrelease software and policy checkpoints of our skill policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent advances in robotic manipulation have come through imitation\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\nform of demonstrations: those collected on the same robot in the same room with\nthe same objects as the trained policy must handle at test time. In contrast,\nlarge pre-recorded human video datasets demonstrating manipulation skills\nin-the-wild already exist, which contain valuable information for robots. Is it\npossible to distill a repository of useful robotic skill policies out of such\ndata without any additional requirements on robot-specific demonstrations or\nexploration? We present the first such system ZeroMimic, that generates\nimmediately deployable image goal-conditioned skill policies for several common\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\ncutting, and stirring) each capable of acting upon diverse objects and across\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\nadvances in semantic and geometric visual understanding of human videos,\ntogether with modern grasp affordance detectors and imitation policy classes.\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\nand simulated kitchen settings with two different robot embodiments,\ndemonstrating its impressive abilities to handle these varied tasks. To enable\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\nrelease software and policy checkpoints of our skill policies."
                },
                "authors": [
                    {
                        "name": "Junyao Shi"
                    },
                    {
                        "name": "Zhuolun Zhao"
                    },
                    {
                        "name": "Tianyou Wang"
                    },
                    {
                        "name": "Ian Pedroza"
                    },
                    {
                        "name": "Amy Luo"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Jason Ma"
                    },
                    {
                        "name": "Dinesh Jayaraman"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Jayaraman"
                },
                "author": "Dinesh Jayaraman",
                "arxiv_comment": "ICRA 2025. Project website: https://zeromimic.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23875v1",
                "updated": "2025-03-31T09:26:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    26,
                    34,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:26:34Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    26,
                    34,
                    0,
                    90,
                    0
                ],
                "title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via\n  Language Models"
                },
                "summary": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm."
                },
                "authors": [
                    {
                        "name": "Wenkang Ji"
                    },
                    {
                        "name": "Huaben Chen"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Guobin Zhu"
                    },
                    {
                        "name": "Lufeng Xu"
                    },
                    {
                        "name": "Roderich Gro"
                    },
                    {
                        "name": "Rui Zhou"
                    },
                    {
                        "name": "Ming Cao"
                    },
                    {
                        "name": "Shiyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Zhao"
                },
                "author": "Shiyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23873v1",
                "updated": "2025-03-31T09:23:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    23,
                    52,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:23:52Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    23,
                    52,
                    0,
                    90,
                    0
                ],
                "title": "Exploring In-Context Learning Capabilities of ChatGPT for Pathological\n  Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring In-Context Learning Capabilities of ChatGPT for Pathological\n  Speech Detection"
                },
                "summary": "Automatic pathological speech detection approaches have shown promising\nresults, gaining attention as potential diagnostic tools alongside costly\ntraditional methods. While these approaches can achieve high accuracy, their\nlack of interpretability limits their applicability in clinical practice. In\nthis paper, we investigate the use of multimodal Large Language Models (LLMs),\nspecifically ChatGPT-4o, for automatic pathological speech detection in a\nfew-shot in-context learning setting. Experimental results show that this\napproach not only delivers promising performance but also provides explanations\nfor its decisions, enhancing model interpretability. To further understand its\neffectiveness, we conduct an ablation study to analyze the impact of different\nfactors, such as input type and system prompts, on the final results. Our\nfindings highlight the potential of multimodal LLMs for further exploration and\nadvancement in automatic pathological speech detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic pathological speech detection approaches have shown promising\nresults, gaining attention as potential diagnostic tools alongside costly\ntraditional methods. While these approaches can achieve high accuracy, their\nlack of interpretability limits their applicability in clinical practice. In\nthis paper, we investigate the use of multimodal Large Language Models (LLMs),\nspecifically ChatGPT-4o, for automatic pathological speech detection in a\nfew-shot in-context learning setting. Experimental results show that this\napproach not only delivers promising performance but also provides explanations\nfor its decisions, enhancing model interpretability. To further understand its\neffectiveness, we conduct an ablation study to analyze the impact of different\nfactors, such as input type and system prompts, on the final results. Our\nfindings highlight the potential of multimodal LLMs for further exploration and\nadvancement in automatic pathological speech detection."
                },
                "authors": [
                    {
                        "name": "Mahdi Amiri"
                    },
                    {
                        "name": "Hatef Otroshi Shahreza"
                    },
                    {
                        "name": "Ina Kodrasi"
                    }
                ],
                "author_detail": {
                    "name": "Ina Kodrasi"
                },
                "author": "Ina Kodrasi",
                "arxiv_comment": "submitted to EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23869v1",
                "updated": "2025-03-31T09:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:18:42Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    18,
                    42,
                    0,
                    90,
                    0
                ],
                "title": "Communication-Efficient and Personalized Federated Foundation Model\n  Fine-Tuning via Tri-Matrix Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-Efficient and Personalized Federated Foundation Model\n  Fine-Tuning via Tri-Matrix Adaptation"
                },
                "summary": "In federated learning, fine-tuning pre-trained foundation models poses\nsignificant challenges, particularly regarding high communication cost and\nsuboptimal model performance due to data heterogeneity between the clients. To\naddress these issues, this paper introduces communication-efficient federated\nLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank\nadaptation approach with personalized model parameter aggregation. We first\npresents a novel LoRA parameter factorization by introducing a small-size dense\nmatrix, which can significantly reduce the communication cost and achieve\ncomparable empirical performance than transferring the low-rank parameter\nmatrix used by existing methods. Without violating data privacy, the server\nconsiders the client similarity in both training dataset and model parameter\nspace, and learns personalized weights for model aggregation. Our experiments\non various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only\nsignificantly reduces communication overhead but also improves performance\nunder not independently and identically distributed data conditions. In\naddition, CE-LoRA improves data privacy protection, effectively mitigating\ngradient-based data reconstruction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In federated learning, fine-tuning pre-trained foundation models poses\nsignificant challenges, particularly regarding high communication cost and\nsuboptimal model performance due to data heterogeneity between the clients. To\naddress these issues, this paper introduces communication-efficient federated\nLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rank\nadaptation approach with personalized model parameter aggregation. We first\npresents a novel LoRA parameter factorization by introducing a small-size dense\nmatrix, which can significantly reduce the communication cost and achieve\ncomparable empirical performance than transferring the low-rank parameter\nmatrix used by existing methods. Without violating data privacy, the server\nconsiders the client similarity in both training dataset and model parameter\nspace, and learns personalized weights for model aggregation. Our experiments\non various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not only\nsignificantly reduces communication overhead but also improves performance\nunder not independently and identically distributed data conditions. In\naddition, CE-LoRA improves data privacy protection, effectively mitigating\ngradient-based data reconstruction attacks."
                },
                "authors": [
                    {
                        "name": "Yongle Li"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Sheng Huang"
                    },
                    {
                        "name": "ZHeng ZHang"
                    },
                    {
                        "name": "Xiaotong Yuan"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23859v1",
                "updated": "2025-03-31T09:06:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:06:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    6,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating small vision-language models as AI assistants for radio\n  astronomical source analysis tasks"
                },
                "summary": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of next-generation radio telescopes is set to transform radio\nastronomy by producing massive data volumes that challenge traditional\nprocessing methods. Deep learning techniques have shown strong potential in\nautomating radio analysis tasks, yet are often constrained by the limited\navailability of large annotated datasets. Recent progress in self-supervised\nlearning has led to foundational radio vision models, but adapting them for new\ntasks typically requires coding expertise, limiting their accessibility to a\nbroader astronomical community. Text-based AI interfaces offer a promising\nalternative by enabling task-specific queries and example-driven learning. In\nthis context, Large Language Models (LLMs), with their remarkable zero-shot\ncapabilities, are increasingly used in scientific domains. However, deploying\nlarge-scale models remains resource-intensive, and there is a growing demand\nfor AI systems that can reason over both visual and textual data in\nastronomical analysis. This study explores small-scale Vision-Language Models\n(VLMs) as AI assistants for radio astronomy, combining LLM capabilities with\nvision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio\nimages from multiple surveys, enriched with 38k image-caption pairs from the\nliterature. The fine-tuned models show clear improvements over base models in\nradio-specific tasks, achieving ~30% F1-score gains in extended source\ndetection, but they underperform pure vision models and exhibit ~20% drop on\ngeneral multimodal tasks. Inclusion of caption data and LoRA fine-tuning\nenhances instruction-following and helps recover ~10% accuracy on standard\nbenchmarks. This work lays the foundation for future advancements in radio\nVLMs, highlighting their potential and limitations, such as the need for better\nmultimodal alignment, higher-quality datasets, and mitigation of catastrophic\nforgetting."
                },
                "authors": [
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "T. Cecconello"
                    },
                    {
                        "name": "A. Pilzer"
                    },
                    {
                        "name": "S. Palazzo"
                    },
                    {
                        "name": "N. Gupta"
                    },
                    {
                        "name": "A. M. Hopkins"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "G. Umana"
                    }
                ],
                "author_detail": {
                    "name": "G. Umana"
                },
                "author": "G. Umana",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23848v1",
                "updated": "2025-03-31T08:52:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    52,
                    21,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T08:52:21Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    52,
                    21,
                    0,
                    90,
                    0
                ],
                "title": "SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to\n  Accelerate Your Speech-LLM Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to\n  Accelerate Your Speech-LLM Development"
                },
                "summary": "High-quality speech dialogue datasets are crucial for Speech-LLM development,\nyet existing acquisition methods face significant limitations. Human recordings\nincur high costs and privacy concerns, while synthetic approaches often lack\nconversational authenticity. To address these challenges, we introduce\n\\textsc{SpeechDialogueFactory}, a production-ready framework for generating\nnatural speech dialogues efficiently. Our solution employs a comprehensive\npipeline including metadata generation, dialogue scripting,\nparalinguistic-enriched utterance simulation, and natural speech synthesis with\nvoice cloning. Additionally, the system provides an interactive UI for detailed\nsample inspection and a high-throughput batch synthesis mode. Evaluations show\nthat dialogues generated by our system achieve a quality comparable to human\nrecordings while significantly reducing production costs. We release our work\nas an open-source toolkit, alongside example datasets available in English and\nChinese, empowering researchers and developers in Speech-LLM research and\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality speech dialogue datasets are crucial for Speech-LLM development,\nyet existing acquisition methods face significant limitations. Human recordings\nincur high costs and privacy concerns, while synthetic approaches often lack\nconversational authenticity. To address these challenges, we introduce\n\\textsc{SpeechDialogueFactory}, a production-ready framework for generating\nnatural speech dialogues efficiently. Our solution employs a comprehensive\npipeline including metadata generation, dialogue scripting,\nparalinguistic-enriched utterance simulation, and natural speech synthesis with\nvoice cloning. Additionally, the system provides an interactive UI for detailed\nsample inspection and a high-throughput batch synthesis mode. Evaluations show\nthat dialogues generated by our system achieve a quality comparable to human\nrecordings while significantly reducing production costs. We release our work\nas an open-source toolkit, alongside example datasets available in English and\nChinese, empowering researchers and developers in Speech-LLM research and\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Ye Bai"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23829v1",
                "updated": "2025-03-31T08:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    22,
                    49,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T08:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    22,
                    49,
                    0,
                    90,
                    0
                ],
                "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding RL with Verifiable Rewards Across Diverse Domains"
                },
                "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23827v1",
                "updated": "2025-03-31T08:21:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    21,
                    11,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T08:21:11Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    21,
                    11,
                    0,
                    90,
                    0
                ],
                "title": "Aud-Sur: An Audio Analyzer Assistant for Audio Surveillance Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aud-Sur: An Audio Analyzer Assistant for Audio Surveillance Applications"
                },
                "summary": "In this paper, we present an audio analyzer assistant tool designed for a\nwide range of audio-based surveillance applications (This work is a part of our\nDEFAME FAKES and EUCINF projects). The proposed tool, refered to as Aud-Sur,\ncomprises two main phases Audio Analysis and Audio Retrieval, respectively. In\nthe first phase, multiple open-source audio models are leveraged to extract\ninformation from input audio recording uploaded by a user. In the second phase,\nusers interact with the Aud-Sur tool via a natural question-and-answer manner,\npowered by a large language model (LLM), to retrieve the information extracted\nfrom the processed audio file. The Aud-Sur tool was deployed using Docker on a\nmicroservices-based architecture design. By leveraging open-source audio models\nfor information extraction, LLM for audio information retrieval, and a\nmicroservices-based deployment approach, the proposed Aud-Sur tool offers a\nhighly extensible and adaptable framework that can integrate more audio tasks,\nand be widely shared within the audio community for further development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an audio analyzer assistant tool designed for a\nwide range of audio-based surveillance applications (This work is a part of our\nDEFAME FAKES and EUCINF projects). The proposed tool, refered to as Aud-Sur,\ncomprises two main phases Audio Analysis and Audio Retrieval, respectively. In\nthe first phase, multiple open-source audio models are leveraged to extract\ninformation from input audio recording uploaded by a user. In the second phase,\nusers interact with the Aud-Sur tool via a natural question-and-answer manner,\npowered by a large language model (LLM), to retrieve the information extracted\nfrom the processed audio file. The Aud-Sur tool was deployed using Docker on a\nmicroservices-based architecture design. By leveraging open-source audio models\nfor information extraction, LLM for audio information retrieval, and a\nmicroservices-based deployment approach, the proposed Aud-Sur tool offers a\nhighly extensible and adaptable framework that can integrate more audio tasks,\nand be widely shared within the audio community for further development."
                },
                "authors": [
                    {
                        "name": "Phat Lam"
                    },
                    {
                        "name": "Lam Pham"
                    },
                    {
                        "name": "Dat Tran"
                    },
                    {
                        "name": "Alexander Schindler"
                    },
                    {
                        "name": "Silvia Poletti"
                    },
                    {
                        "name": "Marcel Hasenbalg"
                    },
                    {
                        "name": "David Fischinger"
                    },
                    {
                        "name": "Martin Boyer"
                    }
                ],
                "author_detail": {
                    "name": "Martin Boyer"
                },
                "author": "Martin Boyer",
                "arxiv_comment": "A preprint for conference paper, 8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23823v1",
                "updated": "2025-03-31T08:19:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    19,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T08:19:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    19,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "Blockchain for Federated Learning in the Internet of Things: Trustworthy\n  Adaptation, Standards, and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain for Federated Learning in the Internet of Things: Trustworthy\n  Adaptation, Standards, and the Road Ahead"
                },
                "summary": "As edge computing gains prominence in Internet of Things (IoTs), smart\ncities, and autonomous systems, the demand for real-time machine intelligence\nwith low latency and model reliability continues to grow. Federated Learning\n(FL) addresses these needs by enabling distributed model training without\ncentralizing user data, yet it remains reliant on centralized servers and lacks\nbuilt-in mechanisms for transparency and trust. Blockchain and Distributed\nLedger Technologies (DLTs) can fill this gap by introducing immutability,\ndecentralized coordination, and verifiability into FL workflows. This article\npresents current standardization efforts from 3GPP, ETSI, ITU-T, IEEE, and\nO-RAN that steer the integration of FL and blockchain in IoT ecosystems. We\nthen propose a blockchain-based FL framework that replaces the centralized\naggregator, incorporates reputation monitoring of IoT devices, and minimizes\noverhead via selective on-chain storage of model updates. We validate our\napproach with IOTA Tangle, demonstrating stable throughput and block\nconfirmations, even under increasing FL workloads. Finally, we discuss\narchitectural considerations and future directions for embedding trustworthy\nand resource-efficient FL in emerging 6G networks and vertical IoT\napplications. Our results underscore the potential of DLT-enhanced FL to meet\nstringent trust and energy requirements of next-generation IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As edge computing gains prominence in Internet of Things (IoTs), smart\ncities, and autonomous systems, the demand for real-time machine intelligence\nwith low latency and model reliability continues to grow. Federated Learning\n(FL) addresses these needs by enabling distributed model training without\ncentralizing user data, yet it remains reliant on centralized servers and lacks\nbuilt-in mechanisms for transparency and trust. Blockchain and Distributed\nLedger Technologies (DLTs) can fill this gap by introducing immutability,\ndecentralized coordination, and verifiability into FL workflows. This article\npresents current standardization efforts from 3GPP, ETSI, ITU-T, IEEE, and\nO-RAN that steer the integration of FL and blockchain in IoT ecosystems. We\nthen propose a blockchain-based FL framework that replaces the centralized\naggregator, incorporates reputation monitoring of IoT devices, and minimizes\noverhead via selective on-chain storage of model updates. We validate our\napproach with IOTA Tangle, demonstrating stable throughput and block\nconfirmations, even under increasing FL workloads. Finally, we discuss\narchitectural considerations and future directions for embedding trustworthy\nand resource-efficient FL in emerging 6G networks and vertical IoT\napplications. Our results underscore the potential of DLT-enhanced FL to meet\nstringent trust and energy requirements of next-generation IoT deployments."
                },
                "authors": [
                    {
                        "name": "Farhana Javed"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Josep Mangues-Bafalluy"
                    },
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Luis Blanco"
                    }
                ],
                "author_detail": {
                    "name": "Luis Blanco"
                },
                "author": "Luis Blanco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18042v2",
                "updated": "2025-03-31T08:16:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    8,
                    16,
                    49,
                    0,
                    90,
                    0
                ],
                "published": "2024-11-27T04:24:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    4,
                    24,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation"
                },
                "summary": "Multimodal LLMs have advanced vision-language tasks but still struggle with\nunderstanding video scenes. To bridge this gap, Video Scene Graph Generation\n(VidSGG) has emerged to capture multi-object relationships across video frames.\nHowever, prior methods rely on pairwise connections, limiting their ability to\nhandle complex multi-object interactions and reasoning. To this end, we propose\nMultimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about\nmulti-way interactions and higher-order relationships. Our approach uniquely\nintegrates entity scene graphs, which capture spatial relationships between\nobjects, with a procedural graph that models their causal transitions, forming\na unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting\nthis unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene\nGraph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,\negocentric, and drone views and supports five tasks: Scene Graph Generation,\nScene Graph Anticipation, Video Question Answering, Video Captioning, and\nRelation Reasoning. Empirically, HyperGLM consistently outperforms\nstate-of-the-art methods across five tasks, effectively modeling and reasoning\ncomplex relationships in diverse video scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs have advanced vision-language tasks but still struggle with\nunderstanding video scenes. To bridge this gap, Video Scene Graph Generation\n(VidSGG) has emerged to capture multi-object relationships across video frames.\nHowever, prior methods rely on pairwise connections, limiting their ability to\nhandle complex multi-object interactions and reasoning. To this end, we propose\nMultimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about\nmulti-way interactions and higher-order relationships. Our approach uniquely\nintegrates entity scene graphs, which capture spatial relationships between\nobjects, with a procedural graph that models their causal transitions, forming\na unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting\nthis unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene\nGraph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,\negocentric, and drone views and supports five tasks: Scene Graph Generation,\nScene Graph Anticipation, Video Question Answering, Video Captioning, and\nRelation Reasoning. Empirically, HyperGLM consistently outperforms\nstate-of-the-art methods across five tasks, effectively modeling and reasoning\ncomplex relationships in diverse video scenes."
                },
                "authors": [
                    {
                        "name": "Trong-Thuan Nguyen"
                    },
                    {
                        "name": "Pha Nguyen"
                    },
                    {
                        "name": "Jackson Cothren"
                    },
                    {
                        "name": "Alper Yilmaz"
                    },
                    {
                        "name": "Khoa Luu"
                    }
                ],
                "author_detail": {
                    "name": "Khoa Luu"
                },
                "author": "Khoa Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23817v1",
                "updated": "2025-03-31T07:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    54,
                    59,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    54,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM\n  Acceleration"
                },
                "summary": "General matrix-vector multiplication (GeMV) remains a critical latency\nbottleneck in large language model (LLM) inference, even with quantized low-bit\nmodels. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has\nthe potential to repurpose on-device DRAM as a GeMV engine, offering additional\nhigh-throughput processing capabilities to widespread consumer devices without\nDRAM modifications. However, applying PUD to GeMV operations in the LLM\ninference pipeline incurs significant overheads $\\textit{before}$ and\n$\\textit{after}$ in-DRAM computation, diminishing the benefits of its\nhigh-throughput processing capabilities.\n  This paper presents MVDRAM, the first practical system to accelerate GeMV\noperations for low-bit LLM inference using unmodified DRAM. By leveraging the\ndata sharing patterns and mathematical linearity in GeMV operations, MVDRAM\norchestrates the processor and DRAM to eliminate the costs associated with\npre-arranging inputs and bit-transposition of outputs required in conventional\nPUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows\nthat MVDRAM achieves comparable or even better inference speed than the\nprocessor-based implementation for GeMV operations in low-bit (under 4-bit)\nLLM. In particular, MVDRAM achieves up to 7.29$\\times$ speedup and 30.5$\\times$\nenergy efficiency for low-bit GeMV operations. For end-to-end LLM inference,\nMVDRAM achieves 2.18$\\times$ and 1.31$\\times$ throughput improvements, along\nwith 3.04$\\times$ and 2.35$\\times$ energy efficiency, for 2-bit and 4-bit\nquantized low-bit models, respectively. MVDRAM has the potential to redefine\nthe AI hardware landscape by demonstrating the feasibility of standard DRAM as\nan LLM accelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General matrix-vector multiplication (GeMV) remains a critical latency\nbottleneck in large language model (LLM) inference, even with quantized low-bit\nmodels. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has\nthe potential to repurpose on-device DRAM as a GeMV engine, offering additional\nhigh-throughput processing capabilities to widespread consumer devices without\nDRAM modifications. However, applying PUD to GeMV operations in the LLM\ninference pipeline incurs significant overheads $\\textit{before}$ and\n$\\textit{after}$ in-DRAM computation, diminishing the benefits of its\nhigh-throughput processing capabilities.\n  This paper presents MVDRAM, the first practical system to accelerate GeMV\noperations for low-bit LLM inference using unmodified DRAM. By leveraging the\ndata sharing patterns and mathematical linearity in GeMV operations, MVDRAM\norchestrates the processor and DRAM to eliminate the costs associated with\npre-arranging inputs and bit-transposition of outputs required in conventional\nPUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows\nthat MVDRAM achieves comparable or even better inference speed than the\nprocessor-based implementation for GeMV operations in low-bit (under 4-bit)\nLLM. In particular, MVDRAM achieves up to 7.29$\\times$ speedup and 30.5$\\times$\nenergy efficiency for low-bit GeMV operations. For end-to-end LLM inference,\nMVDRAM achieves 2.18$\\times$ and 1.31$\\times$ throughput improvements, along\nwith 3.04$\\times$ and 2.35$\\times$ energy efficiency, for 2-bit and 4-bit\nquantized low-bit models, respectively. MVDRAM has the potential to redefine\nthe AI hardware landscape by demonstrating the feasibility of standard DRAM as\nan LLM accelerator."
                },
                "authors": [
                    {
                        "name": "Tatsuya Kubo"
                    },
                    {
                        "name": "Daichi Tokuda"
                    },
                    {
                        "name": "Tomoya Nagatani"
                    },
                    {
                        "name": "Masayuki Usui"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Shinya Takamaeda-Yamazaki"
                    }
                ],
                "author_detail": {
                    "name": "Shinya Takamaeda-Yamazaki"
                },
                "author": "Shinya Takamaeda-Yamazaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23811v2",
                "updated": "2025-04-01T06:56:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    6,
                    56,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T07:44:26Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    26,
                    0,
                    90,
                    0
                ],
                "title": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Did ChatGPT or Copilot use alter the style of internet news headlines? A\n  time series regression analysis"
                },
                "summary": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures."
                },
                "authors": [
                    {
                        "name": "Chris Brogly"
                    },
                    {
                        "name": "Connor McElroy"
                    }
                ],
                "author_detail": {
                    "name": "Connor McElroy"
                },
                "author": "Connor McElroy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23810v1",
                "updated": "2025-03-31T07:44:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    14,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:44:14Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    44,
                    14,
                    0,
                    90,
                    0
                ],
                "title": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization"
                },
                "summary": "Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that can efficiently adapt to varying signal\nconditions and environmental changes. Factors such as multipath interference\nand obstructions introduce different levels of complexity that affect the\naccuracy of the localization. Although generalized models offer broad\napplicability, they often struggle to capture the nuances of specific\nenvironments, leading to suboptimal performance in real-world deployments. In\ncontrast, specialized models can be tailored to particular conditions, enabling\nmore precise localization by effectively handling domain-specific variations\nand noise patterns. However, deploying multiple specialized models requires an\nefficient mechanism to select the most appropriate one for a given scenario. In\nthis work, we develop an adaptive localization framework that combines shallow\nattention-based models with a router/switching mechanism based on a\nsingle-layer perceptron (SLP). This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy, computational efficiency, and robustness to environmental variations.\nWe design three low-complex localization models tailored for distinct\nscenarios, optimized for reduced computational complexity, test time, and model\nsize. The router dynamically selects the most suitable model based on real-time\ninput characteristics. The proposed framework is validated using real-world\nvehicle localization data collected from a massive MIMO base station (BS),\ndemonstrating its ability to seamlessly adapt to diverse deployment conditions\nwhile maintaining high localization accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that can efficiently adapt to varying signal\nconditions and environmental changes. Factors such as multipath interference\nand obstructions introduce different levels of complexity that affect the\naccuracy of the localization. Although generalized models offer broad\napplicability, they often struggle to capture the nuances of specific\nenvironments, leading to suboptimal performance in real-world deployments. In\ncontrast, specialized models can be tailored to particular conditions, enabling\nmore precise localization by effectively handling domain-specific variations\nand noise patterns. However, deploying multiple specialized models requires an\nefficient mechanism to select the most appropriate one for a given scenario. In\nthis work, we develop an adaptive localization framework that combines shallow\nattention-based models with a router/switching mechanism based on a\nsingle-layer perceptron (SLP). This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy, computational efficiency, and robustness to environmental variations.\nWe design three low-complex localization models tailored for distinct\nscenarios, optimized for reduced computational complexity, test time, and model\nsize. The router dynamically selects the most suitable model based on real-time\ninput characteristics. The proposed framework is validated using real-world\nvehicle localization data collected from a massive MIMO base station (BS),\ndemonstrating its ability to seamlessly adapt to diverse deployment conditions\nwhile maintaining high localization accuracy."
                },
                "authors": [
                    {
                        "name": "Ilayda Yaman"
                    },
                    {
                        "name": "Guoda Tian"
                    },
                    {
                        "name": "Fredrik Tufvesson"
                    },
                    {
                        "name": "Ove Edfors"
                    },
                    {
                        "name": "Zhengya Zhang"
                    },
                    {
                        "name": "Liang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Liu"
                },
                "author": "Liang Liu",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21187v2",
                "updated": "2025-03-31T07:41:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    41,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-27T06:08:24Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    6,
                    8,
                    24,
                    3,
                    86,
                    0
                ],
                "title": "DSU-Net:An Improved U-Net Model Based on DINOv2 and SAM2 with\n  Multi-scale Cross-model Feature Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSU-Net:An Improved U-Net Model Based on DINOv2 and SAM2 with\n  Multi-scale Cross-model Feature Enhancement"
                },
                "summary": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant advancements in general image segmentation achieved\nby large-scale pre-trained foundation models (such as Meta's Segment Any-thing\nModel (SAM) series and DINOv2), their performance in specialized fields remains\nlimited by two critical issues: the excessive training costs due to large model\nparameters, and the insufficient ability to represent specific domain\ncharacteristics. This paper proposes a multi-scale feature collabora-tion\nframework guided by DINOv2 for SAM2, with core innovations in three aspects:\n(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2\nbackbones, where high-dimensional semantic features extracted by the\nself-supervised model guide multi-scale feature fusion; (2) Designing\nlightweight adapter modules and cross-modal, cross-layer feature fusion units\nto inject cross-domain knowledge while freezing the base model parameters; (3)\nConstructing a U-shaped network structure based on U-net, which utilizes\nattention mechanisms to achieve adaptive aggregation decoding of\nmulti-granularity features. This framework surpasses existing state-of-the-art\nmeth-ods in downstream tasks such as camouflage target detection and salient\nob-ject detection, without requiring costly training processes. It provides a\ntech-nical pathway for efficient deployment of visual image segmentation,\ndemon-strating significant application value in a wide range of downstream\ntasks and specialized fields within image segmentation.Project page:\nhttps://github.com/CheneyXuYiMin/SAM2DINO-Seg"
                },
                "authors": [
                    {
                        "name": "Yimin Xu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Bin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xu"
                },
                "author": "Bin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23803v1",
                "updated": "2025-03-31T07:31:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    31,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:31:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    31,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute"
                },
                "summary": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner"
                },
                "authors": [
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23801v1",
                "updated": "2025-03-31T07:24:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    24,
                    6,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:24:06Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    24,
                    6,
                    0,
                    90,
                    0
                ],
                "title": "A PINN Methodology for Temperature Field Reconstruction in the PIV\n  Measurement Plane: Case of Rayleigh-Bnard Convection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A PINN Methodology for Temperature Field Reconstruction in the PIV\n  Measurement Plane: Case of Rayleigh-Bnard Convection"
                },
                "summary": "We present a method to infer temperature fields from stereo particle-image\nvelocimetry (PIV) data in turbulent Rayleigh-B\\'enard convection (RBC) using\nPhysics-informed neural networks (PINNs). The physical setup is a cubic RBC\ncell with Rayleigh number $\\text{Ra}=10^7$ and Prandtl number $\\text{Pr}=0.7$.\nWith data only available in a vertical plane $A:x=x_0$, the residuals of the\ngoverning partial differential equations are minimised in an enclosing 3D\ndomain around $A$ with thickness $\\delta_x$. Dynamic collocation point sampling\nstrategies are used to overcome the lack of 3D labelled information and to\noptimize the overall convergence of the PINN. In particular, in the\nout-of-plane direction $x$, the collocation points are distributed according to\na normal distribution, in order to emphasize the region where data is provided.\nAlong the vertical direction, we leverage meshing information and sample points\nfrom a distribution designed based on the grid of a direct numerical simulation\n(DNS). This approach points greater attention to critical regions, particularly\nthe areas with high temperature gradients within the thermal boundary layers.\nUsing planar three-component velocity data from a DNS, we successfully validate\nthe reconstruction of the temperature fields in the PIV plane. We evaluate the\nrobustness of our method with respect to characteristics of the labelled data\nused for training: the data time span, the sampling frequency, some noisy data\nand boundary data omission, aiming to better accommodate the challenges\nassociated with experimental data. Developing PINNs on controlled simulation\ndata is a crucial step toward their effective deployment on experimental data.\nThe key is to systematically introduce noise, gaps, and uncertainties in\nsimulated data to mimic real-world conditions and ensure robust generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method to infer temperature fields from stereo particle-image\nvelocimetry (PIV) data in turbulent Rayleigh-B\\'enard convection (RBC) using\nPhysics-informed neural networks (PINNs). The physical setup is a cubic RBC\ncell with Rayleigh number $\\text{Ra}=10^7$ and Prandtl number $\\text{Pr}=0.7$.\nWith data only available in a vertical plane $A:x=x_0$, the residuals of the\ngoverning partial differential equations are minimised in an enclosing 3D\ndomain around $A$ with thickness $\\delta_x$. Dynamic collocation point sampling\nstrategies are used to overcome the lack of 3D labelled information and to\noptimize the overall convergence of the PINN. In particular, in the\nout-of-plane direction $x$, the collocation points are distributed according to\na normal distribution, in order to emphasize the region where data is provided.\nAlong the vertical direction, we leverage meshing information and sample points\nfrom a distribution designed based on the grid of a direct numerical simulation\n(DNS). This approach points greater attention to critical regions, particularly\nthe areas with high temperature gradients within the thermal boundary layers.\nUsing planar three-component velocity data from a DNS, we successfully validate\nthe reconstruction of the temperature fields in the PIV plane. We evaluate the\nrobustness of our method with respect to characteristics of the labelled data\nused for training: the data time span, the sampling frequency, some noisy data\nand boundary data omission, aiming to better accommodate the challenges\nassociated with experimental data. Developing PINNs on controlled simulation\ndata is a crucial step toward their effective deployment on experimental data.\nThe key is to systematically introduce noise, gaps, and uncertainties in\nsimulated data to mimic real-world conditions and ensure robust generalization."
                },
                "authors": [
                    {
                        "name": "Marie-Christine Volk"
                    },
                    {
                        "name": "Anne Sergent"
                    },
                    {
                        "name": "Didier Lucor"
                    },
                    {
                        "name": "Michael Mommert"
                    },
                    {
                        "name": "Christian Bauer"
                    },
                    {
                        "name": "Claus Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Claus Wagner"
                },
                "author": "Claus Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23798v1",
                "updated": "2025-03-31T07:20:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    20,
                    58,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:20:58Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    20,
                    58,
                    0,
                    90,
                    0
                ],
                "title": "Adaptive Layer-skipping in Pre-trained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer-skipping in Pre-trained LLMs"
                },
                "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration."
                },
                "authors": [
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Weizhi Wang"
                    },
                    {
                        "name": "Xifeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xifeng Yan"
                },
                "author": "Xifeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23791v1",
                "updated": "2025-03-31T07:09:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    9,
                    7,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T07:09:07Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    9,
                    7,
                    0,
                    90,
                    0
                ],
                "title": "LLMigrate: Transforming \"Lazy\" Large Language Models into Efficient\n  Source Code Migrators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMigrate: Transforming \"Lazy\" Large Language Models into Efficient\n  Source Code Migrators"
                },
                "summary": "Rewriting C code in Rust provides stronger memory safety, yet migrating large\ncodebases such as the 32-million-line Linux kernel remains challenging. While\nrule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust\nprograms, recent Large Language Model (LLM) approaches produce more idiomatic,\nsafe Rust programs but frequently exhibit \"laziness\", omitting significant\nportions of the target code. To address the issue, in this paper, we present\nLLMigrate, an LLM-based C-to-Rust translation tool that splits modules into\ndiscrete functions, translating them individually, and then reintegrating them.\nLLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a\nstate-of-the-art LLM) with compiler-driven translation and program-repair\ntechniques for complex core functions, and leverages call-graph-guided\ntranslation to ensure consistent interfaces. Evaluations on three\nrepresentative Linux kernel modules (math, sort, and ramfs) show that LLMigrate\nrequires modifying less than 15\\% of the target code, significantly\noutperforming a pure GPT-4o-based migration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewriting C code in Rust provides stronger memory safety, yet migrating large\ncodebases such as the 32-million-line Linux kernel remains challenging. While\nrule-based translators (e.g., C2Rust) provide accurate yet largely unsafe Rust\nprograms, recent Large Language Model (LLM) approaches produce more idiomatic,\nsafe Rust programs but frequently exhibit \"laziness\", omitting significant\nportions of the target code. To address the issue, in this paper, we present\nLLMigrate, an LLM-based C-to-Rust translation tool that splits modules into\ndiscrete functions, translating them individually, and then reintegrating them.\nLLMigrate uses static analysis to retain necessary context, pairs GPT-4o (a\nstate-of-the-art LLM) with compiler-driven translation and program-repair\ntechniques for complex core functions, and leverages call-graph-guided\ntranslation to ensure consistent interfaces. Evaluations on three\nrepresentative Linux kernel modules (math, sort, and ramfs) show that LLMigrate\nrequires modifying less than 15\\% of the target code, significantly\noutperforming a pure GPT-4o-based migration."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10209v3",
                "updated": "2025-03-31T07:00:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    0,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2024-10-14T07:05:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    5,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "SwiftCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning"
                },
                "summary": "As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce \\dataset to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by directly measuring their\nexecution time and memory usage through local execution. The code solution with\nthe lowest execution time and memory consumption is selected as the final\noutput for each task. Experimental results demonstrate significant improvements\nwhen fine-tuning with \\dataset. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. \\dataset offers a scalable and effective\nsolution for advancing AI-driven code generation, benefiting both software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased in https://github.com/huangd1999/Effi-Code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce \\dataset to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by directly measuring their\nexecution time and memory usage through local execution. The code solution with\nthe lowest execution time and memory consumption is selected as the final\noutput for each task. Experimental results demonstrate significant improvements\nwhen fine-tuning with \\dataset. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. \\dataset offers a scalable and effective\nsolution for advancing AI-driven code generation, benefiting both software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased in https://github.com/huangd1999/Effi-Code."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Han Weng"
                    },
                    {
                        "name": "Yuhao Qing"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Jie M. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie M. Zhang"
                },
                "author": "Jie M. Zhang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23781v1",
                "updated": "2025-03-31T06:56:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    56,
                    13,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T06:56:13Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    56,
                    13,
                    0,
                    90,
                    0
                ],
                "title": "DebFlow: Automating Agent Creation via Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DebFlow: Automating Agent Creation via Agent Debate"
                },
                "summary": "Large language models (LLMs) have demonstrated strong potential and\nimpressive performance in automating the generation and optimization of\nworkflows. However, existing approaches are marked by limited reasoning\ncapabilities, high computational demands, and significant resource\nrequirements. To address these issues, we propose DebFlow, a framework that\nemploys a debate mechanism to optimize workflows and integrates reflexion to\nimprove based on previous experiences. We evaluated our method across six\nbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach\nachieved a 3\\% average performance improvement over the latest baselines,\ndemonstrating its effectiveness in diverse problem domains. In particular,\nduring training, our framework reduces resource consumption by 37\\% compared to\nthe state-of-the-art baselines. Additionally, we performed ablation studies.\nRemoving the Debate component resulted in a 4\\% performance drop across two\nbenchmark datasets, significantly greater than the 2\\% drop observed when the\nReflection component was removed. These findings strongly demonstrate the\ncritical role of Debate in enhancing framework performance, while also\nhighlighting the auxiliary contribution of reflexion to overall optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong potential and\nimpressive performance in automating the generation and optimization of\nworkflows. However, existing approaches are marked by limited reasoning\ncapabilities, high computational demands, and significant resource\nrequirements. To address these issues, we propose DebFlow, a framework that\nemploys a debate mechanism to optimize workflows and integrates reflexion to\nimprove based on previous experiences. We evaluated our method across six\nbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach\nachieved a 3\\% average performance improvement over the latest baselines,\ndemonstrating its effectiveness in diverse problem domains. In particular,\nduring training, our framework reduces resource consumption by 37\\% compared to\nthe state-of-the-art baselines. Additionally, we performed ablation studies.\nRemoving the Debate component resulted in a 4\\% performance drop across two\nbenchmark datasets, significantly greater than the 2\\% drop observed when the\nReflection component was removed. These findings strongly demonstrate the\ncritical role of Debate in enhancing framework performance, while also\nhighlighting the auxiliary contribution of reflexion to overall optimization."
                },
                "authors": [
                    {
                        "name": "Jinwei Su"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Ronghua Shi"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yijin Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yang Jingsong"
                    },
                    {
                        "name": "Lewei He"
                    }
                ],
                "author_detail": {
                    "name": "Lewei He"
                },
                "author": "Lewei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23779v1",
                "updated": "2025-03-31T06:53:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    53,
                    53,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T06:53:53Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    53,
                    53,
                    0,
                    90,
                    0
                ],
                "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with\n  Common Sense Categorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with\n  Common Sense Categorization"
                },
                "summary": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande."
                },
                "authors": [
                    {
                        "name": "Ine Gevers"
                    },
                    {
                        "name": "Victor De Marez"
                    },
                    {
                        "name": "Luna De Bruyne"
                    },
                    {
                        "name": "Walter Daelemans"
                    }
                ],
                "author_detail": {
                    "name": "Walter Daelemans"
                },
                "author": "Walter Daelemans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23777v1",
                "updated": "2025-03-31T06:52:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    52,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T06:52:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    52,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference\n  Alignment"
                },
                "summary": "Naive joint training of large language models (LLMs) for multilingual\npreference alignment can suffer from negative interference. This is a known\nissue in multilingual training, where conflicting objectives degrade overall\nperformance. However, the impact of this phenomenon in the context of\nmultilingual preference alignment remains largely underexplored. To address\nthis issue, we propose CONGRAD, a scalable and effective filtering method that\nselects high-quality preference samples with minimal gradient conflicts across\nlanguages. Our method leverages gradient surgery to retain samples aligned with\nan aggregated multilingual update direction. Additionally, we incorporate a\nsublinear gradient compression strategy that reduces memory overhead during\ngradient accumulation. We integrate CONGRAD into self-rewarding framework and\nevaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that\nCONGRAD consistently outperforms strong baselines in both seen and unseen\nlanguages, with minimal alignment tax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Naive joint training of large language models (LLMs) for multilingual\npreference alignment can suffer from negative interference. This is a known\nissue in multilingual training, where conflicting objectives degrade overall\nperformance. However, the impact of this phenomenon in the context of\nmultilingual preference alignment remains largely underexplored. To address\nthis issue, we propose CONGRAD, a scalable and effective filtering method that\nselects high-quality preference samples with minimal gradient conflicts across\nlanguages. Our method leverages gradient surgery to retain samples aligned with\nan aggregated multilingual update direction. Additionally, we incorporate a\nsublinear gradient compression strategy that reduces memory overhead during\ngradient accumulation. We integrate CONGRAD into self-rewarding framework and\nevaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that\nCONGRAD consistently outperforms strong baselines in both seen and unseen\nlanguages, with minimal alignment tax."
                },
                "authors": [
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Amirhossein Tebbifakhr"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23771v1",
                "updated": "2025-03-31T06:41:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    41,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T06:41:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    41,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large\n  Ultra-High-Resolution Remote Sensing Imagery?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large\n  Ultra-High-Resolution Remote Sensing Imagery?"
                },
                "summary": "The astonishing breakthrough of multimodal large language models (MLLMs) has\nnecessitated new benchmarks to quantitatively assess their capabilities, reveal\ntheir limitations, and indicate future research directions. However, this is\nchallenging in the context of remote sensing (RS), since the imagery features\nultra-high resolution that incorporates extremely complex semantic\nrelationships. Existing benchmarks usually adopt notably smaller image sizes\nthan real-world RS scenarios, suffer from limited annotation quality, and\nconsider insufficient dimensions of evaluation. To address these issues, we\npresent XLRS-Bench: a comprehensive benchmark for evaluating the perception and\nreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.\nXLRS-Bench boasts the largest average image size (8500$\\times$8500) observed\nthus far, with all evaluation samples meticulously annotated manually, assisted\nby a novel semi-automatic captioner on ultra-high-resolution RS images. On top\nof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of\nperceptual capabilities and 6 kinds of reasoning capabilities, with a primary\nemphasis on advanced cognitive processes that facilitate real-world\ndecision-making and the capture of spatiotemporal changes. The results of both\ngeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts are\nneeded for real-world RS applications. We have open-sourced XLRS-Bench to\nsupport further research in developing more powerful MLLMs for remote sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The astonishing breakthrough of multimodal large language models (MLLMs) has\nnecessitated new benchmarks to quantitatively assess their capabilities, reveal\ntheir limitations, and indicate future research directions. However, this is\nchallenging in the context of remote sensing (RS), since the imagery features\nultra-high resolution that incorporates extremely complex semantic\nrelationships. Existing benchmarks usually adopt notably smaller image sizes\nthan real-world RS scenarios, suffer from limited annotation quality, and\nconsider insufficient dimensions of evaluation. To address these issues, we\npresent XLRS-Bench: a comprehensive benchmark for evaluating the perception and\nreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.\nXLRS-Bench boasts the largest average image size (8500$\\times$8500) observed\nthus far, with all evaluation samples meticulously annotated manually, assisted\nby a novel semi-automatic captioner on ultra-high-resolution RS images. On top\nof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of\nperceptual capabilities and 6 kinds of reasoning capabilities, with a primary\nemphasis on advanced cognitive processes that facilitate real-world\ndecision-making and the capture of spatiotemporal changes. The results of both\ngeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts are\nneeded for real-world RS applications. We have open-sourced XLRS-Bench to\nsupport further research in developing more powerful MLLMs for remote sensing."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Hongzhen Wang"
                    },
                    {
                        "name": "Mingshuo Chen"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Zonghao Guo"
                    },
                    {
                        "name": "Qiang Ma"
                    },
                    {
                        "name": "Long Lan"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "It has been accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10913v2",
                "updated": "2025-03-31T06:38:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    38,
                    48,
                    0,
                    90,
                    0
                ],
                "published": "2025-01-19T01:17:05Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    1,
                    17,
                    5,
                    6,
                    19,
                    0
                ],
                "title": "Know \"No'' Better: A Data-Driven Approach for Enhancing Negation\n  Awareness in CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know \"No'' Better: A Data-Driven Approach for Enhancing Negation\n  Awareness in CLIP"
                },
                "summary": "While CLIP has significantly advanced multimodal understanding by bridging\nvision and language, the inability to grasp negation - such as failing to\ndifferentiate concepts like \"parking\" from \"no parking\" - poses substantial\nchallenges. By analyzing the data used in the public CLIP model's pre-training,\nwe posit this limitation stems from a lack of negation-inclusive data. To\naddress this, we introduce data generation pipelines that employ a large\nlanguage model (LLM) and a multimodal LLM to produce negation-inclusive\ncaptions. Fine-tuning CLIP with data generated from our pipelines, we develop\nNegationCLIP, which enhances negation awareness while preserving the\ngenerality. Moreover, to enable a comprehensive evaluation of negation\nunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'\nability to interpret negation across diverse expressions and positions within a\nsentence. Experiments on various CLIP architectures validate the effectiveness\nof our data generation pipelines in enhancing CLIP's ability to perceive\nnegation accurately. Additionally, NegationCLIP's enhanced negation awareness\nhas practical applications across various multimodal tasks, demonstrated by\nperformance gains in text-to-image generation and referring image segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CLIP has significantly advanced multimodal understanding by bridging\nvision and language, the inability to grasp negation - such as failing to\ndifferentiate concepts like \"parking\" from \"no parking\" - poses substantial\nchallenges. By analyzing the data used in the public CLIP model's pre-training,\nwe posit this limitation stems from a lack of negation-inclusive data. To\naddress this, we introduce data generation pipelines that employ a large\nlanguage model (LLM) and a multimodal LLM to produce negation-inclusive\ncaptions. Fine-tuning CLIP with data generated from our pipelines, we develop\nNegationCLIP, which enhances negation awareness while preserving the\ngenerality. Moreover, to enable a comprehensive evaluation of negation\nunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'\nability to interpret negation across diverse expressions and positions within a\nsentence. Experiments on various CLIP architectures validate the effectiveness\nof our data generation pipelines in enhancing CLIP's ability to perceive\nnegation accurately. Additionally, NegationCLIP's enhanced negation awareness\nhas practical applications across various multimodal tasks, demonstrated by\nperformance gains in text-to-image generation and referring image segmentation."
                },
                "authors": [
                    {
                        "name": "Junsung Park"
                    },
                    {
                        "name": "Jungbeom Lee"
                    },
                    {
                        "name": "Jongyoon Song"
                    },
                    {
                        "name": "Sangwon Yu"
                    },
                    {
                        "name": "Dahuin Jung"
                    },
                    {
                        "name": "Sungroh Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Sungroh Yoon"
                },
                "author": "Sungroh Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23764v2",
                "updated": "2025-04-01T02:13:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    2,
                    13,
                    23,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-31T06:28:41Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    28,
                    41,
                    0,
                    90,
                    0
                ],
                "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation\n  for Efficient Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation\n  for Efficient Medical Image Segmentation"
                },
                "summary": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Md Mahfuz Al Hasan"
                    },
                    {
                        "name": "Mahdi Zaman"
                    },
                    {
                        "name": "Abdul Jawad"
                    },
                    {
                        "name": "Alberto Santamaria-Pang"
                    },
                    {
                        "name": "Ho Hin Lee"
                    },
                    {
                        "name": "Ivan Tarapov"
                    },
                    {
                        "name": "Kyle See"
                    },
                    {
                        "name": "Md Shah Imran"
                    },
                    {
                        "name": "Antika Roy"
                    },
                    {
                        "name": "Yaser Pourmohammadi Fallah"
                    },
                    {
                        "name": "Navid Asadizanjani"
                    },
                    {
                        "name": "Reza Forghani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Forghani"
                },
                "author": "Reza Forghani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23762v1",
                "updated": "2025-03-31T06:27:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    27,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T06:27:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    6,
                    27,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "UniSep: Universal Target Audio Separation with Language Models at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSep: Universal Target Audio Separation with Language Models at Scale"
                },
                "summary": "We propose Universal target audio Separation (UniSep), addressing the\nseparation task on arbitrary mixtures of different types of audio.\nDistinguished from previous studies, UniSep is performed on unlimited source\ndomains and unlimited source numbers. We formulate the separation task as a\nsequence-to-sequence problem, and a large language model (LLM) is used to model\nthe audio sequence in the discrete latent space, leveraging the power of LLM in\nhandling complex mixture audios with large-scale data. Moreover, a novel\npre-training strategy is proposed to utilize audio-only data, which reduces the\nefforts of large-scale data simulation and enhances the ability of LLMs to\nunderstand the consistency and correlation of information within audio\nsequences. We also demonstrate the effectiveness of scaling datasets in an\naudio separation task: we use large-scale data (36.5k hours), including speech,\nmusic, and sound, to train a universal target audio separation model that is\nnot limited to a specific domain. Experiments show that UniSep achieves\ncompetitive subjective and objective evaluation results compared with\nsingle-task models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Universal target audio Separation (UniSep), addressing the\nseparation task on arbitrary mixtures of different types of audio.\nDistinguished from previous studies, UniSep is performed on unlimited source\ndomains and unlimited source numbers. We formulate the separation task as a\nsequence-to-sequence problem, and a large language model (LLM) is used to model\nthe audio sequence in the discrete latent space, leveraging the power of LLM in\nhandling complex mixture audios with large-scale data. Moreover, a novel\npre-training strategy is proposed to utilize audio-only data, which reduces the\nefforts of large-scale data simulation and enhances the ability of LLMs to\nunderstand the consistency and correlation of information within audio\nsequences. We also demonstrate the effectiveness of scaling datasets in an\naudio separation task: we use large-scale data (36.5k hours), including speech,\nmusic, and sound, to train a universal target audio separation model that is\nnot limited to a specific domain. Experiments show that UniSep achieves\ncompetitive subjective and objective evaluation results compared with\nsingle-task models."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Hangting Chen"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Weiqin Li"
                    },
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Guangzhi Li"
                    },
                    {
                        "name": "Shan Yang"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Xixin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xixin Wu"
                },
                "author": "Xixin Wu",
                "arxiv_comment": "Accepted by ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23748v1",
                "updated": "2025-03-31T05:58:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    58,
                    57,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T05:58:57Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    58,
                    57,
                    0,
                    90,
                    0
                ],
                "title": "THEMIS: Towards Practical Intellectual Property Protection for\n  Post-Deployment On-Device Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEMIS: Towards Practical Intellectual Property Protection for\n  Post-Deployment On-Device Deep Learning Models"
                },
                "summary": "On-device deep learning (DL) has rapidly gained adoption in mobile apps,\noffering the benefits of offline model inference and user privacy preservation\nover cloud-based approaches. However, it inevitably stores models on user\ndevices, introducing new vulnerabilities, particularly model-stealing attacks\nand intellectual property infringement. While system-level protections like\nTrusted Execution Environments (TEEs) provide a robust solution, practical\nchallenges remain in achieving scalable on-device DL model protection,\nincluding complexities in supporting third-party models and limited adoption in\ncurrent mobile solutions. Advancements in TEE-enabled hardware, such as\nNVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently,\nwatermarking serves as a common defense against model theft but also faces\nchallenges here as many mobile app developers lack corresponding machine\nlearning expertise and the inherent read-only and inference-only nature of\non-device DL models prevents third parties like app stores from implementing\nexisting watermarking techniques in post-deployment models.\n  To protect the intellectual property of on-device DL models, in this paper,\nwe propose THEMIS, an automatic tool that lifts the read-only restriction of\non-device DL models by reconstructing their writable counterparts and leverages\nthe untrainable nature of on-device DL models to solve watermark parameters and\nprotect the model owner's intellectual property. Extensive experimental results\nacross various datasets and model structures show the superiority of THEMIS in\nterms of different metrics. Further, an empirical investigation of 403\nreal-world DL mobile apps from Google Play is performed with a success rate of\n81.14%, showing the practicality of THEMIS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device deep learning (DL) has rapidly gained adoption in mobile apps,\noffering the benefits of offline model inference and user privacy preservation\nover cloud-based approaches. However, it inevitably stores models on user\ndevices, introducing new vulnerabilities, particularly model-stealing attacks\nand intellectual property infringement. While system-level protections like\nTrusted Execution Environments (TEEs) provide a robust solution, practical\nchallenges remain in achieving scalable on-device DL model protection,\nincluding complexities in supporting third-party models and limited adoption in\ncurrent mobile solutions. Advancements in TEE-enabled hardware, such as\nNVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently,\nwatermarking serves as a common defense against model theft but also faces\nchallenges here as many mobile app developers lack corresponding machine\nlearning expertise and the inherent read-only and inference-only nature of\non-device DL models prevents third parties like app stores from implementing\nexisting watermarking techniques in post-deployment models.\n  To protect the intellectual property of on-device DL models, in this paper,\nwe propose THEMIS, an automatic tool that lifts the read-only restriction of\non-device DL models by reconstructing their writable counterparts and leverages\nthe untrainable nature of on-device DL models to solve watermark parameters and\nprotect the model owner's intellectual property. Extensive experimental results\nacross various datasets and model structures show the superiority of THEMIS in\nterms of different metrics. Further, an empirical investigation of 403\nreal-world DL mobile apps from Google Play is performed with a success rate of\n81.14%, showing the practicality of THEMIS."
                },
                "authors": [
                    {
                        "name": "Yujin Huang"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Qingchuan Zhao"
                    },
                    {
                        "name": "Xingliang Yuan"
                    },
                    {
                        "name": "Chunyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chunyang Chen"
                },
                "author": "Chunyang Chen",
                "arxiv_comment": "To Appear in the 34th USENIX Security Symposium, August 13-15, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23746v1",
                "updated": "2025-03-31T05:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    53,
                    15,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T05:53:15Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    53,
                    15,
                    0,
                    90,
                    0
                ],
                "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A\n  New Large Graph Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-video Propagation Influence Rating: A New Real-world Dataset and A\n  New Large Graph Model"
                },
                "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR."
                },
                "authors": [
                    {
                        "name": "Dizhan Xue"
                    },
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Chuanrui Hu"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23743v1",
                "updated": "2025-03-31T05:47:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    47,
                    8,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T05:47:08Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    47,
                    8,
                    0,
                    90,
                    0
                ],
                "title": "DUNE Software and Computing Research and Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUNE Software and Computing Research and Development"
                },
                "summary": "The international collaboration designing and constructing the Deep\nUnderground Neutrino Experiment (DUNE) at the Long-Baseline Neutrino Facility\n(LBNF) has developed a two-phase strategy toward the implementation of this\nleading-edge, large-scale science project. The ambitious physics program of\nPhase I and Phase II of DUNE is dependent upon deployment and utilization of\nsignificant computing resources, and successful research and development of\nsoftware (both infrastructure and algorithmic) in order to achieve these\nscientific goals. This submission discusses the computing resources\nprojections, infrastructure support, and software development needed for DUNE\nduring the coming decades as an input to the European Strategy for Particle\nPhysics Update for 2026. The DUNE collaboration is submitting four main\ncontributions to the 2026 Update of the European Strategy for Particle Physics\nprocess. This submission to the 'Computing' stream focuses on DUNE software and\ncomputing. Additional inputs related to the DUNE science program, DUNE detector\ntechnologies and R&D, and European contributions to Fermilab accelerator\nupgrades and facilities for the DUNE experiment, are also being submitted to\nother streams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The international collaboration designing and constructing the Deep\nUnderground Neutrino Experiment (DUNE) at the Long-Baseline Neutrino Facility\n(LBNF) has developed a two-phase strategy toward the implementation of this\nleading-edge, large-scale science project. The ambitious physics program of\nPhase I and Phase II of DUNE is dependent upon deployment and utilization of\nsignificant computing resources, and successful research and development of\nsoftware (both infrastructure and algorithmic) in order to achieve these\nscientific goals. This submission discusses the computing resources\nprojections, infrastructure support, and software development needed for DUNE\nduring the coming decades as an input to the European Strategy for Particle\nPhysics Update for 2026. The DUNE collaboration is submitting four main\ncontributions to the 2026 Update of the European Strategy for Particle Physics\nprocess. This submission to the 'Computing' stream focuses on DUNE software and\ncomputing. Additional inputs related to the DUNE science program, DUNE detector\ntechnologies and R&D, and European contributions to Fermilab accelerator\nupgrades and facilities for the DUNE experiment, are also being submitted to\nother streams."
                },
                "authors": [
                    {
                        "name": "DUNE Collaboration"
                    },
                    {
                        "name": "A. Abed Abud"
                    },
                    {
                        "name": "R. Acciarri"
                    },
                    {
                        "name": "M. A. Acero"
                    },
                    {
                        "name": "M. R. Adames"
                    },
                    {
                        "name": "G. Adamov"
                    },
                    {
                        "name": "M. Adamowski"
                    },
                    {
                        "name": "D. Adams"
                    },
                    {
                        "name": "M. Adinolfi"
                    },
                    {
                        "name": "C. Adriano"
                    },
                    {
                        "name": "A. Aduszkiewicz"
                    },
                    {
                        "name": "J. Aguilar"
                    },
                    {
                        "name": "F. Akbar"
                    },
                    {
                        "name": "F. Alemanno"
                    },
                    {
                        "name": "N. S. Alex"
                    },
                    {
                        "name": "K. Allison"
                    },
                    {
                        "name": "M. Alrashed"
                    },
                    {
                        "name": "A. Alton"
                    },
                    {
                        "name": "R. Alvarez"
                    },
                    {
                        "name": "T. Alves"
                    },
                    {
                        "name": "A. Aman"
                    },
                    {
                        "name": "H. Amar"
                    },
                    {
                        "name": "P. Amedo"
                    },
                    {
                        "name": "J. Anderson"
                    },
                    {
                        "name": "D. A. Andrade"
                    },
                    {
                        "name": "C. Andreopoulos"
                    },
                    {
                        "name": "M. Andreotti"
                    },
                    {
                        "name": "M. P. Andrews"
                    },
                    {
                        "name": "F. Andrianala"
                    },
                    {
                        "name": "S. Andringa"
                    },
                    {
                        "name": "F. Anjarazafy"
                    },
                    {
                        "name": "D. Antic"
                    },
                    {
                        "name": "M. Antoniassi"
                    },
                    {
                        "name": "M. Antonova"
                    },
                    {
                        "name": "A. Aranda-Fernandez"
                    },
                    {
                        "name": "L. Arellano"
                    },
                    {
                        "name": "E. Arrieta Diaz"
                    },
                    {
                        "name": "M. A. Arroyave"
                    },
                    {
                        "name": "J. Asaadi"
                    },
                    {
                        "name": "M. Ascencio"
                    },
                    {
                        "name": "A. Ashkenazi"
                    },
                    {
                        "name": "D. Asner"
                    },
                    {
                        "name": "L. Asquith"
                    },
                    {
                        "name": "E. Atkin"
                    },
                    {
                        "name": "D. Auguste"
                    },
                    {
                        "name": "A. Aurisano"
                    },
                    {
                        "name": "V. Aushev"
                    },
                    {
                        "name": "D. Autiero"
                    },
                    {
                        "name": "D. vila Gmez"
                    },
                    {
                        "name": "M. B. Azam"
                    },
                    {
                        "name": "F. Azfar"
                    },
                    {
                        "name": "A. Back"
                    },
                    {
                        "name": "H. Back"
                    },
                    {
                        "name": "J. J. Back"
                    },
                    {
                        "name": "I. Bagaturia"
                    },
                    {
                        "name": "L. Bagby"
                    },
                    {
                        "name": "D. Baigarashev"
                    },
                    {
                        "name": "S. Balasubramanian"
                    },
                    {
                        "name": "A. Balboni"
                    },
                    {
                        "name": "P. Baldi"
                    },
                    {
                        "name": "W. Baldini"
                    },
                    {
                        "name": "J. Baldonedo"
                    },
                    {
                        "name": "B. Baller"
                    },
                    {
                        "name": "B. Bambah"
                    },
                    {
                        "name": "R. Banerjee"
                    },
                    {
                        "name": "F. Barao"
                    },
                    {
                        "name": "D. Barbu"
                    },
                    {
                        "name": "G. Barenboim"
                    },
                    {
                        "name": "P. \\ Barham Alzs"
                    },
                    {
                        "name": "G. J. Barker"
                    },
                    {
                        "name": "W. Barkhouse"
                    },
                    {
                        "name": "G. Barr"
                    },
                    {
                        "name": "J. Barranco Monarca"
                    },
                    {
                        "name": "A. Barros"
                    },
                    {
                        "name": "N. Barros"
                    },
                    {
                        "name": "D. Barrow"
                    },
                    {
                        "name": "J. L. Barrow"
                    },
                    {
                        "name": "A. Basharina-Freshville"
                    },
                    {
                        "name": "A. Bashyal"
                    },
                    {
                        "name": "V. Basque"
                    },
                    {
                        "name": "D. Basu"
                    },
                    {
                        "name": "C. Batchelor"
                    },
                    {
                        "name": "L. Bathe-Peters"
                    },
                    {
                        "name": "J. B. R. Battat"
                    },
                    {
                        "name": "F. Battisti"
                    },
                    {
                        "name": "F. Bay"
                    },
                    {
                        "name": "M. C. Q. Bazetto"
                    },
                    {
                        "name": "J. L. L. Bazo Alba"
                    },
                    {
                        "name": "J. F. Beacom"
                    },
                    {
                        "name": "E. Bechetoille"
                    },
                    {
                        "name": "B. Behera"
                    },
                    {
                        "name": "E. Belchior"
                    },
                    {
                        "name": "B. Bell"
                    },
                    {
                        "name": "G. Bell"
                    },
                    {
                        "name": "L. Bellantoni"
                    },
                    {
                        "name": "G. Bellettini"
                    },
                    {
                        "name": "V. Bellini"
                    },
                    {
                        "name": "O. Beltramello"
                    },
                    {
                        "name": "C. Benitez Montiel"
                    },
                    {
                        "name": "D. Benjamin"
                    },
                    {
                        "name": "F. Bento Neves"
                    },
                    {
                        "name": "J. Berger"
                    },
                    {
                        "name": "S. Berkman"
                    },
                    {
                        "name": "J. Bernal"
                    },
                    {
                        "name": "P. Bernardini"
                    },
                    {
                        "name": "A. Bersani"
                    },
                    {
                        "name": "E. Bertolini"
                    },
                    {
                        "name": "S. Bertolucci"
                    },
                    {
                        "name": "M. Betancourt"
                    },
                    {
                        "name": "A. Betancur Rodrguez"
                    },
                    {
                        "name": "Y. Bezawada"
                    },
                    {
                        "name": "A. T. Bezerra"
                    },
                    {
                        "name": "A. Bhat"
                    },
                    {
                        "name": "V. Bhatnagar"
                    },
                    {
                        "name": "J. Bhatt"
                    },
                    {
                        "name": "M. Bhattacharjee"
                    },
                    {
                        "name": "M. Bhattacharya"
                    },
                    {
                        "name": "S. Bhuller"
                    },
                    {
                        "name": "B. Bhuyan"
                    },
                    {
                        "name": "S. Biagi"
                    },
                    {
                        "name": "J. Bian"
                    },
                    {
                        "name": "K. Biery"
                    },
                    {
                        "name": "B. Bilki"
                    },
                    {
                        "name": "M. Bishai"
                    },
                    {
                        "name": "A. Blake"
                    },
                    {
                        "name": "F. D. Blaszczyk"
                    },
                    {
                        "name": "G. C. Blazey"
                    },
                    {
                        "name": "E. Blucher"
                    },
                    {
                        "name": "B. Bogart"
                    },
                    {
                        "name": "J. Bogenschuetz"
                    },
                    {
                        "name": "J. Boissevain"
                    },
                    {
                        "name": "S. Bolognesi"
                    },
                    {
                        "name": "T. Bolton"
                    },
                    {
                        "name": "L. Bomben"
                    },
                    {
                        "name": "M. Bonesini"
                    },
                    {
                        "name": "C. Bonilla-Diaz"
                    },
                    {
                        "name": "A. Booth"
                    },
                    {
                        "name": "F. Boran"
                    },
                    {
                        "name": "R. Borges Merlo"
                    },
                    {
                        "name": "N. Bostan"
                    },
                    {
                        "name": "G. Botogoske"
                    },
                    {
                        "name": "B. Bottino"
                    },
                    {
                        "name": "R. Bouet"
                    },
                    {
                        "name": "J. Boza"
                    },
                    {
                        "name": "J. Bracinik"
                    },
                    {
                        "name": "B. Brahma"
                    },
                    {
                        "name": "D. Brailsford"
                    },
                    {
                        "name": "F. Bramati"
                    },
                    {
                        "name": "A. Branca"
                    },
                    {
                        "name": "A. Brandt"
                    },
                    {
                        "name": "J. Bremer"
                    },
                    {
                        "name": "S. J. Brice"
                    },
                    {
                        "name": "V. Brio"
                    },
                    {
                        "name": "C. Brizzolari"
                    },
                    {
                        "name": "C. Bromberg"
                    },
                    {
                        "name": "J. Brooke"
                    },
                    {
                        "name": "A. Bross"
                    },
                    {
                        "name": "G. Brunetti"
                    },
                    {
                        "name": "M. B. Brunetti"
                    },
                    {
                        "name": "N. Buchanan"
                    },
                    {
                        "name": "H. Budd"
                    },
                    {
                        "name": "J. Buergi"
                    },
                    {
                        "name": "A. Bundock"
                    },
                    {
                        "name": "D. Burgardt"
                    },
                    {
                        "name": "S. Butchart"
                    },
                    {
                        "name": "G. Caceres V."
                    },
                    {
                        "name": "T. Cai"
                    },
                    {
                        "name": "R. Calabrese"
                    },
                    {
                        "name": "R. Calabrese"
                    },
                    {
                        "name": "J. Calcutt"
                    },
                    {
                        "name": "L. Calivers"
                    },
                    {
                        "name": "E. Calvo"
                    },
                    {
                        "name": "A. Caminata"
                    },
                    {
                        "name": "A. F. Camino"
                    },
                    {
                        "name": "W. Campanelli"
                    },
                    {
                        "name": "A. Campani"
                    },
                    {
                        "name": "A. Campos Benitez"
                    },
                    {
                        "name": "N. Canci"
                    },
                    {
                        "name": "J. Cap"
                    },
                    {
                        "name": "I. Caracas"
                    },
                    {
                        "name": "D. Caratelli"
                    },
                    {
                        "name": "D. Carber"
                    },
                    {
                        "name": "J. M. Carceller"
                    },
                    {
                        "name": "G. Carini"
                    },
                    {
                        "name": "B. Carlus"
                    },
                    {
                        "name": "M. F. Carneiro"
                    },
                    {
                        "name": "P. Carniti"
                    },
                    {
                        "name": "I. Caro Terrazas"
                    },
                    {
                        "name": "H. Carranza"
                    },
                    {
                        "name": "N. Carrara"
                    },
                    {
                        "name": "L. Carroll"
                    },
                    {
                        "name": "T. Carroll"
                    },
                    {
                        "name": "A. Carter"
                    },
                    {
                        "name": "E. Casarejos"
                    },
                    {
                        "name": "D. Casazza"
                    },
                    {
                        "name": "J. F. Castao Forero"
                    },
                    {
                        "name": "F. A. Castao"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "C. Castromonte"
                    },
                    {
                        "name": "E. Catano-Mur"
                    },
                    {
                        "name": "C. Cattadori"
                    },
                    {
                        "name": "F. Cavalier"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "S. Centro"
                    },
                    {
                        "name": "G. Cerati"
                    },
                    {
                        "name": "C. Cerna"
                    },
                    {
                        "name": "A. Cervelli"
                    },
                    {
                        "name": "A. Cervera Villanueva"
                    },
                    {
                        "name": "J. Chakrani"
                    },
                    {
                        "name": "M. Chalifour"
                    },
                    {
                        "name": "A. Chappell"
                    },
                    {
                        "name": "A. Chatterjee"
                    },
                    {
                        "name": "B. Chauhan"
                    },
                    {
                        "name": "C. Chavez Barajas"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "M. Chen"
                    },
                    {
                        "name": "W. C. Chen"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "Z. Chen"
                    },
                    {
                        "name": "D. Cherdack"
                    },
                    {
                        "name": "S. S. Chhibra"
                    },
                    {
                        "name": "C. Chi"
                    },
                    {
                        "name": "F. Chiapponi"
                    },
                    {
                        "name": "R. Chirco"
                    },
                    {
                        "name": "N. Chitirasreemadam"
                    },
                    {
                        "name": "K. Cho"
                    },
                    {
                        "name": "S. Choate"
                    },
                    {
                        "name": "G. Choi"
                    },
                    {
                        "name": "D. Chokheli"
                    },
                    {
                        "name": "P. S. Chong"
                    },
                    {
                        "name": "B. Chowdhury"
                    },
                    {
                        "name": "D. Christian"
                    },
                    {
                        "name": "M. Chung"
                    },
                    {
                        "name": "E. Church"
                    },
                    {
                        "name": "M. F. Cicala"
                    },
                    {
                        "name": "M. Cicerchia"
                    },
                    {
                        "name": "V. Cicero"
                    },
                    {
                        "name": "R. Ciolini"
                    },
                    {
                        "name": "P. Clarke"
                    },
                    {
                        "name": "G. Cline"
                    },
                    {
                        "name": "A. G. Cocco"
                    },
                    {
                        "name": "J. A. B. Coelho"
                    },
                    {
                        "name": "A. Cohen"
                    },
                    {
                        "name": "J. Collazo"
                    },
                    {
                        "name": "J. Collot"
                    },
                    {
                        "name": "J. M. Conrad"
                    },
                    {
                        "name": "M. Convery"
                    },
                    {
                        "name": "K. Conway"
                    },
                    {
                        "name": "S. Copello"
                    },
                    {
                        "name": "P. Cova"
                    },
                    {
                        "name": "C. Cox"
                    },
                    {
                        "name": "L. Cremonesi"
                    },
                    {
                        "name": "J. I. Crespo-Anadn"
                    },
                    {
                        "name": "M. Crisler"
                    },
                    {
                        "name": "E. Cristaldo"
                    },
                    {
                        "name": "J. Crnkovic"
                    },
                    {
                        "name": "G. Crone"
                    },
                    {
                        "name": "R. Cross"
                    },
                    {
                        "name": "A. Cudd"
                    },
                    {
                        "name": "C. Cuesta"
                    },
                    {
                        "name": "Y. Cui"
                    },
                    {
                        "name": "F. Curciarello"
                    },
                    {
                        "name": "D. Cussans"
                    },
                    {
                        "name": "J. Dai"
                    },
                    {
                        "name": "O. Dalager"
                    },
                    {
                        "name": "W. Dallaway"
                    },
                    {
                        "name": "R. D'Amico"
                    },
                    {
                        "name": "H. da Motta"
                    },
                    {
                        "name": "Z. A. Dar"
                    },
                    {
                        "name": "R. Darby"
                    },
                    {
                        "name": "L. Da Silva Peres"
                    },
                    {
                        "name": "Q. David"
                    },
                    {
                        "name": "G. S. Davies"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "J. Dawson"
                    },
                    {
                        "name": "R. De Aguiar"
                    },
                    {
                        "name": "P. De Almeida"
                    },
                    {
                        "name": "P. Debbins"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "A. de Gouva"
                    },
                    {
                        "name": "P. C. De Holanda"
                    },
                    {
                        "name": "P. De Jong"
                    },
                    {
                        "name": "P. Del Amo Sanchez"
                    },
                    {
                        "name": "G. De Lauretis"
                    },
                    {
                        "name": "A. Delbart"
                    },
                    {
                        "name": "D. Delepine"
                    },
                    {
                        "name": "M. Delgado"
                    },
                    {
                        "name": "A. Dell'Acqua"
                    },
                    {
                        "name": "G. Delle Monache"
                    },
                    {
                        "name": "N. Delmonte"
                    },
                    {
                        "name": "P. De Lurgio"
                    },
                    {
                        "name": "R. Demario"
                    },
                    {
                        "name": "G. De Matteis"
                    },
                    {
                        "name": "J. R. T. de Mello Neto"
                    },
                    {
                        "name": "A. P. A. De Mendonca"
                    },
                    {
                        "name": "D. M. DeMuth"
                    },
                    {
                        "name": "S. Dennis"
                    },
                    {
                        "name": "C. Densham"
                    },
                    {
                        "name": "P. Denton"
                    },
                    {
                        "name": "G. W. Deptuch"
                    },
                    {
                        "name": "A. De Roeck"
                    },
                    {
                        "name": "V. De Romeri"
                    },
                    {
                        "name": "J. P. Detje"
                    },
                    {
                        "name": "J. Devine"
                    },
                    {
                        "name": "R. Dharmapalan"
                    },
                    {
                        "name": "M. Dias"
                    },
                    {
                        "name": "A. Diaz"
                    },
                    {
                        "name": "J. S. Daz"
                    },
                    {
                        "name": "F. Daz"
                    },
                    {
                        "name": "F. Di Capua"
                    },
                    {
                        "name": "A. Di Domenico"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "S. Di Falco"
                    },
                    {
                        "name": "L. Di Giulio"
                    },
                    {
                        "name": "P. Ding"
                    },
                    {
                        "name": "L. Di Noto"
                    },
                    {
                        "name": "E. Diociaiuti"
                    },
                    {
                        "name": "V. Di Silvestre"
                    },
                    {
                        "name": "C. Distefano"
                    },
                    {
                        "name": "R. Diurba"
                    },
                    {
                        "name": "M. Diwan"
                    },
                    {
                        "name": "Z. Djurcic"
                    },
                    {
                        "name": "S. Dolan"
                    },
                    {
                        "name": "M. Dolce"
                    },
                    {
                        "name": "F. Dolek"
                    },
                    {
                        "name": "M. J. Dolinski"
                    },
                    {
                        "name": "D. Domenici"
                    },
                    {
                        "name": "S. Donati"
                    },
                    {
                        "name": "Y. Donon"
                    },
                    {
                        "name": "S. Doran"
                    },
                    {
                        "name": "D. Douglas"
                    },
                    {
                        "name": "T. A. Doyle"
                    },
                    {
                        "name": "F. Drielsma"
                    },
                    {
                        "name": "L. Duarte"
                    },
                    {
                        "name": "D. Duchesneau"
                    },
                    {
                        "name": "K. Duffy"
                    },
                    {
                        "name": "K. Dugas"
                    },
                    {
                        "name": "P. Dunne"
                    },
                    {
                        "name": "B. Dutta"
                    },
                    {
                        "name": "H. Duyang"
                    },
                    {
                        "name": "D. A. Dwyer"
                    },
                    {
                        "name": "A. S. Dyshkant"
                    },
                    {
                        "name": "S. Dytman"
                    },
                    {
                        "name": "M. Eads"
                    },
                    {
                        "name": "A. Earle"
                    },
                    {
                        "name": "S. Edayath"
                    },
                    {
                        "name": "D. Edmunds"
                    },
                    {
                        "name": "J. Eisch"
                    },
                    {
                        "name": "W. Emark"
                    },
                    {
                        "name": "P. Englezos"
                    },
                    {
                        "name": "A. Ereditato"
                    },
                    {
                        "name": "T. Erjavec"
                    },
                    {
                        "name": "C. O. Escobar"
                    },
                    {
                        "name": "J. J. Evans"
                    },
                    {
                        "name": "E. Ewart"
                    },
                    {
                        "name": "A. C. Ezeribe"
                    },
                    {
                        "name": "K. Fahey"
                    },
                    {
                        "name": "A. Falcone"
                    },
                    {
                        "name": "M. Fani'"
                    },
                    {
                        "name": "C. Farnese"
                    },
                    {
                        "name": "S. Farrell"
                    },
                    {
                        "name": "Y. Farzan"
                    },
                    {
                        "name": "J. Felix"
                    },
                    {
                        "name": "Y. Feng"
                    },
                    {
                        "name": "M. Ferreira da Silva"
                    },
                    {
                        "name": "G. Ferry"
                    },
                    {
                        "name": "E. Fialova"
                    },
                    {
                        "name": "L. Fields"
                    },
                    {
                        "name": "P. Filip"
                    },
                    {
                        "name": "A. Filkins"
                    },
                    {
                        "name": "F. Filthaut"
                    },
                    {
                        "name": "G. Fiorillo"
                    },
                    {
                        "name": "M. Fiorini"
                    },
                    {
                        "name": "S. Fogarty"
                    },
                    {
                        "name": "W. Foreman"
                    },
                    {
                        "name": "J. Fowler"
                    },
                    {
                        "name": "J. Franc"
                    },
                    {
                        "name": "K. Francis"
                    },
                    {
                        "name": "D. Franco"
                    },
                    {
                        "name": "J. Freeman"
                    },
                    {
                        "name": "J. Fried"
                    },
                    {
                        "name": "A. Friedland"
                    },
                    {
                        "name": "M. Fucci"
                    },
                    {
                        "name": "S. Fuess"
                    },
                    {
                        "name": "I. K. Furic"
                    },
                    {
                        "name": "K. Furman"
                    },
                    {
                        "name": "A. P. Furmanski"
                    },
                    {
                        "name": "R. Gaba"
                    },
                    {
                        "name": "A. Gabrielli"
                    },
                    {
                        "name": "A. M. Gago"
                    },
                    {
                        "name": "F. Galizzi"
                    },
                    {
                        "name": "H. Gallagher"
                    },
                    {
                        "name": "M. Galli"
                    },
                    {
                        "name": "N. Gallice"
                    },
                    {
                        "name": "V. Galymov"
                    },
                    {
                        "name": "E. Gamberini"
                    },
                    {
                        "name": "T. Gamble"
                    },
                    {
                        "name": "R. Gandhi"
                    },
                    {
                        "name": "S. Ganguly"
                    },
                    {
                        "name": "F. Gao"
                    },
                    {
                        "name": "S. Gao"
                    },
                    {
                        "name": "D. Garcia-Gamez"
                    },
                    {
                        "name": "M. . Garca-Peris"
                    },
                    {
                        "name": "F. Gardim"
                    },
                    {
                        "name": "S. Gardiner"
                    },
                    {
                        "name": "D. Gastler"
                    },
                    {
                        "name": "A. Gauch"
                    },
                    {
                        "name": "P. Gauzzi"
                    },
                    {
                        "name": "S. Gazzana"
                    },
                    {
                        "name": "G. Ge"
                    },
                    {
                        "name": "N. Geffroy"
                    },
                    {
                        "name": "B. Gelli"
                    },
                    {
                        "name": "S. Gent"
                    },
                    {
                        "name": "L. Gerlach"
                    },
                    {
                        "name": "A. Ghosh"
                    },
                    {
                        "name": "T. Giammaria"
                    },
                    {
                        "name": "D. Gibin"
                    },
                    {
                        "name": "I. Gil-Botella"
                    },
                    {
                        "name": "S. Gilligan"
                    },
                    {
                        "name": "A. Gioiosa"
                    },
                    {
                        "name": "S. Giovannella"
                    },
                    {
                        "name": "A. K. Giri"
                    },
                    {
                        "name": "C. Giugliano"
                    },
                    {
                        "name": "V. Giusti"
                    },
                    {
                        "name": "D. Gnani"
                    },
                    {
                        "name": "O. Gogota"
                    },
                    {
                        "name": "S. Gollapinni"
                    },
                    {
                        "name": "K. Gollwitzer"
                    },
                    {
                        "name": "R. A. Gomes"
                    },
                    {
                        "name": "L. V. Gomez Bermeo"
                    },
                    {
                        "name": "L. S. Gomez Fajardo"
                    },
                    {
                        "name": "D. Gonzalez-Diaz"
                    },
                    {
                        "name": "M. C. Goodman"
                    },
                    {
                        "name": "S. Goswami"
                    },
                    {
                        "name": "C. Gotti"
                    },
                    {
                        "name": "J. Goudeau"
                    },
                    {
                        "name": "E. Goudzovski"
                    },
                    {
                        "name": "C. Grace"
                    },
                    {
                        "name": "E. Gramellini"
                    },
                    {
                        "name": "R. Gran"
                    },
                    {
                        "name": "E. Granados"
                    },
                    {
                        "name": "P. Granger"
                    },
                    {
                        "name": "C. Grant"
                    },
                    {
                        "name": "D. R. Gratieri"
                    },
                    {
                        "name": "G. Grauso"
                    },
                    {
                        "name": "P. Green"
                    },
                    {
                        "name": "S. Greenberg"
                    },
                    {
                        "name": "J. Greer"
                    },
                    {
                        "name": "W. C. Griffith"
                    },
                    {
                        "name": "K. Grzelak"
                    },
                    {
                        "name": "L. Gu"
                    },
                    {
                        "name": "W. Gu"
                    },
                    {
                        "name": "V. Guarino"
                    },
                    {
                        "name": "M. Guarise"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "M. Guerzoni"
                    },
                    {
                        "name": "D. Guffanti"
                    },
                    {
                        "name": "A. Guglielmi"
                    },
                    {
                        "name": "B. Guo"
                    },
                    {
                        "name": "F. Y. Guo"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "G. Gurung"
                    },
                    {
                        "name": "D. Gutierrez"
                    },
                    {
                        "name": "P. Guzowski"
                    },
                    {
                        "name": "M. M. Guzzo"
                    },
                    {
                        "name": "S. Gwon"
                    },
                    {
                        "name": "A. Habig"
                    },
                    {
                        "name": "L. Haegel"
                    },
                    {
                        "name": "R. Hafeji"
                    },
                    {
                        "name": "L. Hagaman"
                    },
                    {
                        "name": "A. Hahn"
                    },
                    {
                        "name": "J. Hakenmller"
                    },
                    {
                        "name": "T. Hamernik"
                    },
                    {
                        "name": "P. Hamilton"
                    },
                    {
                        "name": "J. Hancock"
                    },
                    {
                        "name": "M. Handley"
                    },
                    {
                        "name": "F. Happacher"
                    },
                    {
                        "name": "D. A. Harris"
                    },
                    {
                        "name": "A. L. Hart"
                    },
                    {
                        "name": "J. Hartnell"
                    },
                    {
                        "name": "T. Hartnett"
                    },
                    {
                        "name": "J. Harton"
                    },
                    {
                        "name": "T. Hasegawa"
                    },
                    {
                        "name": "C. M. Hasnip"
                    },
                    {
                        "name": "R. Hatcher"
                    },
                    {
                        "name": "S. Hawkins"
                    },
                    {
                        "name": "J. Hays"
                    },
                    {
                        "name": "M. He"
                    },
                    {
                        "name": "A. Heavey"
                    },
                    {
                        "name": "K. M. Heeger"
                    },
                    {
                        "name": "A. Heindel"
                    },
                    {
                        "name": "J. Heise"
                    },
                    {
                        "name": "P. Hellmuth"
                    },
                    {
                        "name": "L. Henderson"
                    },
                    {
                        "name": "K. Herner"
                    },
                    {
                        "name": "V. Hewes"
                    },
                    {
                        "name": "A. Higuera"
                    },
                    {
                        "name": "C. Hilgenberg"
                    },
                    {
                        "name": "A. Himmel"
                    },
                    {
                        "name": "E. Hinkle"
                    },
                    {
                        "name": "L. R. Hirsch"
                    },
                    {
                        "name": "J. Ho"
                    },
                    {
                        "name": "J. Hoefken Zink"
                    },
                    {
                        "name": "J. Hoff"
                    },
                    {
                        "name": "A. Holin"
                    },
                    {
                        "name": "T. Holvey"
                    },
                    {
                        "name": "C. Hong"
                    },
                    {
                        "name": "E. Hoppe"
                    },
                    {
                        "name": "S. Horiuchi"
                    },
                    {
                        "name": "G. A. Horton-Smith"
                    },
                    {
                        "name": "R. Hosokawa"
                    },
                    {
                        "name": "T. Houdy"
                    },
                    {
                        "name": "B. Howard"
                    },
                    {
                        "name": "R. Howell"
                    },
                    {
                        "name": "I. Hristova"
                    },
                    {
                        "name": "M. S. Hronek"
                    },
                    {
                        "name": "H. Hua"
                    },
                    {
                        "name": "J. Huang"
                    },
                    {
                        "name": "R. G. Huang"
                    },
                    {
                        "name": "X. Huang"
                    },
                    {
                        "name": "Z. Hulcher"
                    },
                    {
                        "name": "G. Iles"
                    },
                    {
                        "name": "N. Ilic"
                    },
                    {
                        "name": "A. M. Iliescu"
                    },
                    {
                        "name": "R. Illingworth"
                    },
                    {
                        "name": "G. Ingratta"
                    },
                    {
                        "name": "A. Ioannisian"
                    },
                    {
                        "name": "B. Irwin"
                    },
                    {
                        "name": "M. Ismerio Oliveira"
                    },
                    {
                        "name": "C. M. Jackson"
                    },
                    {
                        "name": "V. Jain"
                    },
                    {
                        "name": "E. James"
                    },
                    {
                        "name": "W. Jang"
                    },
                    {
                        "name": "B. Jargowsky"
                    },
                    {
                        "name": "D. Jena"
                    },
                    {
                        "name": "I. Jentz"
                    },
                    {
                        "name": "X. Ji"
                    },
                    {
                        "name": "C. Jiang"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "A. Jipa"
                    },
                    {
                        "name": "J. H. Jo"
                    },
                    {
                        "name": "F. R. Joaquim"
                    },
                    {
                        "name": "W. Johnson"
                    },
                    {
                        "name": "C. Jollet"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "N. Jovancevic"
                    },
                    {
                        "name": "M. Judah"
                    },
                    {
                        "name": "C. K. Jung"
                    },
                    {
                        "name": "K. Y. Jung"
                    },
                    {
                        "name": "T. Junk"
                    },
                    {
                        "name": "Y. Jwa"
                    },
                    {
                        "name": "M. Kabirnezhad"
                    },
                    {
                        "name": "A. C. Kaboth"
                    },
                    {
                        "name": "I. Kadenko"
                    },
                    {
                        "name": "O. Kalikulov"
                    },
                    {
                        "name": "D. Kalra"
                    },
                    {
                        "name": "M. Kandemir"
                    },
                    {
                        "name": "D. M. Kaplan"
                    },
                    {
                        "name": "G. Karagiorgi"
                    },
                    {
                        "name": "G. Karaman"
                    },
                    {
                        "name": "A. Karcher"
                    },
                    {
                        "name": "Y. Karyotakis"
                    },
                    {
                        "name": "S. P. Kasetti"
                    },
                    {
                        "name": "L. Kashur"
                    },
                    {
                        "name": "A. Kauther"
                    },
                    {
                        "name": "N. Kazaryan"
                    },
                    {
                        "name": "L. Ke"
                    },
                    {
                        "name": "E. Kearns"
                    },
                    {
                        "name": "P. T. Keener"
                    },
                    {
                        "name": "K. J. Kelly"
                    },
                    {
                        "name": "R. Keloth"
                    },
                    {
                        "name": "E. Kemp"
                    },
                    {
                        "name": "O. Kemularia"
                    },
                    {
                        "name": "Y. Kermaidic"
                    },
                    {
                        "name": "W. Ketchum"
                    },
                    {
                        "name": "S. H. Kettell"
                    },
                    {
                        "name": "N. Khan"
                    },
                    {
                        "name": "A. Khvedelidze"
                    },
                    {
                        "name": "D. Kim"
                    },
                    {
                        "name": "J. Kim"
                    },
                    {
                        "name": "M. J. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "B. King"
                    },
                    {
                        "name": "M. King"
                    },
                    {
                        "name": "M. Kirby"
                    },
                    {
                        "name": "A. Kish"
                    },
                    {
                        "name": "J. Klein"
                    },
                    {
                        "name": "J. Kleykamp"
                    },
                    {
                        "name": "A. Klustova"
                    },
                    {
                        "name": "T. Kobilarcik"
                    },
                    {
                        "name": "L. Koch"
                    },
                    {
                        "name": "K. Koehler"
                    },
                    {
                        "name": "L. W. Koerner"
                    },
                    {
                        "name": "D. H. Koh"
                    },
                    {
                        "name": "M. Kordosky"
                    },
                    {
                        "name": "T. Kosc"
                    },
                    {
                        "name": "V. A. Kosteleck"
                    },
                    {
                        "name": "K. Kothekar"
                    },
                    {
                        "name": "I. Kotler"
                    },
                    {
                        "name": "M. Kovalcuk"
                    },
                    {
                        "name": "R. Kralik"
                    },
                    {
                        "name": "M. Kramer"
                    },
                    {
                        "name": "L. Kreczko"
                    },
                    {
                        "name": "F. Krennrich"
                    },
                    {
                        "name": "T. Kroupova"
                    },
                    {
                        "name": "S. Kubota"
                    },
                    {
                        "name": "M. Kubu"
                    },
                    {
                        "name": "V. A. Kudryavtsev"
                    },
                    {
                        "name": "G. Kufatty"
                    },
                    {
                        "name": "S. Kuhlmann"
                    },
                    {
                        "name": "J. Kumar"
                    },
                    {
                        "name": "M. Kumar"
                    },
                    {
                        "name": "P. Kumar"
                    },
                    {
                        "name": "P. Kumar"
                    },
                    {
                        "name": "S. Kumaran"
                    },
                    {
                        "name": "J. Kunzmann"
                    },
                    {
                        "name": "R. Kuravi"
                    },
                    {
                        "name": "V. Kus"
                    },
                    {
                        "name": "T. Kutter"
                    },
                    {
                        "name": "J. Kvasnicka"
                    },
                    {
                        "name": "T. Labree"
                    },
                    {
                        "name": "T. Lackey"
                    },
                    {
                        "name": "I. Lalu"
                    },
                    {
                        "name": "A. Lambert"
                    },
                    {
                        "name": "B. J. Land"
                    },
                    {
                        "name": "C. E. Lane"
                    },
                    {
                        "name": "N. Lane"
                    },
                    {
                        "name": "K. Lang"
                    },
                    {
                        "name": "T. Langford"
                    },
                    {
                        "name": "M. Langstaff"
                    },
                    {
                        "name": "F. Lanni"
                    },
                    {
                        "name": "J. Larkin"
                    },
                    {
                        "name": "P. Lasorak"
                    },
                    {
                        "name": "D. Last"
                    },
                    {
                        "name": "A. Laundrie"
                    },
                    {
                        "name": "G. Laurenti"
                    },
                    {
                        "name": "E. Lavaut"
                    },
                    {
                        "name": "P. Laycock"
                    },
                    {
                        "name": "I. Lazanu"
                    },
                    {
                        "name": "R. LaZur"
                    },
                    {
                        "name": "M. Lazzaroni"
                    },
                    {
                        "name": "T. Le"
                    },
                    {
                        "name": "S. Leardini"
                    },
                    {
                        "name": "J. Learned"
                    },
                    {
                        "name": "T. LeCompte"
                    },
                    {
                        "name": "G. Lehmann Miotto"
                    },
                    {
                        "name": "R. Lehnert"
                    },
                    {
                        "name": "M. Leitner"
                    },
                    {
                        "name": "H. Lemoine"
                    },
                    {
                        "name": "D. Leon Silverio"
                    },
                    {
                        "name": "L. M. Lepin"
                    },
                    {
                        "name": "J. -Y Li"
                    },
                    {
                        "name": "S. W. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "H. Liao"
                    },
                    {
                        "name": "R. Lima"
                    },
                    {
                        "name": "C. S. Lin"
                    },
                    {
                        "name": "D. Lindebaum"
                    },
                    {
                        "name": "S. Linden"
                    },
                    {
                        "name": "R. A. Lineros"
                    },
                    {
                        "name": "A. Lister"
                    },
                    {
                        "name": "B. R. Littlejohn"
                    },
                    {
                        "name": "H. Liu"
                    },
                    {
                        "name": "J. Liu"
                    },
                    {
                        "name": "Y. Liu"
                    },
                    {
                        "name": "S. Lockwitz"
                    },
                    {
                        "name": "I. Lomidze"
                    },
                    {
                        "name": "K. Long"
                    },
                    {
                        "name": "T. V. Lopes"
                    },
                    {
                        "name": "J. Lopez"
                    },
                    {
                        "name": "I. Lpez de Rego"
                    },
                    {
                        "name": "N. Lpez-March"
                    },
                    {
                        "name": "J. M. LoSecco"
                    },
                    {
                        "name": "W. C. Louis"
                    },
                    {
                        "name": "A. Lozano Sanchez"
                    },
                    {
                        "name": "X. -G. Lu"
                    },
                    {
                        "name": "K. B. Luk"
                    },
                    {
                        "name": "X. Luo"
                    },
                    {
                        "name": "E. Luppi"
                    },
                    {
                        "name": "A. A. Machado"
                    },
                    {
                        "name": "P. Machado"
                    },
                    {
                        "name": "C. T. Macias"
                    },
                    {
                        "name": "J. R. Macier"
                    },
                    {
                        "name": "M. MacMahon"
                    },
                    {
                        "name": "S. Magill"
                    },
                    {
                        "name": "C. Magueur"
                    },
                    {
                        "name": "K. Mahn"
                    },
                    {
                        "name": "A. Maio"
                    },
                    {
                        "name": "A. Major"
                    },
                    {
                        "name": "K. Majumdar"
                    },
                    {
                        "name": "A. Malige"
                    },
                    {
                        "name": "S. Mameli"
                    },
                    {
                        "name": "M. Man"
                    },
                    {
                        "name": "R. C. Mandujano"
                    },
                    {
                        "name": "J. Maneira"
                    },
                    {
                        "name": "S. Manly"
                    },
                    {
                        "name": "A. Mann"
                    },
                    {
                        "name": "K. Manolopoulos"
                    },
                    {
                        "name": "M. Manrique Plata"
                    },
                    {
                        "name": "S. Manthey Corchado"
                    },
                    {
                        "name": "V. N. Manyam"
                    },
                    {
                        "name": "L. Manzanillas-Velez"
                    },
                    {
                        "name": "M. Marchan"
                    },
                    {
                        "name": "A. Marchionni"
                    },
                    {
                        "name": "W. Marciano"
                    },
                    {
                        "name": "D. Marfatia"
                    },
                    {
                        "name": "C. Mariani"
                    },
                    {
                        "name": "J. Maricic"
                    },
                    {
                        "name": "F. Marinho"
                    },
                    {
                        "name": "A. D. Marino"
                    },
                    {
                        "name": "T. Markiewicz"
                    },
                    {
                        "name": "F. Das Chagas Marques"
                    },
                    {
                        "name": "C. Marquet"
                    },
                    {
                        "name": "M. Marshak"
                    },
                    {
                        "name": "C. M. Marshall"
                    },
                    {
                        "name": "J. Marshall"
                    },
                    {
                        "name": "L. Martina"
                    },
                    {
                        "name": "J. Martn-Albo"
                    },
                    {
                        "name": "N. Martinez"
                    },
                    {
                        "name": "D. A. Martinez Caicedo"
                    },
                    {
                        "name": "M. Martinez-Casales"
                    },
                    {
                        "name": "F. Martnez Lpez"
                    },
                    {
                        "name": "P. Martnez Mirav"
                    },
                    {
                        "name": "S. Martynenko"
                    },
                    {
                        "name": "V. Mascagna"
                    },
                    {
                        "name": "A. Mastbaum"
                    },
                    {
                        "name": "M. Masud"
                    },
                    {
                        "name": "F. Matichard"
                    },
                    {
                        "name": "G. Matteucci"
                    },
                    {
                        "name": "J. Matthews"
                    },
                    {
                        "name": "C. Mauger"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "K. Mavrokoridis"
                    },
                    {
                        "name": "I. Mawby"
                    },
                    {
                        "name": "F. Mayhew"
                    },
                    {
                        "name": "R. Mazza"
                    },
                    {
                        "name": "T. McAskill"
                    },
                    {
                        "name": "N. McConkey"
                    },
                    {
                        "name": "K. S. McFarland"
                    },
                    {
                        "name": "C. McGrew"
                    },
                    {
                        "name": "A. McNab"
                    },
                    {
                        "name": "C. McNulty"
                    },
                    {
                        "name": "J. Mead"
                    },
                    {
                        "name": "L. Meazza"
                    },
                    {
                        "name": "V. C. N. Meddage"
                    },
                    {
                        "name": "M. Mehmood"
                    },
                    {
                        "name": "B. Mehta"
                    },
                    {
                        "name": "P. Mehta"
                    },
                    {
                        "name": "F. Mei"
                    },
                    {
                        "name": "P. Melas"
                    },
                    {
                        "name": "L. Mellet"
                    },
                    {
                        "name": "O. Mena"
                    },
                    {
                        "name": "H. Mendez"
                    },
                    {
                        "name": "D. P. Mndez"
                    },
                    {
                        "name": "A. Menegolli"
                    },
                    {
                        "name": "G. Meng"
                    },
                    {
                        "name": "A. C. E. A. Mercuri"
                    },
                    {
                        "name": "A. Meregaglia"
                    },
                    {
                        "name": "M. D. Messier"
                    },
                    {
                        "name": "S. Metallo"
                    },
                    {
                        "name": "W. Metcalf"
                    },
                    {
                        "name": "M. Mewes"
                    },
                    {
                        "name": "H. Meyer"
                    },
                    {
                        "name": "T. Miao"
                    },
                    {
                        "name": "J. Micallef"
                    },
                    {
                        "name": "A. Miccoli"
                    },
                    {
                        "name": "G. Michna"
                    },
                    {
                        "name": "R. Milincic"
                    },
                    {
                        "name": "F. Miller"
                    },
                    {
                        "name": "G. Miller"
                    },
                    {
                        "name": "W. Miller"
                    },
                    {
                        "name": "A. Minotti"
                    },
                    {
                        "name": "L. Miralles Verge"
                    },
                    {
                        "name": "C. Mironov"
                    },
                    {
                        "name": "S. Miryala"
                    },
                    {
                        "name": "S. Miscetti"
                    },
                    {
                        "name": "C. S. Mishra"
                    },
                    {
                        "name": "P. Mishra"
                    },
                    {
                        "name": "S. R. Mishra"
                    },
                    {
                        "name": "A. Mislivec"
                    },
                    {
                        "name": "D. Mladenov"
                    },
                    {
                        "name": "I. Mocioiu"
                    },
                    {
                        "name": "A. Mogan"
                    },
                    {
                        "name": "R. Mohanta"
                    },
                    {
                        "name": "T. A. Mohayai"
                    },
                    {
                        "name": "N. Mokhov"
                    },
                    {
                        "name": "J. Molina"
                    },
                    {
                        "name": "L. Molina Bueno"
                    },
                    {
                        "name": "E. Montagna"
                    },
                    {
                        "name": "A. Montanari"
                    },
                    {
                        "name": "C. Montanari"
                    },
                    {
                        "name": "D. Montanari"
                    },
                    {
                        "name": "D. Montanino"
                    },
                    {
                        "name": "L. M. Montao Zetina"
                    },
                    {
                        "name": "M. Mooney"
                    },
                    {
                        "name": "A. F. Moor"
                    },
                    {
                        "name": "M. Moore"
                    },
                    {
                        "name": "Z. Moore"
                    },
                    {
                        "name": "D. Moreno"
                    },
                    {
                        "name": "G. Moreno-Granados"
                    },
                    {
                        "name": "O. Moreno-Palacios"
                    },
                    {
                        "name": "L. Morescalchi"
                    },
                    {
                        "name": "R. Moretti"
                    },
                    {
                        "name": "C. Morris"
                    },
                    {
                        "name": "C. Mossey"
                    },
                    {
                        "name": "E. Motuk"
                    },
                    {
                        "name": "C. A. Moura"
                    },
                    {
                        "name": "G. Mouster"
                    },
                    {
                        "name": "W. Mu"
                    },
                    {
                        "name": "L. Mualem"
                    },
                    {
                        "name": "J. Mueller"
                    },
                    {
                        "name": "M. Muether"
                    },
                    {
                        "name": "F. Muheim"
                    },
                    {
                        "name": "A. Muir"
                    },
                    {
                        "name": "Y. Mukhamejanov"
                    },
                    {
                        "name": "A. Mukhamejanova"
                    },
                    {
                        "name": "M. Mulhearn"
                    },
                    {
                        "name": "D. Munford"
                    },
                    {
                        "name": "L. J. Munteanu"
                    },
                    {
                        "name": "H. Muramatsu"
                    },
                    {
                        "name": "J. Muraz"
                    },
                    {
                        "name": "M. Murphy"
                    },
                    {
                        "name": "T. Murphy"
                    },
                    {
                        "name": "J. Muse"
                    },
                    {
                        "name": "A. Mytilinaki"
                    },
                    {
                        "name": "J. Nachtman"
                    },
                    {
                        "name": "Y. Nagai"
                    },
                    {
                        "name": "S. Nagu"
                    },
                    {
                        "name": "D. Naples"
                    },
                    {
                        "name": "S. Narita"
                    },
                    {
                        "name": "J. Nava"
                    },
                    {
                        "name": "A. Navrer-Agasson"
                    },
                    {
                        "name": "N. Nayak"
                    },
                    {
                        "name": "M. Nebot-Guinot"
                    },
                    {
                        "name": "A. Nehm"
                    },
                    {
                        "name": "J. K. Nelson"
                    },
                    {
                        "name": "O. Neogi"
                    },
                    {
                        "name": "J. Nesbit"
                    },
                    {
                        "name": "M. Nessi"
                    },
                    {
                        "name": "D. Newbold"
                    },
                    {
                        "name": "M. Newcomer"
                    },
                    {
                        "name": "R. Nichol"
                    },
                    {
                        "name": "F. Nicolas-Arnaldos"
                    },
                    {
                        "name": "A. Nielsen"
                    },
                    {
                        "name": "A. Nikolica"
                    },
                    {
                        "name": "J. Nikolov"
                    },
                    {
                        "name": "E. Niner"
                    },
                    {
                        "name": "X. Ning"
                    },
                    {
                        "name": "K. Nishimura"
                    },
                    {
                        "name": "A. Norman"
                    },
                    {
                        "name": "A. Norrick"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nowak"
                    },
                    {
                        "name": "J. A. Nowak"
                    },
                    {
                        "name": "M. Oberling"
                    },
                    {
                        "name": "J. P. Ochoa-Ricoux"
                    },
                    {
                        "name": "S. Oh"
                    },
                    {
                        "name": "S. B. Oh"
                    },
                    {
                        "name": "A. Olivier"
                    },
                    {
                        "name": "T. Olson"
                    },
                    {
                        "name": "Y. Onel"
                    },
                    {
                        "name": "Y. Onishchuk"
                    },
                    {
                        "name": "A. Oranday"
                    },
                    {
                        "name": "M. Osbiston"
                    },
                    {
                        "name": "J. A. Osorio Vlez"
                    },
                    {
                        "name": "L. O'Sullivan"
                    },
                    {
                        "name": "L. Otiniano Ormachea"
                    },
                    {
                        "name": "L. Pagani"
                    },
                    {
                        "name": "G. Palacio"
                    },
                    {
                        "name": "O. Palamara"
                    },
                    {
                        "name": "S. Palestini"
                    },
                    {
                        "name": "J. M. Paley"
                    },
                    {
                        "name": "M. Pallavicini"
                    },
                    {
                        "name": "C. Palomares"
                    },
                    {
                        "name": "S. Pan"
                    },
                    {
                        "name": "M. Panareo"
                    },
                    {
                        "name": "P. Panda"
                    },
                    {
                        "name": "V. Pandey"
                    },
                    {
                        "name": "W. Panduro Vazquez"
                    },
                    {
                        "name": "E. Pantic"
                    },
                    {
                        "name": "V. Paolone"
                    },
                    {
                        "name": "A. Papadopoulou"
                    },
                    {
                        "name": "R. Papaleo"
                    },
                    {
                        "name": "D. Papoulias"
                    },
                    {
                        "name": "S. Paramesvaran"
                    },
                    {
                        "name": "S. Parke"
                    },
                    {
                        "name": "S. Parsa"
                    },
                    {
                        "name": "Z. Parsa"
                    },
                    {
                        "name": "S. Parveen"
                    },
                    {
                        "name": "M. Parvu"
                    },
                    {
                        "name": "D. Pasciuto"
                    },
                    {
                        "name": "S. Pascoli"
                    },
                    {
                        "name": "L. Pasqualini"
                    },
                    {
                        "name": "J. Pasternak"
                    },
                    {
                        "name": "J. L. Paton"
                    },
                    {
                        "name": "C. Patrick"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "R. B. Patterson"
                    },
                    {
                        "name": "T. Patzak"
                    },
                    {
                        "name": "A. Paudel"
                    },
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "Z. Pavlovic"
                    },
                    {
                        "name": "G. Pawloski"
                    },
                    {
                        "name": "D. Payne"
                    },
                    {
                        "name": "A. Peake"
                    },
                    {
                        "name": "V. Pec"
                    },
                    {
                        "name": "E. Pedreschi"
                    },
                    {
                        "name": "S. J. M. Peeters"
                    },
                    {
                        "name": "W. Pellico"
                    },
                    {
                        "name": "E. Pennacchio"
                    },
                    {
                        "name": "A. Penzo"
                    },
                    {
                        "name": "O. L. G. Peres"
                    },
                    {
                        "name": "L. Prez-Molina"
                    },
                    {
                        "name": "C. Pernas"
                    },
                    {
                        "name": "J. Perry"
                    },
                    {
                        "name": "D. Pershey"
                    },
                    {
                        "name": "G. Pessina"
                    },
                    {
                        "name": "G. Petrillo"
                    },
                    {
                        "name": "C. Petta"
                    },
                    {
                        "name": "R. Petti"
                    },
                    {
                        "name": "M. Pfaff"
                    },
                    {
                        "name": "V. Pia"
                    },
                    {
                        "name": "L. Pickering"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "F. Pietropaolo"
                    },
                    {
                        "name": "V. L. Pimentel"
                    },
                    {
                        "name": "G. Pinaroli"
                    },
                    {
                        "name": "S. Pincha"
                    },
                    {
                        "name": "J. Pinchault"
                    },
                    {
                        "name": "K. Pitts"
                    },
                    {
                        "name": "K. Pletcher"
                    },
                    {
                        "name": "K. Plows"
                    },
                    {
                        "name": "C. Pollack"
                    },
                    {
                        "name": "T. Pollmann"
                    },
                    {
                        "name": "F. Pompa"
                    },
                    {
                        "name": "X. Pons"
                    },
                    {
                        "name": "N. Poonthottathil"
                    },
                    {
                        "name": "V. Popov"
                    },
                    {
                        "name": "F. Poppi"
                    },
                    {
                        "name": "J. Porter"
                    },
                    {
                        "name": "L. G. Porto Paixo"
                    },
                    {
                        "name": "M. Potekhin"
                    },
                    {
                        "name": "M. Pozzato"
                    },
                    {
                        "name": "R. Pradhan"
                    },
                    {
                        "name": "T. Prakash"
                    },
                    {
                        "name": "M. Prest"
                    },
                    {
                        "name": "F. Psihas"
                    },
                    {
                        "name": "D. Pugnere"
                    },
                    {
                        "name": "D. Pullia"
                    },
                    {
                        "name": "X. Qian"
                    },
                    {
                        "name": "J. Queen"
                    },
                    {
                        "name": "J. L. Raaf"
                    },
                    {
                        "name": "M. Rabelhofer"
                    },
                    {
                        "name": "V. Radeka"
                    },
                    {
                        "name": "J. Rademacker"
                    },
                    {
                        "name": "B. Radics"
                    },
                    {
                        "name": "F. Raffaelli"
                    },
                    {
                        "name": "A. Rafique"
                    },
                    {
                        "name": "E. Raguzin"
                    },
                    {
                        "name": "A. Rahe"
                    },
                    {
                        "name": "S. Rajagopalan"
                    },
                    {
                        "name": "M. Rajaoalisoa"
                    },
                    {
                        "name": "I. Rakhno"
                    },
                    {
                        "name": "L. Rakotondravohitra"
                    },
                    {
                        "name": "M. A. Ralaikoto"
                    },
                    {
                        "name": "L. Ralte"
                    },
                    {
                        "name": "M. A. Ramirez Delgado"
                    },
                    {
                        "name": "B. Ramson"
                    },
                    {
                        "name": "S. S. Randriamanampisoa"
                    },
                    {
                        "name": "A. Rappoldi"
                    },
                    {
                        "name": "G. Raselli"
                    },
                    {
                        "name": "T. Rath"
                    },
                    {
                        "name": "P. Ratoff"
                    },
                    {
                        "name": "R. Ray"
                    },
                    {
                        "name": "H. Razafinime"
                    },
                    {
                        "name": "R. F. Razakamiandra"
                    },
                    {
                        "name": "E. M. Rea"
                    },
                    {
                        "name": "J. S. Real"
                    },
                    {
                        "name": "B. Rebel"
                    },
                    {
                        "name": "R. Rechenmacher"
                    },
                    {
                        "name": "J. Reichenbacher"
                    },
                    {
                        "name": "S. D. Reitzner"
                    },
                    {
                        "name": "E. Renner"
                    },
                    {
                        "name": "S. Repetto"
                    },
                    {
                        "name": "S. Rescia"
                    },
                    {
                        "name": "F. Resnati"
                    },
                    {
                        "name": "C. Reynolds"
                    },
                    {
                        "name": "M. Ribas"
                    },
                    {
                        "name": "S. Riboldi"
                    },
                    {
                        "name": "C. Riccio"
                    },
                    {
                        "name": "G. Riccobene"
                    },
                    {
                        "name": "J. S. Ricol"
                    },
                    {
                        "name": "M. Rigan"
                    },
                    {
                        "name": "A. Rikalo"
                    },
                    {
                        "name": "E. V. Rincn"
                    },
                    {
                        "name": "A. Ritchie-Yates"
                    },
                    {
                        "name": "S. Ritter"
                    },
                    {
                        "name": "D. Rivera"
                    },
                    {
                        "name": "A. Robert"
                    },
                    {
                        "name": "A. Roberts"
                    },
                    {
                        "name": "E. Robles"
                    },
                    {
                        "name": "J. L. Rocabado Rocha"
                    },
                    {
                        "name": "M. Roda"
                    },
                    {
                        "name": "D. Rodas Rodrguez"
                    },
                    {
                        "name": "M. J. O. Rodrigues"
                    },
                    {
                        "name": "J. Rodriguez Rondon"
                    },
                    {
                        "name": "S. Rosauro-Alcaraz"
                    },
                    {
                        "name": "P. Rosier"
                    },
                    {
                        "name": "D. Ross"
                    },
                    {
                        "name": "M. Rossella"
                    },
                    {
                        "name": "M. Rossi"
                    },
                    {
                        "name": "M. Ross-Lonergan"
                    },
                    {
                        "name": "N. Roy"
                    },
                    {
                        "name": "P. Roy"
                    },
                    {
                        "name": "P. Roy"
                    },
                    {
                        "name": "C. Rubbia"
                    },
                    {
                        "name": "D. Rudik"
                    },
                    {
                        "name": "A. Ruggeri"
                    },
                    {
                        "name": "G. Ruiz Ferreira"
                    },
                    {
                        "name": "K. Rushiya"
                    },
                    {
                        "name": "B. Russell"
                    },
                    {
                        "name": "S. Sacerdoti"
                    },
                    {
                        "name": "N. Saduyev"
                    },
                    {
                        "name": "S. K. Sahoo"
                    },
                    {
                        "name": "N. Sahu"
                    },
                    {
                        "name": "S. Sakhiyev"
                    },
                    {
                        "name": "P. Sala"
                    },
                    {
                        "name": "G. Salmoria"
                    },
                    {
                        "name": "S. Samanta"
                    },
                    {
                        "name": "N. Samios"
                    },
                    {
                        "name": "M. C. Sanchez"
                    },
                    {
                        "name": "A. Snchez Bravo"
                    },
                    {
                        "name": "A. Snchez-Castillo"
                    },
                    {
                        "name": "P. Sanchez-Lucas"
                    },
                    {
                        "name": "D. A. Sanders"
                    },
                    {
                        "name": "S. Sanfilippo"
                    },
                    {
                        "name": "D. Santoro"
                    },
                    {
                        "name": "N. Saoulidou"
                    },
                    {
                        "name": "P. Sapienza"
                    },
                    {
                        "name": "I. Sarcevic"
                    },
                    {
                        "name": "I. Sarra"
                    },
                    {
                        "name": "G. Savage"
                    },
                    {
                        "name": "V. Savinov"
                    },
                    {
                        "name": "G. Scanavini"
                    },
                    {
                        "name": "A. Scaramelli"
                    },
                    {
                        "name": "A. Scarff"
                    },
                    {
                        "name": "T. Schefke"
                    },
                    {
                        "name": "H. Schellman"
                    },
                    {
                        "name": "S. Schifano"
                    },
                    {
                        "name": "P. Schlabach"
                    },
                    {
                        "name": "D. Schmitz"
                    },
                    {
                        "name": "A. W. Schneider"
                    },
                    {
                        "name": "K. Scholberg"
                    },
                    {
                        "name": "A. Schukraft"
                    },
                    {
                        "name": "B. Schuld"
                    },
                    {
                        "name": "S. Schwartz"
                    },
                    {
                        "name": "A. Segade"
                    },
                    {
                        "name": "E. Segreto"
                    },
                    {
                        "name": "C. R. Senise"
                    },
                    {
                        "name": "J. Sensenig"
                    },
                    {
                        "name": "S. H. Seo"
                    },
                    {
                        "name": "D. Seppela"
                    },
                    {
                        "name": "M. H. Shaevitz"
                    },
                    {
                        "name": "P. Shanahan"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "R. Kumar"
                    },
                    {
                        "name": "S. Sharma Poudel"
                    },
                    {
                        "name": "K. Shaw"
                    },
                    {
                        "name": "T. Shaw"
                    },
                    {
                        "name": "K. Shchablo"
                    },
                    {
                        "name": "J. Shen"
                    },
                    {
                        "name": "C. Shepherd-Themistocleous"
                    },
                    {
                        "name": "J. Shi"
                    },
                    {
                        "name": "W. Shi"
                    },
                    {
                        "name": "S. Shin"
                    },
                    {
                        "name": "S. Shivakoti"
                    },
                    {
                        "name": "A. Shmakov"
                    },
                    {
                        "name": "I. Shoemaker"
                    },
                    {
                        "name": "D. Shooltz"
                    },
                    {
                        "name": "R. Shrock"
                    },
                    {
                        "name": "M. Siden"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "L. Simard"
                    },
                    {
                        "name": "J. Sinclair"
                    },
                    {
                        "name": "G. Sinev"
                    },
                    {
                        "name": "Jaydip Singh"
                    },
                    {
                        "name": "J. Singh"
                    },
                    {
                        "name": "L. Singh"
                    },
                    {
                        "name": "P. Singh"
                    },
                    {
                        "name": "V. Singh"
                    },
                    {
                        "name": "S. Singh Chauhan"
                    },
                    {
                        "name": "R. Sipos"
                    },
                    {
                        "name": "C. Sironneau"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "K. Siyeon"
                    },
                    {
                        "name": "K. Skarpaas"
                    },
                    {
                        "name": "J. Smedley"
                    },
                    {
                        "name": "J. Smith"
                    },
                    {
                        "name": "P. Smith"
                    },
                    {
                        "name": "J. Smolik"
                    },
                    {
                        "name": "M. Smy"
                    },
                    {
                        "name": "M. Snape"
                    },
                    {
                        "name": "E. L. Snider"
                    },
                    {
                        "name": "P. Snopok"
                    },
                    {
                        "name": "M. Soares Nunes"
                    },
                    {
                        "name": "H. Sobel"
                    },
                    {
                        "name": "M. Soderberg"
                    },
                    {
                        "name": "C. J. Solano Salinas"
                    },
                    {
                        "name": "S. Sldner-Rembold"
                    },
                    {
                        "name": "N. Solomey"
                    },
                    {
                        "name": "V. Solovov"
                    },
                    {
                        "name": "W. E. Sondheim"
                    },
                    {
                        "name": "M. Sorbara"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "A. Sousa"
                    },
                    {
                        "name": "K. Soustruznik"
                    },
                    {
                        "name": "D. Souza Correia"
                    },
                    {
                        "name": "F. Spinella"
                    },
                    {
                        "name": "J. Spitz"
                    },
                    {
                        "name": "N. J. C. Spooner"
                    },
                    {
                        "name": "D. Stalder"
                    },
                    {
                        "name": "M. Stancari"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steenis"
                    },
                    {
                        "name": "R. Stein"
                    },
                    {
                        "name": "H. M. Steiner"
                    },
                    {
                        "name": "A. F. Steklain Lisba"
                    },
                    {
                        "name": "J. Stewart"
                    },
                    {
                        "name": "B. Stillwell"
                    },
                    {
                        "name": "J. Stock"
                    },
                    {
                        "name": "T. Stokes"
                    },
                    {
                        "name": "M. Strait"
                    },
                    {
                        "name": "T. Strauss"
                    },
                    {
                        "name": "L. Strigari"
                    },
                    {
                        "name": "A. Stuart"
                    },
                    {
                        "name": "J. G. Suarez"
                    },
                    {
                        "name": "J. Subash"
                    },
                    {
                        "name": "A. Surdo"
                    },
                    {
                        "name": "L. Suter"
                    },
                    {
                        "name": "K. Sutton"
                    },
                    {
                        "name": "Y. Suvorov"
                    },
                    {
                        "name": "R. Svoboda"
                    },
                    {
                        "name": "S. K. Swain"
                    },
                    {
                        "name": "C. Sweeney"
                    },
                    {
                        "name": "B. Szczerbinska"
                    },
                    {
                        "name": "A. M. Szelc"
                    },
                    {
                        "name": "A. Sztuc"
                    },
                    {
                        "name": "A. Taffara"
                    },
                    {
                        "name": "N. Talukdar"
                    },
                    {
                        "name": "J. Tamara"
                    },
                    {
                        "name": "H. A. Tanaka"
                    },
                    {
                        "name": "S. Tang"
                    },
                    {
                        "name": "N. Taniuchi"
                    },
                    {
                        "name": "A. M. Tapia Casanova"
                    },
                    {
                        "name": "A. Tapper"
                    },
                    {
                        "name": "S. Tariq"
                    },
                    {
                        "name": "E. Tarpara"
                    },
                    {
                        "name": "E. Tatar"
                    },
                    {
                        "name": "R. Tayloe"
                    },
                    {
                        "name": "D. Tedeschi"
                    },
                    {
                        "name": "A. M. Teklu"
                    },
                    {
                        "name": "K. Tellez Giron Flores"
                    },
                    {
                        "name": "J. Tena Vidal"
                    },
                    {
                        "name": "P. Tennessen"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "K. Terao"
                    },
                    {
                        "name": "F. Terranova"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "T. Thakore"
                    },
                    {
                        "name": "A. Thea"
                    },
                    {
                        "name": "S. Thomas"
                    },
                    {
                        "name": "A. Thompson"
                    },
                    {
                        "name": "C. Thorn"
                    },
                    {
                        "name": "C. Thorpe"
                    },
                    {
                        "name": "S. C. Timm"
                    },
                    {
                        "name": "E. Tiras"
                    },
                    {
                        "name": "V. Tishchenko"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "N. Todorovi"
                    },
                    {
                        "name": "L. Tomassetti"
                    },
                    {
                        "name": "A. Tonazzo"
                    },
                    {
                        "name": "D. Torbunov"
                    },
                    {
                        "name": "D. Torres Muoz"
                    },
                    {
                        "name": "M. Torti"
                    },
                    {
                        "name": "M. Tortola"
                    },
                    {
                        "name": "Y. Torun"
                    },
                    {
                        "name": "N. Tosi"
                    },
                    {
                        "name": "D. Totani"
                    },
                    {
                        "name": "M. Toups"
                    },
                    {
                        "name": "C. Touramanis"
                    },
                    {
                        "name": "V. Trabattoni"
                    },
                    {
                        "name": "D. Tran"
                    },
                    {
                        "name": "R. Travaglini"
                    },
                    {
                        "name": "J. Trevor"
                    },
                    {
                        "name": "E. Triller"
                    },
                    {
                        "name": "S. Trilov"
                    },
                    {
                        "name": "D. Trotta"
                    },
                    {
                        "name": "J. Truchon"
                    },
                    {
                        "name": "D. Truncali"
                    },
                    {
                        "name": "W. H. Trzaska"
                    },
                    {
                        "name": "Y. Tsai"
                    },
                    {
                        "name": "Y. -T. Tsai"
                    },
                    {
                        "name": "Z. Tsamalaidze"
                    },
                    {
                        "name": "K. V. Tsang"
                    },
                    {
                        "name": "N. Tsverava"
                    },
                    {
                        "name": "S. Z. Tu"
                    },
                    {
                        "name": "S. Tufanli"
                    },
                    {
                        "name": "C. Tunnell"
                    },
                    {
                        "name": "M. Tuzi"
                    },
                    {
                        "name": "J. Tyler"
                    },
                    {
                        "name": "E. Tyley"
                    },
                    {
                        "name": "M. Tzanov"
                    },
                    {
                        "name": "M. A. Uchida"
                    },
                    {
                        "name": "J. Urea Gonzlez"
                    },
                    {
                        "name": "J. Urheim"
                    },
                    {
                        "name": "T. Usher"
                    },
                    {
                        "name": "H. Utaegbulam"
                    },
                    {
                        "name": "S. Uzunyan"
                    },
                    {
                        "name": "M. R. Vagins"
                    },
                    {
                        "name": "P. Vahle"
                    },
                    {
                        "name": "G. A. Valdiviesso"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "E. Valencia"
                    },
                    {
                        "name": "R. Valentim"
                    },
                    {
                        "name": "Z. Vallari"
                    },
                    {
                        "name": "E. Vallazza"
                    },
                    {
                        "name": "J. W. F. Valle"
                    },
                    {
                        "name": "R. Van Berg"
                    },
                    {
                        "name": "D. V. Forero"
                    },
                    {
                        "name": "A. Vannozzi"
                    },
                    {
                        "name": "M. Van Nuland-Troost"
                    },
                    {
                        "name": "F. Varanini"
                    },
                    {
                        "name": "D. Vargas Oliva"
                    },
                    {
                        "name": "N. Vaughan"
                    },
                    {
                        "name": "K. Vaziri"
                    },
                    {
                        "name": "A. Vzquez-Ramos"
                    },
                    {
                        "name": "J. Vega"
                    },
                    {
                        "name": "J. Vences"
                    },
                    {
                        "name": "S. Ventura"
                    },
                    {
                        "name": "A. Verdugo"
                    },
                    {
                        "name": "S. Vergani"
                    },
                    {
                        "name": "M. Verzocchi"
                    },
                    {
                        "name": "K. Vetter"
                    },
                    {
                        "name": "M. Vicenzi"
                    },
                    {
                        "name": "H. Vieira de Souza"
                    },
                    {
                        "name": "C. Vignoli"
                    },
                    {
                        "name": "C. Vilela"
                    },
                    {
                        "name": "E. Villa"
                    },
                    {
                        "name": "S. Viola"
                    },
                    {
                        "name": "B. Viren"
                    },
                    {
                        "name": "G. V. Stenico"
                    },
                    {
                        "name": "R. Vizarreta"
                    },
                    {
                        "name": "A. P. Vizcaya Hernandez"
                    },
                    {
                        "name": "S. Vlachos"
                    },
                    {
                        "name": "G. Vorobyev"
                    },
                    {
                        "name": "Q. Vuong"
                    },
                    {
                        "name": "A. V. Waldron"
                    },
                    {
                        "name": "M. Wallach"
                    },
                    {
                        "name": "J. Walsh"
                    },
                    {
                        "name": "T. Walton"
                    },
                    {
                        "name": "L. Wan"
                    },
                    {
                        "name": "B. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "L. Wang"
                    },
                    {
                        "name": "M. H. L. S. Wang"
                    },
                    {
                        "name": "X. Wang"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "K. Warburton"
                    },
                    {
                        "name": "D. Warner"
                    },
                    {
                        "name": "L. Warsame"
                    },
                    {
                        "name": "M. O. Wascko"
                    },
                    {
                        "name": "D. Waters"
                    },
                    {
                        "name": "A. Watson"
                    },
                    {
                        "name": "K. Wawrowska"
                    },
                    {
                        "name": "A. Weber"
                    },
                    {
                        "name": "C. M. Weber"
                    },
                    {
                        "name": "M. Weber"
                    },
                    {
                        "name": "H. Wei"
                    },
                    {
                        "name": "A. Weinstein"
                    },
                    {
                        "name": "S. Westerdale"
                    },
                    {
                        "name": "M. Wetstein"
                    },
                    {
                        "name": "K. Whalen"
                    },
                    {
                        "name": "A. White"
                    },
                    {
                        "name": "L. H. Whitehead"
                    },
                    {
                        "name": "D. Whittington"
                    },
                    {
                        "name": "F. Wieler"
                    },
                    {
                        "name": "J. Wilhlemi"
                    },
                    {
                        "name": "M. J. Wilking"
                    },
                    {
                        "name": "A. Wilkinson"
                    },
                    {
                        "name": "C. Wilkinson"
                    },
                    {
                        "name": "F. Wilson"
                    },
                    {
                        "name": "R. J. Wilson"
                    },
                    {
                        "name": "P. Winter"
                    },
                    {
                        "name": "J. Wolcott"
                    },
                    {
                        "name": "J. Wolfs"
                    },
                    {
                        "name": "T. Wongjirad"
                    },
                    {
                        "name": "A. Wood"
                    },
                    {
                        "name": "K. Wood"
                    },
                    {
                        "name": "E. Worcester"
                    },
                    {
                        "name": "M. Worcester"
                    },
                    {
                        "name": "K. Wresilo"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "M. Wrobel"
                    },
                    {
                        "name": "S. Wu"
                    },
                    {
                        "name": "W. Wu"
                    },
                    {
                        "name": "W. Wu"
                    },
                    {
                        "name": "M. Wurm"
                    },
                    {
                        "name": "J. Wyenberg"
                    },
                    {
                        "name": "B. M. Wynne"
                    },
                    {
                        "name": "Y. Xiao"
                    },
                    {
                        "name": "I. Xiotidis"
                    },
                    {
                        "name": "B. Yaeggy"
                    },
                    {
                        "name": "N. Yahlali"
                    },
                    {
                        "name": "E. Yandel"
                    },
                    {
                        "name": "G. Yang"
                    },
                    {
                        "name": "J. Yang"
                    },
                    {
                        "name": "T. Yang"
                    },
                    {
                        "name": "A. Yankelevich"
                    },
                    {
                        "name": "L. Yates"
                    },
                    {
                        "name": "K. Yonehara"
                    },
                    {
                        "name": "T. Young"
                    },
                    {
                        "name": "B. Yu"
                    },
                    {
                        "name": "H. Yu"
                    },
                    {
                        "name": "J. Yu"
                    },
                    {
                        "name": "Y. Yu"
                    },
                    {
                        "name": "W. Yuan"
                    },
                    {
                        "name": "R. Zaki"
                    },
                    {
                        "name": "J. Zalesak"
                    },
                    {
                        "name": "L. Zambelli"
                    },
                    {
                        "name": "B. Zamorano"
                    },
                    {
                        "name": "A. Zani"
                    },
                    {
                        "name": "O. Zapata"
                    },
                    {
                        "name": "L. Zazueta"
                    },
                    {
                        "name": "G. P. Zeller"
                    },
                    {
                        "name": "J. Zennamo"
                    },
                    {
                        "name": "J. Zettlemoyer"
                    },
                    {
                        "name": "K. Zeug"
                    },
                    {
                        "name": "C. Zhang"
                    },
                    {
                        "name": "S. Zhang"
                    },
                    {
                        "name": "M. Zhao"
                    },
                    {
                        "name": "E. Zhivun"
                    },
                    {
                        "name": "E. D. Zimmerman"
                    },
                    {
                        "name": "S. Zucchelli"
                    },
                    {
                        "name": "V. Zutshi"
                    },
                    {
                        "name": "R. Zwaska"
                    }
                ],
                "author_detail": {
                    "name": "R. Zwaska"
                },
                "author": "R. Zwaska",
                "arxiv_comment": "Submitted to the 2026 Update of the European Strategy for Particle\n  Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23740v1",
                "updated": "2025-03-31T05:34:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    34,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T05:34:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    34,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "LANID: LLM-assisted New Intent Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LANID: LLM-assisted New Intent Discovery"
                },
                "summary": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID."
                },
                "authors": [
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Jiashu Pu"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "arxiv_comment": "Published in LREC-COLING 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11546v2",
                "updated": "2025-03-31T05:25:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    25,
                    57,
                    0,
                    90,
                    0
                ],
                "published": "2025-02-17T08:28:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    28,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection"
                },
                "summary": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and clean multilingual datasets. In this\npaper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus built using newly extracted Common Crawl data\nand existing multilingual datasets. DCAD-2000 includes over 2,282 languages,\n46.72TB of data, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof current data cleaning methods, which rely on manual heuristic thresholds, we\npropose reframing data cleaning as an anomaly detection task. This dynamic\nfiltering approach significantly enhances data quality by identifying and\nremoving noisy or anomalous content. We evaluate the quality of DCAD-2000 on\nthe FineTask benchmark, demonstrating substantial improvements in multilingual\ndataset quality and task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and clean multilingual datasets. In this\npaper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus built using newly extracted Common Crawl data\nand existing multilingual datasets. DCAD-2000 includes over 2,282 languages,\n46.72TB of data, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof current data cleaning methods, which rely on manual heuristic thresholds, we\npropose reframing data cleaning as an anomaly detection task. This dynamic\nfiltering approach significantly enhances data quality by identifying and\nremoving noisy or anomalous content. We evaluate the quality of DCAD-2000 on\nthe FineTask benchmark, demonstrating substantial improvements in multilingual\ndataset quality and task performance."
                },
                "authors": [
                    {
                        "name": "Yingli Shen"
                    },
                    {
                        "name": "Wen Lai"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xueren Zhang"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23733v1",
                "updated": "2025-03-31T05:13:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    13,
                    2,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T05:13:02Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    5,
                    13,
                    2,
                    0,
                    90,
                    0
                ],
                "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization"
                },
                "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks."
                },
                "authors": [
                    {
                        "name": "Yiyang Du"
                    },
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Chi Chen"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Yiru Wang"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19037v2",
                "updated": "2025-03-31T04:48:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    4,
                    48,
                    28,
                    0,
                    90,
                    0
                ],
                "published": "2024-12-26T03:13:03Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    13,
                    3,
                    3,
                    361,
                    0
                ],
                "title": "CL-Attack: Textual Backdoor Attacks via Cross-Lingual Triggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL-Attack: Textual Backdoor Attacks via Cross-Lingual Triggers"
                },
                "summary": "Backdoor attacks significantly compromise the security of large language\nmodels by triggering them to output specific and controlled content. Currently,\ntriggers for textual backdoor attacks fall into two categories: fixed-token\ntriggers and sentence-pattern triggers. However, the former are typically easy\nto identify and filter, while the latter, such as syntax and style, do not\napply to all original samples and may lead to semantic shifts. In this paper,\ninspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we\npropose a higher-dimensional trigger method at the paragraph level, namely\nCL-attack. CL-attack injects the backdoor by using texts with specific\nstructures that incorporate multiple languages, thereby offering greater\nstealthiness and universality compared to existing backdoor attack techniques.\nExtensive experiments on different tasks and model architectures demonstrate\nthat CL-attack can achieve nearly 100% attack success rate with a low poisoning\nrate in both classification and generation tasks. We also empirically show that\nthe CL-attack is more robust against current major defense methods compared to\nbaseline backdoor attacks. Additionally, to mitigate CL-attack, we further\ndevelop a new defense called TranslateDefense, which can partially mitigate the\nimpact of CL-attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks significantly compromise the security of large language\nmodels by triggering them to output specific and controlled content. Currently,\ntriggers for textual backdoor attacks fall into two categories: fixed-token\ntriggers and sentence-pattern triggers. However, the former are typically easy\nto identify and filter, while the latter, such as syntax and style, do not\napply to all original samples and may lead to semantic shifts. In this paper,\ninspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we\npropose a higher-dimensional trigger method at the paragraph level, namely\nCL-attack. CL-attack injects the backdoor by using texts with specific\nstructures that incorporate multiple languages, thereby offering greater\nstealthiness and universality compared to existing backdoor attack techniques.\nExtensive experiments on different tasks and model architectures demonstrate\nthat CL-attack can achieve nearly 100% attack success rate with a low poisoning\nrate in both classification and generation tasks. We also empirically show that\nthe CL-attack is more robust against current major defense methods compared to\nbaseline backdoor attacks. Additionally, to mitigate CL-attack, we further\ndevelop a new defense called TranslateDefense, which can partially mitigate the\nimpact of CL-attack."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Tianyi Hu"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "arxiv_comment": "The paper has been accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]