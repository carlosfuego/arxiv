[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v1",
                "updated": "2025-01-09T06:00:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v1",
                "updated": "2025-01-04T20:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04012v1",
                "updated": "2024-12-18T00:35:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T00:35:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "FlexCache: Flexible Approximate Cache System for Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexCache: Flexible Approximate Cache System for Video Diffusion"
                },
                "summary": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Henry Tian"
                    },
                    {
                        "name": "Tim Lu"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06186v1",
                "updated": "2025-01-10T18:59:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    59,
                    51,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T18:59:51Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    59,
                    51,
                    4,
                    10,
                    0
                ],
                "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs"
                },
                "summary": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available."
                },
                "authors": [
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Dinura Dissanayake"
                    },
                    {
                        "name": "Ketan More"
                    },
                    {
                        "name": "Ritesh Thawkar"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Noor Ahsan"
                    },
                    {
                        "name": "Yuhao Li"
                    },
                    {
                        "name": "Mohammed Zumri"
                    },
                    {
                        "name": "Jean Lahoud"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "15 pages, 5 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05450v2",
                "updated": "2025-01-10T18:58:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    58,
                    11,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-09T18:59:56Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    56,
                    3,
                    9,
                    0
                ],
                "title": "Decentralized Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Diffusion Models"
                },
                "summary": "Large-scale AI model training divides work across thousands of GPUs, then\nsynchronizes gradients across them at each step. This incurs a significant\nnetwork burden that only centralized, monolithic clusters can support, driving\nup infrastructure costs and straining power systems. We propose Decentralized\nDiffusion Models, a scalable framework for distributing diffusion model\ntraining across independent clusters or datacenters by eliminating the\ndependence on a centralized, high-bandwidth networking fabric. Our method\ntrains a set of expert diffusion models over partitions of the dataset, each in\nfull isolation from one another. At inference time, the experts ensemble\nthrough a lightweight router. We show that the ensemble collectively optimizes\nthe same objective as a single model trained over the whole dataset. This means\nwe can divide the training burden among a number of \"compute islands,\" lowering\ninfrastructure costs and improving resilience to localized GPU failures.\nDecentralized diffusion models empower researchers to take advantage of\nsmaller, more cost-effective and more readily available compute like on-demand\nGPU nodes rather than central integrated systems. We conduct extensive\nexperiments on ImageNet and LAION Aesthetics, showing that decentralized\ndiffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\nscale our approach to 24 billion parameters, demonstrating that high-quality\ndiffusion models can now be trained with just eight individual GPU nodes in\nless than a week.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale AI model training divides work across thousands of GPUs, then\nsynchronizes gradients across them at each step. This incurs a significant\nnetwork burden that only centralized, monolithic clusters can support, driving\nup infrastructure costs and straining power systems. We propose Decentralized\nDiffusion Models, a scalable framework for distributing diffusion model\ntraining across independent clusters or datacenters by eliminating the\ndependence on a centralized, high-bandwidth networking fabric. Our method\ntrains a set of expert diffusion models over partitions of the dataset, each in\nfull isolation from one another. At inference time, the experts ensemble\nthrough a lightweight router. We show that the ensemble collectively optimizes\nthe same objective as a single model trained over the whole dataset. This means\nwe can divide the training burden among a number of \"compute islands,\" lowering\ninfrastructure costs and improving resilience to localized GPU failures.\nDecentralized diffusion models empower researchers to take advantage of\nsmaller, more cost-effective and more readily available compute like on-demand\nGPU nodes rather than central integrated systems. We conduct extensive\nexperiments on ImageNet and LAION Aesthetics, showing that decentralized\ndiffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\nscale our approach to 24 billion parameters, demonstrating that high-quality\ndiffusion models can now be trained with just eight individual GPU nodes in\nless than a week."
                },
                "authors": [
                    {
                        "name": "David McAllister"
                    },
                    {
                        "name": "Matthew Tancik"
                    },
                    {
                        "name": "Jiaming Song"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "arxiv_comment": "Project webpage: https://decentralizeddiffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v6",
                "updated": "2025-01-10T18:45:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    45,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads"
                },
                "summary": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02767v2",
                "updated": "2025-01-10T18:24:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    24,
                    55,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-03T19:09:48Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    19,
                    9,
                    48,
                    1,
                    338,
                    0
                ],
                "title": "Endogenous Heteroskedasticity in Linear Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogenous Heteroskedasticity in Linear Models"
                },
                "summary": "Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a framework that has two common issues, endogeneity\nof the regressors, and heteroskedasticity that is allowed to depend on\nendogenous regressors, i.e., endogenous heteroskedasticity. We show that the\npresence of such conditional heteroskedasticity in the structural regression\nrenders the two-stages least squares estimator inconsistent. To solve this\nissue, we propose sufficient conditions together with a control function\napproach to identify and estimate the causal parameters of interest. We\nestablish the limiting properties of the estimator, say consistency and\nasymptotic normality, and propose inference procedures. Monte Carlo simulations\nprovide evidence of the finite sample performance of the proposed methods, and\nevaluate different implementation procedures. We revisit an empirical\napplication about job training to illustrate the methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a framework that has two common issues, endogeneity\nof the regressors, and heteroskedasticity that is allowed to depend on\nendogenous regressors, i.e., endogenous heteroskedasticity. We show that the\npresence of such conditional heteroskedasticity in the structural regression\nrenders the two-stages least squares estimator inconsistent. To solve this\nissue, we propose sufficient conditions together with a control function\napproach to identify and estimate the causal parameters of interest. We\nestablish the limiting properties of the estimator, say consistency and\nasymptotic normality, and propose inference procedures. Monte Carlo simulations\nprovide evidence of the finite sample performance of the proposed methods, and\nevaluate different implementation procedures. We revisit an empirical\napplication about job training to illustrate the methods."
                },
                "authors": [
                    {
                        "name": "Javier Alejo"
                    },
                    {
                        "name": "Antonio F. Galvao"
                    },
                    {
                        "name": "Julian Martinez-Iriarte"
                    },
                    {
                        "name": "Gabriel Montes-Rojas"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Montes-Rojas"
                },
                "author": "Gabriel Montes-Rojas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06149v1",
                "updated": "2025-01-10T18:19:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    19,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T18:19:50Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    19,
                    50,
                    4,
                    10,
                    0
                ],
                "title": "Understanding what helium absorption tells us about atmospheric escape\n  from exoplanets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what helium absorption tells us about atmospheric escape\n  from exoplanets"
                },
                "summary": "Atmospheric escape is now considered the major contributing factor in shaping\nthe demographic of detected exoplanets. However, inferences about the exoplanet\npopulations strongly depend on the accuracy of the models. Direct observational\ntests of atmospheric models are still in their infancy. Helium escape from\nplanetary atmospheres has rapidly become the primary observational probe,\nalready observed in $\\gtrsim$20 exoplanets. Grounding our understanding in the\nbasic physics of atmospheric escape, we present a new theoretical model to\npredict the excess absorption from the helium absorption line. We constrain the\natmosphere properties, such as mass-loss rates and outflow temperatures, by\nimplementing a Parker wind solution with an energy limited evaporating outflow.\nImportantly, we self-consistently link the mass-loss rates and outflow\ntemperatures, which are critical to understanding helium absorption as the\ntriplet-level population is typically exponentially sensitive to temperature.\nFurthermore, helium absorption is typically optically thin and the absorption\nis dominated far from the planet. Therefore, the absorption depth is not a\nmeasure of the size of the helium outflow. Our results indicate that for\nplanets with a detectable signal, typically the helium triplet population in\nthe atmosphere rapidly approaches a statistical equilibrium between populations\nby recombination and depopulation caused by electron collisions. We suggest\nthat excess helium absorption can be quantified by a scaled equivalent width,\nwhich is positively correlated with the mass loss rate. We also show that the\nhelium absorption scales with incident radiation, particularly with the XEUV to\nFUV flux ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric escape is now considered the major contributing factor in shaping\nthe demographic of detected exoplanets. However, inferences about the exoplanet\npopulations strongly depend on the accuracy of the models. Direct observational\ntests of atmospheric models are still in their infancy. Helium escape from\nplanetary atmospheres has rapidly become the primary observational probe,\nalready observed in $\\gtrsim$20 exoplanets. Grounding our understanding in the\nbasic physics of atmospheric escape, we present a new theoretical model to\npredict the excess absorption from the helium absorption line. We constrain the\natmosphere properties, such as mass-loss rates and outflow temperatures, by\nimplementing a Parker wind solution with an energy limited evaporating outflow.\nImportantly, we self-consistently link the mass-loss rates and outflow\ntemperatures, which are critical to understanding helium absorption as the\ntriplet-level population is typically exponentially sensitive to temperature.\nFurthermore, helium absorption is typically optically thin and the absorption\nis dominated far from the planet. Therefore, the absorption depth is not a\nmeasure of the size of the helium outflow. Our results indicate that for\nplanets with a detectable signal, typically the helium triplet population in\nthe atmosphere rapidly approaches a statistical equilibrium between populations\nby recombination and depopulation caused by electron collisions. We suggest\nthat excess helium absorption can be quantified by a scaled equivalent width,\nwhich is positively correlated with the mass loss rate. We also show that the\nhelium absorption scales with incident radiation, particularly with the XEUV to\nFUV flux ratios."
                },
                "authors": [
                    {
                        "name": "Giulia Ballabio"
                    },
                    {
                        "name": "James E. Owen"
                    }
                ],
                "author_detail": {
                    "name": "James E. Owen"
                },
                "author": "James E. Owen",
                "arxiv_comment": "15 pages, 12 figures, accepted for publication by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06137v1",
                "updated": "2025-01-10T17:52:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    34,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    34,
                    4,
                    10,
                    0
                ],
                "title": "Supervision policies can shape long-term risk management in\n  general-purpose AI models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervision policies can shape long-term risk management in\n  general-purpose AI models"
                },
                "summary": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society."
                },
                "authors": [
                    {
                        "name": "Manuel Cebrian"
                    },
                    {
                        "name": "Emilia Gomez"
                    },
                    {
                        "name": "David Fernandez Llorca"
                    }
                ],
                "author_detail": {
                    "name": "David Fernandez Llorca"
                },
                "author": "David Fernandez Llorca",
                "arxiv_comment": "24 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06129v1",
                "updated": "2025-01-10T17:35:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    35,
                    6,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:35:06Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    35,
                    6,
                    4,
                    10,
                    0
                ],
                "title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI"
                },
                "summary": "General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives."
                },
                "authors": [
                    {
                        "name": "Yuya Asano"
                    },
                    {
                        "name": "Sabit Hassan"
                    },
                    {
                        "name": "Paras Sharma"
                    },
                    {
                        "name": "Anthony Sicilia"
                    },
                    {
                        "name": "Katherine Atwell"
                    },
                    {
                        "name": "Diane Litman"
                    },
                    {
                        "name": "Malihe Alikhani"
                    }
                ],
                "author_detail": {
                    "name": "Malihe Alikhani"
                },
                "author": "Malihe Alikhani",
                "arxiv_comment": "Accepted to COLING 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06128v1",
                "updated": "2025-01-10T17:33:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    33,
                    18,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:33:18Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    33,
                    18,
                    4,
                    10,
                    0
                ],
                "title": "Benchmarking Different Application Types across Heterogeneous Cloud\n  Compute Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Different Application Types across Heterogeneous Cloud\n  Compute Services"
                },
                "summary": "Infrastructure as a Service (IaaS) clouds have become the predominant\nunderlying infrastructure for the operation of modern and smart technology.\nIaaS clouds have proven to be useful for multiple reasons such as reduced\ncosts, increased speed and efficiency, and better reliability and scalability.\nCompute services offered by such clouds are heterogeneous -- they offer a set\nof architecturally diverse machines that fit efficiently executing different\nworkloads. However, there has been little study to shed light on the\nperformance of popular application types on these heterogeneous compute servers\nacross different clouds. Such a study can help organizations to optimally (in\nterms of cost, latency, throughput, consumed energy, carbon footprint, etc.)\nemploy cloud compute services. At HPCC lab, we have focused on such benchmarks\nin different research projects and, in this report, we curate those benchmarks\nin a single document to help other researchers in the community using them.\nSpecifically, we introduce our benchmarks datasets for three application types\nin three different domains, namely: Deep Neural Networks (DNN) Inference for\nindustrial applications, Machine Learning (ML) Inference for assistive\ntechnology applications, and video transcoding for multimedia use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure as a Service (IaaS) clouds have become the predominant\nunderlying infrastructure for the operation of modern and smart technology.\nIaaS clouds have proven to be useful for multiple reasons such as reduced\ncosts, increased speed and efficiency, and better reliability and scalability.\nCompute services offered by such clouds are heterogeneous -- they offer a set\nof architecturally diverse machines that fit efficiently executing different\nworkloads. However, there has been little study to shed light on the\nperformance of popular application types on these heterogeneous compute servers\nacross different clouds. Such a study can help organizations to optimally (in\nterms of cost, latency, throughput, consumed energy, carbon footprint, etc.)\nemploy cloud compute services. At HPCC lab, we have focused on such benchmarks\nin different research projects and, in this report, we curate those benchmarks\nin a single document to help other researchers in the community using them.\nSpecifically, we introduce our benchmarks datasets for three application types\nin three different domains, namely: Deep Neural Networks (DNN) Inference for\nindustrial applications, Machine Learning (ML) Inference for assistive\ntechnology applications, and video transcoding for multimedia use cases."
                },
                "authors": [
                    {
                        "name": "Nivedhitha Duggi"
                    },
                    {
                        "name": "Masoud Rafiei"
                    },
                    {
                        "name": "Mohsen Amini Salehi"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Amini Salehi"
                },
                "author": "Mohsen Amini Salehi",
                "arxiv_comment": "Technical Report. arXiv admin note: text overlap with\n  arXiv:2011.11711 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06108v1",
                "updated": "2025-01-10T17:01:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:01:09Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "title": "Inferring High-Order Couplings with Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring High-Order Couplings with Neural Networks"
                },
                "summary": "Maximum-entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical mechanics, have become indispensable tools for modeling pairwise\ninteractions in disciplines such as bioinformatics, ecology, and neuroscience.\nDespite their remarkable success, these methods often overlook high-order\ninteractions that may be crucial in complex systems. Conversely, while modern\nmachine learning approaches can capture such interactions, existing\ninterpretable frameworks are computationally expensive, making it impractical\nto assess the relevance of high-order interactions in real-world scenarios.\nRestricted Boltzmann Machines (RBMs) offer a computationally efficient\nalternative by encoding statistical correlations via hidden nodes in a\nbipartite neural network. Here, we present a method that maps RBMs exactly onto\ngeneralized Potts models with interactions of arbitrary high order. This\napproach leverages large-$N$ approximations, facilitated by the simple\narchitecture of the RBM, to enable the efficient extraction of effective\nmany-body couplings with minimal computational cost. This mapping also enables\nthe development of a general formal framework for the extraction of effective\nhigher-order interactions in arbitrarily complex probabilistic models.\nAdditionally, we introduce a robust formalism for gauge fixing within the\ngeneralized Potts model. We validate our method by accurately recovering two-\nand three-body interactions from synthetic datasets. Additionally, applying our\nframework to protein sequence data demonstrates its effectiveness in\nreconstructing protein contact maps, achieving performance comparable to\nstate-of-the-art inverse Potts models. These results position RBMs as a\npowerful and efficient tool for investigating high-order interactions in\ncomplex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum-entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical mechanics, have become indispensable tools for modeling pairwise\ninteractions in disciplines such as bioinformatics, ecology, and neuroscience.\nDespite their remarkable success, these methods often overlook high-order\ninteractions that may be crucial in complex systems. Conversely, while modern\nmachine learning approaches can capture such interactions, existing\ninterpretable frameworks are computationally expensive, making it impractical\nto assess the relevance of high-order interactions in real-world scenarios.\nRestricted Boltzmann Machines (RBMs) offer a computationally efficient\nalternative by encoding statistical correlations via hidden nodes in a\nbipartite neural network. Here, we present a method that maps RBMs exactly onto\ngeneralized Potts models with interactions of arbitrary high order. This\napproach leverages large-$N$ approximations, facilitated by the simple\narchitecture of the RBM, to enable the efficient extraction of effective\nmany-body couplings with minimal computational cost. This mapping also enables\nthe development of a general formal framework for the extraction of effective\nhigher-order interactions in arbitrarily complex probabilistic models.\nAdditionally, we introduce a robust formalism for gauge fixing within the\ngeneralized Potts model. We validate our method by accurately recovering two-\nand three-body interactions from synthetic datasets. Additionally, applying our\nframework to protein sequence data demonstrates its effectiveness in\nreconstructing protein contact maps, achieving performance comparable to\nstate-of-the-art inverse Potts models. These results position RBMs as a\npowerful and efficient tool for investigating high-order interactions in\ncomplex systems."
                },
                "authors": [
                    {
                        "name": "Aurélien Decelle"
                    },
                    {
                        "name": "Alfonso de Jesús Navas Gómez"
                    },
                    {
                        "name": "Beatriz Seoane"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Seoane"
                },
                "author": "Beatriz Seoane",
                "arxiv_comment": "13 Pages and 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19606v2",
                "updated": "2025-01-10T16:57:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    57,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-03-28T17:28:24Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    17,
                    28,
                    24,
                    3,
                    88,
                    0
                ],
                "title": "Positivity violations in marginal structural survival models with\n  time-dependent confounding: a simulation study on IPTW-estimator performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positivity violations in marginal structural survival models with\n  time-dependent confounding: a simulation study on IPTW-estimator performance"
                },
                "summary": "In longitudinal observational studies, marginal structural models (MSMs) are\na class of causal models used to analyse the effect of an exposure on the\n(time-to-event) outcome of interest, while accounting for exposure-affected\ntime-dependent confounding. In the applied literature, inverse probability of\ntreatment weighting (IPTW) has been widely adopted to estimate MSMs. An\nessential assumption for IPTW-based MSMs is the positivity assumption, which\nensures that, for any combination of measured confounders among individuals,\nthere is a non-zero probability of receiving each possible treatment strategy.\nPositivity is crucial for valid causal inference through IPTW-based MSMs, but\nis often overlooked compared to confounding bias. Positivity violations may\nalso arise due to randomness, in situations where the assignment to a specific\ntreatment is theoretically possible but is either absent or rarely observed in\nthe data, leading to near violations. These situations are common in practical\napplications, particularly when the sample size is small, and they pose\nsignificant challenges for causal inference. This study investigates the impact\nof near-positivity violations on estimates from IPTW-based MSMs in survival\nanalysis. Two algorithms are proposed for simulating longitudinal data from\nhazard-MSMs, accommodating near-positivity violations, a time-varying binary\nexposure, and a time-to-event outcome. Cases of near-positivity violations,\nwhere remaining unexposed is rare within certain confounder levels, are\nanalysed across various scenarios and weight truncation (WT) strategies. This\nwork aims to serve as a critical warning against overlooking the positivity\nassumption or naively applying WT in causal studies using longitudinal\nobservational data and IPTW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In longitudinal observational studies, marginal structural models (MSMs) are\na class of causal models used to analyse the effect of an exposure on the\n(time-to-event) outcome of interest, while accounting for exposure-affected\ntime-dependent confounding. In the applied literature, inverse probability of\ntreatment weighting (IPTW) has been widely adopted to estimate MSMs. An\nessential assumption for IPTW-based MSMs is the positivity assumption, which\nensures that, for any combination of measured confounders among individuals,\nthere is a non-zero probability of receiving each possible treatment strategy.\nPositivity is crucial for valid causal inference through IPTW-based MSMs, but\nis often overlooked compared to confounding bias. Positivity violations may\nalso arise due to randomness, in situations where the assignment to a specific\ntreatment is theoretically possible but is either absent or rarely observed in\nthe data, leading to near violations. These situations are common in practical\napplications, particularly when the sample size is small, and they pose\nsignificant challenges for causal inference. This study investigates the impact\nof near-positivity violations on estimates from IPTW-based MSMs in survival\nanalysis. Two algorithms are proposed for simulating longitudinal data from\nhazard-MSMs, accommodating near-positivity violations, a time-varying binary\nexposure, and a time-to-event outcome. Cases of near-positivity violations,\nwhere remaining unexposed is rare within certain confounder levels, are\nanalysed across various scenarios and weight truncation (WT) strategies. This\nwork aims to serve as a critical warning against overlooking the positivity\nassumption or naively applying WT in causal studies using longitudinal\nobservational data and IPTW."
                },
                "authors": [
                    {
                        "name": "Marta Spreafico"
                    }
                ],
                "author_detail": {
                    "name": "Marta Spreafico"
                },
                "author": "Marta Spreafico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06101v1",
                "updated": "2025-01-10T16:54:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    54,
                    20,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T16:54:20Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    54,
                    20,
                    4,
                    10,
                    0
                ],
                "title": "From Conversation to Automation: Leveraging Large Language Models to\n  Analyze Strategies in Problem Solving Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Conversation to Automation: Leveraging Large Language Models to\n  Analyze Strategies in Problem Solving Therapy"
                },
                "summary": "Problem-solving therapy (PST) is a structured psychological approach that\nhelps individuals manage stress and resolve personal issues by guiding them\nthrough problem identification, solution brainstorming, decision-making, and\noutcome evaluation. As mental health care increasingly integrates technologies\nlike chatbots and large language models (LLMs), understanding how PST can be\neffectively automated is important. This study leverages anonymized therapy\ntranscripts to analyze and classify therapeutic interventions using various\nLLMs and transformer-based models. Our results show that GPT-4o achieved the\nhighest accuracy (0.76) in identifying PST strategies, outperforming other\nmodels. Additionally, we introduced a new dimension of communication strategies\nthat enhances the current PST framework, offering deeper insights into\ntherapist-client interactions. This research demonstrates the potential of LLMs\nto automate complex therapeutic dialogue analysis, providing a scalable,\nefficient tool for mental health interventions. Our annotation framework can\nenhance the accessibility, effectiveness, and personalization of PST,\nsupporting therapists in real-time with more precise, targeted interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-solving therapy (PST) is a structured psychological approach that\nhelps individuals manage stress and resolve personal issues by guiding them\nthrough problem identification, solution brainstorming, decision-making, and\noutcome evaluation. As mental health care increasingly integrates technologies\nlike chatbots and large language models (LLMs), understanding how PST can be\neffectively automated is important. This study leverages anonymized therapy\ntranscripts to analyze and classify therapeutic interventions using various\nLLMs and transformer-based models. Our results show that GPT-4o achieved the\nhighest accuracy (0.76) in identifying PST strategies, outperforming other\nmodels. Additionally, we introduced a new dimension of communication strategies\nthat enhances the current PST framework, offering deeper insights into\ntherapist-client interactions. This research demonstrates the potential of LLMs\nto automate complex therapeutic dialogue analysis, providing a scalable,\nefficient tool for mental health interventions. Our annotation framework can\nenhance the accessibility, effectiveness, and personalization of PST,\nsupporting therapists in real-time with more precise, targeted interventions."
                },
                "authors": [
                    {
                        "name": "Elham Aghakhani"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Karla T. Washington"
                    },
                    {
                        "name": "George Demiris"
                    },
                    {
                        "name": "Jina Huh-Yoo"
                    },
                    {
                        "name": "Rezvaneh Rezapour"
                    }
                ],
                "author_detail": {
                    "name": "Rezvaneh Rezapour"
                },
                "author": "Rezvaneh Rezapour",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06092v1",
                "updated": "2025-01-10T16:45:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    45,
                    0,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T16:45:00Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    45,
                    0,
                    4,
                    10,
                    0
                ],
                "title": "Molecular Communication-Inspired Particle Collector-Transmitter (PaCoT)\n  for Heavy Metal Removal from Human Circulatory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Communication-Inspired Particle Collector-Transmitter (PaCoT)\n  for Heavy Metal Removal from Human Circulatory System"
                },
                "summary": "This study proposes a novel molecular communication (MC)-inspired\nnanomachine, PArticle COllector-Transmitter (PaCoT), to remove toxic heavy\nmetals from the human circulatory system. PaCoT collects these toxic metals and\ntransmits them to release nodes, such as lymph capillaries, before they reach\ncritical organs. The design incorporates key physical parameters and operates\nthrough particle reception and release mechanisms. In the reception process,\ndescribed as ligand-receptor binding reactions, modeled as a continuous-time\nMarkov process (CTMP), PaCoT uses metallothionein proteins as receptors and\nheavy metals (e.g., Zn, Pb, Cd) as ligands. We assume that the toxicity\ncondition (toxic (bit-1), non-toxic (bit-0)) is encoded into the concentration\nof heavy metal molecules. Thus, we consider that heavy metal concentration\nwithin the MC channel (e.g., human circulatory system) employs binary\nconcentration shift keying (binary CSK). The concentration ratio of specific\nheavy metals is estimated to infer toxicity, i.e., a high ratio indicates\ntoxicity and a low ratio suggests non-toxicity. Toxicity detection is achieved\nby monitoring the receptor bound duration in the presence of interferers and\nvarious types of heavy metals. After detecting and collecting toxic heavy\nmetals, PaCoT securely retains them in a liquid medium (e.g., water) until\nrelease, employing two mechanisms: (1) a single-disc viscous micropump to\nregulate flow rate, and (2) Brownian motion to facilitate diffusion. PaCoT's\nperformance is evaluated through MATLAB simulations, focusing on bit error\nprobability (BEP) of the toxicity detection method, release time of molecules\nfrom PaCoT and energy consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a novel molecular communication (MC)-inspired\nnanomachine, PArticle COllector-Transmitter (PaCoT), to remove toxic heavy\nmetals from the human circulatory system. PaCoT collects these toxic metals and\ntransmits them to release nodes, such as lymph capillaries, before they reach\ncritical organs. The design incorporates key physical parameters and operates\nthrough particle reception and release mechanisms. In the reception process,\ndescribed as ligand-receptor binding reactions, modeled as a continuous-time\nMarkov process (CTMP), PaCoT uses metallothionein proteins as receptors and\nheavy metals (e.g., Zn, Pb, Cd) as ligands. We assume that the toxicity\ncondition (toxic (bit-1), non-toxic (bit-0)) is encoded into the concentration\nof heavy metal molecules. Thus, we consider that heavy metal concentration\nwithin the MC channel (e.g., human circulatory system) employs binary\nconcentration shift keying (binary CSK). The concentration ratio of specific\nheavy metals is estimated to infer toxicity, i.e., a high ratio indicates\ntoxicity and a low ratio suggests non-toxicity. Toxicity detection is achieved\nby monitoring the receptor bound duration in the presence of interferers and\nvarious types of heavy metals. After detecting and collecting toxic heavy\nmetals, PaCoT securely retains them in a liquid medium (e.g., water) until\nrelease, employing two mechanisms: (1) a single-disc viscous micropump to\nregulate flow rate, and (2) Brownian motion to facilitate diffusion. PaCoT's\nperformance is evaluated through MATLAB simulations, focusing on bit error\nprobability (BEP) of the toxicity detection method, release time of molecules\nfrom PaCoT and energy consumption."
                },
                "authors": [
                    {
                        "name": "Hilal Esra Yaldiz"
                    },
                    {
                        "name": "Ozgur B. Akan"
                    }
                ],
                "author_detail": {
                    "name": "Ozgur B. Akan"
                },
                "author": "Ozgur B. Akan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06077v1",
                "updated": "2025-01-10T16:14:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    14,
                    8,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T16:14:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    14,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "Explainable Federated Bayesian Causal Inference and Its Application in\n  Advanced Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Federated Bayesian Causal Inference and Its Application in\n  Advanced Manufacturing"
                },
                "summary": "Causal inference has recently gained notable attention across various fields\nlike biology, healthcare, and environmental science, especially within\nexplainable artificial intelligence (xAI) systems, for uncovering the causal\nrelationships among multiple variables and outcomes. Yet, it has not been fully\nrecognized and deployed in the manufacturing systems. In this paper, we\nintroduce an explainable, scalable, and flexible federated Bayesian learning\nframework, \\texttt{xFBCI}, designed to explore causality through treatment\neffect estimation in distributed manufacturing systems. By leveraging federated\nBayesian learning, we efficiently estimate posterior of local parameters to\nderive the propensity score for each client without accessing local private\ndata. These scores are then used to estimate the treatment effect using\npropensity score matching (PSM). Through simulations on various datasets and a\nreal-world Electrohydrodynamic (EHD) printing data, we demonstrate that our\napproach outperforms standard Bayesian causal inference methods and several\nstate-of-the-art federated learning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference has recently gained notable attention across various fields\nlike biology, healthcare, and environmental science, especially within\nexplainable artificial intelligence (xAI) systems, for uncovering the causal\nrelationships among multiple variables and outcomes. Yet, it has not been fully\nrecognized and deployed in the manufacturing systems. In this paper, we\nintroduce an explainable, scalable, and flexible federated Bayesian learning\nframework, \\texttt{xFBCI}, designed to explore causality through treatment\neffect estimation in distributed manufacturing systems. By leveraging federated\nBayesian learning, we efficiently estimate posterior of local parameters to\nderive the propensity score for each client without accessing local private\ndata. These scores are then used to estimate the treatment effect using\npropensity score matching (PSM). Through simulations on various datasets and a\nreal-world Electrohydrodynamic (EHD) printing data, we demonstrate that our\napproach outperforms standard Bayesian causal inference methods and several\nstate-of-the-art federated learning benchmarks."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Xiao"
                    },
                    {
                        "name": "Khawlah Alharbi"
                    },
                    {
                        "name": "Pengyu Zhang"
                    },
                    {
                        "name": "Hantang Qin"
                    },
                    {
                        "name": "Xubo Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xubo Yue"
                },
                "author": "Xubo Yue",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v1",
                "updated": "2025-01-10T15:57:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15314v2",
                "updated": "2025-01-10T15:47:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    47,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2024-08-27T18:00:00Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    0,
                    0,
                    1,
                    240,
                    0
                ],
                "title": "Bayesian inference for the Markov-modulated Poisson process with an\n  outcome process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for the Markov-modulated Poisson process with an\n  outcome process"
                },
                "summary": "In medical research, understanding changes in outcome measurements is crucial\nfor inferring shifts in health conditions. However, traditional methods often\nstruggle with large, irregularly longitudinal data and fail to account for the\ntendency of individuals in poorer health to interact more frequently with the\nhealthcare system. Additionally, clinical data can lack information on\nterminating events like death. To address these challenges, we start from the\ncontinuous-time hidden Markov model which models observed data as outcomes\ninfluenced by latent health states. Our extension incorporates a point process\nto account for the impact of health states on observation timings and includes\na \"death\" state to model unobserved terminating events through a Poisson\nprocess, where transition rates depend on the latent health state. This\napproach captures both the severity of the disease and the timing of healthcare\ninteractions. We present an exact Gibbs sampler procedure that alternates\nbetween sampling the latent health state paths and the model parameters. By\nincluding the \"death\" state, we mitigate biases in parameter estimation that\nwould arise from solely modelling \"live\" health states. Simulation studies\ndemonstrate that the proposed Gibbs sampler performs effectively. We apply our\nmethod to Canadian healthcare data, offering valuable insights for healthcare\nmanagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical research, understanding changes in outcome measurements is crucial\nfor inferring shifts in health conditions. However, traditional methods often\nstruggle with large, irregularly longitudinal data and fail to account for the\ntendency of individuals in poorer health to interact more frequently with the\nhealthcare system. Additionally, clinical data can lack information on\nterminating events like death. To address these challenges, we start from the\ncontinuous-time hidden Markov model which models observed data as outcomes\ninfluenced by latent health states. Our extension incorporates a point process\nto account for the impact of health states on observation timings and includes\na \"death\" state to model unobserved terminating events through a Poisson\nprocess, where transition rates depend on the latent health state. This\napproach captures both the severity of the disease and the timing of healthcare\ninteractions. We present an exact Gibbs sampler procedure that alternates\nbetween sampling the latent health state paths and the model parameters. By\nincluding the \"death\" state, we mitigate biases in parameter estimation that\nwould arise from solely modelling \"live\" health states. Simulation studies\ndemonstrate that the proposed Gibbs sampler performs effectively. We apply our\nmethod to Canadian healthcare data, offering valuable insights for healthcare\nmanagement."
                },
                "authors": [
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Chris Sherlock"
                    }
                ],
                "author_detail": {
                    "name": "Chris Sherlock"
                },
                "author": "Chris Sherlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06062v1",
                "updated": "2025-01-10T15:46:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    46,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:46:19Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    46,
                    19,
                    4,
                    10,
                    0
                ],
                "title": "Personalized Language Model Learning on Text Data Without User\n  Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Language Model Learning on Text Data Without User\n  Identifiers"
                },
                "summary": "In many practical natural language applications, user data are highly\nsensitive, requiring anonymous uploads of text data from mobile devices to the\ncloud without user identifiers. However, the absence of user identifiers\nrestricts the ability of cloud-based language models to provide personalized\nservices, which are essential for catering to diverse user needs. The trivial\nmethod of replacing an explicit user identifier with a static user embedding as\nmodel input still compromises data anonymization. In this work, we propose to\nlet each mobile device maintain a user-specific distribution to dynamically\ngenerate user embeddings, thereby breaking the one-to-one mapping between an\nembedding and a specific user. We further theoretically demonstrate that to\nprevent the cloud from tracking users via uploaded embeddings, the local\ndistributions of different users should either be derived from a linearly\ndependent space to avoid identifiability or be close to each other to prevent\naccurate attribution. Evaluation on both public and industrial datasets using\ndifferent language models reveals a remarkable improvement in accuracy from\nincorporating anonymous user embeddings, while preserving real-time inference\nrequirement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many practical natural language applications, user data are highly\nsensitive, requiring anonymous uploads of text data from mobile devices to the\ncloud without user identifiers. However, the absence of user identifiers\nrestricts the ability of cloud-based language models to provide personalized\nservices, which are essential for catering to diverse user needs. The trivial\nmethod of replacing an explicit user identifier with a static user embedding as\nmodel input still compromises data anonymization. In this work, we propose to\nlet each mobile device maintain a user-specific distribution to dynamically\ngenerate user embeddings, thereby breaking the one-to-one mapping between an\nembedding and a specific user. We further theoretically demonstrate that to\nprevent the cloud from tracking users via uploaded embeddings, the local\ndistributions of different users should either be derived from a linearly\ndependent space to avoid identifiability or be close to each other to prevent\naccurate attribution. Evaluation on both public and industrial datasets using\ndifferent language models reveals a remarkable improvement in accuracy from\nincorporating anonymous user embeddings, while preserving real-time inference\nrequirement."
                },
                "authors": [
                    {
                        "name": "Yucheng Ding"
                    },
                    {
                        "name": "Yangwenjian Tan"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05177v2",
                "updated": "2025-01-10T15:44:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    44,
                    28,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-09T11:52:54Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    11,
                    52,
                    54,
                    3,
                    9,
                    0
                ],
                "title": "FaceMe: Robust Blind Face Restoration with Personal Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceMe: Robust Blind Face Restoration with Personal Identification"
                },
                "summary": "Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blind face restoration is a highly ill-posed problem due to the lack of\nnecessary context. Although existing methods produce high-quality outputs, they\noften fail to faithfully preserve the individual's identity. In this paper, we\npropose a personalized face restoration method, FaceMe, based on a diffusion\nmodel. Given a single or a few reference images, we use an identity encoder to\nextract identity-related features, which serve as prompts to guide the\ndiffusion model in restoring high-quality and identity-consistent facial\nimages. By simply combining identity-related features, we effectively minimize\nthe impact of identity-irrelevant features during training and support any\nnumber of reference image inputs during inference. Additionally, thanks to the\nrobustness of the identity encoder, synthesized images can be used as reference\nimages during training, and identity changing during inference does not require\nfine-tuning the model. We also propose a pipeline for constructing a reference\nimage training pool that simulates the poses and expressions that may appear in\nreal-world scenarios. Experimental results demonstrate that our FaceMe can\nrestore high-quality facial images while maintaining identity consistency,\nachieving excellent performance and robustness."
                },
                "authors": [
                    {
                        "name": "Siyu Liu"
                    },
                    {
                        "name": "Zheng-Peng Duan"
                    },
                    {
                        "name": "Jia OuYang"
                    },
                    {
                        "name": "Jiayi Fu"
                    },
                    {
                        "name": "Hyunhee Park"
                    },
                    {
                        "name": "Zikun Liu"
                    },
                    {
                        "name": "Chun-Le Guo"
                    },
                    {
                        "name": "Chongyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongyi Li"
                },
                "author": "Chongyi Li",
                "arxiv_comment": "To appear at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06043v1",
                "updated": "2025-01-10T15:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    24,
                    10,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    24,
                    10,
                    4,
                    10,
                    0
                ],
                "title": "Axon: A novel systolic array architecture for improved run time and\n  energy efficient GeMM and Conv operation with on-chip im2col",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axon: A novel systolic array architecture for improved run time and\n  energy efficient GeMM and Conv operation with on-chip im2col"
                },
                "summary": "General matrix multiplication (GeMM) is a core operation in virtually all AI\napplications. Systolic array (SA) based architectures have shown great promise\nas GeMM hardware accelerators thanks to their speed and energy efficiency.\nUnfortunately, SAs incur a linear delay in filling the operands, due to\nunidirectional propagation via pipeline latches. In this work, we propose a\nnovel in-array data orchestration technique in SAs where we enable data feeding\non the principal diagonal followed by bi-directional propagation. This improves\nthe runtime by up to 2X at minimal hardware overhead. In addition, the proposed\ndata orchestration enables convolution lowering (known as im2col) using a\nsimple hardware support to fully exploit input feature map reuse opportunity\nand significantly lower the off-chip memory traffic resulting in 1.2X\nthroughput improvement and 2.17X inference energy reduction during YOLOv3 and\nRESNET50 workload on average. In contrast, conventional data orchestration\nwould require more elaborate hardware and control signals to implement im2col\nin hardware because of the data skew. We have synthesized and conducted place\nand route for 16X16 systolic arrays based on the novel and conventional\norchestrations using ASAP 7nm PDK and found that our proposed approach results\nin 0.211% area and 1.6% power overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General matrix multiplication (GeMM) is a core operation in virtually all AI\napplications. Systolic array (SA) based architectures have shown great promise\nas GeMM hardware accelerators thanks to their speed and energy efficiency.\nUnfortunately, SAs incur a linear delay in filling the operands, due to\nunidirectional propagation via pipeline latches. In this work, we propose a\nnovel in-array data orchestration technique in SAs where we enable data feeding\non the principal diagonal followed by bi-directional propagation. This improves\nthe runtime by up to 2X at minimal hardware overhead. In addition, the proposed\ndata orchestration enables convolution lowering (known as im2col) using a\nsimple hardware support to fully exploit input feature map reuse opportunity\nand significantly lower the off-chip memory traffic resulting in 1.2X\nthroughput improvement and 2.17X inference energy reduction during YOLOv3 and\nRESNET50 workload on average. In contrast, conventional data orchestration\nwould require more elaborate hardware and control signals to implement im2col\nin hardware because of the data skew. We have synthesized and conducted place\nand route for 16X16 systolic arrays based on the novel and conventional\norchestrations using ASAP 7nm PDK and found that our proposed approach results\nin 0.211% area and 1.6% power overheads."
                },
                "authors": [
                    {
                        "name": "Md Mizanur Rahaman Nayan"
                    },
                    {
                        "name": "Ritik Raj"
                    },
                    {
                        "name": "Gouse Basha Shaik"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Azad J Naeemi"
                    }
                ],
                "author_detail": {
                    "name": "Azad J Naeemi"
                },
                "author": "Azad J Naeemi",
                "arxiv_comment": "Accepted for Design Automation and Test in Europe (DATE), 2025. This\n  is preprint of the accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v3",
                "updated": "2025-01-10T15:08:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    8,
                    44,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06031v1",
                "updated": "2025-01-10T15:07:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    7,
                    57,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:07:57Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    7,
                    57,
                    4,
                    10,
                    0
                ],
                "title": "Generate, Transduct, Adapt: Iterative Transduction with VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate, Transduct, Adapt: Iterative Transduction with VLMs"
                },
                "summary": "Transductive zero-shot learning with vision-language models leverages\nimage-image similarities within the dataset to achieve better classification\naccuracy compared to the inductive setting. However, there is little work that\nexplores the structure of the language space in this context. We propose\nGTA-CLIP, a novel technique that incorporates supervision from language models\nfor joint transduction in language and vision spaces. Our approach is iterative\nand consists of three steps: (i) incrementally exploring the attribute space by\nquerying language models, (ii) an attribute-augmented transductive inference\nprocedure, and (iii) fine-tuning the language and vision encoders based on\ninferred labels within the dataset. Through experiments with CLIP encoders, we\ndemonstrate that GTA-CLIP, yields an average performance improvement of 8.6%\nand 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIP\nrespectively in the zero-shot setting. We also observe similar improvements in\na few-shot setting. We present ablation studies that demonstrate the value of\neach step and visualize how the vision and language spaces evolve over\niterations driven by the transductive learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transductive zero-shot learning with vision-language models leverages\nimage-image similarities within the dataset to achieve better classification\naccuracy compared to the inductive setting. However, there is little work that\nexplores the structure of the language space in this context. We propose\nGTA-CLIP, a novel technique that incorporates supervision from language models\nfor joint transduction in language and vision spaces. Our approach is iterative\nand consists of three steps: (i) incrementally exploring the attribute space by\nquerying language models, (ii) an attribute-augmented transductive inference\nprocedure, and (iii) fine-tuning the language and vision encoders based on\ninferred labels within the dataset. Through experiments with CLIP encoders, we\ndemonstrate that GTA-CLIP, yields an average performance improvement of 8.6%\nand 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIP\nrespectively in the zero-shot setting. We also observe similar improvements in\na few-shot setting. We present ablation studies that demonstrate the value of\neach step and visualize how the vision and language spaces evolve over\niterations driven by the transductive learning."
                },
                "authors": [
                    {
                        "name": "Oindrila Saha"
                    },
                    {
                        "name": "Logan Lawrence"
                    },
                    {
                        "name": "Grant Van Horn"
                    },
                    {
                        "name": "Subhransu Maji"
                    }
                ],
                "author_detail": {
                    "name": "Subhransu Maji"
                },
                "author": "Subhransu Maji",
                "arxiv_comment": "Code will be released at https://github.com/cvl-umass/GTA-CLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06024v1",
                "updated": "2025-01-10T15:01:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    1,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:01:50Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    1,
                    50,
                    4,
                    10,
                    0
                ],
                "title": "Doubly-Robust Functional Average Treatment Effect Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly-Robust Functional Average Treatment Effect Estimation"
                },
                "summary": "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset."
                },
                "authors": [
                    {
                        "name": "Lorenzo Testa"
                    },
                    {
                        "name": "Tobia Boschi"
                    },
                    {
                        "name": "Francesca Chiaromonte"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Matthew Reimherr"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Reimherr"
                },
                "author": "Matthew Reimherr",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06022v1",
                "updated": "2025-01-10T15:01:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    1,
                    20,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:01:20Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    1,
                    20,
                    4,
                    10,
                    0
                ],
                "title": "Modern Bayesian Sampling Methods for Cosmological Inference: A\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Bayesian Sampling Methods for Cosmological Inference: A\n  Comparative Study"
                },
                "summary": "We present a comprehensive comparison of different Markov Chain Monte Carlo\n(MCMC) sampling methods, evaluating their performance on both standard test\nproblems and cosmological parameter estimation. Our analysis includes\ntraditional Metropolis-Hastings MCMC, Hamiltonian Monte Carlo (HMC), slice\nsampling, nested sampling as implemented in dynesty, and PolyChord. We examine\nsamplers through multiple metrics including runtime, memory usage, effective\nsample size, and parameter accuracy, testing their scaling with dimension and\nresponse to different probability distributions. While all samplers perform\nwell with simple Gaussian distributions, we find that HMC and nested sampling\nshow advantages for more complex distributions typical of cosmological\nproblems. Traditional MCMC and slice sampling become less efficient in higher\ndimensions, while nested methods maintain accuracy but at higher computational\ncost. In cosmological applications using BAO data, we observe similar patterns,\nwith particular challenges arising from parameter degeneracies and poorly\nconstrained parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive comparison of different Markov Chain Monte Carlo\n(MCMC) sampling methods, evaluating their performance on both standard test\nproblems and cosmological parameter estimation. Our analysis includes\ntraditional Metropolis-Hastings MCMC, Hamiltonian Monte Carlo (HMC), slice\nsampling, nested sampling as implemented in dynesty, and PolyChord. We examine\nsamplers through multiple metrics including runtime, memory usage, effective\nsample size, and parameter accuracy, testing their scaling with dimension and\nresponse to different probability distributions. While all samplers perform\nwell with simple Gaussian distributions, we find that HMC and nested sampling\nshow advantages for more complex distributions typical of cosmological\nproblems. Traditional MCMC and slice sampling become less efficient in higher\ndimensions, while nested methods maintain accuracy but at higher computational\ncost. In cosmological applications using BAO data, we observe similar patterns,\nwith particular challenges arising from parameter degeneracies and poorly\nconstrained parameters."
                },
                "authors": [
                    {
                        "name": "Denitsa Staicova"
                    }
                ],
                "author_detail": {
                    "name": "Denitsa Staicova"
                },
                "author": "Denitsa Staicova",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17437v2",
                "updated": "2025-01-10T14:59:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    59,
                    16,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-26T13:51:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems"
                },
                "summary": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners."
                },
                "authors": [
                    {
                        "name": "Mireia Hernandez Caralt"
                    },
                    {
                        "name": "Ivan Sekulić"
                    },
                    {
                        "name": "Filip Carević"
                    },
                    {
                        "name": "Nghia Khau"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Bruna Guedes"
                    },
                    {
                        "name": "Victor Guimarães"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Andre Manso"
                    },
                    {
                        "name": "Meghana Reddy"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Roland Mathis"
                    }
                ],
                "author_detail": {
                    "name": "Roland Mathis"
                },
                "author": "Roland Mathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16745v2",
                "updated": "2025-01-10T14:40:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    40,
                    49,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-21T19:41:10Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    19,
                    41,
                    10,
                    5,
                    356,
                    0
                ],
                "title": "ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for\n  Disparity Map Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for\n  Disparity Map Generation"
                },
                "summary": "In this work we propose a Visual Mamba (ViM) based architecture, to dissolve\nthe existing trade-off for real-time and accurate model with low computation\noverhead for disparity map generation (DMG). Moreover, we proposed a\nperformance measure that can jointly evaluate the inference speed, computation\noverhead and the accurateness of a DMG model. The code implementation and\ncorresponding models are available at: https://github.com/MBora/ViM-Disparity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we propose a Visual Mamba (ViM) based architecture, to dissolve\nthe existing trade-off for real-time and accurate model with low computation\noverhead for disparity map generation (DMG). Moreover, we proposed a\nperformance measure that can jointly evaluate the inference speed, computation\noverhead and the accurateness of a DMG model. The code implementation and\ncorresponding models are available at: https://github.com/MBora/ViM-Disparity."
                },
                "authors": [
                    {
                        "name": "Maheswar Bora"
                    },
                    {
                        "name": "Tushar Anand"
                    },
                    {
                        "name": "Saurabh Atreya"
                    },
                    {
                        "name": "Aritra Mukherjee"
                    },
                    {
                        "name": "Abhijit Das"
                    }
                ],
                "author_detail": {
                    "name": "Abhijit Das"
                },
                "author": "Abhijit Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04127v3",
                "updated": "2025-01-10T14:31:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    31,
                    21,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-06T14:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    14,
                    49,
                    6,
                    3,
                    158,
                    0
                ],
                "title": "Are We Done with MMLU?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We Done with MMLU?"
                },
                "summary": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0."
                },
                "authors": [
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Alberto Carlo Maria Mancino"
                    },
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Xiaotang Du"
                    },
                    {
                        "name": "Mohammad Reza Ghasemi Madani"
                    },
                    {
                        "name": "Claire Barale"
                    },
                    {
                        "name": "Robert McHardy"
                    },
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Emile van Krieken"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05989v1",
                "updated": "2025-01-10T14:20:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    20,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:20:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    20,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Addressing speaker gender bias in large scale speech translation systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing speaker gender bias in large scale speech translation systems"
                },
                "summary": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set."
                },
                "authors": [
                    {
                        "name": "Shubham Bansal"
                    },
                    {
                        "name": "Vikas Joshi"
                    },
                    {
                        "name": "Harveen Chadha"
                    },
                    {
                        "name": "Rupeshkumar Mehta"
                    },
                    {
                        "name": "Jinyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Li"
                },
                "author": "Jinyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05985v1",
                "updated": "2025-01-10T14:17:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys"
                },
                "summary": "Designing culturally relevant questionnaires for ICTD research is\nchallenging, particularly when adapting surveys for populations to non-western\ncontexts. Prior work adapted questionnaires through expert reviews and pilot\nstudies, which are resource-intensive and time-consuming. To address these\nchallenges, we propose using large language models (LLMs) to automate the\nquestionnaire pretesting process in cross-cultural settings. Our study used\nLLMs to adapt a U.S.-focused climate opinion survey for a South African\naudience. We then tested the adapted questionnaire with 116 South African\nparticipants via Prolific, asking them to provide feedback on both versions.\nParticipants perceived the LLM-adapted questions as slightly more favorable\nthan the traditional version. Our note opens discussions on the potential role\nof LLMs in adapting surveys and facilitating cross-cultural questionnaire\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing culturally relevant questionnaires for ICTD research is\nchallenging, particularly when adapting surveys for populations to non-western\ncontexts. Prior work adapted questionnaires through expert reviews and pilot\nstudies, which are resource-intensive and time-consuming. To address these\nchallenges, we propose using large language models (LLMs) to automate the\nquestionnaire pretesting process in cross-cultural settings. Our study used\nLLMs to adapt a U.S.-focused climate opinion survey for a South African\naudience. We then tested the adapted questionnaire with 116 South African\nparticipants via Prolific, asking them to provide feedback on both versions.\nParticipants perceived the LLM-adapted questions as slightly more favorable\nthan the traditional version. Our note opens discussions on the potential role\nof LLMs in adapting surveys and facilitating cross-cultural questionnaire\ndesign."
                },
                "authors": [
                    {
                        "name": "Divya Mani Adhikari"
                    },
                    {
                        "name": "Vikram Kamath Cannanure"
                    },
                    {
                        "name": "Alexander Hartland"
                    },
                    {
                        "name": "Ingmar Weber"
                    }
                ],
                "author_detail": {
                    "name": "Ingmar Weber"
                },
                "author": "Ingmar Weber",
                "arxiv_comment": "Accepted to ICTD 2024 (Notes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06342v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06342v3",
                "updated": "2025-01-10T14:16:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    16,
                    36,
                    4,
                    10,
                    0
                ],
                "published": "2024-10-08T20:25:09Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    20,
                    25,
                    9,
                    1,
                    282,
                    0
                ],
                "title": "Describing Hadronization via Histories and Observables for Monte-Carlo\n  Event Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Describing Hadronization via Histories and Observables for Monte-Carlo\n  Event Reweighting"
                },
                "summary": "We introduce a novel method for extracting a fragmentation model directly\nfrom experimental data without requiring an explicit parametric form, called\nHistories and Observables for Monte-Carlo Event Reweighting (HOMER), consisting\nof three steps: the training of a classifier between simulation and data, the\ninference of single fragmentation weights, and the calculation of the weight\nfor the full hadronization chain. We illustrate the use of HOMER on a\nsimplified hadronization problem, a $q\\bar{q}$ string fragmenting into pions,\nand extract a modified Lund string fragmentation function $f(z)$. We then\ndemonstrate the use of HOMER on three types of experimental data: (i) binned\ndistributions of high level observables, (ii) unbinned event-by-event\ndistributions of these observables, and (iii) full particle cloud information.\nAfter demonstrating that $f(z)$ can be extracted from data (the inverse of\nhadronization), we also show that, at least in this limited setup, the fidelity\nof the extracted $f(z)$ suffers only limited loss when moving from (i) to (ii)\nto (iii). Public code is available at https://gitlab.com/uchep/mlhad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel method for extracting a fragmentation model directly\nfrom experimental data without requiring an explicit parametric form, called\nHistories and Observables for Monte-Carlo Event Reweighting (HOMER), consisting\nof three steps: the training of a classifier between simulation and data, the\ninference of single fragmentation weights, and the calculation of the weight\nfor the full hadronization chain. We illustrate the use of HOMER on a\nsimplified hadronization problem, a $q\\bar{q}$ string fragmenting into pions,\nand extract a modified Lund string fragmentation function $f(z)$. We then\ndemonstrate the use of HOMER on three types of experimental data: (i) binned\ndistributions of high level observables, (ii) unbinned event-by-event\ndistributions of these observables, and (iii) full particle cloud information.\nAfter demonstrating that $f(z)$ can be extracted from data (the inverse of\nhadronization), we also show that, at least in this limited setup, the fidelity\nof the extracted $f(z)$ suffers only limited loss when moving from (i) to (ii)\nto (iii). Public code is available at https://gitlab.com/uchep/mlhad."
                },
                "authors": [
                    {
                        "name": "Christian Bierlich"
                    },
                    {
                        "name": "Phil Ilten"
                    },
                    {
                        "name": "Tony Menzo"
                    },
                    {
                        "name": "Stephen Mrenna"
                    },
                    {
                        "name": "Manuel Szewc"
                    },
                    {
                        "name": "Michael K. Wilkinson"
                    },
                    {
                        "name": "Ahmed Youssef"
                    },
                    {
                        "name": "Jure Zupan"
                    }
                ],
                "author_detail": {
                    "name": "Jure Zupan"
                },
                "author": "Jure Zupan",
                "arxiv_comment": "42 pages, 21 figures. Updated version with minor revision recommended\n  by SciPost Physics. Public code available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06342v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06342v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05981v1",
                "updated": "2025-01-10T14:08:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    8,
                    59,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:08:59Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    8,
                    59,
                    4,
                    10,
                    0
                ],
                "title": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study\n  of LLM Hallucination on North Korea",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study\n  of LLM Hallucination on North Korea"
                },
                "summary": "Hallucination in large language models (LLMs) remains a significant challenge\nfor their safe deployment, particularly due to its potential to spread\nmisinformation. Most existing solutions address this challenge by focusing on\naligning the models with credible sources or by improving how models\ncommunicate their confidence (or lack thereof) in their outputs. While these\nmeasures may be effective in most contexts, they may fall short in scenarios\nrequiring more nuanced approaches, especially in situations where access to\naccurate data is limited or determining credible sources is challenging. In\nthis study, we take North Korea - a country characterised by an extreme lack of\nreliable sources and the prevalence of sensationalist falsehoods - as a case\nstudy. We explore and evaluate how some of the best-performing multilingual\nLLMs and specific language-based models generate information about North Korea\nin three languages spoken in countries with significant geo-political\ninterests: English (United States, United Kingdom), Korean (South Korea), and\nMandarin Chinese (China). Our findings reveal significant differences,\nsuggesting that the choice of model and language can lead to vastly different\nunderstandings of North Korea, which has important implications given the\nglobal security challenges the country poses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination in large language models (LLMs) remains a significant challenge\nfor their safe deployment, particularly due to its potential to spread\nmisinformation. Most existing solutions address this challenge by focusing on\naligning the models with credible sources or by improving how models\ncommunicate their confidence (or lack thereof) in their outputs. While these\nmeasures may be effective in most contexts, they may fall short in scenarios\nrequiring more nuanced approaches, especially in situations where access to\naccurate data is limited or determining credible sources is challenging. In\nthis study, we take North Korea - a country characterised by an extreme lack of\nreliable sources and the prevalence of sensationalist falsehoods - as a case\nstudy. We explore and evaluate how some of the best-performing multilingual\nLLMs and specific language-based models generate information about North Korea\nin three languages spoken in countries with significant geo-political\ninterests: English (United States, United Kingdom), Korean (South Korea), and\nMandarin Chinese (China). Our findings reveal significant differences,\nsuggesting that the choice of model and language can lead to vastly different\nunderstandings of North Korea, which has important implications given the\nglobal security challenges the country poses."
                },
                "authors": [
                    {
                        "name": "Eunjung Cho"
                    },
                    {
                        "name": "Won Ik Cho"
                    },
                    {
                        "name": "Soomin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Soomin Seo"
                },
                "author": "Soomin Seo",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05965v1",
                "updated": "2025-01-10T13:47:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    47,
                    13,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T13:47:13Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    47,
                    13,
                    4,
                    10,
                    0
                ],
                "title": "Model Inversion in Split Learning for Personalized LLMs: New Insights\n  from Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Inversion in Split Learning for Personalized LLMs: New Insights\n  from Information Bottleneck Theory"
                },
                "summary": "Personalized Large Language Models (LLMs) have become increasingly prevalent,\nshowcasing the impressive capabilities of models like GPT-4. This trend has\nalso catalyzed extensive research on deploying LLMs on mobile devices. Feasible\napproaches for such edge-cloud deployment include using split learning.\nHowever, previous research has largely overlooked the privacy leakage\nassociated with intermediate representations transmitted from devices to\nservers. This work is the first to identify model inversion attacks in the\nsplit learning framework for LLMs, emphasizing the necessity of secure defense.\nFor the first time, we introduce mutual information entropy to understand the\ninformation propagation of Transformer-based LLMs and assess privacy attack\nperformance for LLM blocks. To address the issue of representations being\nsparser and containing less information than embeddings, we propose a two-stage\nattack system in which the first part projects representations into the\nembedding space, and the second part uses a generative model to recover text\nfrom these embeddings. This design breaks down the complexity and achieves\nattack scores of 38%-75% in various scenarios, with an over 60% improvement\nover the SOTA. This work comprehensively highlights the potential privacy risks\nduring the deployment of personalized LLMs on the edge side.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models (LLMs) have become increasingly prevalent,\nshowcasing the impressive capabilities of models like GPT-4. This trend has\nalso catalyzed extensive research on deploying LLMs on mobile devices. Feasible\napproaches for such edge-cloud deployment include using split learning.\nHowever, previous research has largely overlooked the privacy leakage\nassociated with intermediate representations transmitted from devices to\nservers. This work is the first to identify model inversion attacks in the\nsplit learning framework for LLMs, emphasizing the necessity of secure defense.\nFor the first time, we introduce mutual information entropy to understand the\ninformation propagation of Transformer-based LLMs and assess privacy attack\nperformance for LLM blocks. To address the issue of representations being\nsparser and containing less information than embeddings, we propose a two-stage\nattack system in which the first part projects representations into the\nembedding space, and the second part uses a generative model to recover text\nfrom these embeddings. This design breaks down the complexity and achieves\nattack scores of 38%-75% in various scenarios, with an over 60% improvement\nover the SOTA. This work comprehensively highlights the potential privacy risks\nduring the deployment of personalized LLMs on the edge side."
                },
                "authors": [
                    {
                        "name": "Yunmeng Shu"
                    },
                    {
                        "name": "Shaofeng Li"
                    },
                    {
                        "name": "Tian Dong"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Haojin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haojin Zhu"
                },
                "author": "Haojin Zhu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.10798v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.10798v5",
                "updated": "2025-01-10T13:43:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    43,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2023-02-17T09:37:17Z",
                "published_parsed": [
                    2023,
                    2,
                    17,
                    9,
                    37,
                    17,
                    4,
                    48,
                    0
                ],
                "title": "Learning a Consensus Sub-Network with Polarization Regularization and\n  One Pass Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a Consensus Sub-Network with Polarization Regularization and\n  One Pass Training"
                },
                "summary": "The subject of green AI has been gaining attention within the deep learning\ncommunity given the recent trend of ever larger and more complex neural network\nmodels. Existing solutions for reducing the computational load of training at\ninference time usually involve pruning the network parameters. Pruning schemes\noften create extra overhead either by iterative training and fine-tuning for\nstatic pruning or repeated computation of a dynamic pruning graph. We propose a\nnew parameter pruning strategy for learning a lighter-weight sub-network that\nminimizes the energy cost while maintaining comparable performance to the fully\nparameterised network on given downstream tasks. Our proposed pruning scheme is\ngreen-oriented, as it only requires a one-off training to discover the optimal\nstatic sub-networks by dynamic pruning methods. The pruning scheme consists of\na binary gating module and a polarizing loss function to uncover sub-networks\nwith user-defined sparsity. Our method enables pruning and training\nsimultaneously, which saves energy in both the training and inference phases\nand avoids extra computational overhead from gating modules at inference time.\nOur results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme\ncan remove 50% of connections in deep networks with <1% reduction in\nclassification accuracy. Compared to other related pruning methods, our method\ndemonstrates a lower drop in accuracy for equivalent reductions in\ncomputational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The subject of green AI has been gaining attention within the deep learning\ncommunity given the recent trend of ever larger and more complex neural network\nmodels. Existing solutions for reducing the computational load of training at\ninference time usually involve pruning the network parameters. Pruning schemes\noften create extra overhead either by iterative training and fine-tuning for\nstatic pruning or repeated computation of a dynamic pruning graph. We propose a\nnew parameter pruning strategy for learning a lighter-weight sub-network that\nminimizes the energy cost while maintaining comparable performance to the fully\nparameterised network on given downstream tasks. Our proposed pruning scheme is\ngreen-oriented, as it only requires a one-off training to discover the optimal\nstatic sub-networks by dynamic pruning methods. The pruning scheme consists of\na binary gating module and a polarizing loss function to uncover sub-networks\nwith user-defined sparsity. Our method enables pruning and training\nsimultaneously, which saves energy in both the training and inference phases\nand avoids extra computational overhead from gating modules at inference time.\nOur results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme\ncan remove 50% of connections in deep networks with <1% reduction in\nclassification accuracy. Compared to other related pruning methods, our method\ndemonstrates a lower drop in accuracy for equivalent reductions in\ncomputational cost."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhi"
                    },
                    {
                        "name": "Varun Babbar"
                    },
                    {
                        "name": "Rundong Liu"
                    },
                    {
                        "name": "Pheobe Sun"
                    },
                    {
                        "name": "Fran Silavong"
                    },
                    {
                        "name": "Ruibo Shi"
                    },
                    {
                        "name": "Sean Moran"
                    }
                ],
                "author_detail": {
                    "name": "Sean Moran"
                },
                "author": "Sean Moran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.10798v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.10798v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05959v1",
                "updated": "2025-01-10T13:40:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    40,
                    32,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T13:40:32Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    40,
                    32,
                    4,
                    10,
                    0
                ],
                "title": "Estimation and Restoration of Unknown Nonlinear Distortion using\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Restoration of Unknown Nonlinear Distortion using\n  Diffusion"
                },
                "summary": "The restoration of nonlinearly distorted audio signals, alongside the\nidentification of the applied memoryless nonlinear operation, is studied. The\npaper focuses on the difficult but practically important case in which both the\nnonlinearity and the original input signal are unknown. The proposed method\nuses a generative diffusion model trained unconditionally on guitar or speech\nsignals to jointly model and invert the nonlinear system at inference time.\nBoth the memoryless nonlinear function model and the restored audio signal are\nobtained as output. Successful example case studies are presented including\ninversion of hard and soft clipping, digital quantization, half-wave\nrectification, and wavefolding nonlinearities. Our results suggest that, out of\nthe nonlinear functions tested here, the cubic Catmull-Rom spline is best\nsuited to approximating these nonlinearities. In the case of guitar recordings,\ncomparisons with informed and supervised methods show that the proposed blind\nmethod is at least as good as they are in terms of objective metrics.\nExperiments on distorted speech show that the proposed blind method outperforms\ngeneral-purpose speech enhancement techniques and restores the original voice\nquality. The proposed method can be applied to audio effects modeling,\nrestoration of music and speech recordings, and characterization of analog\nrecording media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The restoration of nonlinearly distorted audio signals, alongside the\nidentification of the applied memoryless nonlinear operation, is studied. The\npaper focuses on the difficult but practically important case in which both the\nnonlinearity and the original input signal are unknown. The proposed method\nuses a generative diffusion model trained unconditionally on guitar or speech\nsignals to jointly model and invert the nonlinear system at inference time.\nBoth the memoryless nonlinear function model and the restored audio signal are\nobtained as output. Successful example case studies are presented including\ninversion of hard and soft clipping, digital quantization, half-wave\nrectification, and wavefolding nonlinearities. Our results suggest that, out of\nthe nonlinear functions tested here, the cubic Catmull-Rom spline is best\nsuited to approximating these nonlinearities. In the case of guitar recordings,\ncomparisons with informed and supervised methods show that the proposed blind\nmethod is at least as good as they are in terms of objective metrics.\nExperiments on distorted speech show that the proposed blind method outperforms\ngeneral-purpose speech enhancement techniques and restores the original voice\nquality. The proposed method can be applied to audio effects modeling,\nrestoration of music and speech recordings, and characterization of analog\nrecording media."
                },
                "authors": [
                    {
                        "name": "Michal Švento"
                    },
                    {
                        "name": "Eloi Moliner"
                    },
                    {
                        "name": "Lauri Juvela"
                    },
                    {
                        "name": "Alec Wright"
                    },
                    {
                        "name": "Vesa Välimäki"
                    }
                ],
                "author_detail": {
                    "name": "Vesa Välimäki"
                },
                "author": "Vesa Välimäki",
                "arxiv_comment": "Submitted to the Journal of Audio Engineering Society, special issue\n  \"The Sound of Digital Audio Effects\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11698v2",
                "updated": "2025-01-10T13:35:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    35,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-16T12:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?"
                },
                "summary": "Context. The security of critical infrastructure has been a pressing concern\nsince the advent of computers and has become even more critical in today's era\nof cyber warfare. Protecting mission-critical systems (MCSs), essential for\nnational security, requires swift and robust governance, yet recent events\nreveal the increasing difficulty of meeting these challenges. Aim. Building on\nprior research showcasing the potential of Generative AI (GAI), such as Large\nLanguage Models, in enhancing risk analysis, we aim to explore practitioners'\nviews on integrating GAI into the governance of IT MCSs. Our goal is to provide\nactionable insights and recommendations for stakeholders, including\nresearchers, practitioners, and policymakers. Method. We designed a survey to\ncollect practical experiences, concerns, and expectations of practitioners who\ndevelop and implement security solutions in the context of MCSs. Conclusions\nand Future Works. Our findings highlight that the safe use of LLMs in MCS\ngovernance requires interdisciplinary collaboration. Researchers should focus\non designing regulation-oriented models and focus on accountability;\npractitioners emphasize data protection and transparency, while policymakers\nmust establish a unified AI framework with global benchmarks to ensure ethical\nand secure LLMs-based MCS governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The security of critical infrastructure has been a pressing concern\nsince the advent of computers and has become even more critical in today's era\nof cyber warfare. Protecting mission-critical systems (MCSs), essential for\nnational security, requires swift and robust governance, yet recent events\nreveal the increasing difficulty of meeting these challenges. Aim. Building on\nprior research showcasing the potential of Generative AI (GAI), such as Large\nLanguage Models, in enhancing risk analysis, we aim to explore practitioners'\nviews on integrating GAI into the governance of IT MCSs. Our goal is to provide\nactionable insights and recommendations for stakeholders, including\nresearchers, practitioners, and policymakers. Method. We designed a survey to\ncollect practical experiences, concerns, and expectations of practitioners who\ndevelop and implement security solutions in the context of MCSs. Conclusions\nand Future Works. Our findings highlight that the safe use of LLMs in MCS\ngovernance requires interdisciplinary collaboration. Researchers should focus\non designing regulation-oriented models and focus on accountability;\npractitioners emphasize data protection and transparency, while policymakers\nmust establish a unified AI framework with global benchmarks to ensure ethical\nand secure LLMs-based MCS governance."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Francesco Palagiano"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    },
                    {
                        "name": "Davide Taibi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Taibi"
                },
                "author": "Davide Taibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09352v2",
                "updated": "2025-01-10T13:23:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    23,
                    58,
                    4,
                    10,
                    0
                ],
                "published": "2024-09-14T07:46:28Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    7,
                    46,
                    28,
                    5,
                    258,
                    0
                ],
                "title": "MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent\n  Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent\n  Conversion"
                },
                "summary": "In accented voice conversion or accent conversion, we seek to convert the\naccent in speech from one another while preserving speaker identity and\nsemantic content. In this study, we formulate a novel method for creating\nmulti-accented speech samples, thus pairs of accented speech samples by the\nsame speaker, through text transliteration for training accent conversion\nsystems. We begin by generating transliterated text with Large Language Models\n(LLMs), which is then fed into multilingual TTS models to synthesize accented\nEnglish speech. As a reference system, we built a sequence-to-sequence model on\nthe synthetic parallel corpus for accent conversion. We validated the proposed\nmethod for both native and non-native English speakers. Subjective and\nobjective evaluations further validate our dataset's effectiveness in accent\nconversion studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In accented voice conversion or accent conversion, we seek to convert the\naccent in speech from one another while preserving speaker identity and\nsemantic content. In this study, we formulate a novel method for creating\nmulti-accented speech samples, thus pairs of accented speech samples by the\nsame speaker, through text transliteration for training accent conversion\nsystems. We begin by generating transliterated text with Large Language Models\n(LLMs), which is then fed into multilingual TTS models to synthesize accented\nEnglish speech. As a reference system, we built a sequence-to-sequence model on\nthe synthetic parallel corpus for accent conversion. We validated the proposed\nmethod for both native and non-native English speakers. Subjective and\nobjective evaluations further validate our dataset's effectiveness in accent\nconversion studies."
                },
                "authors": [
                    {
                        "name": "Sho Inoue"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Wanxing Wang"
                    },
                    {
                        "name": "Pengcheng Zhu"
                    },
                    {
                        "name": "Mengxiao Bi"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "This is accepted to IEEE ICASSP 2025; Project page with Speech Demo:\n  https://github.com/shinshoji01/MacST-project-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11629v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11629v5",
                "updated": "2025-01-10T13:23:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    23,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-17T15:11:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    11,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study"
                },
                "summary": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Yue Pan"
                    }
                ],
                "author_detail": {
                    "name": "Yue Pan"
                },
                "author": "Yue Pan",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11629v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11629v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12498v2",
                "updated": "2025-01-10T13:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    21,
                    57,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-17T03:02:05Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    2,
                    5,
                    1,
                    352,
                    0
                ],
                "title": "Hierarchical Control of Emotion Rendering in Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Control of Emotion Rendering in Speech Synthesis"
                },
                "summary": "Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional\nspeech from input text. However, quantitatively controlling multi-level emotion\nrendering remains challenging. In this paper, we propose a diffusion-based\nemotional TTS framework with a novel approach for emotion intensity modeling to\nfacilitate fine-grained control over emotion rendering at the phoneme, word,\nand utterance levels. We introduce a hierarchical emotion distribution (ED)\nextractor that captures a quantifiable ED embedding across different speech\nsegment levels. Additionally, we explore various acoustic features and assess\ntheir impact on emotion intensity modeling. During TTS training, the\nhierarchical ED embedding effectively captures the variance in emotion\nintensity from the reference audio and correlates it with linguistic and\nspeaker information. The TTS model not only generates emotional speech during\ninference, but also quantitatively controls the emotion rendering over the\nspeech constituents. Both objective and subjective evaluations demonstrate the\neffectiveness of our framework in terms of speech quality, emotional\nexpressiveness, and hierarchical emotion control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional\nspeech from input text. However, quantitatively controlling multi-level emotion\nrendering remains challenging. In this paper, we propose a diffusion-based\nemotional TTS framework with a novel approach for emotion intensity modeling to\nfacilitate fine-grained control over emotion rendering at the phoneme, word,\nand utterance levels. We introduce a hierarchical emotion distribution (ED)\nextractor that captures a quantifiable ED embedding across different speech\nsegment levels. Additionally, we explore various acoustic features and assess\ntheir impact on emotion intensity modeling. During TTS training, the\nhierarchical ED embedding effectively captures the variance in emotion\nintensity from the reference audio and correlates it with linguistic and\nspeaker information. The TTS model not only generates emotional speech during\ninference, but also quantitatively controls the emotion rendering over the\nspeech constituents. Both objective and subjective evaluations demonstrate the\neffectiveness of our framework in terms of speech quality, emotional\nexpressiveness, and hierarchical emotion control."
                },
                "authors": [
                    {
                        "name": "Sho Inoue"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Submitted to IEEE Transactions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05945v1",
                "updated": "2025-01-10T13:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    15,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T13:15:37Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    15,
                    37,
                    4,
                    10,
                    0
                ],
                "title": "Reusable specimen-level inference in computational pathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusable specimen-level inference in computational pathology"
                },
                "summary": "Foundation models for computational pathology have shown great promise for\nspecimen-level tasks and are increasingly accessible to researchers. However,\nspecimen-level models built on these foundation models remain largely\nunavailable, hindering their broader utility and impact. To address this gap,\nwe developed SpinPath, a toolkit designed to democratize specimen-level deep\nlearning by providing a zoo of pretrained specimen-level models, a Python-based\ninference engine, and a JavaScript-based inference platform. We demonstrate the\nutility of SpinPath in metastasis detection tasks across nine foundation\nmodels. SpinPath may foster reproducibility, simplify experimentation, and\naccelerate the adoption of specimen-level deep learning in computational\npathology research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models for computational pathology have shown great promise for\nspecimen-level tasks and are increasingly accessible to researchers. However,\nspecimen-level models built on these foundation models remain largely\nunavailable, hindering their broader utility and impact. To address this gap,\nwe developed SpinPath, a toolkit designed to democratize specimen-level deep\nlearning by providing a zoo of pretrained specimen-level models, a Python-based\ninference engine, and a JavaScript-based inference platform. We demonstrate the\nutility of SpinPath in metastasis detection tasks across nine foundation\nmodels. SpinPath may foster reproducibility, simplify experimentation, and\naccelerate the adoption of specimen-level deep learning in computational\npathology research."
                },
                "authors": [
                    {
                        "name": "Jakub R. Kaczmarzyk"
                    },
                    {
                        "name": "Rishul Sharma"
                    },
                    {
                        "name": "Peter K. Koo"
                    },
                    {
                        "name": "Joel H. Saltz"
                    }
                ],
                "author_detail": {
                    "name": "Joel H. Saltz"
                },
                "author": "Joel H. Saltz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03056v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03056v4",
                "updated": "2025-01-10T13:01:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    1,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2023-11-06T12:22:19Z",
                "published_parsed": [
                    2023,
                    11,
                    6,
                    12,
                    22,
                    19,
                    0,
                    310,
                    0
                ],
                "title": "LitSumm: Large language models for literature summarisation of\n  non-coding RNAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitSumm: Large language models for literature summarisation of\n  non-coding RNAs"
                },
                "summary": "Curation of literature in life sciences is a growing challenge. The continued\nincrease in the rate of publication, coupled with the relatively fixed number\nof curators worldwide presents a major challenge to developers of biomedical\nknowledgebases. Very few knowledgebases have resources to scale to the whole\nrelevant literature and all have to prioritise their efforts.\n  In this work, we take a first step to alleviating the lack of curator time in\nRNA science by generating summaries of literature for non-coding RNAs using\nlarge language models (LLMs). We demonstrate that high-quality, factually\naccurate summaries with accurate references can be automatically generated from\nthe literature using a commercial LLM and a chain of prompts and checks. Manual\nassessment was carried out for a subset of summaries, with the majority being\nrated extremely high quality.\n  We apply our tool to a selection of over 4,600 ncRNAs and make the generated\nsummaries available via the RNAcentral resource. We conclude that automated\nliterature summarization is feasible with the current generation of LLMs,\nprovided careful prompting and automated checking are applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curation of literature in life sciences is a growing challenge. The continued\nincrease in the rate of publication, coupled with the relatively fixed number\nof curators worldwide presents a major challenge to developers of biomedical\nknowledgebases. Very few knowledgebases have resources to scale to the whole\nrelevant literature and all have to prioritise their efforts.\n  In this work, we take a first step to alleviating the lack of curator time in\nRNA science by generating summaries of literature for non-coding RNAs using\nlarge language models (LLMs). We demonstrate that high-quality, factually\naccurate summaries with accurate references can be automatically generated from\nthe literature using a commercial LLM and a chain of prompts and checks. Manual\nassessment was carried out for a subset of summaries, with the majority being\nrated extremely high quality.\n  We apply our tool to a selection of over 4,600 ncRNAs and make the generated\nsummaries available via the RNAcentral resource. We conclude that automated\nliterature summarization is feasible with the current generation of LLMs,\nprovided careful prompting and automated checking are applied."
                },
                "authors": [
                    {
                        "name": "Andrew Green"
                    },
                    {
                        "name": "Carlos Ribas"
                    },
                    {
                        "name": "Nancy Ontiveros-Palacios"
                    },
                    {
                        "name": "Sam Griffiths-Jones"
                    },
                    {
                        "name": "Anton I. Petrov"
                    },
                    {
                        "name": "Alex Bateman"
                    },
                    {
                        "name": "Blake Sweeney"
                    }
                ],
                "author_detail": {
                    "name": "Blake Sweeney"
                },
                "author": "Blake Sweeney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.03056v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03056v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05933v1",
                "updated": "2025-01-10T12:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    56,
                    18,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    56,
                    18,
                    4,
                    10,
                    0
                ],
                "title": "Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2"
                },
                "summary": "Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy."
                },
                "authors": [
                    {
                        "name": "Olivier Morelle"
                    },
                    {
                        "name": "Justus Bisten"
                    },
                    {
                        "name": "Maximilian W. M. Wintergerst"
                    },
                    {
                        "name": "Robert P. Finger"
                    },
                    {
                        "name": "Thomas Schultz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schultz"
                },
                "arxiv_affiliation": "Lamarr Institute for Machine Learning and Artificial Intelligence",
                "author": "Thomas Schultz",
                "arxiv_comment": "7 pages, 1 figure, accepted at German Conference on Medical Image\n  Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05926v1",
                "updated": "2025-01-10T12:46:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities"
                },
                "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case."
                },
                "authors": [
                    {
                        "name": "Ruby Ostrow"
                    },
                    {
                        "name": "Adam Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Adam Lopez"
                },
                "author": "Adam Lopez",
                "arxiv_comment": "10 pages, 8 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05925v1",
                "updated": "2025-01-10T12:44:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    44,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:44:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    44,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction"
                },
                "summary": "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements."
                },
                "authors": [
                    {
                        "name": "Petraq Nako"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20918v2",
                "updated": "2025-01-10T12:15:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    15,
                    6,
                    4,
                    10,
                    0
                ],
                "published": "2024-05-31T15:21:59Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    21,
                    59,
                    4,
                    152,
                    0
                ],
                "title": "Flexible inference in heterogeneous and attributed multilayer networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible inference in heterogeneous and attributed multilayer networks"
                },
                "summary": "Networked datasets can be enriched by different types of information about\nindividual nodes or edges. However, most existing methods for analyzing such\ndatasets struggle to handle the complexity of heterogeneous data, often\nrequiring substantial model-specific analysis. In this paper, we develop a\nprobabilistic generative model to perform inference in multilayer networks with\narbitrary types of information. Our approach employs a Bayesian framework\ncombined with the Laplace matching technique to ease interpretation of inferred\nparameters. Furthermore, the algorithmic implementation relies on automatic\ndifferentiation, avoiding the need for explicit derivations. This makes our\nmodel scalable and flexible to adapt to any combination of input data. We\ndemonstrate the effectiveness of our method in detecting overlapping community\nstructures and performing various prediction tasks on heterogeneous multilayer\ndata, where nodes and edges have different types of attributes. Additionally,\nwe showcase its ability to unveil a variety of patterns in a social support\nnetwork among villagers in rural India by effectively utilizing all input\ninformation in a meaningful way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networked datasets can be enriched by different types of information about\nindividual nodes or edges. However, most existing methods for analyzing such\ndatasets struggle to handle the complexity of heterogeneous data, often\nrequiring substantial model-specific analysis. In this paper, we develop a\nprobabilistic generative model to perform inference in multilayer networks with\narbitrary types of information. Our approach employs a Bayesian framework\ncombined with the Laplace matching technique to ease interpretation of inferred\nparameters. Furthermore, the algorithmic implementation relies on automatic\ndifferentiation, avoiding the need for explicit derivations. This makes our\nmodel scalable and flexible to adapt to any combination of input data. We\ndemonstrate the effectiveness of our method in detecting overlapping community\nstructures and performing various prediction tasks on heterogeneous multilayer\ndata, where nodes and edges have different types of attributes. Additionally,\nwe showcase its ability to unveil a variety of patterns in a social support\nnetwork among villagers in rural India by effectively utilizing all input\ninformation in a meaningful way."
                },
                "authors": [
                    {
                        "name": "Martina Contisciani"
                    },
                    {
                        "name": "Marius Hobbhahn"
                    },
                    {
                        "name": "Eleanor A. Power"
                    },
                    {
                        "name": "Philipp Hennig"
                    },
                    {
                        "name": "Caterina De Bacco"
                    }
                ],
                "author_detail": {
                    "name": "Caterina De Bacco"
                },
                "author": "Caterina De Bacco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14269v2",
                "updated": "2025-01-10T12:14:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    14,
                    55,
                    4,
                    10,
                    0
                ],
                "published": "2024-04-22T15:15:06Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    15,
                    15,
                    6,
                    0,
                    113,
                    0
                ],
                "title": "Hybrid Fusion for 802.11ax Wi-Fi-based Passive Radars Exploiting\n  Beamforming Feedbacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Fusion for 802.11ax Wi-Fi-based Passive Radars Exploiting\n  Beamforming Feedbacks"
                },
                "summary": "Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of\ntargets using Wi-Fi signals of opportunity transmitted by an access point.\nUnlike active radars that optimize their transmitted waveform for localization,\nPWRs align with the 802.11 amendments. Specifically, during the channel\nsounding session preceding a multi-user multiple-input multiple-output downlink\ntransmission, an access point isotropically transmits a null data packet (NDP)\nwith a known preamble. From these known symbols, client user equipments derive\ntheir channel state information and transmit an unencrypted beamforming\nfeedback (BFF) back to the access point. The BFF comprises the right singular\nmatrix of the channel and the corresponding stream gain for each subcarrier,\nwhich allows the computation of a beamforming matrix at the access point. In a\nclassical PWR processing, only the preamble symbols from the NDP are exploited\nduring the channel sounding session. In this study, we investigate multiple\ntarget localization by a PWR exploiting hybrid information sources. On one\nhand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP.\nOn another hand, the line-of-sight angle-of-departures inferred from the BFFs.\nThe processing steps at the PWR are defined and an optimal hybrid fusion rule\nis derived in the maximum likelihood framework. Monte-Carlo simulations assess\nthe enhanced accuracy of the proposed combination method compared to classical\nPWR processing based solely on the NDP, and compare the localisation\nperformance between client and non-client targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of\ntargets using Wi-Fi signals of opportunity transmitted by an access point.\nUnlike active radars that optimize their transmitted waveform for localization,\nPWRs align with the 802.11 amendments. Specifically, during the channel\nsounding session preceding a multi-user multiple-input multiple-output downlink\ntransmission, an access point isotropically transmits a null data packet (NDP)\nwith a known preamble. From these known symbols, client user equipments derive\ntheir channel state information and transmit an unencrypted beamforming\nfeedback (BFF) back to the access point. The BFF comprises the right singular\nmatrix of the channel and the corresponding stream gain for each subcarrier,\nwhich allows the computation of a beamforming matrix at the access point. In a\nclassical PWR processing, only the preamble symbols from the NDP are exploited\nduring the channel sounding session. In this study, we investigate multiple\ntarget localization by a PWR exploiting hybrid information sources. On one\nhand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP.\nOn another hand, the line-of-sight angle-of-departures inferred from the BFFs.\nThe processing steps at the PWR are defined and an optimal hybrid fusion rule\nis derived in the maximum likelihood framework. Monte-Carlo simulations assess\nthe enhanced accuracy of the proposed combination method compared to classical\nPWR processing based solely on the NDP, and compare the localisation\nperformance between client and non-client targets."
                },
                "authors": [
                    {
                        "name": "Martin Willame"
                    },
                    {
                        "name": "Hasan Can Yildirim"
                    },
                    {
                        "name": "Laurent Storrer"
                    },
                    {
                        "name": "Francois Horlin"
                    },
                    {
                        "name": "Jerome Louveaux"
                    }
                ],
                "author_detail": {
                    "name": "Jerome Louveaux"
                },
                "author": "Jerome Louveaux",
                "arxiv_comment": "5 pages, 3 figures, Accepted for SPAWC24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05899v1",
                "updated": "2025-01-10T11:49:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    49,
                    31,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:49:31Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    49,
                    31,
                    4,
                    10,
                    0
                ],
                "title": "Prompt engineering and its implications on the energy consumption of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering and its implications on the energy consumption of\n  Large Language Models"
                },
                "summary": "Reducing the environmental impact of AI-based software systems has become\ncritical. The intensive use of large language models (LLMs) in software\nengineering poses severe challenges regarding computational resources, data\ncenters, and carbon emissions. In this paper, we investigate how prompt\nengineering techniques (PETs) can impact the carbon emission of the Llama 3\nmodel for the code generation task. We experimented with the CodeXGLUE\nbenchmark to evaluate both energy consumption and the accuracy of the generated\ncode using an isolated testing environment. Our initial results show that the\nenergy consumption of LLMs can be reduced by using specific tags that\ndistinguish different prompt parts. Even though a more in-depth evaluation is\nneeded to confirm our findings, this work suggests that prompt engineering can\nreduce LLMs' energy consumption during the inference phase without compromising\nperformance, paving the way for further investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the environmental impact of AI-based software systems has become\ncritical. The intensive use of large language models (LLMs) in software\nengineering poses severe challenges regarding computational resources, data\ncenters, and carbon emissions. In this paper, we investigate how prompt\nengineering techniques (PETs) can impact the carbon emission of the Llama 3\nmodel for the code generation task. We experimented with the CodeXGLUE\nbenchmark to evaluate both energy consumption and the accuracy of the generated\ncode using an isolated testing environment. Our initial results show that the\nenergy consumption of LLMs can be reduced by using specific tags that\ndistinguish different prompt parts. Even though a more in-depth evaluation is\nneeded to confirm our findings, this work suggests that prompt engineering can\nreduce LLMs' energy consumption during the inference phase without compromising\nperformance, paving the way for further investigations."
                },
                "authors": [
                    {
                        "name": "Riccardo Rubei"
                    },
                    {
                        "name": "Aicha Moussaid"
                    },
                    {
                        "name": "Claudio di Sipio"
                    },
                    {
                        "name": "Davide di Ruscio"
                    }
                ],
                "author_detail": {
                    "name": "Davide di Ruscio"
                },
                "author": "Davide di Ruscio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05892v1",
                "updated": "2025-01-10T11:44:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    59,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:44:59Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    59,
                    4,
                    10,
                    0
                ],
                "title": "Beyond Flat Text: Dual Self-inherited Guidance for Visual Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Flat Text: Dual Self-inherited Guidance for Visual Text\n  Generation"
                },
                "summary": "In real-world images, slanted or curved texts, especially those on cans,\nbanners, or badges, appear as frequently, if not more so, than flat texts due\nto artistic design or layout constraints. While high-quality visual text\ngeneration has become available with the advanced generative capabilities of\ndiffusion models, these models often produce distorted text and inharmonious\ntext background when given slanted or curved text layouts due to training data\nlimitation. In this paper, we introduce a new training-free framework, STGen,\nwhich accurately generates visual texts in challenging scenarios (\\eg, slanted\nor curved text layouts) while harmonizing them with the text background. Our\nframework decomposes the visual text generation process into two branches: (i)\n\\textbf{Semantic Rectification Branch}, which leverages the ability in\ngenerating flat but accurate visual texts of the model to guide the generation\nof challenging scenarios. The generated latent of flat text is abundant in\naccurate semantic information related both to the text itself and its\nbackground. By incorporating this, we rectify the semantic information of the\ntexts and harmonize the integration of the text with its background in complex\nlayouts. (ii) \\textbf{Structure Injection Branch}, which reinforces the visual\ntext structure during inference. We incorporate the latent information of the\nglyph image, rich in glyph structure, as a new condition to further strengthen\nthe text structure. To enhance image harmony, we also apply an effective\ncombination method to merge the priors, providing a solid foundation for\ngeneration. Extensive experiments across a variety of visual text layouts\ndemonstrate that our framework achieves superior accuracy and outstanding\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world images, slanted or curved texts, especially those on cans,\nbanners, or badges, appear as frequently, if not more so, than flat texts due\nto artistic design or layout constraints. While high-quality visual text\ngeneration has become available with the advanced generative capabilities of\ndiffusion models, these models often produce distorted text and inharmonious\ntext background when given slanted or curved text layouts due to training data\nlimitation. In this paper, we introduce a new training-free framework, STGen,\nwhich accurately generates visual texts in challenging scenarios (\\eg, slanted\nor curved text layouts) while harmonizing them with the text background. Our\nframework decomposes the visual text generation process into two branches: (i)\n\\textbf{Semantic Rectification Branch}, which leverages the ability in\ngenerating flat but accurate visual texts of the model to guide the generation\nof challenging scenarios. The generated latent of flat text is abundant in\naccurate semantic information related both to the text itself and its\nbackground. By incorporating this, we rectify the semantic information of the\ntexts and harmonize the integration of the text with its background in complex\nlayouts. (ii) \\textbf{Structure Injection Branch}, which reinforces the visual\ntext structure during inference. We incorporate the latent information of the\nglyph image, rich in glyph structure, as a new condition to further strengthen\nthe text structure. To enhance image harmony, we also apply an effective\ncombination method to merge the priors, providing a solid foundation for\ngeneration. Extensive experiments across a variety of visual text layouts\ndemonstrate that our framework achieves superior accuracy and outstanding\nquality."
                },
                "authors": [
                    {
                        "name": "Minxing Luo"
                    },
                    {
                        "name": "Zixun Xia"
                    },
                    {
                        "name": "Liaojun Chen"
                    },
                    {
                        "name": "Zhenhang Li"
                    },
                    {
                        "name": "Weichao Zeng"
                    },
                    {
                        "name": "Jianye Wang"
                    },
                    {
                        "name": "Wentao Cheng"
                    },
                    {
                        "name": "Yaxing Wang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05891v1",
                "updated": "2025-01-10T11:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    35,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    35,
                    4,
                    10,
                    0
                ],
                "title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs"
                },
                "summary": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable."
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Saverio Giallorenzo"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "The 40th ACM/SIGAPP Symposium On Applied Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05886v1",
                "updated": "2025-01-10T11:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    39,
                    25,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:39:25Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    39,
                    25,
                    4,
                    10,
                    0
                ],
                "title": "Estimating variability power spectra of active galaxies from irregular\n  time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating variability power spectra of active galaxies from irregular\n  time series"
                },
                "summary": "A common feature of Active Galactic Nuclei (AGN) is their random variations\nin brightness across the whole emission spectrum, from radio to $\\gamma$-rays.\nStudying the nature and origin of these fluctuations is critical to\ncharacterising the underlying variability process of the accretion flow that\npowers AGN. Random timing fluctuations are often studied with the power\nspectrum; this quantifies how the amplitude of variations is distributed over\ntemporal frequencies. Red noise variability -- when the power spectrum\nincreases smoothly towards low frequencies -- is ubiquitous in AGN. The\ncommonly used Fourier analysis methods, have significant challenges when\napplied to arbitrarily sampled light curves of red noise variability. Several\ntime-domain methods exist to infer the power spectral shape in the case of\nirregular sampling but they suffer from biases which can be difficult to\nmitigate, or are computationally expensive. In this paper, we demonstrate a\nmethod infer the shape of broad-band power spectra for irregular time series,\nusing a Gaussian process regression method scalable to large datasets. The\npower spectrum is modelled as a power-law model with one or two bends with\nflexible slopes. The method is fully Bayesian and we demonstrate its utility\nusing simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1\ngalaxy, is used as a test case and we find consistent results with the\nliterature using independent X-ray data from XMM-Newton and Swift. We provide\npublicly available, documented and tested implementations in Python and Julia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common feature of Active Galactic Nuclei (AGN) is their random variations\nin brightness across the whole emission spectrum, from radio to $\\gamma$-rays.\nStudying the nature and origin of these fluctuations is critical to\ncharacterising the underlying variability process of the accretion flow that\npowers AGN. Random timing fluctuations are often studied with the power\nspectrum; this quantifies how the amplitude of variations is distributed over\ntemporal frequencies. Red noise variability -- when the power spectrum\nincreases smoothly towards low frequencies -- is ubiquitous in AGN. The\ncommonly used Fourier analysis methods, have significant challenges when\napplied to arbitrarily sampled light curves of red noise variability. Several\ntime-domain methods exist to infer the power spectral shape in the case of\nirregular sampling but they suffer from biases which can be difficult to\nmitigate, or are computationally expensive. In this paper, we demonstrate a\nmethod infer the shape of broad-band power spectra for irregular time series,\nusing a Gaussian process regression method scalable to large datasets. The\npower spectrum is modelled as a power-law model with one or two bends with\nflexible slopes. The method is fully Bayesian and we demonstrate its utility\nusing simulated light curves. Finally, Ark 564, a well-known variable Seyfert 1\ngalaxy, is used as a test case and we find consistent results with the\nliterature using independent X-ray data from XMM-Newton and Swift. We provide\npublicly available, documented and tested implementations in Python and Julia."
                },
                "authors": [
                    {
                        "name": "Mehdy Lefkir"
                    },
                    {
                        "name": "Simon Vaughan"
                    },
                    {
                        "name": "Daniela Huppenkothen"
                    },
                    {
                        "name": "Phil Uttley"
                    },
                    {
                        "name": "Vysakh Anilkumar"
                    }
                ],
                "author_detail": {
                    "name": "Vysakh Anilkumar"
                },
                "author": "Vysakh Anilkumar",
                "arxiv_comment": "21 pages, 16 figures, submitted to MNRAS, code available here:\n  https://github.com/mlefkir/Pioran.jl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05885v1",
                "updated": "2025-01-10T11:37:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    37,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:37:50Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    37,
                    50,
                    4,
                    10,
                    0
                ],
                "title": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster\n  Context Attention, Better Feature Fusion, and Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster\n  Context Attention, Better Feature Fusion, and Hardware Acceleration"
                },
                "summary": "Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet."
                },
                "authors": [
                    {
                        "name": "Zhifan Song"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Abd Al Rahman M. Abu Ebayyeh"
                    }
                ],
                "author_detail": {
                    "name": "Abd Al Rahman M. Abu Ebayyeh"
                },
                "author": "Abd Al Rahman M. Abu Ebayyeh",
                "arxiv_doi": "10.1109/SWC62898.2024.00141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SWC62898.2024.00141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in 21st IEEE International Conference on Ubiquitous\n  Intelligence and Computing (UIC 2024)\n  https://www.ieee-smart-world.org/2024/uic",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05884v1",
                "updated": "2025-01-10T11:35:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    35,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:35:43Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    35,
                    43,
                    4,
                    10,
                    0
                ],
                "title": "Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal\n  LLMs"
                },
                "summary": "The exponential growth of short-video content has ignited a surge in the\nnecessity for efficient, automated solutions to video editing, with challenges\narising from the need to understand videos and tailor the editing according to\nuser requirements. Addressing this need, we propose an innovative end-to-end\nfoundational framework, ultimately actualizing precise control over the final\nvideo content editing. Leveraging the flexibility and generalizability of\nMultimodal Large Language Models (MLLMs), we defined clear input-output\nmappings for efficient video creation. To bolster the model's capability in\nprocessing and comprehending video content, we introduce a strategic\ncombination of a denser frame rate and a slow-fast processing technique,\nsignificantly enhancing the extraction and understanding of both temporal and\nspatial video information. Furthermore, we introduce a text-to-edit mechanism\nthat allows users to achieve desired video outcomes through textual input,\nthereby enhancing the quality and controllability of the edited videos. Through\ncomprehensive experimentation, our method has not only showcased significant\neffectiveness within advertising datasets, but also yields universally\napplicable conclusions on public datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of short-video content has ignited a surge in the\nnecessity for efficient, automated solutions to video editing, with challenges\narising from the need to understand videos and tailor the editing according to\nuser requirements. Addressing this need, we propose an innovative end-to-end\nfoundational framework, ultimately actualizing precise control over the final\nvideo content editing. Leveraging the flexibility and generalizability of\nMultimodal Large Language Models (MLLMs), we defined clear input-output\nmappings for efficient video creation. To bolster the model's capability in\nprocessing and comprehending video content, we introduce a strategic\ncombination of a denser frame rate and a slow-fast processing technique,\nsignificantly enhancing the extraction and understanding of both temporal and\nspatial video information. Furthermore, we introduce a text-to-edit mechanism\nthat allows users to achieve desired video outcomes through textual input,\nthereby enhancing the quality and controllability of the edited videos. Through\ncomprehensive experimentation, our method has not only showcased significant\neffectiveness within advertising datasets, but also yields universally\napplicable conclusions on public datasets."
                },
                "authors": [
                    {
                        "name": "Dabing Cheng"
                    },
                    {
                        "name": "Haosen Zhan"
                    },
                    {
                        "name": "Xingchen Zhao"
                    },
                    {
                        "name": "Guisheng Liu"
                    },
                    {
                        "name": "Zemin Li"
                    },
                    {
                        "name": "Jinghui Xie"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Weiguo Feng"
                    },
                    {
                        "name": "Bingyue Peng"
                    }
                ],
                "author_detail": {
                    "name": "Bingyue Peng"
                },
                "author": "Bingyue Peng",
                "arxiv_comment": "16pages conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05880v1",
                "updated": "2025-01-10T11:32:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    32,
                    56,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:32:56Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    32,
                    56,
                    4,
                    10,
                    0
                ],
                "title": "TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV\n  systems in Emergency Response Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV\n  systems in Emergency Response Scenarios"
                },
                "summary": "Designing efficient neural networks for embedded devices is a critical\nchallenge, particularly in applications requiring real-time performance, such\nas aerial imaging with drones and UAVs for emergency responses. In this work,\nwe introduce TakuNet, a novel light-weight architecture which employs\ntechniques such as depth-wise convolutions and an early downsampling stem to\nreduce computational complexity while maintaining high accuracy. It leverages\ndense connections for fast convergence during training and uses 16-bit\nfloating-point precision for optimization on embedded hardware accelerators.\nExperimental evaluation on two public datasets shows that TakuNet achieves\nnear-state-of-the-art accuracy in classifying aerial images of emergency\nsituations, despite its minimal parameter count. Real-world tests on embedded\ndevices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's\nefficiency, achieving more than 650 fps on the 15W Jetson board, making it\nsuitable for real-time AI processing on resource-constrained platforms and\nadvancing the applicability of drones in emergency scenarios. The code and\nimplementation details are publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing efficient neural networks for embedded devices is a critical\nchallenge, particularly in applications requiring real-time performance, such\nas aerial imaging with drones and UAVs for emergency responses. In this work,\nwe introduce TakuNet, a novel light-weight architecture which employs\ntechniques such as depth-wise convolutions and an early downsampling stem to\nreduce computational complexity while maintaining high accuracy. It leverages\ndense connections for fast convergence during training and uses 16-bit\nfloating-point precision for optimization on embedded hardware accelerators.\nExperimental evaluation on two public datasets shows that TakuNet achieves\nnear-state-of-the-art accuracy in classifying aerial images of emergency\nsituations, despite its minimal parameter count. Real-world tests on embedded\ndevices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's\nefficiency, achieving more than 650 fps on the 15W Jetson board, making it\nsuitable for real-time AI processing on resource-constrained platforms and\nadvancing the applicability of drones in emergency scenarios. The code and\nimplementation details are publicly released."
                },
                "authors": [
                    {
                        "name": "Daniel Rossi"
                    },
                    {
                        "name": "Guido Borghi"
                    },
                    {
                        "name": "Roberto Vezzani"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Vezzani"
                },
                "author": "Roberto Vezzani",
                "arxiv_comment": "This paper has been accepted at WACVW 2025, which will take place on\n  28/02/2025. The official conference proceedings have not yet been published\n  at the time of submission to arXiv. The final version of the paper,\n  incorporating any changes based on feedback received during the conference,\n  will be included in the proceedings once they are made available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05870v1",
                "updated": "2025-01-10T11:11:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    11,
                    8,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:11:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    11,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "A Neighbor-based Approach to Pitch Ownership Models in Soccer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neighbor-based Approach to Pitch Ownership Models in Soccer"
                },
                "summary": "Pitch ownership models allow many types of analysis in soccer and provide\nvaluable assistance to tactical analysts in understanding the game's dynamics.\nThe novelty they provide over event-based analysis is that tracking data\nincorporates context that event-based data does not possess, like player\npositioning. This paper proposes a novel approach to building pitch ownership\nmodels in soccer games using the K-Nearest Neighbors (KNN) algorithm. Our\napproach provides a fast inference mechanism that can model different\napproaches to pitch control using the same algorithm. Despite its flexibility,\nit uses only three hyperparameters to tune the model, facilitating the tuning\nprocess for different player skill levels. The flexibility of the approach\nallows for the emulation of different methods available in the literature by\nadjusting a small number of parameters, including adjusting for different\nlevels of uncertainty. In summary, the proposed model provides a new and more\nflexible strategy for building pitch ownership models, extending beyond just\nreplicating existing algorithms, and can provide valuable insights for tactical\nanalysts and open up new avenues for future research. We thoroughly visualize\nseveral examples demonstrating the presented models' strengths and weaknesses.\nThe code is available at github.com/nvsclub/KNNPitchControl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pitch ownership models allow many types of analysis in soccer and provide\nvaluable assistance to tactical analysts in understanding the game's dynamics.\nThe novelty they provide over event-based analysis is that tracking data\nincorporates context that event-based data does not possess, like player\npositioning. This paper proposes a novel approach to building pitch ownership\nmodels in soccer games using the K-Nearest Neighbors (KNN) algorithm. Our\napproach provides a fast inference mechanism that can model different\napproaches to pitch control using the same algorithm. Despite its flexibility,\nit uses only three hyperparameters to tune the model, facilitating the tuning\nprocess for different player skill levels. The flexibility of the approach\nallows for the emulation of different methods available in the literature by\nadjusting a small number of parameters, including adjusting for different\nlevels of uncertainty. In summary, the proposed model provides a new and more\nflexible strategy for building pitch ownership models, extending beyond just\nreplicating existing algorithms, and can provide valuable insights for tactical\nanalysts and open up new avenues for future research. We thoroughly visualize\nseveral examples demonstrating the presented models' strengths and weaknesses.\nThe code is available at github.com/nvsclub/KNNPitchControl."
                },
                "authors": [
                    {
                        "name": "Tiago Mendes-Neves"
                    },
                    {
                        "name": "Luís Meireles"
                    },
                    {
                        "name": "João Mendes-Moreira"
                    }
                ],
                "author_detail": {
                    "name": "João Mendes-Moreira"
                },
                "author": "João Mendes-Moreira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09504v2",
                "updated": "2025-01-10T11:03:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    3,
                    13,
                    4,
                    10,
                    0
                ],
                "published": "2024-10-12T11:45:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    11,
                    45,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach"
                },
                "summary": "Building artificially intelligent geospatial systems require rapid delivery\nof spatial data analysis at massive scales with minimal human intervention.\nDepending upon their intended use, data analysis may also entail model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate its effectiveness in rapidly\nanalyzing massive data sets. Furthermore, we make inference feasible in a\nreasonable amount of time, and without excessively demanding hardware settings.\nWe illustrate the effectiveness of this approach in extensive simulation\nexperiments and subsequently analyze massive data sets in climate science on\nsea surface temperatures and on vegetation index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building artificially intelligent geospatial systems require rapid delivery\nof spatial data analysis at massive scales with minimal human intervention.\nDepending upon their intended use, data analysis may also entail model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate its effectiveness in rapidly\nanalyzing massive data sets. Furthermore, we make inference feasible in a\nreasonable amount of time, and without excessively demanding hardware settings.\nWe illustrate the effectiveness of this approach in extensive simulation\nexperiments and subsequently analyze massive data sets in climate science on\nsea surface temperatures and on vegetation index."
                },
                "authors": [
                    {
                        "name": "Luca Presicce"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v1",
                "updated": "2025-01-10T10:53:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim"
                },
                "authors": [
                    {
                        "name": "Antonin Poché"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03630v2",
                "updated": "2025-01-10T10:45:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    45,
                    49,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-07T09:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    0,
                    7,
                    1,
                    7,
                    0
                ],
                "title": "MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer"
                },
                "summary": "Virtual try-on methods based on diffusion models achieve realistic try-on\neffects. They use an extra reference network or an additional image encoder to\nprocess multiple conditional image inputs, which adds complexity pre-processing\nand additional computational costs. Besides, they require more than 25\ninference steps, bringing longer inference time. In this work, with the\ndevelopment of diffusion transformer (DiT), we rethink the necessity of\nadditional reference network or image encoder and introduce MC-VTON, which\nleverages DiT's intrinsic backbone to seamlessly integrate minimal conditional\ntry-on inputs. Compared to existing methods, the superiority of MC-VTON is\ndemonstrated in four aspects: (1) Superior detail fidelity. Our DiT-based\nMC-VTON exhibits superior fidelity in preserving fine-grained details. (2)\nSimplified network and inputs. We remove any extra reference network or image\nencoder. We also remove unnecessary conditions like the long prompt, pose\nestimation, human parsing, and depth map. We require only the masked person\nimage and the garment image. (3) Parameter-efficient training. To process the\ntry-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters\n(0.33% of the backbone parameters). (4) Less inference steps. We apply\ndistillation diffusion on MC-VTON and only need 8 steps to generate a realistic\ntry-on image, with only 86.8M additional parameters (0.72% of the backbone\nparameters). Experiments show that MC-VTON achieves superior qualitative and\nquantitative results with fewer condition inputs, trainable parameters, and\ninference steps than baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual try-on methods based on diffusion models achieve realistic try-on\neffects. They use an extra reference network or an additional image encoder to\nprocess multiple conditional image inputs, which adds complexity pre-processing\nand additional computational costs. Besides, they require more than 25\ninference steps, bringing longer inference time. In this work, with the\ndevelopment of diffusion transformer (DiT), we rethink the necessity of\nadditional reference network or image encoder and introduce MC-VTON, which\nleverages DiT's intrinsic backbone to seamlessly integrate minimal conditional\ntry-on inputs. Compared to existing methods, the superiority of MC-VTON is\ndemonstrated in four aspects: (1) Superior detail fidelity. Our DiT-based\nMC-VTON exhibits superior fidelity in preserving fine-grained details. (2)\nSimplified network and inputs. We remove any extra reference network or image\nencoder. We also remove unnecessary conditions like the long prompt, pose\nestimation, human parsing, and depth map. We require only the masked person\nimage and the garment image. (3) Parameter-efficient training. To process the\ntry-on task, we fine-tune the FLUX.1-dev with only 39.7M additional parameters\n(0.33% of the backbone parameters). (4) Less inference steps. We apply\ndistillation diffusion on MC-VTON and only need 8 steps to generate a realistic\ntry-on image, with only 86.8M additional parameters (0.72% of the backbone\nparameters). Experiments show that MC-VTON achieves superior qualitative and\nquantitative results with fewer condition inputs, trainable parameters, and\ninference steps than baseline methods."
                },
                "authors": [
                    {
                        "name": "Junsheng Luan"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03968v2",
                "updated": "2025-01-10T10:38:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    38,
                    49,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-07T18:06:27Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-driven Behavior Tree for Context-aware Task Planning"
                },
                "summary": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations."
                },
                "authors": [
                    {
                        "name": "Naoki Wake"
                    },
                    {
                        "name": "Atsushi Kanehira"
                    },
                    {
                        "name": "Jun Takamatsu"
                    },
                    {
                        "name": "Kazuhiro Sasabuchi"
                    },
                    {
                        "name": "Katsushi Ikeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Katsushi Ikeuchi"
                },
                "author": "Katsushi Ikeuchi",
                "arxiv_comment": "10 pages, 11 figures, 5 tables. Last updated on January 9th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10221v2",
                "updated": "2025-01-10T10:36:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    36,
                    58,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-14T17:54:54Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    17,
                    54,
                    54,
                    4,
                    166,
                    0
                ],
                "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Story Short: Story-level Video Understanding from 20K Short Films"
                },
                "summary": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding."
                },
                "authors": [
                    {
                        "name": "Ridouane Ghermi"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Vicky Kalogeiton"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15627v3",
                "updated": "2025-01-10T10:24:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    24,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-21T20:06:31Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    20,
                    6,
                    31,
                    4,
                    173,
                    0
                ],
                "title": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph"
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Daniil Vasilev"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Abdelrahman Boda Sadallah"
                    },
                    {
                        "name": "Kirill Grishchenkov"
                    },
                    {
                        "name": "Sergey Petrakov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_comment": "Accepted to TACL 2025, pre-MIT Press publication version. Roman\n  Vashurin, Ekaterina Fadeeva, Artem Vazhentsev contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05836v1",
                "updated": "2025-01-10T10:23:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    23,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T10:23:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    23,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations"
                },
                "summary": "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising."
                },
                "authors": [
                    {
                        "name": "Charlotte Voinot"
                    },
                    {
                        "name": "Clément Berenfeld"
                    },
                    {
                        "name": "Imke Mayer"
                    },
                    {
                        "name": "Bernard Sebastien"
                    },
                    {
                        "name": "Julie Josse"
                    }
                ],
                "author_detail": {
                    "name": "Julie Josse"
                },
                "arxiv_affiliation": "PREMEDICAL",
                "author": "Julie Josse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v2",
                "updated": "2025-01-10T10:08:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    8,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09278v2",
                "updated": "2025-01-10T10:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    7,
                    55,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-12T13:41:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    41,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine"
                },
                "summary": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB."
                },
                "authors": [
                    {
                        "name": "Xiaoshuang Huang"
                    },
                    {
                        "name": "Lingdong Shen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Fangxin Shang"
                    },
                    {
                        "name": "Hongxiang Li"
                    },
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Yehui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yehui Yang"
                },
                "author": "Yehui Yang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13902v2",
                "updated": "2025-01-10T09:55:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    55,
                    54,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-18T14:42:43Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    42,
                    43,
                    2,
                    353,
                    0
                ],
                "title": "Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference"
                },
                "summary": "Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication."
                },
                "authors": [
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05399v2",
                "updated": "2025-01-10T09:54:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    54,
                    54,
                    4,
                    10,
                    0
                ],
                "published": "2024-04-08T10:57:25Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    10,
                    57,
                    25,
                    0,
                    99,
                    0
                ],
                "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and\n  Improving Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and\n  Improving Large Language Model Safety"
                },
                "summary": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops."
                },
                "authors": [
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Fabio Pernisi"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "Accepted at AAAI 2025 (Special Track on AI Alignment)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06154v2",
                "updated": "2025-01-10T09:44:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    44,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2024-09-10T01:57:57Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    1,
                    57,
                    57,
                    1,
                    254,
                    0
                ],
                "title": "Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial\n  Expressions Using Static Expression Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial\n  Expressions Using Static Expression Data"
                },
                "summary": "Dynamic facial expression recognition (DFER) infers emotions from the\ntemporal evolution of expressions, unlike static facial expression recognition\n(SFER), which relies solely on a single snapshot. This temporal analysis\nprovides richer information and promises greater recognition capability.\nHowever, current DFER methods often exhibit unsatisfied performance largely due\nto fewer training samples compared to SFER. Given the inherent correlation\nbetween static and dynamic expressions, we hypothesize that leveraging the\nabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic\n(S4D), a unified dual-modal learning framework that integrates SFER data as a\ncomplementary resource for DFER. Specifically, S4D employs dual-modal\nself-supervised pre-training on facial images and videos using a shared Vision\nTransformer (ViT) encoder-decoder architecture, yielding improved\nspatiotemporal representations. The pre-trained encoder is then fine-tuned on\nstatic and dynamic expression datasets in a multi-task learning setup to\nfacilitate emotional information interaction. Unfortunately, vanilla multi-task\nlearning in our study results in negative transfer. To address this, we propose\nan innovative Mixture of Adapter Experts (MoAE) module that facilitates\ntask-specific knowledge acquisition while effectively extracting shared\nknowledge from both static and dynamic expression data. Extensive experiments\ndemonstrate that S4D achieves a deeper understanding of DFER, setting new\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nAdditionally, a systematic correlation analysis between SFER and DFER tasks is\npresented, which further elucidates the potential benefits of leveraging SFER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic facial expression recognition (DFER) infers emotions from the\ntemporal evolution of expressions, unlike static facial expression recognition\n(SFER), which relies solely on a single snapshot. This temporal analysis\nprovides richer information and promises greater recognition capability.\nHowever, current DFER methods often exhibit unsatisfied performance largely due\nto fewer training samples compared to SFER. Given the inherent correlation\nbetween static and dynamic expressions, we hypothesize that leveraging the\nabundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic\n(S4D), a unified dual-modal learning framework that integrates SFER data as a\ncomplementary resource for DFER. Specifically, S4D employs dual-modal\nself-supervised pre-training on facial images and videos using a shared Vision\nTransformer (ViT) encoder-decoder architecture, yielding improved\nspatiotemporal representations. The pre-trained encoder is then fine-tuned on\nstatic and dynamic expression datasets in a multi-task learning setup to\nfacilitate emotional information interaction. Unfortunately, vanilla multi-task\nlearning in our study results in negative transfer. To address this, we propose\nan innovative Mixture of Adapter Experts (MoAE) module that facilitates\ntask-specific knowledge acquisition while effectively extracting shared\nknowledge from both static and dynamic expression data. Extensive experiments\ndemonstrate that S4D achieves a deeper understanding of DFER, setting new\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nAdditionally, a systematic correlation analysis between SFER and DFER tasks is\npresented, which further elucidates the potential benefits of leveraging SFER."
                },
                "authors": [
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zhenzhen Hu"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "arxiv_comment": "The code and model are publicly available here\n  https://github.com/MSA-LMC/S4D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07523v2",
                "updated": "2025-01-10T09:23:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    23,
                    33,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-10T14:00:09Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    0,
                    9,
                    1,
                    345,
                    0
                ],
                "title": "JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust\n  Scattering in the HH 30 disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust\n  Scattering in the HH 30 disk"
                },
                "summary": "We present near- and mid-infrared (IR) broadband imaging observations of the\nedge-on protoplanetary disk around HH 30 with the James Webb Space\nTelescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI).\nWe combine these observations with archival optical/near-IR scattered light\nimages obtained with the Hubble Space Telescope (HST) and a\nmillimeter-wavelength dust continuum image obtained with the Atacama Large\nMillimeter/submillimeter Array (ALMA) with the highest spatial resolution ever\nobtained for this target. Our multiwavelength images clearly reveal the\nvertical and radial segregation of micron-sized and sub-mm-sized grains in the\ndisk. In the near- and mid-IR, the images capture not only bi-reflection\nnebulae separated by a dark lane but also diverse dynamical processes occurring\nin the HH 30 disk, such as spiral- and tail-like structures, a conical outflow,\nand a collimated jet. In contrast, the ALMA image reveals a flat dust disk in\nthe disk midplane. By performing radiative transfer simulations, we show that\ngrains of about 3 $\\mu$m in radius or larger are fully vertically mixed to\nexplain the observed mid-IR scattered light flux and its morphology, whereas\nmillimeter-sized grains are settled into a layer with a scale height of\n$\\gtrsim1$ au at $100$ au from the central star. We also find a tension in the\ndisk inclination angle inferred from optical/near-IR and mm observations with\nthe latter being closer to an exactly edge-on. Finally, we report the first\ndetection of the proper motion of an emission knot associated with the mid-IR\ncollimated jet detected by combining two epochs of our MIRI 12.8-$\\mu$m\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present near- and mid-infrared (IR) broadband imaging observations of the\nedge-on protoplanetary disk around HH 30 with the James Webb Space\nTelescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI).\nWe combine these observations with archival optical/near-IR scattered light\nimages obtained with the Hubble Space Telescope (HST) and a\nmillimeter-wavelength dust continuum image obtained with the Atacama Large\nMillimeter/submillimeter Array (ALMA) with the highest spatial resolution ever\nobtained for this target. Our multiwavelength images clearly reveal the\nvertical and radial segregation of micron-sized and sub-mm-sized grains in the\ndisk. In the near- and mid-IR, the images capture not only bi-reflection\nnebulae separated by a dark lane but also diverse dynamical processes occurring\nin the HH 30 disk, such as spiral- and tail-like structures, a conical outflow,\nand a collimated jet. In contrast, the ALMA image reveals a flat dust disk in\nthe disk midplane. By performing radiative transfer simulations, we show that\ngrains of about 3 $\\mu$m in radius or larger are fully vertically mixed to\nexplain the observed mid-IR scattered light flux and its morphology, whereas\nmillimeter-sized grains are settled into a layer with a scale height of\n$\\gtrsim1$ au at $100$ au from the central star. We also find a tension in the\ndisk inclination angle inferred from optical/near-IR and mm observations with\nthe latter being closer to an exactly edge-on. Finally, we report the first\ndetection of the proper motion of an emission knot associated with the mid-IR\ncollimated jet detected by combining two epochs of our MIRI 12.8-$\\mu$m\nobservations."
                },
                "authors": [
                    {
                        "name": "Ryo Tazaki"
                    },
                    {
                        "name": "François Ménard"
                    },
                    {
                        "name": "Gaspard Duchêne"
                    },
                    {
                        "name": "Marion Villenave"
                    },
                    {
                        "name": "Álvaro Ribas"
                    },
                    {
                        "name": "Karl R. Stapelfeldt"
                    },
                    {
                        "name": "Marshall D. Perrin"
                    },
                    {
                        "name": "Christophe Pinte"
                    },
                    {
                        "name": "Schuyler G. Wolff"
                    },
                    {
                        "name": "Deborah L. Padgett"
                    },
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Laurine Martinien"
                    },
                    {
                        "name": "Maxime Roumesy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Roumesy"
                },
                "author": "Maxime Roumesy",
                "arxiv_comment": "29 pages, 23 figures; Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05790v1",
                "updated": "2025-01-10T08:50:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T08:50:38Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "title": "Understanding Impact of Human Feedback via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Impact of Human Feedback via Influence Functions"
                },
                "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF"
                },
                "authors": [
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Haeone Lee"
                    },
                    {
                        "name": "Hanho Ryu"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Source code: https://github.com/mintaywon/IF_RLHF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05091v2",
                "updated": "2025-01-10T08:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    43,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-09T09:15:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    15,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResPanDiff: Diffusion Model for Pansharpening by Inferring Residual\n  Inference"
                },
                "summary": "The implementation of diffusion-based pansharpening task is predominantly\nconstrained by its slow inference speed, which results from numerous sampling\nsteps. Despite the existing techniques aiming to accelerate sampling, they\noften compromise performance when fusing multi-source images. To ease this\nlimitation, we introduce a novel and efficient diffusion model named Diffusion\nModel for Pansharpening by Inferring Residual Inference (ResPanDiff), which\nsignificantly reduces the number of diffusion steps without sacrificing the\nperformance to tackle pansharpening task. In ResPanDiff, we innovatively\npropose a Markov chain that transits from noisy residuals to the residuals\nbetween the LRMS and HRMS images, thereby reducing the number of sampling steps\nand enhancing performance. Additionally, we design the latent space to help\nmodel extract more features at the encoding stage, Shallow\nCond-Injection~(SC-I) to help model fetch cond-injected hidden features with\nhigher dimensions, and loss functions to give a better guidance for the\nresidual generation task. enabling the model to achieve superior performance in\nresidual generation. Furthermore, experimental evaluations on pansharpening\ndatasets demonstrate that the proposed method achieves superior outcomes\ncompared to recent state-of-the-art~(SOTA) techniques, requiring only 15\nsampling steps, which reduces over $90\\%$ step compared with the benchmark\ndiffusion models. Our experiments also include thorough discussions and\nablation studies to underscore the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of diffusion-based pansharpening task is predominantly\nconstrained by its slow inference speed, which results from numerous sampling\nsteps. Despite the existing techniques aiming to accelerate sampling, they\noften compromise performance when fusing multi-source images. To ease this\nlimitation, we introduce a novel and efficient diffusion model named Diffusion\nModel for Pansharpening by Inferring Residual Inference (ResPanDiff), which\nsignificantly reduces the number of diffusion steps without sacrificing the\nperformance to tackle pansharpening task. In ResPanDiff, we innovatively\npropose a Markov chain that transits from noisy residuals to the residuals\nbetween the LRMS and HRMS images, thereby reducing the number of sampling steps\nand enhancing performance. Additionally, we design the latent space to help\nmodel extract more features at the encoding stage, Shallow\nCond-Injection~(SC-I) to help model fetch cond-injected hidden features with\nhigher dimensions, and loss functions to give a better guidance for the\nresidual generation task. enabling the model to achieve superior performance in\nresidual generation. Furthermore, experimental evaluations on pansharpening\ndatasets demonstrate that the proposed method achieves superior outcomes\ncompared to recent state-of-the-art~(SOTA) techniques, requiring only 15\nsampling steps, which reduces over $90\\%$ step compared with the benchmark\ndiffusion models. Our experiments also include thorough discussions and\nablation studies to underscore the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Shiqi Cao"
                    },
                    {
                        "name": "Liangjian Deng"
                    },
                    {
                        "name": "Shangqi Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shangqi Deng"
                },
                "author": "Shangqi Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05787v1",
                "updated": "2025-01-10T08:41:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    41,
                    42,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T08:41:42Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    41,
                    42,
                    4,
                    10,
                    0
                ],
                "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model"
                },
                "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with\nzero-shot voice cloning abilities. However, they often struggle with more\nexpressive references or complex text inputs. We present MARS6, a robust\nencoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent\nimprovements in spoken language modelling. Utilizing a hierarchical setup for\nits decoder, new speech tokens are processed at a rate of only 12 Hz, enabling\nefficient modelling of long-form text while retaining reconstruction quality.\nWe combine several recent training and inference techniques to reduce\nrepetitive generation and improve output stability and quality. This enables\nthe 70M-parameter MARS6 to achieve similar performance to models many times\nlarger. We show this in objective and subjective evaluations, comparing TTS\noutput quality and reference speaker cloning ability. Project page:\nhttps://camb-ai.github.io/mars6-turbo/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codec-based text-to-speech (TTS) models have shown impressive quality with\nzero-shot voice cloning abilities. However, they often struggle with more\nexpressive references or complex text inputs. We present MARS6, a robust\nencoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent\nimprovements in spoken language modelling. Utilizing a hierarchical setup for\nits decoder, new speech tokens are processed at a rate of only 12 Hz, enabling\nefficient modelling of long-form text while retaining reconstruction quality.\nWe combine several recent training and inference techniques to reduce\nrepetitive generation and improve output stability and quality. This enables\nthe 70M-parameter MARS6 to achieve similar performance to models many times\nlarger. We show this in objective and subjective evaluations, comparing TTS\noutput quality and reference speaker cloning ability. Project page:\nhttps://camb-ai.github.io/mars6-turbo/"
                },
                "authors": [
                    {
                        "name": "Matthew Baas"
                    },
                    {
                        "name": "Pieter Scholtz"
                    },
                    {
                        "name": "Arnav Mehta"
                    },
                    {
                        "name": "Elliott Dyson"
                    },
                    {
                        "name": "Akshat Prakash"
                    },
                    {
                        "name": "Herman Kamper"
                    }
                ],
                "author_detail": {
                    "name": "Herman Kamper"
                },
                "author": "Herman Kamper",
                "arxiv_comment": "5 pages, 2 figures, 1 table. Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05777v1",
                "updated": "2025-01-10T08:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    18,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T08:18:37Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    18,
                    37,
                    4,
                    10,
                    0
                ],
                "title": "StructSR: Refuse Spurious Details in Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructSR: Refuse Spurious Details in Real-World Image Super-Resolution"
                },
                "summary": "Diffusion-based models have shown great promise in real-world image\nsuper-resolution (Real-ISR), but often generate content with structural errors\nand spurious texture details due to the empirical priors and illusions of these\nmodels. To address this issue, we introduce StructSR, a simple, effective, and\nplug-and-play method that enhances structural fidelity and suppresses spurious\ndetails for diffusion-based Real-ISR. StructSR operates without the need for\nadditional fine-tuning, external model priors, or high-level semantic\nknowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which\nidentifies the image with the highest structural similarity to the\nlow-resolution (LR) input in the early inference stage, allowing us to leverage\nit as a historical structure knowledge to suppress the generation of spurious\ndetails. By intervening in the diffusion inference process, StructSR seamlessly\nintegrates with existing diffusion-based Real-ISR models. Our experimental\nresults demonstrate that StructSR significantly improves the fidelity of\nstructure and texture, improving the PSNR and SSIM metrics by an average of\n5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two\nreal-world datasets (RealSR and DRealSR) when integrated with four\nstate-of-the-art diffusion-based Real-ISR methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based models have shown great promise in real-world image\nsuper-resolution (Real-ISR), but often generate content with structural errors\nand spurious texture details due to the empirical priors and illusions of these\nmodels. To address this issue, we introduce StructSR, a simple, effective, and\nplug-and-play method that enhances structural fidelity and suppresses spurious\ndetails for diffusion-based Real-ISR. StructSR operates without the need for\nadditional fine-tuning, external model priors, or high-level semantic\nknowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which\nidentifies the image with the highest structural similarity to the\nlow-resolution (LR) input in the early inference stage, allowing us to leverage\nit as a historical structure knowledge to suppress the generation of spurious\ndetails. By intervening in the diffusion inference process, StructSR seamlessly\nintegrates with existing diffusion-based Real-ISR models. Our experimental\nresults demonstrate that StructSR significantly improves the fidelity of\nstructure and texture, improving the PSNR and SSIM metrics by an average of\n5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two\nreal-world datasets (RealSR and DRealSR) when integrated with four\nstate-of-the-art diffusion-based Real-ISR methods."
                },
                "authors": [
                    {
                        "name": "Yachao Li"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Tianyu Ding"
                    },
                    {
                        "name": "Sheng-Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng-Jun Huang"
                },
                "author": "Sheng-Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02565v2",
                "updated": "2025-01-10T08:01:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "published": "2023-11-05T04:43:48Z",
                "published_parsed": [
                    2023,
                    11,
                    5,
                    4,
                    43,
                    48,
                    6,
                    309,
                    0
                ],
                "title": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy"
                },
                "summary": "Sensors are commonly deployed to perceive the environment. However, due to\nthe high cost, sensors are usually sparsely deployed. Kriging is the tailored\ntask to infer the unobserved nodes (without sensors) using the observed source\nnodes (with sensors). The essence of kriging task is transferability. Recently,\nseveral inductive spatio-temporal kriging methods have been proposed based on\ngraph neural networks, being trained based on a graph built on top of observed\nnodes via pretext tasks such as masking nodes out and reconstructing them.\nHowever, the graph in training is inevitably much sparser than the graph in\ninference that includes all the observed and unobserved nodes. The learned\npattern cannot be well generalized for inference, denoted as graph gap. To\naddress this issue, we first present a novel Increment training strategy:\ninstead of masking nodes (and reconstructing them), we add virtual nodes into\nthe training graph so as to mitigate the graph gap issue naturally.\nNevertheless, the empty-shell virtual nodes without labels could have\nbad-learned features and lack supervision signals. To solve these issues, we\npair each virtual node with its most similar observed node and fuse their\nfeatures together; to enhance the supervision signal, we construct reliable\npseudo labels for virtual nodes. As a result, the learned pattern of virtual\nnodes could be safely transferred to real unobserved nodes for reliable\nkriging. We name our new Kriging model with Increment Training Strategy as\nKITS. Extensive experiments demonstrate that KITS consistently outperforms\nexisting kriging methods by large margins, e.g., the improvement over MAE score\ncould be as high as 18.33%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensors are commonly deployed to perceive the environment. However, due to\nthe high cost, sensors are usually sparsely deployed. Kriging is the tailored\ntask to infer the unobserved nodes (without sensors) using the observed source\nnodes (with sensors). The essence of kriging task is transferability. Recently,\nseveral inductive spatio-temporal kriging methods have been proposed based on\ngraph neural networks, being trained based on a graph built on top of observed\nnodes via pretext tasks such as masking nodes out and reconstructing them.\nHowever, the graph in training is inevitably much sparser than the graph in\ninference that includes all the observed and unobserved nodes. The learned\npattern cannot be well generalized for inference, denoted as graph gap. To\naddress this issue, we first present a novel Increment training strategy:\ninstead of masking nodes (and reconstructing them), we add virtual nodes into\nthe training graph so as to mitigate the graph gap issue naturally.\nNevertheless, the empty-shell virtual nodes without labels could have\nbad-learned features and lack supervision signals. To solve these issues, we\npair each virtual node with its most similar observed node and fuse their\nfeatures together; to enhance the supervision signal, we construct reliable\npseudo labels for virtual nodes. As a result, the learned pattern of virtual\nnodes could be safely transferred to real unobserved nodes for reliable\nkriging. We name our new Kriging model with Increment Training Strategy as\nKITS. Extensive experiments demonstrate that KITS consistently outperforms\nexisting kriging methods by large margins, e.g., the improvement over MAE score\ncould be as high as 18.33%."
                },
                "authors": [
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Sijie Ruan"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Zhishuai Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhishuai Li"
                },
                "author": "Zhishuai Li",
                "arxiv_comment": "This paper is accepted by AAAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15938v2",
                "updated": "2025-01-10T08:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    0,
                    34,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-22T18:00:05Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    18,
                    0,
                    5,
                    0,
                    204,
                    0
                ],
                "title": "Large dark matter content and steep metallicity profile predicted for\n  Ultra-Diffuse Galaxies formed in high-spin halos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large dark matter content and steep metallicity profile predicted for\n  Ultra-Diffuse Galaxies formed in high-spin halos"
                },
                "summary": "We study the stellar properties of a sample of simulated ultra-diffuse\ngalaxies (UDGs) with stellar mass $\\rm{M_\\star=10^{7.5} - 10^{9} ~ M_{\\odot}}$,\nselected from the TNG50 simulation, where UDGs form mainly in high-spin\ndwarf-mass halos. We divide our sample into star-forming and quenched UDGs,\nfinding good agreement with the stellar assembly history measured in\nobservations. Star-forming UDGs and quenched UDGs with $\\rm{M_\\star \\geq 10^8 ~\nM_\\odot}$ in our sample are particularly inefficient at forming stars, having\n$2$ - $10$ times less stellar mass than non-UDGs for the same virial mass halo.\nThese results are consistent with recent mass inferences in UDG samples and\nsuggest that the most inefficient UDGs arise from a late assembly of the dark\nmatter mass followed by a stellar growth that is comparatively slower (for\nstar-forming UDGs) or that was interrupted due to environmental removal of the\ngas (for quenched UDGs). Regardless of efficiency, UDGs are $60\\%$ poorer in\n[Fe/H] than the population of non-UDGs at a fixed stellar mass, with the most\nextreme objects having metal content consistent with the simulated\nmass-metallicity relation at $z \\sim 2$. Quenched UDGs stop their star\nformation in shorter timescales than non-UDGs of similar mass and are, as a\nconsequence, alpha-enhanced with respect to non-UDGs. We identify metallicity\nprofiles in UDGs as a potential avenue to distinguish between different\nformation paths for these galaxies, where gentle formation as a result of\nhigh-spin halos would present well-defined declining metallicity radial\nprofiles while powerful-outflows or tidal stripping formation models would lead\nto flatter or constant metallicity as a function of radius due to the inherent\nmixing of stellar orbits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the stellar properties of a sample of simulated ultra-diffuse\ngalaxies (UDGs) with stellar mass $\\rm{M_\\star=10^{7.5} - 10^{9} ~ M_{\\odot}}$,\nselected from the TNG50 simulation, where UDGs form mainly in high-spin\ndwarf-mass halos. We divide our sample into star-forming and quenched UDGs,\nfinding good agreement with the stellar assembly history measured in\nobservations. Star-forming UDGs and quenched UDGs with $\\rm{M_\\star \\geq 10^8 ~\nM_\\odot}$ in our sample are particularly inefficient at forming stars, having\n$2$ - $10$ times less stellar mass than non-UDGs for the same virial mass halo.\nThese results are consistent with recent mass inferences in UDG samples and\nsuggest that the most inefficient UDGs arise from a late assembly of the dark\nmatter mass followed by a stellar growth that is comparatively slower (for\nstar-forming UDGs) or that was interrupted due to environmental removal of the\ngas (for quenched UDGs). Regardless of efficiency, UDGs are $60\\%$ poorer in\n[Fe/H] than the population of non-UDGs at a fixed stellar mass, with the most\nextreme objects having metal content consistent with the simulated\nmass-metallicity relation at $z \\sim 2$. Quenched UDGs stop their star\nformation in shorter timescales than non-UDGs of similar mass and are, as a\nconsequence, alpha-enhanced with respect to non-UDGs. We identify metallicity\nprofiles in UDGs as a potential avenue to distinguish between different\nformation paths for these galaxies, where gentle formation as a result of\nhigh-spin halos would present well-defined declining metallicity radial\nprofiles while powerful-outflows or tidal stripping formation models would lead\nto flatter or constant metallicity as a function of radius due to the inherent\nmixing of stellar orbits."
                },
                "authors": [
                    {
                        "name": "José A. Benavides"
                    },
                    {
                        "name": "Laura V. Sales"
                    },
                    {
                        "name": "Mario. G. Abadi"
                    },
                    {
                        "name": "Mark Vogelsberger"
                    },
                    {
                        "name": "Federico Marinacci"
                    },
                    {
                        "name": "Lars Hernquist"
                    }
                ],
                "author_detail": {
                    "name": "Lars Hernquist"
                },
                "author": "Lars Hernquist",
                "arxiv_comment": "19 pages, 11 figures, Published in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05764v1",
                "updated": "2025-01-10T07:41:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T07:41:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Controlling Large Language Models Through Concept Activation Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Large Language Models Through Concept Activation Vectors"
                },
                "summary": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Chengao Li"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05763v1",
                "updated": "2025-01-10T07:41:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    47,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T07:41:47Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    47,
                    4,
                    10,
                    0
                ],
                "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion\n  Model for Scalable and Controllable Scene Generation"
                },
                "summary": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large reconstruction and generative models have\nsignificantly improved scene reconstruction and novel view generation. However,\ndue to compute limitations, each inference with these large models is confined\nto a small area, making long-range consistent scene generation challenging. To\naddress this, we propose StarGen, a novel framework that employs a pre-trained\nvideo diffusion model in an autoregressive manner for long-range scene\ngeneration. The generation of each video clip is conditioned on the 3D warping\nof spatially adjacent images and the temporally overlapping image from\npreviously generated clips, improving spatiotemporal consistency in long-range\nscene generation with precise pose control. The spatiotemporal condition is\ncompatible with various input conditions, facilitating diverse tasks, including\nsparse view interpolation, perpetual view generation, and layout-conditioned\ncity generation. Quantitative and qualitative evaluations demonstrate StarGen's\nsuperior scalability, fidelity, and pose accuracy compared to state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Shangjin Zhai"
                    },
                    {
                        "name": "Zhichao Ye"
                    },
                    {
                        "name": "Jialin Liu"
                    },
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhen Peng"
                    },
                    {
                        "name": "Hua Xue"
                    },
                    {
                        "name": "Danpeng Chen"
                    },
                    {
                        "name": "Xiaomeng Wang"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Haomin Liu"
                    },
                    {
                        "name": "Guofeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guofeng Zhang"
                },
                "author": "Guofeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12746v2",
                "updated": "2025-01-10T07:38:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    38,
                    53,
                    4,
                    10,
                    0
                ],
                "published": "2023-10-19T13:50:56Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    13,
                    50,
                    56,
                    3,
                    292,
                    0
                ],
                "title": "TabuLa: Harnessing Language Models for Tabular Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabuLa: Harnessing Language Models for Tabular Data Synthesis"
                },
                "summary": "Tabular data synthesis is crucial for addressing privacy and security\nconcerns in industries reliant on tabular data. While recent advancements adopt\nlarge language models (LLMs) for realistic tabular data generation, their long\ntraining times and limited reusability hinder practical applications. In this\npaper, we propose Tabula, a tabular data synthesizer that leverages the\nstructure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data\nsynthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained\nweights originally designed for natural language tasks, focusing instead on a\ntailored approach for tabular data. In addition, Tabula introduces a token\nsequence compression strategy that significantly reduces training time while\nmaintaining data quality, alongside a novel token padding method that improves\nsequence alignment across training batches. Experiments on six datasets show\nthat Tabula achieves superior synthetic data utility compared to current SOTA\nmethods. Additionally, the results demonstrate that Tabula model trained on\ntabular datasets serves effectively as a foundational model for synthesizing\nnew tabular datasets. Furthermore, the proposed padding method outperforms the\nconventional left and right padding strategies. Finally, the results highlight\nthat Tabula averagely reduces training time per epoch by 46.2% compared to\nstate-of-the-art LLM approaches while achieving higher data utility. Our code\nis available at https://github.com/zhao-zilong/Tabula",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial for addressing privacy and security\nconcerns in industries reliant on tabular data. While recent advancements adopt\nlarge language models (LLMs) for realistic tabular data generation, their long\ntraining times and limited reusability hinder practical applications. In this\npaper, we propose Tabula, a tabular data synthesizer that leverages the\nstructure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data\nsynthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained\nweights originally designed for natural language tasks, focusing instead on a\ntailored approach for tabular data. In addition, Tabula introduces a token\nsequence compression strategy that significantly reduces training time while\nmaintaining data quality, alongside a novel token padding method that improves\nsequence alignment across training batches. Experiments on six datasets show\nthat Tabula achieves superior synthetic data utility compared to current SOTA\nmethods. Additionally, the results demonstrate that Tabula model trained on\ntabular datasets serves effectively as a foundational model for synthesizing\nnew tabular datasets. Furthermore, the proposed padding method outperforms the\nconventional left and right padding strategies. Finally, the results highlight\nthat Tabula averagely reduces training time per epoch by 46.2% compared to\nstate-of-the-art LLM approaches while achieving higher data utility. Our code\nis available at https://github.com/zhao-zilong/Tabula"
                },
                "authors": [
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Robert Birke"
                    },
                    {
                        "name": "Lydia Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Chen"
                },
                "author": "Lydia Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11210v2",
                "updated": "2025-01-10T07:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    26,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-15T15:04:27Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    15,
                    4,
                    27,
                    6,
                    350,
                    0
                ],
                "title": "ViPOcc: Leveraging Visual Priors from Vision Foundation Models for\n  Single-View 3D Occupancy Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViPOcc: Leveraging Visual Priors from Vision Foundation Models for\n  Single-View 3D Occupancy Prediction"
                },
                "summary": "Inferring the 3D structure of a scene from a single image is an ill-posed and\nchallenging problem in the field of vision-centric autonomous driving. Existing\nmethods usually employ neural radiance fields to produce voxelized 3D\noccupancy, lacking instance-level semantic reasoning and temporal photometric\nconsistency. In this paper, we propose ViPOcc, which leverages the visual\npriors from vision foundation models (VFMs) for fine-grained 3D occupancy\nprediction. Unlike previous works that solely employ volume rendering for RGB\nand depth image reconstruction, we introduce a metric depth estimation branch,\nin which an inverse depth alignment module is proposed to bridge the domain gap\nin depth distribution between VFM predictions and the ground truth. The\nrecovered metric depth is then utilized in temporal photometric alignment and\nspatial geometric alignment to ensure accurate and consistent 3D occupancy\nprediction. Additionally, we also propose a semantic-guided non-overlapping\nGaussian mixture sampler for efficient, instance-aware ray sampling, which\naddresses the redundant and imbalanced sampling issue that still exists in\nprevious state-of-the-art methods. Extensive experiments demonstrate the\nsuperior performance of ViPOcc in both 3D occupancy prediction and depth\nestimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available\nat: \\url{https://mias.group/ViPOcc}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the 3D structure of a scene from a single image is an ill-posed and\nchallenging problem in the field of vision-centric autonomous driving. Existing\nmethods usually employ neural radiance fields to produce voxelized 3D\noccupancy, lacking instance-level semantic reasoning and temporal photometric\nconsistency. In this paper, we propose ViPOcc, which leverages the visual\npriors from vision foundation models (VFMs) for fine-grained 3D occupancy\nprediction. Unlike previous works that solely employ volume rendering for RGB\nand depth image reconstruction, we introduce a metric depth estimation branch,\nin which an inverse depth alignment module is proposed to bridge the domain gap\nin depth distribution between VFM predictions and the ground truth. The\nrecovered metric depth is then utilized in temporal photometric alignment and\nspatial geometric alignment to ensure accurate and consistent 3D occupancy\nprediction. Additionally, we also propose a semantic-guided non-overlapping\nGaussian mixture sampler for efficient, instance-aware ray sampling, which\naddresses the redundant and imbalanced sampling issue that still exists in\nprevious state-of-the-art methods. Extensive experiments demonstrate the\nsuperior performance of ViPOcc in both 3D occupancy prediction and depth\nestimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available\nat: \\url{https://mias.group/ViPOcc}."
                },
                "authors": [
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Xijing Zhang"
                    },
                    {
                        "name": "Tanghui Li"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Rui Fan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Fan"
                },
                "author": "Rui Fan",
                "arxiv_comment": "accepted to AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10718v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10718v3",
                "updated": "2025-01-10T07:20:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    20,
                    26,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-14T07:22:03Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    7,
                    22,
                    3,
                    5,
                    349,
                    0
                ],
                "title": "GridShow: Omni Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GridShow: Omni Visual Generation"
                },
                "summary": "In this paper, we introduce GRID, a novel paradigm that reframes a broad\nrange of visual generation tasks as the problem of arranging grids, akin to\nfilm strips. At its core, GRID transforms temporal sequences into grid layouts,\nenabling image generation models to process visual sequences holistically. To\nachieve both layout consistency and motion coherence, we develop a parallel\nflow-matching training strategy that combines layout matching and temporal\nlosses, guided by a coarse-to-fine schedule that evolves from basic layouts to\nprecise motion control. Our approach demonstrates remarkable efficiency,\nachieving up to 35 faster inference speeds while using 1/1000 of the\ncomputational resources compared to specialized models. Extensive experiments\nshow that GRID exhibits exceptional versatility across diverse visual\ngeneration tasks, from Text-to-Video to 3D Editing, while maintaining its\nfoundational image generation capabilities. This dual strength in both expanded\napplications and preserved core competencies establishes GRID as an efficient\nand versatile omni-solution for visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce GRID, a novel paradigm that reframes a broad\nrange of visual generation tasks as the problem of arranging grids, akin to\nfilm strips. At its core, GRID transforms temporal sequences into grid layouts,\nenabling image generation models to process visual sequences holistically. To\nachieve both layout consistency and motion coherence, we develop a parallel\nflow-matching training strategy that combines layout matching and temporal\nlosses, guided by a coarse-to-fine schedule that evolves from basic layouts to\nprecise motion control. Our approach demonstrates remarkable efficiency,\nachieving up to 35 faster inference speeds while using 1/1000 of the\ncomputational resources compared to specialized models. Extensive experiments\nshow that GRID exhibits exceptional versatility across diverse visual\ngeneration tasks, from Text-to-Video to 3D Editing, while maintaining its\nfoundational image generation capabilities. This dual strength in both expanded\napplications and preserved core competencies establishes GRID as an efficient\nand versatile omni-solution for visual generation."
                },
                "authors": [
                    {
                        "name": "Cong Wan"
                    },
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Zijian Cai"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "arxiv_comment": "Codes: https://github.com/Should-AI-Lab/GRID",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10718v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10718v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05752v1",
                "updated": "2025-01-10T07:02:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    2,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T07:02:43Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    2,
                    43,
                    4,
                    10,
                    0
                ],
                "title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving\n  with Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in various complex tasks requiring multi-step reasoning methods like\ntree search to explore diverse reasoning paths. However, existing methods often\nsuffer from computational inefficiency and redundancy. First, they overlook the\ndiversity of task difficulties, leading to unnecessarily extensive searches\neven for easy tasks. Second, they neglect the semantics of reasoning paths,\nresulting in redundant exploration of semantically identical paths. To address\nthese limitations, we propose Semantic Exploration with Adaptive Gating (SEAG),\na computationally efficient method. SEAG employs an adaptive gating mechanism\nthat dynamically decides whether to conduct a tree search, based on the\nconfidence level of answers from a preceding simple reasoning method.\nFurthermore, its tree-based exploration consolidates semantically identical\nreasoning steps, reducing redundant explorations while maintaining or even\nimproving accuracy. Our extensive experiments demonstrate that SEAG\nsignificantly improves accuracy by 4.3% on average while requiring only 31% of\ncomputational costs compared to existing tree search-based methods on complex\nreasoning benchmarks including GSM8K and ARC with diverse language models such\nas Llama2, Llama3, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in various complex tasks requiring multi-step reasoning methods like\ntree search to explore diverse reasoning paths. However, existing methods often\nsuffer from computational inefficiency and redundancy. First, they overlook the\ndiversity of task difficulties, leading to unnecessarily extensive searches\neven for easy tasks. Second, they neglect the semantics of reasoning paths,\nresulting in redundant exploration of semantically identical paths. To address\nthese limitations, we propose Semantic Exploration with Adaptive Gating (SEAG),\na computationally efficient method. SEAG employs an adaptive gating mechanism\nthat dynamically decides whether to conduct a tree search, based on the\nconfidence level of answers from a preceding simple reasoning method.\nFurthermore, its tree-based exploration consolidates semantically identical\nreasoning steps, reducing redundant explorations while maintaining or even\nimproving accuracy. Our extensive experiments demonstrate that SEAG\nsignificantly improves accuracy by 4.3% on average while requiring only 31% of\ncomputational costs compared to existing tree search-based methods on complex\nreasoning benchmarks including GSM8K and ARC with diverse language models such\nas Llama2, Llama3, and Mistral."
                },
                "authors": [
                    {
                        "name": "Sungjae Lee"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Jaechang Kim"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01735v3",
                "updated": "2025-01-10T06:45:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    6,
                    45,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2024-09-03T09:24:04Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    24,
                    4,
                    1,
                    247,
                    0
                ],
                "title": "Multi-objective Bayesian optimization for Likelihood-Free inference in\n  sequential sampling models of decision making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective Bayesian optimization for Likelihood-Free inference in\n  sequential sampling models of decision making"
                },
                "summary": "Scientifically motivated statistical models can sometimes be defined by a\ngenerative process for simulating synthetic data. Models specified this way can\nhave likelihoods which are intractable, and this is the case for many\nsequential sampling models (SSMs) widely used in psychology and consumer\nbehavior modelling. Researchers have developed likelihood-free inference (LFI)\nmethods to make Bayesian inferences on parameters in models with intractable\nlikelihood. Extending a popular approach to simulation efficient LFI for\nsingle-source data, we propose Multi-objective Bayesian Optimization for\nLikelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs\ncalibrated using multi-source data, such as those based on response times and\nchoice outcomes. MOBOLFI models a multi-dimensional discrepancy between\nobserved and simulated data, using a discrepancy for each data source.\nMulti-objective Bayesian Optimization is then used to ensure simulation\nefficient approximation of the SSM likelihood. The use of a multivariate\ndiscrepancy allows for approximations to individual data source likelihoods in\naddition to the joint likelihood, enabling both the detection of conflicting\ninformation and a deeper understanding of the importance of different data\nsources in estimating individual SSM parameters. We illustrate the advantages\nof our approach in comparison with the use of a single discrepancy in a simple\nsynthetic data example and an SSM example with real-world data assessing\npreferences of ride-hailing drivers in Singapore to rent electric vehicles.\nAlthough we focus on applications to SSMs, our approach applies to the\nlikelihood-free calibration of other models using multi-source data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientifically motivated statistical models can sometimes be defined by a\ngenerative process for simulating synthetic data. Models specified this way can\nhave likelihoods which are intractable, and this is the case for many\nsequential sampling models (SSMs) widely used in psychology and consumer\nbehavior modelling. Researchers have developed likelihood-free inference (LFI)\nmethods to make Bayesian inferences on parameters in models with intractable\nlikelihood. Extending a popular approach to simulation efficient LFI for\nsingle-source data, we propose Multi-objective Bayesian Optimization for\nLikelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs\ncalibrated using multi-source data, such as those based on response times and\nchoice outcomes. MOBOLFI models a multi-dimensional discrepancy between\nobserved and simulated data, using a discrepancy for each data source.\nMulti-objective Bayesian Optimization is then used to ensure simulation\nefficient approximation of the SSM likelihood. The use of a multivariate\ndiscrepancy allows for approximations to individual data source likelihoods in\naddition to the joint likelihood, enabling both the detection of conflicting\ninformation and a deeper understanding of the importance of different data\nsources in estimating individual SSM parameters. We illustrate the advantages\nof our approach in comparison with the use of a single discrepancy in a simple\nsynthetic data example and an SSM example with real-world data assessing\npreferences of ride-hailing drivers in Singapore to rent electric vehicles.\nAlthough we focus on applications to SSMs, our approach applies to the\nlikelihood-free calibration of other models using multi-source data."
                },
                "authors": [
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Xinwei Li"
                    },
                    {
                        "name": "Eui-Jin Kim"
                    },
                    {
                        "name": "Prateek Bansal"
                    },
                    {
                        "name": "David Nott"
                    }
                ],
                "author_detail": {
                    "name": "David Nott"
                },
                "author": "David Nott",
                "arxiv_comment": "39 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05745v1",
                "updated": "2025-01-10T06:21:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    6,
                    21,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T06:21:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    6,
                    21,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Covariate Dependent Mixture of Bayesian Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariate Dependent Mixture of Bayesian Networks"
                },
                "summary": "Learning the structure of Bayesian networks from data provides insights into\nunderlying processes and the causal relationships that generate the data, but\nits usefulness depends on the homogeneity of the data population, a condition\noften violated in real-world applications. In such cases, using a single\nnetwork structure for inference can be misleading, as it may not capture\nsub-population differences. To address this, we propose a novel approach of\nmodelling a mixture of Bayesian networks where component probabilities depend\non individual characteristics. Our method identifies both network structures\nand demographic predictors of sub-population membership, aiding personalised\ninterventions. We evaluate our method through simulations and a youth mental\nhealth case study, demonstrating its potential to improve tailored\ninterventions in health, education, and social policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the structure of Bayesian networks from data provides insights into\nunderlying processes and the causal relationships that generate the data, but\nits usefulness depends on the homogeneity of the data population, a condition\noften violated in real-world applications. In such cases, using a single\nnetwork structure for inference can be misleading, as it may not capture\nsub-population differences. To address this, we propose a novel approach of\nmodelling a mixture of Bayesian networks where component probabilities depend\non individual characteristics. Our method identifies both network structures\nand demographic predictors of sub-population membership, aiding personalised\ninterventions. We evaluate our method through simulations and a youth mental\nhealth case study, demonstrating its potential to improve tailored\ninterventions in health, education, and social policy."
                },
                "authors": [
                    {
                        "name": "Roman Marchant"
                    },
                    {
                        "name": "Dario Draca"
                    },
                    {
                        "name": "Gilad Francis"
                    },
                    {
                        "name": "Sahand Assadzadeh"
                    },
                    {
                        "name": "Mathew Varidel"
                    },
                    {
                        "name": "Frank Iorfino"
                    },
                    {
                        "name": "Sally Cripps"
                    }
                ],
                "author_detail": {
                    "name": "Sally Cripps"
                },
                "author": "Sally Cripps",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09093v2",
                "updated": "2025-01-10T06:04:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    6,
                    4,
                    24,
                    4,
                    10,
                    0
                ],
                "published": "2024-08-17T04:43:26Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    4,
                    43,
                    26,
                    5,
                    230,
                    0
                ],
                "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yirui Zhang"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05730v1",
                "updated": "2025-01-10T05:54:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    54,
                    4,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:54:04Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    54,
                    4,
                    4,
                    10,
                    0
                ],
                "title": "Element-wise Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Element-wise Attention Is All You Need"
                },
                "summary": "The self-attention (SA) mechanism has demonstrated superior performance\nacross various domains, yet it suffers from substantial complexity during both\ntraining and inference. The next-generation architecture, aiming at retaining\nthe competitive performance of SA while achieving low-cost inference and\nefficient long-sequence training, primarily focuses on three approaches: linear\nattention, linear RNNs, and state space models. Although these approaches\nachieve reduced complexity than SA, they all have built-in performance\ndegradation factors, such as diminished “spikiness” and compression of\nhistorical information. In contrast to these approaches, we propose a novel\nelement-wise attention mechanism, which uses the element-wise squared Euclidean\ndistance, instead of the dot product operation, to compute similarity and\napproximates the quadratic complexity term $\\exp(q_{ic}k_{jc})$ with a Taylor\npolynomial. This design achieves remarkable efficiency: during training, the\nelement-wise attention has a complexity of $\\mathcal{O}(tLD)$, making\nlong-sequence training both computationally and memory efficient, where $L$ is\nthe sequence length, $D$ is the feature dimension, and $t$ is the highest order\nof the polynomial; during inference, it can be reformulated as recurrent neural\nnetworks, achieving a inference complexity of $\\mathcal{O}(tD)$. Furthermore,\nthe element-wise attention circumvents the performance degradation factors\npresent in these approaches and achieves performance comparable to SA in both\ncausal and non-causal forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-attention (SA) mechanism has demonstrated superior performance\nacross various domains, yet it suffers from substantial complexity during both\ntraining and inference. The next-generation architecture, aiming at retaining\nthe competitive performance of SA while achieving low-cost inference and\nefficient long-sequence training, primarily focuses on three approaches: linear\nattention, linear RNNs, and state space models. Although these approaches\nachieve reduced complexity than SA, they all have built-in performance\ndegradation factors, such as diminished “spikiness” and compression of\nhistorical information. In contrast to these approaches, we propose a novel\nelement-wise attention mechanism, which uses the element-wise squared Euclidean\ndistance, instead of the dot product operation, to compute similarity and\napproximates the quadratic complexity term $\\exp(q_{ic}k_{jc})$ with a Taylor\npolynomial. This design achieves remarkable efficiency: during training, the\nelement-wise attention has a complexity of $\\mathcal{O}(tLD)$, making\nlong-sequence training both computationally and memory efficient, where $L$ is\nthe sequence length, $D$ is the feature dimension, and $t$ is the highest order\nof the polynomial; during inference, it can be reformulated as recurrent neural\nnetworks, achieving a inference complexity of $\\mathcal{O}(tD)$. Furthermore,\nthe element-wise attention circumvents the performance degradation factors\npresent in these approaches and achieves performance comparable to SA in both\ncausal and non-causal forms."
                },
                "authors": [
                    {
                        "name": "Guoxin Feng"
                    }
                ],
                "author_detail": {
                    "name": "Guoxin Feng"
                },
                "author": "Guoxin Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05728v1",
                "updated": "2025-01-10T05:53:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    53,
                    32,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:53:32Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    53,
                    32,
                    4,
                    10,
                    0
                ],
                "title": "Super-class guided Transformer for Zero-Shot Attribute Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-class guided Transformer for Zero-Shot Attribute Classification"
                },
                "summary": "Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns SugaFormer's features with VLMs using\nregion-specific prompts, and during inference, Zero-shot Retrieval-based Score\nEnhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute classification is crucial for identifying specific characteristics\nwithin image regions. Vision-Language Models (VLMs) have been effective in\nzero-shot tasks by leveraging their general knowledge from large-scale\ndatasets. Recent studies demonstrate that transformer-based models with\nclass-wise queries can effectively address zero-shot multi-label\nclassification. However, poor utilization of the relationship between seen and\nunseen attributes makes the model lack generalizability. Additionally,\nattribute classification generally involves many attributes, making maintaining\nthe model's scalability difficult. To address these issues, we propose\nSuper-class guided transFormer (SugaFormer), a novel framework that leverages\nsuper-classes to enhance scalability and generalizability for zero-shot\nattribute classification. SugaFormer employs Super-class Query Initialization\n(SQI) to reduce the number of queries, utilizing common semantic information\nfrom super-classes, and incorporates Multi-context Decoding (MD) to handle\ndiverse visual cues. To strengthen generalizability, we introduce two knowledge\ntransfer strategies that utilize VLMs. During training, Super-class guided\nConsistency Regularization (SCR) aligns SugaFormer's features with VLMs using\nregion-specific prompts, and during inference, Zero-shot Retrieval-based Score\nEnhancement (ZRSE) refines predictions for unseen attributes. Extensive\nexperiments demonstrate that SugaFormer achieves state-of-the-art performance\nacross three widely-used attribute classification benchmarks under zero-shot,\nand cross-dataset transfer settings. Our code is available at\nhttps://github.com/mlvlab/SugaFormer."
                },
                "authors": [
                    {
                        "name": "Sehyung Kim"
                    },
                    {
                        "name": "Chanhyeong Yang"
                    },
                    {
                        "name": "Jihwan Park"
                    },
                    {
                        "name": "Taehoon Song"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05727v1",
                "updated": "2025-01-10T05:51:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    51,
                    52,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:51:52Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    51,
                    52,
                    4,
                    10,
                    0
                ],
                "title": "Enabling Scalable Oversight via Self-Evolving Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Scalable Oversight via Self-Evolving Critic"
                },
                "summary": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component."
                },
                "authors": [
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Zhenyang Xiao"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05724v1",
                "updated": "2025-01-10T05:43:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    36,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:43:36Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    36,
                    4,
                    10,
                    0
                ],
                "title": "I Can't Share Code, but I need Translation -- An Empirical Study on Code\n  Translation through Federated LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can't Share Code, but I need Translation -- An Empirical Study on Code\n  Translation through Federated LLM"
                },
                "summary": "Owing to the rapid evolution of technologies and project requirements,\norganizations need to upgrade the code base in their software projects to a new\nversion of the programming language or even translating to an entirely new one.\nHowever, code translation is resource-intensive and requires expertise in both\nthe source and target languages. While researchers have made progress in\nautomating translations between legacy and modern languages, recent work has\nincreasingly turned to pre-trained Large Language Models (LLMs) to translate\nefficiently.\n  Given the proprietary nature of code, organizations prefer fine-tuning LLMs\nlocally rather than relying on external APIs. This is one of the first\nempirical studies that proposes a Federated LLM-based approach for code\ntranslation. The proposed approach enables clients to jointly train a code\ntranslator without sharing sensitive data. This study demonstrates that\nparticipants can collaboratively develop a FedLLM for efficient code\ntranslation (particularly C\\# to Java and vice-versa) with superior results\n(more than 40\\% improvement in CodeLLaMA's CodeBLEU score) compared to\nindividual client models. Our findings indicate that FedLLM offers a\ncollaborative approach to code translation and could serve as a promising\ndirection for future research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the rapid evolution of technologies and project requirements,\norganizations need to upgrade the code base in their software projects to a new\nversion of the programming language or even translating to an entirely new one.\nHowever, code translation is resource-intensive and requires expertise in both\nthe source and target languages. While researchers have made progress in\nautomating translations between legacy and modern languages, recent work has\nincreasingly turned to pre-trained Large Language Models (LLMs) to translate\nefficiently.\n  Given the proprietary nature of code, organizations prefer fine-tuning LLMs\nlocally rather than relying on external APIs. This is one of the first\nempirical studies that proposes a Federated LLM-based approach for code\ntranslation. The proposed approach enables clients to jointly train a code\ntranslator without sharing sensitive data. This study demonstrates that\nparticipants can collaboratively develop a FedLLM for efficient code\ntranslation (particularly C\\# to Java and vice-versa) with superior results\n(more than 40\\% improvement in CodeLLaMA's CodeBLEU score) compared to\nindividual client models. Our findings indicate that FedLLM offers a\ncollaborative approach to code translation and could serve as a promising\ndirection for future research in this field."
                },
                "authors": [
                    {
                        "name": "Jahnavi Kumar"
                    },
                    {
                        "name": "Venkata Lakshmana Sasaank Janapati"
                    },
                    {
                        "name": "Mokshith Reddy Tanguturi"
                    },
                    {
                        "name": "Sridhar Chimalakonda"
                    }
                ],
                "author_detail": {
                    "name": "Sridhar Chimalakonda"
                },
                "author": "Sridhar Chimalakonda",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00778v3",
                "updated": "2025-01-10T05:35:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    35,
                    58,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-02T15:35:45Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    15,
                    35,
                    45,
                    6,
                    154,
                    0
                ],
                "title": "Bayesian Joint Additive Factor Models for Multiview Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Joint Additive Factor Models for Multiview Learning"
                },
                "summary": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar."
                },
                "authors": [
                    {
                        "name": "Niccolo Anceschi"
                    },
                    {
                        "name": "Federico Ferrari"
                    },
                    {
                        "name": "David B. Dunson"
                    },
                    {
                        "name": "Himel Mallick"
                    }
                ],
                "author_detail": {
                    "name": "Himel Mallick"
                },
                "author": "Himel Mallick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01350v2",
                "updated": "2025-01-10T05:35:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    35,
                    32,
                    4,
                    10,
                    0
                ],
                "published": "2024-10-02T09:07:33Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    9,
                    7,
                    33,
                    2,
                    276,
                    0
                ],
                "title": "Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid\n  Content Encoding and Enhanced Timbre Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid\n  Content Encoding and Enhanced Timbre Modeling"
                },
                "summary": "Expressive zero-shot voice conversion (VC) is a critical and challenging task\nthat aims to transform the source timbre into an arbitrary unseen speaker while\npreserving the original content and expressive qualities. Despite recent\nprogress in zero-shot VC, there remains considerable potential for improvements\nin speaker similarity and speech naturalness. Moreover, existing zero-shot VC\nsystems struggle to fully reproduce paralinguistic information in highly\nexpressive speech, such as breathing, crying, and emotional nuances, limiting\ntheir practical applicability. To address these issues, we propose Takin-VC, a\nnovel expressive zero-shot VC framework via adaptive hybrid content encoding\nand memory-augmented context-aware timbre modeling. Specifically, we introduce\nan innovative hybrid content encoder that incorporates an adaptive fusion\nmodule, capable of effectively integrating quantized features of the\npre-trained WavLM and HybridFormer in an implicit manner, so as to extract\nprecise linguistic features while enriching paralinguistic elements. For timbre\nmodeling, we propose advanced memory-augmented and context-aware modules to\ngenerate high-quality target timbre features and fused representations that\nseamlessly align source content with target timbre. To enhance real-time\nperformance, we advocate a conditional flow matching model to reconstruct the\nMel-spectrogram of the source speech. Experimental results show that our\nTakin-VC consistently surpasses state-of-the-art VC systems, achieving notable\nimprovements in terms of speech naturalness, speech expressiveness, and speaker\nsimilarity, while offering enhanced inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressive zero-shot voice conversion (VC) is a critical and challenging task\nthat aims to transform the source timbre into an arbitrary unseen speaker while\npreserving the original content and expressive qualities. Despite recent\nprogress in zero-shot VC, there remains considerable potential for improvements\nin speaker similarity and speech naturalness. Moreover, existing zero-shot VC\nsystems struggle to fully reproduce paralinguistic information in highly\nexpressive speech, such as breathing, crying, and emotional nuances, limiting\ntheir practical applicability. To address these issues, we propose Takin-VC, a\nnovel expressive zero-shot VC framework via adaptive hybrid content encoding\nand memory-augmented context-aware timbre modeling. Specifically, we introduce\nan innovative hybrid content encoder that incorporates an adaptive fusion\nmodule, capable of effectively integrating quantized features of the\npre-trained WavLM and HybridFormer in an implicit manner, so as to extract\nprecise linguistic features while enriching paralinguistic elements. For timbre\nmodeling, we propose advanced memory-augmented and context-aware modules to\ngenerate high-quality target timbre features and fused representations that\nseamlessly align source content with target timbre. To enhance real-time\nperformance, we advocate a conditional flow matching model to reconstruct the\nMel-spectrogram of the source speech. Experimental results show that our\nTakin-VC consistently surpasses state-of-the-art VC systems, achieving notable\nimprovements in terms of speech naturalness, speech expressiveness, and speaker\nsimilarity, while offering enhanced inference speed."
                },
                "authors": [
                    {
                        "name": "Yuguang Yang"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Jixun Yao"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Jianhao Ye"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Jianjun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Zhao"
                },
                "author": "Jianjun Zhao",
                "arxiv_comment": "Work in Progress; Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05714v1",
                "updated": "2025-01-10T05:15:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    15,
                    14,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:15:14Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    15,
                    14,
                    4,
                    10,
                    0
                ],
                "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond"
                },
                "summary": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Wenqiang Lei"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jimmy Xiangji Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Xiangji Huang"
                },
                "author": "Jimmy Xiangji Huang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05712v1",
                "updated": "2025-01-10T05:07:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    7,
                    27,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:07:27Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    7,
                    27,
                    4,
                    10,
                    0
                ],
                "title": "Multi-Step Reasoning in Korean and the Emergent Mirage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning in Korean and the Emergent Mirage"
                },
                "summary": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Dasol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Dasol Choi"
                },
                "author": "Dasol Choi",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04934v2",
                "updated": "2025-01-10T04:43:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    43,
                    24,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-09T02:52:30Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    2,
                    52,
                    30,
                    3,
                    9,
                    0
                ],
                "title": "Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel\n  Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel\n  Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images"
                },
                "summary": "Existing Weakly-Supervised Change Detection (WSCD) methods often encounter\nthe problem of \"instance lumping\" under scene-level supervision, particularly\nin scenarios with a dense distribution of changed instances (i.e., changed\nobjects). In these scenarios, unchanged pixels between changed instances are\nalso mistakenly identified as changed, causing multiple changes to be\nmistakenly viewed as one. In practical applications, this issue prevents the\naccurate quantification of the number of changes. To address this issue, we\npropose a Dense Instance Separation (DISep) method as a plug-and-play solution,\nrefining pixel features from a unified instance perspective under scene-level\nsupervision. Specifically, our DISep comprises a three-step iterative training\nprocess: 1) Instance Localization: We locate instance candidate regions for\nchanged pixels using high-pass class activation maps. 2) Instance Retrieval: We\nidentify and group these changed pixels into different instance IDs through\nconnectivity searching. Then, based on the assigned instance IDs, we extract\ncorresponding pixel-level features on a per-instance basis. 3) Instance\nSeparation: We introduce a separation loss to enforce intra-instance pixel\nconsistency in the embedding space, thereby ensuring separable instance feature\nrepresentations. The proposed DISep adds only minimal training cost and no\ninference cost. It can be seamlessly integrated to enhance existing WSCD\nmethods. We achieve state-of-the-art performance by enhancing {three\nTransformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD,\nDSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to\nimprove fully-supervised change detection methods. Code is available at\nhttps://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Weakly-Supervised Change Detection (WSCD) methods often encounter\nthe problem of \"instance lumping\" under scene-level supervision, particularly\nin scenarios with a dense distribution of changed instances (i.e., changed\nobjects). In these scenarios, unchanged pixels between changed instances are\nalso mistakenly identified as changed, causing multiple changes to be\nmistakenly viewed as one. In practical applications, this issue prevents the\naccurate quantification of the number of changes. To address this issue, we\npropose a Dense Instance Separation (DISep) method as a plug-and-play solution,\nrefining pixel features from a unified instance perspective under scene-level\nsupervision. Specifically, our DISep comprises a three-step iterative training\nprocess: 1) Instance Localization: We locate instance candidate regions for\nchanged pixels using high-pass class activation maps. 2) Instance Retrieval: We\nidentify and group these changed pixels into different instance IDs through\nconnectivity searching. Then, based on the assigned instance IDs, we extract\ncorresponding pixel-level features on a per-instance basis. 3) Instance\nSeparation: We introduce a separation loss to enforce intra-instance pixel\nconsistency in the embedding space, thereby ensuring separable instance feature\nrepresentations. The proposed DISep adds only minimal training cost and no\ninference cost. It can be seamlessly integrated to enhance existing WSCD\nmethods. We achieve state-of-the-art performance by enhancing {three\nTransformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD,\nDSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to\nimprove fully-supervised change detection methods. Code is available at\nhttps://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection."
                },
                "authors": [
                    {
                        "name": "Zhenghui Zhao"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Lixiang Ru"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Hongruixuan Chen"
                    },
                    {
                        "name": "Cuiqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cuiqun Chen"
                },
                "author": "Cuiqun Chen",
                "arxiv_comment": "Accepted by ISPRS Journal of Photogrammetry and Remote Sensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05707v1",
                "updated": "2025-01-10T04:35:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    35,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T04:35:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    35,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Vighnesh Subramaniam"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Antonio Torralba"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Igor Mordatch"
                    }
                ],
                "author_detail": {
                    "name": "Igor Mordatch"
                },
                "author": "Igor Mordatch",
                "arxiv_comment": "22 pages, 13 figures, 7 tables; Project page at\n  https://llm-multiagent-ft.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05706v1",
                "updated": "2025-01-10T04:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    32,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T04:32:19Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    32,
                    19,
                    4,
                    10,
                    0
                ],
                "title": "Debugging Without Error Messages: How LLM Prompting Strategy Affects\n  Programming Error Explanation Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging Without Error Messages: How LLM Prompting Strategy Affects\n  Programming Error Explanation Effectiveness"
                },
                "summary": "Making errors is part of the programming process -- even for the most\nseasoned professionals. Novices in particular are bound to make many errors\nwhile learning. It is well known that traditional (compiler/interpreter)\nprogramming error messages have been less than helpful for many novices and can\nhave effects such as being frustrating, containing confusing jargon, and being\ndownright misleading. Recent work has found that large language models (LLMs)\ncan generate excellent error explanations, but that the effectiveness of these\nerror messages heavily depends on whether the LLM has been provided with\ncontext -- typically the original source code where the problem occurred.\nKnowing that programming error messages can be misleading and/or contain that\nserves little-to-no use (particularly for novices) we explore the reverse: what\nhappens when GPT-3.5 is prompted for error explanations on just the erroneous\nsource code itself -- original compiler/interpreter produced error message\nexcluded. We utilized various strategies to make more effective error\nexplanations, including one-shot prompting and fine-tuning. We report the\nbaseline results of how effective the error explanations are at providing\nfeedback, as well as how various prompting strategies might improve the\nexplanations' effectiveness. Our results can help educators by understanding\nhow LLMs respond to such prompts that novices are bound to make, and hopefully\nlead to more effective use of Generative AI in the classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making errors is part of the programming process -- even for the most\nseasoned professionals. Novices in particular are bound to make many errors\nwhile learning. It is well known that traditional (compiler/interpreter)\nprogramming error messages have been less than helpful for many novices and can\nhave effects such as being frustrating, containing confusing jargon, and being\ndownright misleading. Recent work has found that large language models (LLMs)\ncan generate excellent error explanations, but that the effectiveness of these\nerror messages heavily depends on whether the LLM has been provided with\ncontext -- typically the original source code where the problem occurred.\nKnowing that programming error messages can be misleading and/or contain that\nserves little-to-no use (particularly for novices) we explore the reverse: what\nhappens when GPT-3.5 is prompted for error explanations on just the erroneous\nsource code itself -- original compiler/interpreter produced error message\nexcluded. We utilized various strategies to make more effective error\nexplanations, including one-shot prompting and fine-tuning. We report the\nbaseline results of how effective the error explanations are at providing\nfeedback, as well as how various prompting strategies might improve the\nexplanations' effectiveness. Our results can help educators by understanding\nhow LLMs respond to such prompts that novices are bound to make, and hopefully\nlead to more effective use of Generative AI in the classroom."
                },
                "authors": [
                    {
                        "name": "Audrey Salmon"
                    },
                    {
                        "name": "Katie Hammer"
                    },
                    {
                        "name": "Eddie Antonio Santos"
                    },
                    {
                        "name": "Brett A. Becker"
                    }
                ],
                "author_detail": {
                    "name": "Brett A. Becker"
                },
                "author": "Brett A. Becker",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v3",
                "updated": "2025-01-10T04:09:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    9,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "9 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12924v2",
                "updated": "2025-01-10T03:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    3,
                    55,
                    57,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-19T23:22:33Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    22,
                    33,
                    1,
                    324,
                    0
                ],
                "title": "Human-In-the-Loop Software Development Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-In-the-Loop Software Development Agents"
                },
                "summary": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, rarely considers\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality remain a concern in some cases. We\ndraw lessons learned and discuss opportunities for future work, which will pave\nthe way for the advancement of LLM-based agents in software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, rarely considers\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality remain a concern in some cases. We\ndraw lessons learned and discuss opportunities for future work, which will pave\nthe way for the advancement of LLM-based agents in software development."
                },
                "authors": [
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Jirat Pasuksmit"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Ruixiong Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Evan Cook"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Wu"
                },
                "author": "Ming Wu",
                "arxiv_comment": "10 pages, 9 figures, ICSE SEIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02631v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02631v3",
                "updated": "2025-01-10T03:50:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    3,
                    50,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-04-03T10:37:56Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    10,
                    37,
                    56,
                    2,
                    94,
                    0
                ],
                "title": "Two-Stage Super-Resolution Simulation Method of Three-Dimensional\n  Street-Scale Atmospheric Flows for Real-Time Urban Micrometeorology\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Super-Resolution Simulation Method of Three-Dimensional\n  Street-Scale Atmospheric Flows for Real-Time Urban Micrometeorology\n  Prediction"
                },
                "summary": "A two-stage super-resolution simulation method is proposed for street-scale\nair temperature and wind velocity, which considerably reduces computation time\nwhile maintaining accuracy. The first stage employs a convolutional neural\nnetwork (CNN) to correct large-scale flows above buildings in the input\nlow-resolution simulation results. The second stage uses another CNN to\nreconstruct small-scale flows between buildings from the output of the first\nstage, resulting in high-resolution inferences. The CNNs are trained using\nhigh-resolution simulation data for the second stage and their coarse-grained\nversion for the first stage as the ground truth, where the high-resolution\nsimulations are conducted independently of the low-resolution simulations used\nas input. This learning approach separates the spatial scales of inference in\neach stage. The effectiveness of the proposed method was evaluated using\nmicrometeorological simulations in an actual urban area around Tokyo Station in\nJapan. The super-resolution simulation successfully inferred high-resolution\natmospheric flows, reducing errors by approximately 50% compared to the\nlow-resolution simulations. Furthermore, the two-stage approach enabled\nlocalized high-resolution inferences, reducing GPU memory usage to as low as\n12% during training. The total wall-clock time for 60-min predictions was\nreduced to 6.83 min, which was 3.32% of the high-resolution simulation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A two-stage super-resolution simulation method is proposed for street-scale\nair temperature and wind velocity, which considerably reduces computation time\nwhile maintaining accuracy. The first stage employs a convolutional neural\nnetwork (CNN) to correct large-scale flows above buildings in the input\nlow-resolution simulation results. The second stage uses another CNN to\nreconstruct small-scale flows between buildings from the output of the first\nstage, resulting in high-resolution inferences. The CNNs are trained using\nhigh-resolution simulation data for the second stage and their coarse-grained\nversion for the first stage as the ground truth, where the high-resolution\nsimulations are conducted independently of the low-resolution simulations used\nas input. This learning approach separates the spatial scales of inference in\neach stage. The effectiveness of the proposed method was evaluated using\nmicrometeorological simulations in an actual urban area around Tokyo Station in\nJapan. The super-resolution simulation successfully inferred high-resolution\natmospheric flows, reducing errors by approximately 50% compared to the\nlow-resolution simulations. Furthermore, the two-stage approach enabled\nlocalized high-resolution inferences, reducing GPU memory usage to as low as\n12% during training. The total wall-clock time for 60-min predictions was\nreduced to 6.83 min, which was 3.32% of the high-resolution simulation time."
                },
                "authors": [
                    {
                        "name": "Yuki Yasuda"
                    },
                    {
                        "name": "Ryo Onishi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Onishi"
                },
                "author": "Ryo Onishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02631v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02631v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.02329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.02329v3",
                "updated": "2025-01-10T03:00:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    3,
                    0,
                    30,
                    4,
                    10,
                    0
                ],
                "published": "2021-06-04T08:25:47Z",
                "published_parsed": [
                    2021,
                    6,
                    4,
                    8,
                    25,
                    47,
                    4,
                    155,
                    0
                ],
                "title": "Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series\n  Forecasting with Regime Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series\n  Forecasting with Regime Switching"
                },
                "summary": "Modern time series data often display complex nonlinear dependencies along\nwith irregular regime-switching behaviors. These features present technical\nchallenges in modeling, inference, and in offering insightful understanding\ninto the underlying stochastic phenomena. To tackle these challenges, we\nintroduce a novel modeling framework known as the Deep Switching State Space\nModel (DS$^3$M). This framework is engineered to make accurate forecasts for\nsuch time series while adeptly identifying the irregular regimes hidden within\nthe dynamics. These identifications not only have significant economic\nramifications but also contribute to a deeper understanding of the underlying\nphenomena. In DS$^3$M, the architecture employs discrete latent variables to\nrepresent regimes and continuous latent variables to account for random driving\nfactors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching\nState Space Model (SSSM), we manage to capture the nonlinear dependencies and\nirregular regime-switching behaviors, governed by a Markov chain and\nparameterized using multilayer perceptrons. We validate the effectiveness and\nregime identification capabilities of DS$^3$M through short- and long-term\nforecasting tests on a wide array of simulated and real-world datasets,\nspanning sectors such as healthcare, economics, traffic, meteorology, and\nenergy. Experimental results reveal that DS$^3$M outperforms several\nstate-of-the-art models in terms of forecasting accuracy, while providing\nmeaningful regime identifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern time series data often display complex nonlinear dependencies along\nwith irregular regime-switching behaviors. These features present technical\nchallenges in modeling, inference, and in offering insightful understanding\ninto the underlying stochastic phenomena. To tackle these challenges, we\nintroduce a novel modeling framework known as the Deep Switching State Space\nModel (DS$^3$M). This framework is engineered to make accurate forecasts for\nsuch time series while adeptly identifying the irregular regimes hidden within\nthe dynamics. These identifications not only have significant economic\nramifications but also contribute to a deeper understanding of the underlying\nphenomena. In DS$^3$M, the architecture employs discrete latent variables to\nrepresent regimes and continuous latent variables to account for random driving\nfactors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching\nState Space Model (SSSM), we manage to capture the nonlinear dependencies and\nirregular regime-switching behaviors, governed by a Markov chain and\nparameterized using multilayer perceptrons. We validate the effectiveness and\nregime identification capabilities of DS$^3$M through short- and long-term\nforecasting tests on a wide array of simulated and real-world datasets,\nspanning sectors such as healthcare, economics, traffic, meteorology, and\nenergy. Experimental results reveal that DS$^3$M outperforms several\nstate-of-the-art models in terms of forecasting accuracy, while providing\nmeaningful regime identifications."
                },
                "authors": [
                    {
                        "name": "Xiuqin Xu"
                    },
                    {
                        "name": "Hanqiu Peng"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.02329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.02329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05675v1",
                "updated": "2025-01-10T02:57:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T02:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "Facilitate Collaboration between Large Language Model and Task-specific\n  Model for Time Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitate Collaboration between Large Language Model and Task-specific\n  Model for Time Series Anomaly Detection"
                },
                "summary": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge, while task-specific smaller models excel at\nextracting normal patterns and detecting value fluctuations. Inspired by the\nhuman nervous system, where the brain stores expert knowledge and the\nperipheral nervous system and spinal cord handle specific tasks like withdrawal\nand knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate\ncollaboration between LLMs and task-specific models, leveraging the strengths\nof both.\n  In this work, we first formulate the collaboration process and identify two\nkey challenges in the collaboration between LLMs and task-specific models: (1)\nthe misalignment between the expression domains of LLMs and smaller models, and\n(2) error accumulation arising from the predictions of both models.\n  To address these challenges, we introduce two key components in CoLLaTe: the\nalignment module and the collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan LLM based methods and task-specific smaller model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge, while task-specific smaller models excel at\nextracting normal patterns and detecting value fluctuations. Inspired by the\nhuman nervous system, where the brain stores expert knowledge and the\nperipheral nervous system and spinal cord handle specific tasks like withdrawal\nand knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate\ncollaboration between LLMs and task-specific models, leveraging the strengths\nof both.\n  In this work, we first formulate the collaboration process and identify two\nkey challenges in the collaboration between LLMs and task-specific models: (1)\nthe misalignment between the expression domains of LLMs and smaller models, and\n(2) error accumulation arising from the predictions of both models.\n  To address these challenges, we introduce two key components in CoLLaTe: the\nalignment module and the collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan LLM based methods and task-specific smaller model."
                },
                "authors": [
                    {
                        "name": "Feiyi Chen"
                    },
                    {
                        "name": "Leilei Zhang"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05662v1",
                "updated": "2025-01-10T02:28:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    28,
                    4,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T02:28:04Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    28,
                    4,
                    4,
                    10,
                    0
                ],
                "title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal\n  Large Language Models"
                },
                "summary": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced\nrecently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step\nself-evaluation has improved their performance. However, limited parameters\noften hinder EMLLMs from effectively using self-evaluation during inference.\nKey challenges include synthesizing evaluation data, determining its quantity,\noptimizing training and inference strategies, and selecting appropriate\nprompts.\n  To address these issues, we introduce Self-Evaluation Augmented Training\n(SEAT). SEAT uses more powerful EMLLMs for CoT reasoning, data selection, and\nevaluation generation, then trains EMLLMs with the synthesized data. However,\nhandling long prompts and maintaining CoT reasoning quality are problematic.\nTherefore, we propose Cascaded Self-Evaluation Augmented Training (Cas-SEAT),\nwhich breaks down lengthy prompts into shorter, task-specific cascaded prompts\nand reduces costs for resource-limited settings. During data synthesis, we\nemploy open-source 7B-parameter EMLLMs and annotate a small dataset with short\nprompts.\n  Experiments demonstrate that Cas-SEAT significantly boosts EMLLMs'\nself-evaluation abilities, improving performance by 19.68%, 55.57%, and 46.79%\non the MathVista, Math-V, and We-Math datasets, respectively. Additionally, our\nCas-SEAT Dataset serves as a valuable resource for future research in enhancing\nEMLLM self-evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced\nrecently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step\nself-evaluation has improved their performance. However, limited parameters\noften hinder EMLLMs from effectively using self-evaluation during inference.\nKey challenges include synthesizing evaluation data, determining its quantity,\noptimizing training and inference strategies, and selecting appropriate\nprompts.\n  To address these issues, we introduce Self-Evaluation Augmented Training\n(SEAT). SEAT uses more powerful EMLLMs for CoT reasoning, data selection, and\nevaluation generation, then trains EMLLMs with the synthesized data. However,\nhandling long prompts and maintaining CoT reasoning quality are problematic.\nTherefore, we propose Cascaded Self-Evaluation Augmented Training (Cas-SEAT),\nwhich breaks down lengthy prompts into shorter, task-specific cascaded prompts\nand reduces costs for resource-limited settings. During data synthesis, we\nemploy open-source 7B-parameter EMLLMs and annotate a small dataset with short\nprompts.\n  Experiments demonstrate that Cas-SEAT significantly boosts EMLLMs'\nself-evaluation abilities, improving performance by 19.68%, 55.57%, and 46.79%\non the MathVista, Math-V, and We-Math datasets, respectively. Additionally, our\nCas-SEAT Dataset serves as a valuable resource for future research in enhancing\nEMLLM self-evaluation."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Wenkai Wang"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v9",
                "updated": "2025-01-10T02:18:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    18,
                    1,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05647v1",
                "updated": "2025-01-10T01:27:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T01:27:12Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation"
                },
                "summary": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Tianyu Zhan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3690624.3709335",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709335",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05645v1",
                "updated": "2025-01-10T01:17:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    17,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T01:17:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    17,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "k-Sample inference via Multimarginal Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "k-Sample inference via Multimarginal Optimal Transport"
                },
                "summary": "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020."
                },
                "authors": [
                    {
                        "name": "Natalia Kravtsova"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Kravtsova"
                },
                "author": "Natalia Kravtsova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18544v2",
                "updated": "2025-01-10T01:06:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    6,
                    6,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-24T16:51:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Consistency Checks for Language Model Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Checks for Language Model Forecasters"
                },
                "summary": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting."
                },
                "authors": [
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Abhimanyu Pallavi Sudhir"
                    },
                    {
                        "name": "Alejandro Alvarez"
                    },
                    {
                        "name": "Vineeth Bhat"
                    },
                    {
                        "name": "Adam Shen"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "55 pages, 25 figures. Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06186v1",
                "updated": "2025-01-10T18:59:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    59,
                    51,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T18:59:51Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    59,
                    51,
                    4,
                    10,
                    0
                ],
                "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs"
                },
                "summary": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available."
                },
                "authors": [
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Dinura Dissanayake"
                    },
                    {
                        "name": "Ketan More"
                    },
                    {
                        "name": "Ritesh Thawkar"
                    },
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Noor Ahsan"
                    },
                    {
                        "name": "Yuhao Li"
                    },
                    {
                        "name": "Mohammed Zumri"
                    },
                    {
                        "name": "Jean Lahoud"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "arxiv_comment": "15 pages, 5 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v6",
                "updated": "2025-01-10T18:45:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    18,
                    45,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads"
                },
                "summary": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06138v1",
                "updated": "2025-01-10T17:52:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    47,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:52:47Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    47,
                    4,
                    10,
                    0
                ],
                "title": "MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action\n  Detection"
                },
                "summary": "Action detection in real-world scenarios is particularly challenging due to\ndensely distributed actions in hour-long untrimmed videos. It requires modeling\nboth short- and long-term temporal relationships while handling significant\nintra-class temporal variations. Previous state-of-the-art (SOTA)\nTransformer-based architectures, though effective, are impractical for\nreal-world deployment due to their high parameter count, GPU memory usage, and\nlimited throughput, making them unsuitable for very long videos. In this work,\nwe innovatively adapt the Mamba architecture for action detection and propose\nMulti-scale Temporal Mamba (MS-Temba), comprising two key components: Temporal\nMamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include the\nTemporal Local Module (TLM) for short-range temporal modeling and the Dilated\nTemporal SSM (DTS) for long-range dependencies. By introducing dilations, a\nnovel concept for Mamba, TLM and DTS capture local and global features at\nmultiple scales. The Temba Fuser aggregates these scale-specific features using\nMamba to learn comprehensive multi-scale representations of untrimmed videos.\nMS-Temba is validated on three public datasets, outperforming SOTA methods on\nlong videos and matching prior methods on short videos while using only\none-eighth of the parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action detection in real-world scenarios is particularly challenging due to\ndensely distributed actions in hour-long untrimmed videos. It requires modeling\nboth short- and long-term temporal relationships while handling significant\nintra-class temporal variations. Previous state-of-the-art (SOTA)\nTransformer-based architectures, though effective, are impractical for\nreal-world deployment due to their high parameter count, GPU memory usage, and\nlimited throughput, making them unsuitable for very long videos. In this work,\nwe innovatively adapt the Mamba architecture for action detection and propose\nMulti-scale Temporal Mamba (MS-Temba), comprising two key components: Temporal\nMamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include the\nTemporal Local Module (TLM) for short-range temporal modeling and the Dilated\nTemporal SSM (DTS) for long-range dependencies. By introducing dilations, a\nnovel concept for Mamba, TLM and DTS capture local and global features at\nmultiple scales. The Temba Fuser aggregates these scale-specific features using\nMamba to learn comprehensive multi-scale representations of untrimmed videos.\nMS-Temba is validated on three public datasets, outperforming SOTA methods on\nlong videos and matching prior methods on short videos while using only\none-eighth of the parameters."
                },
                "authors": [
                    {
                        "name": "Arkaprava Sinha"
                    },
                    {
                        "name": "Monish Soundar Raj"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Ahmed Helmy"
                    },
                    {
                        "name": "Srijan Das"
                    }
                ],
                "author_detail": {
                    "name": "Srijan Das"
                },
                "author": "Srijan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06137v1",
                "updated": "2025-01-10T17:52:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    34,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    52,
                    34,
                    4,
                    10,
                    0
                ],
                "title": "Supervision policies can shape long-term risk management in\n  general-purpose AI models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervision policies can shape long-term risk management in\n  general-purpose AI models"
                },
                "summary": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society."
                },
                "authors": [
                    {
                        "name": "Manuel Cebrian"
                    },
                    {
                        "name": "Emilia Gomez"
                    },
                    {
                        "name": "David Fernandez Llorca"
                    }
                ],
                "author_detail": {
                    "name": "David Fernandez Llorca"
                },
                "author": "David Fernandez Llorca",
                "arxiv_comment": "24 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06132v1",
                "updated": "2025-01-10T17:44:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    44,
                    57,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:44:57Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    44,
                    57,
                    4,
                    10,
                    0
                ],
                "title": "CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion\n  Planning for Future Autonomous Mobility on Demand Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion\n  Planning for Future Autonomous Mobility on Demand Systems"
                },
                "summary": "The increasing demand for flexible and efficient urban transportation\nsolutions has spotlighted the limitations of traditional Demand Responsive\nTransport (DRT) systems, particularly in accommodating diverse passenger needs\nand dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems\nhave emerged as a promising alternative, leveraging connected and autonomous\nvehicles (CAVs) to provide responsive and adaptable services. However, existing\nmethods primarily focus on either vehicle scheduling or path planning, which\noften simplify complex urban layouts and neglect the necessity for simultaneous\ncoordination and mutual avoidance among CAVs. This oversimplification poses\nsignificant challenges to the deployment of AMoD systems in real-world\nscenarios. To address these gaps, we propose CoDriveVLM, a novel framework that\nintegrates high-fidelity simultaneous dispatching and cooperative motion\nplanning for future AMoD systems. Our method harnesses Vision-Language Models\n(VLMs) to enhance multi-modality information processing, and this enables\ncomprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV\ndispatching coordinator is introduced to effectively manage complex and\nunforeseen AMoD conditions, thus supporting efficient scheduling\ndecision-making. Furthermore, we propose a scalable decentralized cooperative\nmotion planning method via consensus alternating direction method of\nmultipliers (ADMM) focusing on collision risk evaluation and decentralized\ntrajectory optimization. Simulation results demonstrate the feasibility and\nrobustness of CoDriveVLM in various traffic conditions, showcasing its\npotential to significantly improve the fidelity and effectiveness of AMoD\nsystems in future urban transportation networks. The code is available at\nhttps://github.com/henryhcliu/CoDriveVLM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for flexible and efficient urban transportation\nsolutions has spotlighted the limitations of traditional Demand Responsive\nTransport (DRT) systems, particularly in accommodating diverse passenger needs\nand dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems\nhave emerged as a promising alternative, leveraging connected and autonomous\nvehicles (CAVs) to provide responsive and adaptable services. However, existing\nmethods primarily focus on either vehicle scheduling or path planning, which\noften simplify complex urban layouts and neglect the necessity for simultaneous\ncoordination and mutual avoidance among CAVs. This oversimplification poses\nsignificant challenges to the deployment of AMoD systems in real-world\nscenarios. To address these gaps, we propose CoDriveVLM, a novel framework that\nintegrates high-fidelity simultaneous dispatching and cooperative motion\nplanning for future AMoD systems. Our method harnesses Vision-Language Models\n(VLMs) to enhance multi-modality information processing, and this enables\ncomprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV\ndispatching coordinator is introduced to effectively manage complex and\nunforeseen AMoD conditions, thus supporting efficient scheduling\ndecision-making. Furthermore, we propose a scalable decentralized cooperative\nmotion planning method via consensus alternating direction method of\nmultipliers (ADMM) focusing on collision risk evaluation and decentralized\ntrajectory optimization. Simulation results demonstrate the feasibility and\nrobustness of CoDriveVLM in various traffic conditions, showcasing its\npotential to significantly improve the fidelity and effectiveness of AMoD\nsystems in future urban transportation networks. The code is available at\nhttps://github.com/henryhcliu/CoDriveVLM.git."
                },
                "authors": [
                    {
                        "name": "Haichao Liu"
                    },
                    {
                        "name": "Ruoyu Yao"
                    },
                    {
                        "name": "Wenru Liu"
                    },
                    {
                        "name": "Zhenmin Huang"
                    },
                    {
                        "name": "Shaojie Shen"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06129v1",
                "updated": "2025-01-10T17:35:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    35,
                    6,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:35:06Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    35,
                    6,
                    4,
                    10,
                    0
                ],
                "title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented\n  Conversational AI"
                },
                "summary": "General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose automatic speech recognition (ASR) systems do not always\nperform well in goal-oriented dialogue. Existing ASR correction methods rely on\nprior user data or named entities. We extend correction to tasks that have no\nprior user data and exhibit linguistic flexibility such as lexical and\nsyntactic variations. We propose a novel context augmentation with a large\nlanguage model and a ranking strategy that incorporates contextual information\nfrom the dialogue states of a goal-oriented conversational AI and its tasks.\nOur method ranks (1) n-best ASR hypotheses by their lexical and semantic\nsimilarity with context and (2) context by phonetic correspondence with ASR\nhypotheses. Evaluated in home improvement and cooking domains with real-world\nusers, our method improves recall and F1 of correction by 34% and 16%,\nrespectively, while maintaining precision and false positive rate. Users rated\n.8-1 point (out of 5) higher when our correction method worked properly, with\nno decrease due to false positives."
                },
                "authors": [
                    {
                        "name": "Yuya Asano"
                    },
                    {
                        "name": "Sabit Hassan"
                    },
                    {
                        "name": "Paras Sharma"
                    },
                    {
                        "name": "Anthony Sicilia"
                    },
                    {
                        "name": "Katherine Atwell"
                    },
                    {
                        "name": "Diane Litman"
                    },
                    {
                        "name": "Malihe Alikhani"
                    }
                ],
                "author_detail": {
                    "name": "Malihe Alikhani"
                },
                "author": "Malihe Alikhani",
                "arxiv_comment": "Accepted to COLING 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06113v1",
                "updated": "2025-01-10T17:05:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    5,
                    59,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T17:05:59Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    5,
                    59,
                    4,
                    10,
                    0
                ],
                "title": "Vehicle-in-Virtual-Environment (VVE) Based Autonomous Driving Function\n  Development and Evaluation Methodology for Vulnerable Road User Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle-in-Virtual-Environment (VVE) Based Autonomous Driving Function\n  Development and Evaluation Methodology for Vulnerable Road User Safety"
                },
                "summary": "Traditional methods for developing and evaluating autonomous driving\nfunctions, such as model-in-the-loop (MIL) and hardware-in-the-loop (HIL)\nsimulations, heavily depend on the accuracy of simulated vehicle models and\nhuman factors, especially for vulnerable road user safety systems. Continuation\nof development during public road deployment forces other road users including\nvulnerable ones to involuntarily participate in the development process,\nleading to safety risks, inefficiencies, and a decline in public trust. To\naddress these deficiencies, the Vehicle-in-Virtual-Environment (VVE) method was\nproposed as a safer, more efficient, and cost-effective solution for developing\nand testing connected and autonomous driving technologies by operating the real\nvehicle and multiple other actors like vulnerable road users in different test\nareas while being immersed within the same highly realistic virtual\nenvironment. This VVE approach synchronizes real-world vehicle and vulnerable\nroad user motion within the same virtual scenario, enabling the safe and\nrealistic testing of various traffic situations in a safe and repeatable\nmanner. In this paper, we propose a new testing pipeline that sequentially\nintegrates MIL, HIL, and VVE methods to comprehensively develop and evaluate\nautonomous driving functions. The effectiveness of this testing pipeline will\nbe demonstrated using an autonomous driving path-tracking algorithm with local\ndeep reinforcement learning modification for vulnerable road user collision\navoidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for developing and evaluating autonomous driving\nfunctions, such as model-in-the-loop (MIL) and hardware-in-the-loop (HIL)\nsimulations, heavily depend on the accuracy of simulated vehicle models and\nhuman factors, especially for vulnerable road user safety systems. Continuation\nof development during public road deployment forces other road users including\nvulnerable ones to involuntarily participate in the development process,\nleading to safety risks, inefficiencies, and a decline in public trust. To\naddress these deficiencies, the Vehicle-in-Virtual-Environment (VVE) method was\nproposed as a safer, more efficient, and cost-effective solution for developing\nand testing connected and autonomous driving technologies by operating the real\nvehicle and multiple other actors like vulnerable road users in different test\nareas while being immersed within the same highly realistic virtual\nenvironment. This VVE approach synchronizes real-world vehicle and vulnerable\nroad user motion within the same virtual scenario, enabling the safe and\nrealistic testing of various traffic situations in a safe and repeatable\nmanner. In this paper, we propose a new testing pipeline that sequentially\nintegrates MIL, HIL, and VVE methods to comprehensively develop and evaluate\nautonomous driving functions. The effectiveness of this testing pipeline will\nbe demonstrated using an autonomous driving path-tracking algorithm with local\ndeep reinforcement learning modification for vulnerable road user collision\navoidance."
                },
                "authors": [
                    {
                        "name": "Haochong Chen"
                    },
                    {
                        "name": "Xincheng Cao"
                    },
                    {
                        "name": "Levent Guvenc"
                    },
                    {
                        "name": "Bilin Aksun Guvenc"
                    }
                ],
                "author_detail": {
                    "name": "Bilin Aksun Guvenc"
                },
                "author": "Bilin Aksun Guvenc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06101v1",
                "updated": "2025-01-10T16:54:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    54,
                    20,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T16:54:20Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    16,
                    54,
                    20,
                    4,
                    10,
                    0
                ],
                "title": "From Conversation to Automation: Leveraging Large Language Models to\n  Analyze Strategies in Problem Solving Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Conversation to Automation: Leveraging Large Language Models to\n  Analyze Strategies in Problem Solving Therapy"
                },
                "summary": "Problem-solving therapy (PST) is a structured psychological approach that\nhelps individuals manage stress and resolve personal issues by guiding them\nthrough problem identification, solution brainstorming, decision-making, and\noutcome evaluation. As mental health care increasingly integrates technologies\nlike chatbots and large language models (LLMs), understanding how PST can be\neffectively automated is important. This study leverages anonymized therapy\ntranscripts to analyze and classify therapeutic interventions using various\nLLMs and transformer-based models. Our results show that GPT-4o achieved the\nhighest accuracy (0.76) in identifying PST strategies, outperforming other\nmodels. Additionally, we introduced a new dimension of communication strategies\nthat enhances the current PST framework, offering deeper insights into\ntherapist-client interactions. This research demonstrates the potential of LLMs\nto automate complex therapeutic dialogue analysis, providing a scalable,\nefficient tool for mental health interventions. Our annotation framework can\nenhance the accessibility, effectiveness, and personalization of PST,\nsupporting therapists in real-time with more precise, targeted interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-solving therapy (PST) is a structured psychological approach that\nhelps individuals manage stress and resolve personal issues by guiding them\nthrough problem identification, solution brainstorming, decision-making, and\noutcome evaluation. As mental health care increasingly integrates technologies\nlike chatbots and large language models (LLMs), understanding how PST can be\neffectively automated is important. This study leverages anonymized therapy\ntranscripts to analyze and classify therapeutic interventions using various\nLLMs and transformer-based models. Our results show that GPT-4o achieved the\nhighest accuracy (0.76) in identifying PST strategies, outperforming other\nmodels. Additionally, we introduced a new dimension of communication strategies\nthat enhances the current PST framework, offering deeper insights into\ntherapist-client interactions. This research demonstrates the potential of LLMs\nto automate complex therapeutic dialogue analysis, providing a scalable,\nefficient tool for mental health interventions. Our annotation framework can\nenhance the accessibility, effectiveness, and personalization of PST,\nsupporting therapists in real-time with more precise, targeted interventions."
                },
                "authors": [
                    {
                        "name": "Elham Aghakhani"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Karla T. Washington"
                    },
                    {
                        "name": "George Demiris"
                    },
                    {
                        "name": "Jina Huh-Yoo"
                    },
                    {
                        "name": "Rezvaneh Rezapour"
                    }
                ],
                "author_detail": {
                    "name": "Rezvaneh Rezapour"
                },
                "author": "Rezvaneh Rezapour",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v1",
                "updated": "2025-01-10T15:57:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04860v2",
                "updated": "2025-01-10T15:47:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    47,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-08T22:22:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    22,
                    22,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "Exploring the Use of Robots for Diary Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Use of Robots for Diary Studies"
                },
                "summary": "As interest in studying in-the-wild human-robot interaction grows, there is a\nneed for methods to collect data over time and in naturalistic or potentially\nprivate environments. HRI researchers have increasingly used the diary method\nfor these studies, asking study participants to self-administer a structured\ndata collection instrument, i.e., a diary, over a period of time. Although the\ndiary method offers a unique window into settings that researchers may not have\naccess to, they also lack the interactivity and probing that interview-based\nmethods offer. In this paper, we explore a novel data collection method in\nwhich a robot plays the role of an interactive diary. We developed the Diary\nRobot system and performed in-home deployments for a week to evaluate the\nfeasibility and effectiveness of this approach. Using traditional text-based\nand audio-based diaries as benchmarks, we found that robots are able to\neffectively elicit the intended information. We reflect on our findings, and\ndescribe scenarios where the utilization of robots in diary studies as a data\ncollection instrument may be especially applicable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As interest in studying in-the-wild human-robot interaction grows, there is a\nneed for methods to collect data over time and in naturalistic or potentially\nprivate environments. HRI researchers have increasingly used the diary method\nfor these studies, asking study participants to self-administer a structured\ndata collection instrument, i.e., a diary, over a period of time. Although the\ndiary method offers a unique window into settings that researchers may not have\naccess to, they also lack the interactivity and probing that interview-based\nmethods offer. In this paper, we explore a novel data collection method in\nwhich a robot plays the role of an interactive diary. We developed the Diary\nRobot system and performed in-home deployments for a week to evaluate the\nfeasibility and effectiveness of this approach. Using traditional text-based\nand audio-based diaries as benchmarks, we found that robots are able to\neffectively elicit the intended information. We reflect on our findings, and\ndescribe scenarios where the utilization of robots in diary studies as a data\ncollection instrument may be especially applicable."
                },
                "authors": [
                    {
                        "name": "Michael F. Xu"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_comment": "Proceedings of the 20th ACM/IEEE International Conference on Human\n  Robot Interaction (HRI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19876v3",
                "updated": "2025-01-10T15:08:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    8,
                    44,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-29T17:38:56Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    17,
                    38,
                    56,
                    4,
                    334,
                    0
                ],
                "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  Attacks leveraging internal LLM states"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments."
                },
                "authors": [
                    {
                        "name": "Luis Ibanez-Lissen"
                    },
                    {
                        "name": "Lorena Gonzalez-Manzano"
                    },
                    {
                        "name": "Jose Maria de Fuentes"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Joaquin Garcia-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Joaquin Garcia-Alfaro"
                },
                "author": "Joaquin Garcia-Alfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17437v2",
                "updated": "2025-01-10T14:59:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    59,
                    16,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-26T13:51:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems"
                },
                "summary": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners."
                },
                "authors": [
                    {
                        "name": "Mireia Hernandez Caralt"
                    },
                    {
                        "name": "Ivan Sekulić"
                    },
                    {
                        "name": "Filip Carević"
                    },
                    {
                        "name": "Nghia Khau"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Bruna Guedes"
                    },
                    {
                        "name": "Victor Guimarães"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Andre Manso"
                    },
                    {
                        "name": "Meghana Reddy"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Roland Mathis"
                    }
                ],
                "author_detail": {
                    "name": "Roland Mathis"
                },
                "author": "Roland Mathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04127v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04127v3",
                "updated": "2025-01-10T14:31:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    31,
                    21,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-06T14:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    14,
                    49,
                    6,
                    3,
                    158,
                    0
                ],
                "title": "Are We Done with MMLU?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We Done with MMLU?"
                },
                "summary": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0."
                },
                "authors": [
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Alberto Carlo Maria Mancino"
                    },
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Xiaotang Du"
                    },
                    {
                        "name": "Mohammad Reza Ghasemi Madani"
                    },
                    {
                        "name": "Claire Barale"
                    },
                    {
                        "name": "Robert McHardy"
                    },
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Emile van Krieken"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04127v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05989v1",
                "updated": "2025-01-10T14:20:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    20,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:20:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    20,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Addressing speaker gender bias in large scale speech translation systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing speaker gender bias in large scale speech translation systems"
                },
                "summary": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the issue of speaker gender bias in Speech Translation\n(ST) systems, which can lead to offensive and inaccurate translations. The\nmasculine bias often found in large-scale ST systems is typically perpetuated\nthrough training data derived from Machine Translation (MT) systems. Our\napproach involves two key steps. First, we employ Large Language Models (LLMs)\nto rectify translations based on the speaker's gender in a cost-effective\nmanner. Second, we fine-tune the ST model with the corrected data, enabling the\nmodel to generate gender-specific translations directly from audio cues,\nwithout the need for explicit gender input. Additionally, we propose a\nthree-mode fine-tuned model for scenarios where the speaker's gender is either\npredefined or should not be inferred from speech cues. We demonstrate a 70%\nimprovement in translations for female speakers compared to our baseline and\nother large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE\ntest set."
                },
                "authors": [
                    {
                        "name": "Shubham Bansal"
                    },
                    {
                        "name": "Vikas Joshi"
                    },
                    {
                        "name": "Harveen Chadha"
                    },
                    {
                        "name": "Rupeshkumar Mehta"
                    },
                    {
                        "name": "Jinyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Li"
                },
                "author": "Jinyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05985v1",
                "updated": "2025-01-10T14:17:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Automated Pre-Testing of Cross-Cultural Surveys"
                },
                "summary": "Designing culturally relevant questionnaires for ICTD research is\nchallenging, particularly when adapting surveys for populations to non-western\ncontexts. Prior work adapted questionnaires through expert reviews and pilot\nstudies, which are resource-intensive and time-consuming. To address these\nchallenges, we propose using large language models (LLMs) to automate the\nquestionnaire pretesting process in cross-cultural settings. Our study used\nLLMs to adapt a U.S.-focused climate opinion survey for a South African\naudience. We then tested the adapted questionnaire with 116 South African\nparticipants via Prolific, asking them to provide feedback on both versions.\nParticipants perceived the LLM-adapted questions as slightly more favorable\nthan the traditional version. Our note opens discussions on the potential role\nof LLMs in adapting surveys and facilitating cross-cultural questionnaire\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing culturally relevant questionnaires for ICTD research is\nchallenging, particularly when adapting surveys for populations to non-western\ncontexts. Prior work adapted questionnaires through expert reviews and pilot\nstudies, which are resource-intensive and time-consuming. To address these\nchallenges, we propose using large language models (LLMs) to automate the\nquestionnaire pretesting process in cross-cultural settings. Our study used\nLLMs to adapt a U.S.-focused climate opinion survey for a South African\naudience. We then tested the adapted questionnaire with 116 South African\nparticipants via Prolific, asking them to provide feedback on both versions.\nParticipants perceived the LLM-adapted questions as slightly more favorable\nthan the traditional version. Our note opens discussions on the potential role\nof LLMs in adapting surveys and facilitating cross-cultural questionnaire\ndesign."
                },
                "authors": [
                    {
                        "name": "Divya Mani Adhikari"
                    },
                    {
                        "name": "Vikram Kamath Cannanure"
                    },
                    {
                        "name": "Alexander Hartland"
                    },
                    {
                        "name": "Ingmar Weber"
                    }
                ],
                "author_detail": {
                    "name": "Ingmar Weber"
                },
                "author": "Ingmar Weber",
                "arxiv_comment": "Accepted to ICTD 2024 (Notes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05981v1",
                "updated": "2025-01-10T14:08:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    8,
                    59,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T14:08:59Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    8,
                    59,
                    4,
                    10,
                    0
                ],
                "title": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study\n  of LLM Hallucination on North Korea",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study\n  of LLM Hallucination on North Korea"
                },
                "summary": "Hallucination in large language models (LLMs) remains a significant challenge\nfor their safe deployment, particularly due to its potential to spread\nmisinformation. Most existing solutions address this challenge by focusing on\naligning the models with credible sources or by improving how models\ncommunicate their confidence (or lack thereof) in their outputs. While these\nmeasures may be effective in most contexts, they may fall short in scenarios\nrequiring more nuanced approaches, especially in situations where access to\naccurate data is limited or determining credible sources is challenging. In\nthis study, we take North Korea - a country characterised by an extreme lack of\nreliable sources and the prevalence of sensationalist falsehoods - as a case\nstudy. We explore and evaluate how some of the best-performing multilingual\nLLMs and specific language-based models generate information about North Korea\nin three languages spoken in countries with significant geo-political\ninterests: English (United States, United Kingdom), Korean (South Korea), and\nMandarin Chinese (China). Our findings reveal significant differences,\nsuggesting that the choice of model and language can lead to vastly different\nunderstandings of North Korea, which has important implications given the\nglobal security challenges the country poses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination in large language models (LLMs) remains a significant challenge\nfor their safe deployment, particularly due to its potential to spread\nmisinformation. Most existing solutions address this challenge by focusing on\naligning the models with credible sources or by improving how models\ncommunicate their confidence (or lack thereof) in their outputs. While these\nmeasures may be effective in most contexts, they may fall short in scenarios\nrequiring more nuanced approaches, especially in situations where access to\naccurate data is limited or determining credible sources is challenging. In\nthis study, we take North Korea - a country characterised by an extreme lack of\nreliable sources and the prevalence of sensationalist falsehoods - as a case\nstudy. We explore and evaluate how some of the best-performing multilingual\nLLMs and specific language-based models generate information about North Korea\nin three languages spoken in countries with significant geo-political\ninterests: English (United States, United Kingdom), Korean (South Korea), and\nMandarin Chinese (China). Our findings reveal significant differences,\nsuggesting that the choice of model and language can lead to vastly different\nunderstandings of North Korea, which has important implications given the\nglobal security challenges the country poses."
                },
                "authors": [
                    {
                        "name": "Eunjung Cho"
                    },
                    {
                        "name": "Won Ik Cho"
                    },
                    {
                        "name": "Soomin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Soomin Seo"
                },
                "author": "Soomin Seo",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05965v1",
                "updated": "2025-01-10T13:47:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    47,
                    13,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T13:47:13Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    47,
                    13,
                    4,
                    10,
                    0
                ],
                "title": "Model Inversion in Split Learning for Personalized LLMs: New Insights\n  from Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Inversion in Split Learning for Personalized LLMs: New Insights\n  from Information Bottleneck Theory"
                },
                "summary": "Personalized Large Language Models (LLMs) have become increasingly prevalent,\nshowcasing the impressive capabilities of models like GPT-4. This trend has\nalso catalyzed extensive research on deploying LLMs on mobile devices. Feasible\napproaches for such edge-cloud deployment include using split learning.\nHowever, previous research has largely overlooked the privacy leakage\nassociated with intermediate representations transmitted from devices to\nservers. This work is the first to identify model inversion attacks in the\nsplit learning framework for LLMs, emphasizing the necessity of secure defense.\nFor the first time, we introduce mutual information entropy to understand the\ninformation propagation of Transformer-based LLMs and assess privacy attack\nperformance for LLM blocks. To address the issue of representations being\nsparser and containing less information than embeddings, we propose a two-stage\nattack system in which the first part projects representations into the\nembedding space, and the second part uses a generative model to recover text\nfrom these embeddings. This design breaks down the complexity and achieves\nattack scores of 38%-75% in various scenarios, with an over 60% improvement\nover the SOTA. This work comprehensively highlights the potential privacy risks\nduring the deployment of personalized LLMs on the edge side.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Large Language Models (LLMs) have become increasingly prevalent,\nshowcasing the impressive capabilities of models like GPT-4. This trend has\nalso catalyzed extensive research on deploying LLMs on mobile devices. Feasible\napproaches for such edge-cloud deployment include using split learning.\nHowever, previous research has largely overlooked the privacy leakage\nassociated with intermediate representations transmitted from devices to\nservers. This work is the first to identify model inversion attacks in the\nsplit learning framework for LLMs, emphasizing the necessity of secure defense.\nFor the first time, we introduce mutual information entropy to understand the\ninformation propagation of Transformer-based LLMs and assess privacy attack\nperformance for LLM blocks. To address the issue of representations being\nsparser and containing less information than embeddings, we propose a two-stage\nattack system in which the first part projects representations into the\nembedding space, and the second part uses a generative model to recover text\nfrom these embeddings. This design breaks down the complexity and achieves\nattack scores of 38%-75% in various scenarios, with an over 60% improvement\nover the SOTA. This work comprehensively highlights the potential privacy risks\nduring the deployment of personalized LLMs on the edge side."
                },
                "authors": [
                    {
                        "name": "Yunmeng Shu"
                    },
                    {
                        "name": "Shaofeng Li"
                    },
                    {
                        "name": "Tian Dong"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Haojin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haojin Zhu"
                },
                "author": "Haojin Zhu",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11698v2",
                "updated": "2025-01-10T13:35:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    35,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-16T12:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?"
                },
                "summary": "Context. The security of critical infrastructure has been a pressing concern\nsince the advent of computers and has become even more critical in today's era\nof cyber warfare. Protecting mission-critical systems (MCSs), essential for\nnational security, requires swift and robust governance, yet recent events\nreveal the increasing difficulty of meeting these challenges. Aim. Building on\nprior research showcasing the potential of Generative AI (GAI), such as Large\nLanguage Models, in enhancing risk analysis, we aim to explore practitioners'\nviews on integrating GAI into the governance of IT MCSs. Our goal is to provide\nactionable insights and recommendations for stakeholders, including\nresearchers, practitioners, and policymakers. Method. We designed a survey to\ncollect practical experiences, concerns, and expectations of practitioners who\ndevelop and implement security solutions in the context of MCSs. Conclusions\nand Future Works. Our findings highlight that the safe use of LLMs in MCS\ngovernance requires interdisciplinary collaboration. Researchers should focus\non designing regulation-oriented models and focus on accountability;\npractitioners emphasize data protection and transparency, while policymakers\nmust establish a unified AI framework with global benchmarks to ensure ethical\nand secure LLMs-based MCS governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The security of critical infrastructure has been a pressing concern\nsince the advent of computers and has become even more critical in today's era\nof cyber warfare. Protecting mission-critical systems (MCSs), essential for\nnational security, requires swift and robust governance, yet recent events\nreveal the increasing difficulty of meeting these challenges. Aim. Building on\nprior research showcasing the potential of Generative AI (GAI), such as Large\nLanguage Models, in enhancing risk analysis, we aim to explore practitioners'\nviews on integrating GAI into the governance of IT MCSs. Our goal is to provide\nactionable insights and recommendations for stakeholders, including\nresearchers, practitioners, and policymakers. Method. We designed a survey to\ncollect practical experiences, concerns, and expectations of practitioners who\ndevelop and implement security solutions in the context of MCSs. Conclusions\nand Future Works. Our findings highlight that the safe use of LLMs in MCS\ngovernance requires interdisciplinary collaboration. Researchers should focus\non designing regulation-oriented models and focus on accountability;\npractitioners emphasize data protection and transparency, while policymakers\nmust establish a unified AI framework with global benchmarks to ensure ethical\nand secure LLMs-based MCS governance."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Francesco Palagiano"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    },
                    {
                        "name": "Davide Taibi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Taibi"
                },
                "author": "Davide Taibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09352v2",
                "updated": "2025-01-10T13:23:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    23,
                    58,
                    4,
                    10,
                    0
                ],
                "published": "2024-09-14T07:46:28Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    7,
                    46,
                    28,
                    5,
                    258,
                    0
                ],
                "title": "MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent\n  Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent\n  Conversion"
                },
                "summary": "In accented voice conversion or accent conversion, we seek to convert the\naccent in speech from one another while preserving speaker identity and\nsemantic content. In this study, we formulate a novel method for creating\nmulti-accented speech samples, thus pairs of accented speech samples by the\nsame speaker, through text transliteration for training accent conversion\nsystems. We begin by generating transliterated text with Large Language Models\n(LLMs), which is then fed into multilingual TTS models to synthesize accented\nEnglish speech. As a reference system, we built a sequence-to-sequence model on\nthe synthetic parallel corpus for accent conversion. We validated the proposed\nmethod for both native and non-native English speakers. Subjective and\nobjective evaluations further validate our dataset's effectiveness in accent\nconversion studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In accented voice conversion or accent conversion, we seek to convert the\naccent in speech from one another while preserving speaker identity and\nsemantic content. In this study, we formulate a novel method for creating\nmulti-accented speech samples, thus pairs of accented speech samples by the\nsame speaker, through text transliteration for training accent conversion\nsystems. We begin by generating transliterated text with Large Language Models\n(LLMs), which is then fed into multilingual TTS models to synthesize accented\nEnglish speech. As a reference system, we built a sequence-to-sequence model on\nthe synthetic parallel corpus for accent conversion. We validated the proposed\nmethod for both native and non-native English speakers. Subjective and\nobjective evaluations further validate our dataset's effectiveness in accent\nconversion studies."
                },
                "authors": [
                    {
                        "name": "Sho Inoue"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Wanxing Wang"
                    },
                    {
                        "name": "Pengcheng Zhu"
                    },
                    {
                        "name": "Mengxiao Bi"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "This is accepted to IEEE ICASSP 2025; Project page with Speech Demo:\n  https://github.com/shinshoji01/MacST-project-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11629v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11629v5",
                "updated": "2025-01-10T13:23:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    23,
                    37,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-17T15:11:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    11,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study"
                },
                "summary": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Yue Pan"
                    }
                ],
                "author_detail": {
                    "name": "Yue Pan"
                },
                "author": "Yue Pan",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11629v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11629v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.03056v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.03056v4",
                "updated": "2025-01-10T13:01:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    13,
                    1,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2023-11-06T12:22:19Z",
                "published_parsed": [
                    2023,
                    11,
                    6,
                    12,
                    22,
                    19,
                    0,
                    310,
                    0
                ],
                "title": "LitSumm: Large language models for literature summarisation of\n  non-coding RNAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitSumm: Large language models for literature summarisation of\n  non-coding RNAs"
                },
                "summary": "Curation of literature in life sciences is a growing challenge. The continued\nincrease in the rate of publication, coupled with the relatively fixed number\nof curators worldwide presents a major challenge to developers of biomedical\nknowledgebases. Very few knowledgebases have resources to scale to the whole\nrelevant literature and all have to prioritise their efforts.\n  In this work, we take a first step to alleviating the lack of curator time in\nRNA science by generating summaries of literature for non-coding RNAs using\nlarge language models (LLMs). We demonstrate that high-quality, factually\naccurate summaries with accurate references can be automatically generated from\nthe literature using a commercial LLM and a chain of prompts and checks. Manual\nassessment was carried out for a subset of summaries, with the majority being\nrated extremely high quality.\n  We apply our tool to a selection of over 4,600 ncRNAs and make the generated\nsummaries available via the RNAcentral resource. We conclude that automated\nliterature summarization is feasible with the current generation of LLMs,\nprovided careful prompting and automated checking are applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curation of literature in life sciences is a growing challenge. The continued\nincrease in the rate of publication, coupled with the relatively fixed number\nof curators worldwide presents a major challenge to developers of biomedical\nknowledgebases. Very few knowledgebases have resources to scale to the whole\nrelevant literature and all have to prioritise their efforts.\n  In this work, we take a first step to alleviating the lack of curator time in\nRNA science by generating summaries of literature for non-coding RNAs using\nlarge language models (LLMs). We demonstrate that high-quality, factually\naccurate summaries with accurate references can be automatically generated from\nthe literature using a commercial LLM and a chain of prompts and checks. Manual\nassessment was carried out for a subset of summaries, with the majority being\nrated extremely high quality.\n  We apply our tool to a selection of over 4,600 ncRNAs and make the generated\nsummaries available via the RNAcentral resource. We conclude that automated\nliterature summarization is feasible with the current generation of LLMs,\nprovided careful prompting and automated checking are applied."
                },
                "authors": [
                    {
                        "name": "Andrew Green"
                    },
                    {
                        "name": "Carlos Ribas"
                    },
                    {
                        "name": "Nancy Ontiveros-Palacios"
                    },
                    {
                        "name": "Sam Griffiths-Jones"
                    },
                    {
                        "name": "Anton I. Petrov"
                    },
                    {
                        "name": "Alex Bateman"
                    },
                    {
                        "name": "Blake Sweeney"
                    }
                ],
                "author_detail": {
                    "name": "Blake Sweeney"
                },
                "author": "Blake Sweeney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.03056v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.03056v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05931v1",
                "updated": "2025-01-10T12:54:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    54,
                    33,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:54:33Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    54,
                    33,
                    4,
                    10,
                    0
                ],
                "title": "Environment Modeling for Service Robots From a Task Execution\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment Modeling for Service Robots From a Task Execution\n  Perspective"
                },
                "summary": "Service robots are increasingly entering the home to provide domestic tasks\nfor residents. However, when working in an open, dynamic, and unstructured home\nenvironment, service robots still face challenges such as low intelligence for\ntask execution and poor long-term autonomy (LTA), which has limited their\ndeployment. As the basis of robotic task execution, environment modeling has\nattracted significant attention. This integrates core technologies such as\nenvironment perception, understanding, and representation to accurately\nrecognize environmental information. This paper presents a comprehensive survey\nof environmental modeling from a new task-executionoriented perspective. In\nparticular, guided by the requirements of robots in performing domestic service\ntasks in the home environment, we systematically review the progress that has\nbeen made in task-execution-oriented environmental modeling in four respects:\n1) localization, 2) navigation, 3) manipulation, and 4) LTA. Current challenges\nare discussed, and potential research opportunities are also highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service robots are increasingly entering the home to provide domestic tasks\nfor residents. However, when working in an open, dynamic, and unstructured home\nenvironment, service robots still face challenges such as low intelligence for\ntask execution and poor long-term autonomy (LTA), which has limited their\ndeployment. As the basis of robotic task execution, environment modeling has\nattracted significant attention. This integrates core technologies such as\nenvironment perception, understanding, and representation to accurately\nrecognize environmental information. This paper presents a comprehensive survey\nof environmental modeling from a new task-executionoriented perspective. In\nparticular, guided by the requirements of robots in performing domestic service\ntasks in the home environment, we systematically review the progress that has\nbeen made in task-execution-oriented environmental modeling in four respects:\n1) localization, 2) navigation, 3) manipulation, and 4) LTA. Current challenges\nare discussed, and potential research opportunities are also highlighted."
                },
                "authors": [
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Guohui Tian"
                    },
                    {
                        "name": "Cui-Hua Zhang"
                    },
                    {
                        "name": "Changchun Hua"
                    },
                    {
                        "name": "Weili Ding"
                    },
                    {
                        "name": "Choon Ki Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Choon Ki Ahn"
                },
                "author": "Choon Ki Ahn",
                "arxiv_doi": "10.1109/JAS.2025.125168",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JAS.2025.125168",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 9 figures; This article has been accepted for publication\n  in a future issue of IEEE/CAA Journal of Automatica Sinica, but has not been\n  fully edited. Content may change prior to final publication",
                "arxiv_journal_ref": "IEEE/CAA Journal of Automatica Sinica, 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05926v1",
                "updated": "2025-01-10T12:46:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities"
                },
                "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used psychological framework -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom LLMs, just as they do from humans. We then extend this framework to a more\nrealistic use case: text generation. Our analysis shows that LLMs generate\nstereotyped representations of sexual and gender minorities in this setting,\nraising concerns about their capacity to amplify representational harms in\ncreative writing, a widely promoted use case."
                },
                "authors": [
                    {
                        "name": "Ruby Ostrow"
                    },
                    {
                        "name": "Adam Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Adam Lopez"
                },
                "author": "Adam Lopez",
                "arxiv_comment": "10 pages, 8 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05925v1",
                "updated": "2025-01-10T12:44:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    44,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T12:44:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    44,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction"
                },
                "summary": "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements."
                },
                "authors": [
                    {
                        "name": "Petraq Nako"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05899v1",
                "updated": "2025-01-10T11:49:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    49,
                    31,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:49:31Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    49,
                    31,
                    4,
                    10,
                    0
                ],
                "title": "Prompt engineering and its implications on the energy consumption of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering and its implications on the energy consumption of\n  Large Language Models"
                },
                "summary": "Reducing the environmental impact of AI-based software systems has become\ncritical. The intensive use of large language models (LLMs) in software\nengineering poses severe challenges regarding computational resources, data\ncenters, and carbon emissions. In this paper, we investigate how prompt\nengineering techniques (PETs) can impact the carbon emission of the Llama 3\nmodel for the code generation task. We experimented with the CodeXGLUE\nbenchmark to evaluate both energy consumption and the accuracy of the generated\ncode using an isolated testing environment. Our initial results show that the\nenergy consumption of LLMs can be reduced by using specific tags that\ndistinguish different prompt parts. Even though a more in-depth evaluation is\nneeded to confirm our findings, this work suggests that prompt engineering can\nreduce LLMs' energy consumption during the inference phase without compromising\nperformance, paving the way for further investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the environmental impact of AI-based software systems has become\ncritical. The intensive use of large language models (LLMs) in software\nengineering poses severe challenges regarding computational resources, data\ncenters, and carbon emissions. In this paper, we investigate how prompt\nengineering techniques (PETs) can impact the carbon emission of the Llama 3\nmodel for the code generation task. We experimented with the CodeXGLUE\nbenchmark to evaluate both energy consumption and the accuracy of the generated\ncode using an isolated testing environment. Our initial results show that the\nenergy consumption of LLMs can be reduced by using specific tags that\ndistinguish different prompt parts. Even though a more in-depth evaluation is\nneeded to confirm our findings, this work suggests that prompt engineering can\nreduce LLMs' energy consumption during the inference phase without compromising\nperformance, paving the way for further investigations."
                },
                "authors": [
                    {
                        "name": "Riccardo Rubei"
                    },
                    {
                        "name": "Aicha Moussaid"
                    },
                    {
                        "name": "Claudio di Sipio"
                    },
                    {
                        "name": "Davide di Ruscio"
                    }
                ],
                "author_detail": {
                    "name": "Davide di Ruscio"
                },
                "author": "Davide di Ruscio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05891v1",
                "updated": "2025-01-10T11:44:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    35,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    44,
                    35,
                    4,
                    10,
                    0
                ],
                "title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs"
                },
                "summary": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable."
                },
                "authors": [
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Saverio Giallorenzo"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "The 40th ACM/SIGAPP Symposium On Applied Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05885v1",
                "updated": "2025-01-10T11:37:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    37,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:37:50Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    37,
                    50,
                    4,
                    10,
                    0
                ],
                "title": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster\n  Context Attention, Better Feature Fusion, and Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster\n  Context Attention, Better Feature Fusion, and Hardware Acceleration"
                },
                "summary": "Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet."
                },
                "authors": [
                    {
                        "name": "Zhifan Song"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Abd Al Rahman M. Abu Ebayyeh"
                    }
                ],
                "author_detail": {
                    "name": "Abd Al Rahman M. Abu Ebayyeh"
                },
                "author": "Abd Al Rahman M. Abu Ebayyeh",
                "arxiv_doi": "10.1109/SWC62898.2024.00141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SWC62898.2024.00141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in 21st IEEE International Conference on Ubiquitous\n  Intelligence and Computing (UIC 2024)\n  https://www.ieee-smart-world.org/2024/uic",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05884v1",
                "updated": "2025-01-10T11:35:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    35,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T11:35:43Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    35,
                    43,
                    4,
                    10,
                    0
                ],
                "title": "Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal\n  LLMs"
                },
                "summary": "The exponential growth of short-video content has ignited a surge in the\nnecessity for efficient, automated solutions to video editing, with challenges\narising from the need to understand videos and tailor the editing according to\nuser requirements. Addressing this need, we propose an innovative end-to-end\nfoundational framework, ultimately actualizing precise control over the final\nvideo content editing. Leveraging the flexibility and generalizability of\nMultimodal Large Language Models (MLLMs), we defined clear input-output\nmappings for efficient video creation. To bolster the model's capability in\nprocessing and comprehending video content, we introduce a strategic\ncombination of a denser frame rate and a slow-fast processing technique,\nsignificantly enhancing the extraction and understanding of both temporal and\nspatial video information. Furthermore, we introduce a text-to-edit mechanism\nthat allows users to achieve desired video outcomes through textual input,\nthereby enhancing the quality and controllability of the edited videos. Through\ncomprehensive experimentation, our method has not only showcased significant\neffectiveness within advertising datasets, but also yields universally\napplicable conclusions on public datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of short-video content has ignited a surge in the\nnecessity for efficient, automated solutions to video editing, with challenges\narising from the need to understand videos and tailor the editing according to\nuser requirements. Addressing this need, we propose an innovative end-to-end\nfoundational framework, ultimately actualizing precise control over the final\nvideo content editing. Leveraging the flexibility and generalizability of\nMultimodal Large Language Models (MLLMs), we defined clear input-output\nmappings for efficient video creation. To bolster the model's capability in\nprocessing and comprehending video content, we introduce a strategic\ncombination of a denser frame rate and a slow-fast processing technique,\nsignificantly enhancing the extraction and understanding of both temporal and\nspatial video information. Furthermore, we introduce a text-to-edit mechanism\nthat allows users to achieve desired video outcomes through textual input,\nthereby enhancing the quality and controllability of the edited videos. Through\ncomprehensive experimentation, our method has not only showcased significant\neffectiveness within advertising datasets, but also yields universally\napplicable conclusions on public datasets."
                },
                "authors": [
                    {
                        "name": "Dabing Cheng"
                    },
                    {
                        "name": "Haosen Zhan"
                    },
                    {
                        "name": "Xingchen Zhao"
                    },
                    {
                        "name": "Guisheng Liu"
                    },
                    {
                        "name": "Zemin Li"
                    },
                    {
                        "name": "Jinghui Xie"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Weiguo Feng"
                    },
                    {
                        "name": "Bingyue Peng"
                    }
                ],
                "author_detail": {
                    "name": "Bingyue Peng"
                },
                "author": "Bingyue Peng",
                "arxiv_comment": "16pages conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09504v2",
                "updated": "2025-01-10T11:03:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    11,
                    3,
                    13,
                    4,
                    10,
                    0
                ],
                "published": "2024-10-12T11:45:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    11,
                    45,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Transfer Learning for Artificially Intelligent Geospatial\n  Systems: A Predictive Stacking Approach"
                },
                "summary": "Building artificially intelligent geospatial systems require rapid delivery\nof spatial data analysis at massive scales with minimal human intervention.\nDepending upon their intended use, data analysis may also entail model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate its effectiveness in rapidly\nanalyzing massive data sets. Furthermore, we make inference feasible in a\nreasonable amount of time, and without excessively demanding hardware settings.\nWe illustrate the effectiveness of this approach in extensive simulation\nexperiments and subsequently analyze massive data sets in climate science on\nsea surface temperatures and on vegetation index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building artificially intelligent geospatial systems require rapid delivery\nof spatial data analysis at massive scales with minimal human intervention.\nDepending upon their intended use, data analysis may also entail model\nassessment and uncertainty quantification. This article devises transfer\nlearning frameworks for deployment in artificially intelligent systems, where a\nmassive data set is split into smaller data sets that stream into the\nanalytical framework to propagate learning and assimilate inference for the\nentire data set. Specifically, we introduce Bayesian predictive stacking for\nmultivariate spatial data and demonstrate its effectiveness in rapidly\nanalyzing massive data sets. Furthermore, we make inference feasible in a\nreasonable amount of time, and without excessively demanding hardware settings.\nWe illustrate the effectiveness of this approach in extensive simulation\nexperiments and subsequently analyze massive data sets in climate science on\nsea surface temperatures and on vegetation index."
                },
                "authors": [
                    {
                        "name": "Luca Presicce"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v1",
                "updated": "2025-01-10T10:53:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim"
                },
                "authors": [
                    {
                        "name": "Antonin Poché"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03968v2",
                "updated": "2025-01-10T10:38:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    38,
                    49,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-07T18:06:27Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-driven Behavior Tree for Context-aware Task Planning"
                },
                "summary": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations."
                },
                "authors": [
                    {
                        "name": "Naoki Wake"
                    },
                    {
                        "name": "Atsushi Kanehira"
                    },
                    {
                        "name": "Jun Takamatsu"
                    },
                    {
                        "name": "Kazuhiro Sasabuchi"
                    },
                    {
                        "name": "Katsushi Ikeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Katsushi Ikeuchi"
                },
                "author": "Katsushi Ikeuchi",
                "arxiv_comment": "10 pages, 11 figures, 5 tables. Last updated on January 9th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10221v2",
                "updated": "2025-01-10T10:36:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    36,
                    58,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-14T17:54:54Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    17,
                    54,
                    54,
                    4,
                    166,
                    0
                ],
                "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Story Short: Story-level Video Understanding from 20K Short Films"
                },
                "summary": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in vision-language models have significantly advanced\nvideo understanding. Existing datasets and tasks, however, have notable\nlimitations. Most datasets are confined to short videos with limited events and\nnarrow narratives. For example, datasets with instructional and egocentric\nvideos often depict activities of one person in a single scene. Although\nexisting movie datasets offer richer content, they are often limited to\nshort-term tasks, lack publicly available videos, and frequently encounter data\nleakage issues given the use of subtitles and other information about\ncommercial movies during LLM pretraining. To address the above limitations, we\npropose Short-Films 20K (SF20K), the largest publicly available movie dataset.\nSF20K is composed of 20,143 amateur films and offers long-term video tasks in\nthe form of multiple-choice and open-ended question answering. Our extensive\nanalysis of SF20K reveals minimal data leakage, emphasizes the need for\nlong-term reasoning, and demonstrates the strong performance of recent VLMs.\nFinally, we show that instruction tuning on the SF20K-Train set substantially\nimproves model performance, paving the way for future progress in long-term\nvideo understanding."
                },
                "authors": [
                    {
                        "name": "Ridouane Ghermi"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Vicky Kalogeiton"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05840v1",
                "updated": "2025-01-10T10:29:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    29,
                    27,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T10:29:27Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    29,
                    27,
                    4,
                    10,
                    0
                ],
                "title": "Applying Think-Aloud in ICTD: A Case Study of a Chatbot Use by Teachers\n  in Rural Côte d'Ivoire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Think-Aloud in ICTD: A Case Study of a Chatbot Use by Teachers\n  in Rural Côte d'Ivoire"
                },
                "summary": "Think-alouds are a common HCI usability method where participants verbalize\ntheir thoughts while using interfaces. However, their utility in cross-cultural\nsettings, particularly in the Global South, is unclear, where cultural\ndifferences impact user interactions. This paper investigates the usability\nchallenges teachers in rural C\\^ote d'Ivoire faced when using a chatbot\ndesigned to support an educational program. We conducted think-aloud sessions\nwith 20 teachers two weeks after a chatbot deployment, analyzing their\nnavigation, errors, and time spent on tasks. We discuss our approach and\nfindings that helped us identify usability issues and challenging features for\nimproving the chatbot designs. Our note summarizes our reflections on using\nthink-aloud and contributes to discussions on its culturally sensitive\nadaptation in the Global South.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-alouds are a common HCI usability method where participants verbalize\ntheir thoughts while using interfaces. However, their utility in cross-cultural\nsettings, particularly in the Global South, is unclear, where cultural\ndifferences impact user interactions. This paper investigates the usability\nchallenges teachers in rural C\\^ote d'Ivoire faced when using a chatbot\ndesigned to support an educational program. We conducted think-aloud sessions\nwith 20 teachers two weeks after a chatbot deployment, analyzing their\nnavigation, errors, and time spent on tasks. We discuss our approach and\nfindings that helped us identify usability issues and challenging features for\nimproving the chatbot designs. Our note summarizes our reflections on using\nthink-aloud and contributes to discussions on its culturally sensitive\nadaptation in the Global South."
                },
                "authors": [
                    {
                        "name": "Vikram Kamath Cannanure"
                    },
                    {
                        "name": "Sharon Wolf"
                    },
                    {
                        "name": "Kaja Jasińska"
                    },
                    {
                        "name": "Timothy X Brown"
                    },
                    {
                        "name": "Amy Ogan"
                    }
                ],
                "author_detail": {
                    "name": "Amy Ogan"
                },
                "author": "Amy Ogan",
                "arxiv_comment": "ICTD 24, Notes track. International Conference on Information &\n  Communication Technologies and Development 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; K.3.1; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15627v3",
                "updated": "2025-01-10T10:24:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    24,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2024-06-21T20:06:31Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    20,
                    6,
                    31,
                    4,
                    173,
                    0
                ],
                "title": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph"
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Ekaterina Fadeeva"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Lyudmila Rvanova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Daniil Vasilev"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Abdelrahman Boda Sadallah"
                    },
                    {
                        "name": "Kirill Grishchenkov"
                    },
                    {
                        "name": "Sergey Petrakov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_comment": "Accepted to TACL 2025, pre-MIT Press publication version. Roman\n  Vashurin, Ekaterina Fadeeva, Artem Vazhentsev contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v2",
                "updated": "2025-01-10T10:08:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    8,
                    50,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05820v1",
                "updated": "2025-01-10T09:59:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    59,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T09:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    59,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays"
                },
                "summary": "Modular Arrays (MAs) are a promising architecture to enable multi-user\ncommunications in next-generation multiple-input multiple-output (MIMO) systems\nbased on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an\nimproved spatial resolution with characteristic interference patterns\nassociated to grating lobes. In this work, we analyze whether MAs can\noutperform conventional collocated deployments, in terms of achievable sum-rate\nand served users in a multi-user downlink set-up. First, we provide a rigorous\nanalytical characterization of the inter-user interference for modular gMIMO\nsystems operating in the near field. Then, we leverage these results to\noptimize the user selection and precoding mechanisms, designing two algorithms\nthat largely outperform existing alternatives in the literature, with different\nalgorithmic complexities. Results show that the proposed algorithms yield over\n70% improvements in achievable sum-spectral efficiencies compared to the state\nof the art. We also illustrate how MAs allow to serve a larger number of users\nthanks to their improved spatial resolution, compared to the collocated\ncounterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Arrays (MAs) are a promising architecture to enable multi-user\ncommunications in next-generation multiple-input multiple-output (MIMO) systems\nbased on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an\nimproved spatial resolution with characteristic interference patterns\nassociated to grating lobes. In this work, we analyze whether MAs can\noutperform conventional collocated deployments, in terms of achievable sum-rate\nand served users in a multi-user downlink set-up. First, we provide a rigorous\nanalytical characterization of the inter-user interference for modular gMIMO\nsystems operating in the near field. Then, we leverage these results to\noptimize the user selection and precoding mechanisms, designing two algorithms\nthat largely outperform existing alternatives in the literature, with different\nalgorithmic complexities. Results show that the proposed algorithms yield over\n70% improvements in achievable sum-spectral efficiencies compared to the state\nof the art. We also illustrate how MAs allow to serve a larger number of users\nthanks to their improved spatial resolution, compared to the collocated\ncounterpart."
                },
                "authors": [
                    {
                        "name": "José P. González-Coma"
                    },
                    {
                        "name": "Santiago Fernández"
                    },
                    {
                        "name": "F. Javier López-Martínez"
                    }
                ],
                "author_detail": {
                    "name": "F. Javier López-Martínez"
                },
                "author": "F. Javier López-Martínez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05399v2",
                "updated": "2025-01-10T09:54:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    54,
                    54,
                    4,
                    10,
                    0
                ],
                "published": "2024-04-08T10:57:25Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    10,
                    57,
                    25,
                    0,
                    99,
                    0
                ],
                "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and\n  Improving Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and\n  Improving Large Language Model Safety"
                },
                "summary": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops."
                },
                "authors": [
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Fabio Pernisi"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "Accepted at AAAI 2025 (Special Track on AI Alignment)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05817v1",
                "updated": "2025-01-10T09:44:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    44,
                    30,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T09:44:30Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    44,
                    30,
                    4,
                    10,
                    0
                ],
                "title": "RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT"
                },
                "summary": "This paper evaluates the performance of reconfigurable intelligent surface\n(RIS) optimization algorithms, which utilize channel estimation methods, in ray\ntracing (RT) simulations within urban digital twin environments. Beyond\nSionna's native capabilities, we implement and benchmark additional RIS\noptimization algorithms based on channel estimation, enabling an evaluation of\nRIS strategies under various deployment conditions. Coverage maps for\nRIS-assisted communication systems are generated through the integration of\nSionna's RT simulations. Moreover, real-world experimentation underscores the\nnecessity of validating algorithms in near-realistic simulation environments,\nas minor variations in measurement setups can significantly affect performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the performance of reconfigurable intelligent surface\n(RIS) optimization algorithms, which utilize channel estimation methods, in ray\ntracing (RT) simulations within urban digital twin environments. Beyond\nSionna's native capabilities, we implement and benchmark additional RIS\noptimization algorithms based on channel estimation, enabling an evaluation of\nRIS strategies under various deployment conditions. Coverage maps for\nRIS-assisted communication systems are generated through the integration of\nSionna's RT simulations. Moreover, real-world experimentation underscores the\nnecessity of validating algorithms in near-realistic simulation environments,\nas minor variations in measurement setups can significantly affect performance."
                },
                "authors": [
                    {
                        "name": "Ahmet Esad Güneşer"
                    },
                    {
                        "name": "Berkay Şekeroğlu"
                    },
                    {
                        "name": "Sefa Kayraklık"
                    },
                    {
                        "name": "Erhan Karakoca"
                    },
                    {
                        "name": "İbrahim Hökelek"
                    },
                    {
                        "name": "Sultan Aldirmaz-Colak"
                    },
                    {
                        "name": "Ali Görçin"
                    }
                ],
                "author_detail": {
                    "name": "Ali Görçin"
                },
                "author": "Ali Görçin",
                "arxiv_comment": "Submitted to possible IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02221v2",
                "updated": "2025-01-10T09:26:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    26,
                    32,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-04T07:53:38Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    7,
                    53,
                    38,
                    5,
                    4,
                    0
                ],
                "title": "CORD: Generalizable Cooperation via Role Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Generalizable Cooperation via Role Diversity"
                },
                "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to develop agents\nthat can collaborate effectively. However, most cooperative MARL methods\noverfit training agents, making learned policies not generalize well to unseen\ncollaborators, which is a critical issue for real-world deployment. Some\nmethods attempt to address the generalization problem but require prior\nknowledge or predefined policies of new teammates, limiting real-world\napplications. To this end, we propose a hierarchical MARL approach to enable\ngeneralizable cooperation via role diversity, namely CORD. CORD's high-level\ncontroller assigns roles to low-level agents by maximizing the role entropy\nwith constraints. We show this constrained objective can be decomposed into\ncausal influence in role that enables reasonable role assignment, and role\nheterogeneity that yields coherent, non-redundant role clusters. Evaluated on a\nvariety of cooperative multi-agent tasks, CORD achieves better performance than\nbaselines, especially in generalization tests. Ablation studies further\ndemonstrate the efficacy of the constrained objective in generalizable\ncooperation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative multi-agent reinforcement learning (MARL) aims to develop agents\nthat can collaborate effectively. However, most cooperative MARL methods\noverfit training agents, making learned policies not generalize well to unseen\ncollaborators, which is a critical issue for real-world deployment. Some\nmethods attempt to address the generalization problem but require prior\nknowledge or predefined policies of new teammates, limiting real-world\napplications. To this end, we propose a hierarchical MARL approach to enable\ngeneralizable cooperation via role diversity, namely CORD. CORD's high-level\ncontroller assigns roles to low-level agents by maximizing the role entropy\nwith constraints. We show this constrained objective can be decomposed into\ncausal influence in role that enables reasonable role assignment, and role\nheterogeneity that yields coherent, non-redundant role clusters. Evaluated on a\nvariety of cooperative multi-agent tasks, CORD achieves better performance than\nbaselines, especially in generalization tests. Ablation studies further\ndemonstrate the efficacy of the constrained objective in generalizable\ncooperation."
                },
                "authors": [
                    {
                        "name": "Kanefumi Matsuyama"
                    },
                    {
                        "name": "Kefan Su"
                    },
                    {
                        "name": "Jiangxing Wang"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05790v1",
                "updated": "2025-01-10T08:50:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T08:50:38Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "title": "Understanding Impact of Human Feedback via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Impact of Human Feedback via Influence Functions"
                },
                "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. In our experiments, we demonstrate\ntwo key applications of influence functions: (1) detecting common forms of\nlabeler bias in human feedback datasets and (2) guiding labelers to refine\ntheir strategies to align more closely with expert feedback. By quantifying the\nimpact of human feedback on reward models, we believe that influence functions\ncan enhance feedback interpretability and contribute to scalable oversight in\nRLHF, helping labelers provide more accurate and consistent feedback. Source\ncode is available at https://github.com/mintaywon/IF_RLHF"
                },
                "authors": [
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Haeone Lee"
                    },
                    {
                        "name": "Hanho Ryu"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Source code: https://github.com/mintaywon/IF_RLHF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05772v1",
                "updated": "2025-01-10T08:07:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    7,
                    14,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T08:07:14Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    7,
                    14,
                    4,
                    10,
                    0
                ],
                "title": "rmlnomogram: An R package to construct an explainable nomogram for any\n  machine learning algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rmlnomogram: An R package to construct an explainable nomogram for any\n  machine learning algorithms"
                },
                "summary": "Background: Current nomogram can only be created for regression algorithm.\nProviding nomogram for any machine learning (ML) algorithms may accelerate\nmodel deployment in clinical settings or improve model availability. We\ndeveloped an R package and web application to construct nomogram with model\nexplainability of any ML algorithms. Methods: We formulated a function to\ntransform an ML prediction model into a nomogram, requiring datasets with: (1)\nall possible combinations of predictor values; (2) the corresponding outputs of\nthe model; and (3) the corresponding explainability values for each predictor\n(optional). Web application was also created. Results: Our R package could\ncreate 5 types of nomograms for categorical predictors and binary outcome\nwithout probability (1), categorical predictors and binary outcome with\nprobability (2) or continuous outcome (3), and categorical with single\nnumerical predictors and binary outcome with probability (4) or continuous\noutcome (5). Respectively, the first and remaining types optimally allowed\nmaximum 15 and 5 predictors with maximum 3,200 combinations. Web application is\nprovided with such limits. The explainability values were possible for types 2\nto 5. Conclusions: Our R package and web application could construct nomogram\nwith model explainability of any ML algorithms using a fair number of\npredictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Current nomogram can only be created for regression algorithm.\nProviding nomogram for any machine learning (ML) algorithms may accelerate\nmodel deployment in clinical settings or improve model availability. We\ndeveloped an R package and web application to construct nomogram with model\nexplainability of any ML algorithms. Methods: We formulated a function to\ntransform an ML prediction model into a nomogram, requiring datasets with: (1)\nall possible combinations of predictor values; (2) the corresponding outputs of\nthe model; and (3) the corresponding explainability values for each predictor\n(optional). Web application was also created. Results: Our R package could\ncreate 5 types of nomograms for categorical predictors and binary outcome\nwithout probability (1), categorical predictors and binary outcome with\nprobability (2) or continuous outcome (3), and categorical with single\nnumerical predictors and binary outcome with probability (4) or continuous\noutcome (5). Respectively, the first and remaining types optimally allowed\nmaximum 15 and 5 predictors with maximum 3,200 combinations. Web application is\nprovided with such limits. The explainability values were possible for types 2\nto 5. Conclusions: Our R package and web application could construct nomogram\nwith model explainability of any ML algorithms using a fair number of\npredictors."
                },
                "authors": [
                    {
                        "name": "Herdiantri Sufriyana"
                    },
                    {
                        "name": "Emily Chia-Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Emily Chia-Yu Su"
                },
                "author": "Emily Chia-Yu Su",
                "arxiv_comment": "16 pages, 2 figures, 1 table, 3 equations, 1 algorithm, 4 code\n  snippets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.m; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05764v1",
                "updated": "2025-01-10T07:41:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    48,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T07:41:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    41,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Controlling Large Language Models Through Concept Activation Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Large Language Models Through Concept Activation Vectors"
                },
                "summary": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Chengao Li"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12746v2",
                "updated": "2025-01-10T07:38:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    38,
                    53,
                    4,
                    10,
                    0
                ],
                "published": "2023-10-19T13:50:56Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    13,
                    50,
                    56,
                    3,
                    292,
                    0
                ],
                "title": "TabuLa: Harnessing Language Models for Tabular Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabuLa: Harnessing Language Models for Tabular Data Synthesis"
                },
                "summary": "Tabular data synthesis is crucial for addressing privacy and security\nconcerns in industries reliant on tabular data. While recent advancements adopt\nlarge language models (LLMs) for realistic tabular data generation, their long\ntraining times and limited reusability hinder practical applications. In this\npaper, we propose Tabula, a tabular data synthesizer that leverages the\nstructure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data\nsynthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained\nweights originally designed for natural language tasks, focusing instead on a\ntailored approach for tabular data. In addition, Tabula introduces a token\nsequence compression strategy that significantly reduces training time while\nmaintaining data quality, alongside a novel token padding method that improves\nsequence alignment across training batches. Experiments on six datasets show\nthat Tabula achieves superior synthetic data utility compared to current SOTA\nmethods. Additionally, the results demonstrate that Tabula model trained on\ntabular datasets serves effectively as a foundational model for synthesizing\nnew tabular datasets. Furthermore, the proposed padding method outperforms the\nconventional left and right padding strategies. Finally, the results highlight\nthat Tabula averagely reduces training time per epoch by 46.2% compared to\nstate-of-the-art LLM approaches while achieving higher data utility. Our code\nis available at https://github.com/zhao-zilong/Tabula",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial for addressing privacy and security\nconcerns in industries reliant on tabular data. While recent advancements adopt\nlarge language models (LLMs) for realistic tabular data generation, their long\ntraining times and limited reusability hinder practical applications. In this\npaper, we propose Tabula, a tabular data synthesizer that leverages the\nstructure of LLM. Unlike state-of-the-art (SOTA) LLM-based tabular data\nsynthesizers that rely on pre-trained LLMs, Tabula discards the pre-trained\nweights originally designed for natural language tasks, focusing instead on a\ntailored approach for tabular data. In addition, Tabula introduces a token\nsequence compression strategy that significantly reduces training time while\nmaintaining data quality, alongside a novel token padding method that improves\nsequence alignment across training batches. Experiments on six datasets show\nthat Tabula achieves superior synthetic data utility compared to current SOTA\nmethods. Additionally, the results demonstrate that Tabula model trained on\ntabular datasets serves effectively as a foundational model for synthesizing\nnew tabular datasets. Furthermore, the proposed padding method outperforms the\nconventional left and right padding strategies. Finally, the results highlight\nthat Tabula averagely reduces training time per epoch by 46.2% compared to\nstate-of-the-art LLM approaches while achieving higher data utility. Our code\nis available at https://github.com/zhao-zilong/Tabula"
                },
                "authors": [
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Robert Birke"
                    },
                    {
                        "name": "Lydia Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Chen"
                },
                "author": "Lydia Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05752v1",
                "updated": "2025-01-10T07:02:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    2,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T07:02:43Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    7,
                    2,
                    43,
                    4,
                    10,
                    0
                ],
                "title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving\n  with Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in various complex tasks requiring multi-step reasoning methods like\ntree search to explore diverse reasoning paths. However, existing methods often\nsuffer from computational inefficiency and redundancy. First, they overlook the\ndiversity of task difficulties, leading to unnecessarily extensive searches\neven for easy tasks. Second, they neglect the semantics of reasoning paths,\nresulting in redundant exploration of semantically identical paths. To address\nthese limitations, we propose Semantic Exploration with Adaptive Gating (SEAG),\na computationally efficient method. SEAG employs an adaptive gating mechanism\nthat dynamically decides whether to conduct a tree search, based on the\nconfidence level of answers from a preceding simple reasoning method.\nFurthermore, its tree-based exploration consolidates semantically identical\nreasoning steps, reducing redundant explorations while maintaining or even\nimproving accuracy. Our extensive experiments demonstrate that SEAG\nsignificantly improves accuracy by 4.3% on average while requiring only 31% of\ncomputational costs compared to existing tree search-based methods on complex\nreasoning benchmarks including GSM8K and ARC with diverse language models such\nas Llama2, Llama3, and Mistral.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in various complex tasks requiring multi-step reasoning methods like\ntree search to explore diverse reasoning paths. However, existing methods often\nsuffer from computational inefficiency and redundancy. First, they overlook the\ndiversity of task difficulties, leading to unnecessarily extensive searches\neven for easy tasks. Second, they neglect the semantics of reasoning paths,\nresulting in redundant exploration of semantically identical paths. To address\nthese limitations, we propose Semantic Exploration with Adaptive Gating (SEAG),\na computationally efficient method. SEAG employs an adaptive gating mechanism\nthat dynamically decides whether to conduct a tree search, based on the\nconfidence level of answers from a preceding simple reasoning method.\nFurthermore, its tree-based exploration consolidates semantically identical\nreasoning steps, reducing redundant explorations while maintaining or even\nimproving accuracy. Our extensive experiments demonstrate that SEAG\nsignificantly improves accuracy by 4.3% on average while requiring only 31% of\ncomputational costs compared to existing tree search-based methods on complex\nreasoning benchmarks including GSM8K and ARC with diverse language models such\nas Llama2, Llama3, and Mistral."
                },
                "authors": [
                    {
                        "name": "Sungjae Lee"
                    },
                    {
                        "name": "Hyejin Park"
                    },
                    {
                        "name": "Jaechang Kim"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09093v2",
                "updated": "2025-01-10T06:04:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    6,
                    4,
                    24,
                    4,
                    10,
                    0
                ],
                "published": "2024-08-17T04:43:26Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    4,
                    43,
                    26,
                    5,
                    230,
                    0
                ],
                "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language\n  Models by Treating Harmful Instruction as Backdoor Trigger"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have showcased impressive\nperformance in a variety of multimodal tasks. On the other hand, the\nintegration of additional image modality may allow the malicious users to\ninject harmful content inside the images for jailbreaking. Unlike text-based\nLLMs, where adversaries need to select discrete tokens to conceal their\nmalicious intent using specific algorithms, the continuous nature of image\nsignals provides a direct opportunity for adversaries to inject harmful\nintentions. In this work, we propose $\\textbf{BaThe}$ ($\\textbf{Ba}$ckdoor\n$\\textbf{T}$rigger S$\\textbf{h}$i$\\textbf{e}$ld), a simple yet effective\njailbreak defense mechanism. Our work is motivated by recent research on\njailbreak backdoor attack and virtual prompt backdoor attack in generative\nlanguage models. Jailbreak backdoor attack uses harmful instructions combined\nwith manually crafted strings as triggers to make the backdoored model generate\nprohibited responses. We assume that harmful instructions can function as\ntriggers, and if we alternatively set rejection responses as the triggered\nresponse, the backdoored model then can defend against jailbreak attacks. We\nachieve this by utilizing virtual rejection prompt, similar to the virtual\nprompt backdoor attack. We embed the virtual rejection prompt into the soft\ntext embeddings, which we call ``wedge''. Our comprehensive experiments\ndemonstrate that BaThe effectively mitigates various types of jailbreak attacks\nand is adaptable to defend against unseen attacks, with minimal impact on\nMLLMs' performance."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yirui Zhang"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05727v1",
                "updated": "2025-01-10T05:51:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    51,
                    52,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:51:52Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    51,
                    52,
                    4,
                    10,
                    0
                ],
                "title": "Enabling Scalable Oversight via Self-Evolving Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Scalable Oversight via Self-Evolving Critic"
                },
                "summary": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component."
                },
                "authors": [
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Zhenyang Xiao"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05724v1",
                "updated": "2025-01-10T05:43:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    36,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:43:36Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    36,
                    4,
                    10,
                    0
                ],
                "title": "I Can't Share Code, but I need Translation -- An Empirical Study on Code\n  Translation through Federated LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can't Share Code, but I need Translation -- An Empirical Study on Code\n  Translation through Federated LLM"
                },
                "summary": "Owing to the rapid evolution of technologies and project requirements,\norganizations need to upgrade the code base in their software projects to a new\nversion of the programming language or even translating to an entirely new one.\nHowever, code translation is resource-intensive and requires expertise in both\nthe source and target languages. While researchers have made progress in\nautomating translations between legacy and modern languages, recent work has\nincreasingly turned to pre-trained Large Language Models (LLMs) to translate\nefficiently.\n  Given the proprietary nature of code, organizations prefer fine-tuning LLMs\nlocally rather than relying on external APIs. This is one of the first\nempirical studies that proposes a Federated LLM-based approach for code\ntranslation. The proposed approach enables clients to jointly train a code\ntranslator without sharing sensitive data. This study demonstrates that\nparticipants can collaboratively develop a FedLLM for efficient code\ntranslation (particularly C\\# to Java and vice-versa) with superior results\n(more than 40\\% improvement in CodeLLaMA's CodeBLEU score) compared to\nindividual client models. Our findings indicate that FedLLM offers a\ncollaborative approach to code translation and could serve as a promising\ndirection for future research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the rapid evolution of technologies and project requirements,\norganizations need to upgrade the code base in their software projects to a new\nversion of the programming language or even translating to an entirely new one.\nHowever, code translation is resource-intensive and requires expertise in both\nthe source and target languages. While researchers have made progress in\nautomating translations between legacy and modern languages, recent work has\nincreasingly turned to pre-trained Large Language Models (LLMs) to translate\nefficiently.\n  Given the proprietary nature of code, organizations prefer fine-tuning LLMs\nlocally rather than relying on external APIs. This is one of the first\nempirical studies that proposes a Federated LLM-based approach for code\ntranslation. The proposed approach enables clients to jointly train a code\ntranslator without sharing sensitive data. This study demonstrates that\nparticipants can collaboratively develop a FedLLM for efficient code\ntranslation (particularly C\\# to Java and vice-versa) with superior results\n(more than 40\\% improvement in CodeLLaMA's CodeBLEU score) compared to\nindividual client models. Our findings indicate that FedLLM offers a\ncollaborative approach to code translation and could serve as a promising\ndirection for future research in this field."
                },
                "authors": [
                    {
                        "name": "Jahnavi Kumar"
                    },
                    {
                        "name": "Venkata Lakshmana Sasaank Janapati"
                    },
                    {
                        "name": "Mokshith Reddy Tanguturi"
                    },
                    {
                        "name": "Sridhar Chimalakonda"
                    }
                ],
                "author_detail": {
                    "name": "Sridhar Chimalakonda"
                },
                "author": "Sridhar Chimalakonda",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05722v1",
                "updated": "2025-01-10T05:43:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    31,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:43:31Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    43,
                    31,
                    4,
                    10,
                    0
                ],
                "title": "An Efficiency Firmware Verification Framework for Public Key\n  Infrastructure with Smart Grid and Energy Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficiency Firmware Verification Framework for Public Key\n  Infrastructure with Smart Grid and Energy Storage System"
                },
                "summary": "As a critical component of electrical energy infrastructure, the smart grid\nsystem has become indispensable to the energy sector. However, the rapid\nevolution of smart grids has attracted numerous nation-state actors seeking to\ndisrupt the power infrastructure of adversarial nations. This development\nunderscores the urgent need to establish secure mechanisms for firmware\nupdates, with firmware signing and verification serving as pivotal elements in\nsafeguarding system integrity. In this work, we propose a digital signing and\nverification framework grounded in Public Key Infrastructure (PKI),\nspecifically tailored for resource-constrained devices such as smart meters.\nThe framework utilizes the Concise Binary Object Representation (CBOR) and\nObject Signing and Encryption (COSE) formats to achieve efficient da-ta\nencapsulation and robust security features. Our approach not only en-sures the\nsecure deployment of firmware updates against the convergence of information\ntechnology (IT) and operational technology (OT) attacks but also addresses\nperformance bottlenecks stemming from device limitations, thereby enhancing the\noverall reliability and stability of the smart grid sys-tem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a critical component of electrical energy infrastructure, the smart grid\nsystem has become indispensable to the energy sector. However, the rapid\nevolution of smart grids has attracted numerous nation-state actors seeking to\ndisrupt the power infrastructure of adversarial nations. This development\nunderscores the urgent need to establish secure mechanisms for firmware\nupdates, with firmware signing and verification serving as pivotal elements in\nsafeguarding system integrity. In this work, we propose a digital signing and\nverification framework grounded in Public Key Infrastructure (PKI),\nspecifically tailored for resource-constrained devices such as smart meters.\nThe framework utilizes the Concise Binary Object Representation (CBOR) and\nObject Signing and Encryption (COSE) formats to achieve efficient da-ta\nencapsulation and robust security features. Our approach not only en-sures the\nsecure deployment of firmware updates against the convergence of information\ntechnology (IT) and operational technology (OT) attacks but also addresses\nperformance bottlenecks stemming from device limitations, thereby enhancing the\noverall reliability and stability of the smart grid sys-tem."
                },
                "authors": [
                    {
                        "name": "Jhih-Zen Shih"
                    },
                    {
                        "name": "Cheng-Che Chuang"
                    },
                    {
                        "name": "Hong-Sheng Huang"
                    },
                    {
                        "name": "Hsuan-Tung Chen"
                    },
                    {
                        "name": "Hung-Min Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hung-Min Sun"
                },
                "author": "Hung-Min Sun",
                "arxiv_comment": "10pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05714v1",
                "updated": "2025-01-10T05:15:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    15,
                    14,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:15:14Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    15,
                    14,
                    4,
                    10,
                    0
                ],
                "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond"
                },
                "summary": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Wenqiang Lei"
                    },
                    {
                        "name": "Jiancheng Lv"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jimmy Xiangji Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Xiangji Huang"
                },
                "author": "Jimmy Xiangji Huang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05712v1",
                "updated": "2025-01-10T05:07:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    7,
                    27,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T05:07:27Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    7,
                    27,
                    4,
                    10,
                    0
                ],
                "title": "Multi-Step Reasoning in Korean and the Emergent Mirage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning in Korean and the Emergent Mirage"
                },
                "summary": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Dasol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Dasol Choi"
                },
                "author": "Dasol Choi",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05707v1",
                "updated": "2025-01-10T04:35:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    35,
                    46,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T04:35:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    35,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Vighnesh Subramaniam"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Antonio Torralba"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Igor Mordatch"
                    }
                ],
                "author_detail": {
                    "name": "Igor Mordatch"
                },
                "author": "Igor Mordatch",
                "arxiv_comment": "22 pages, 13 figures, 7 tables; Project page at\n  https://llm-multiagent-ft.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05706v1",
                "updated": "2025-01-10T04:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    32,
                    19,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T04:32:19Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    32,
                    19,
                    4,
                    10,
                    0
                ],
                "title": "Debugging Without Error Messages: How LLM Prompting Strategy Affects\n  Programming Error Explanation Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging Without Error Messages: How LLM Prompting Strategy Affects\n  Programming Error Explanation Effectiveness"
                },
                "summary": "Making errors is part of the programming process -- even for the most\nseasoned professionals. Novices in particular are bound to make many errors\nwhile learning. It is well known that traditional (compiler/interpreter)\nprogramming error messages have been less than helpful for many novices and can\nhave effects such as being frustrating, containing confusing jargon, and being\ndownright misleading. Recent work has found that large language models (LLMs)\ncan generate excellent error explanations, but that the effectiveness of these\nerror messages heavily depends on whether the LLM has been provided with\ncontext -- typically the original source code where the problem occurred.\nKnowing that programming error messages can be misleading and/or contain that\nserves little-to-no use (particularly for novices) we explore the reverse: what\nhappens when GPT-3.5 is prompted for error explanations on just the erroneous\nsource code itself -- original compiler/interpreter produced error message\nexcluded. We utilized various strategies to make more effective error\nexplanations, including one-shot prompting and fine-tuning. We report the\nbaseline results of how effective the error explanations are at providing\nfeedback, as well as how various prompting strategies might improve the\nexplanations' effectiveness. Our results can help educators by understanding\nhow LLMs respond to such prompts that novices are bound to make, and hopefully\nlead to more effective use of Generative AI in the classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making errors is part of the programming process -- even for the most\nseasoned professionals. Novices in particular are bound to make many errors\nwhile learning. It is well known that traditional (compiler/interpreter)\nprogramming error messages have been less than helpful for many novices and can\nhave effects such as being frustrating, containing confusing jargon, and being\ndownright misleading. Recent work has found that large language models (LLMs)\ncan generate excellent error explanations, but that the effectiveness of these\nerror messages heavily depends on whether the LLM has been provided with\ncontext -- typically the original source code where the problem occurred.\nKnowing that programming error messages can be misleading and/or contain that\nserves little-to-no use (particularly for novices) we explore the reverse: what\nhappens when GPT-3.5 is prompted for error explanations on just the erroneous\nsource code itself -- original compiler/interpreter produced error message\nexcluded. We utilized various strategies to make more effective error\nexplanations, including one-shot prompting and fine-tuning. We report the\nbaseline results of how effective the error explanations are at providing\nfeedback, as well as how various prompting strategies might improve the\nexplanations' effectiveness. Our results can help educators by understanding\nhow LLMs respond to such prompts that novices are bound to make, and hopefully\nlead to more effective use of Generative AI in the classroom."
                },
                "authors": [
                    {
                        "name": "Audrey Salmon"
                    },
                    {
                        "name": "Katie Hammer"
                    },
                    {
                        "name": "Eddie Antonio Santos"
                    },
                    {
                        "name": "Brett A. Becker"
                    }
                ],
                "author_detail": {
                    "name": "Brett A. Becker"
                },
                "author": "Brett A. Becker",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v3",
                "updated": "2025-01-10T04:09:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    4,
                    9,
                    43,
                    4,
                    10,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "9 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12924v2",
                "updated": "2025-01-10T03:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    3,
                    55,
                    57,
                    4,
                    10,
                    0
                ],
                "published": "2024-11-19T23:22:33Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    22,
                    33,
                    1,
                    324,
                    0
                ],
                "title": "Human-In-the-Loop Software Development Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-In-the-Loop Software Development Agents"
                },
                "summary": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, rarely considers\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality remain a concern in some cases. We\ndraw lessons learned and discuss opportunities for future work, which will pave\nthe way for the advancement of LLM-based agents in software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, rarely considers\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality remain a concern in some cases. We\ndraw lessons learned and discuss opportunities for future work, which will pave\nthe way for the advancement of LLM-based agents in software development."
                },
                "authors": [
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Jirat Pasuksmit"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Ruixiong Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Evan Cook"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Wu"
                },
                "author": "Ming Wu",
                "arxiv_comment": "10 pages, 9 figures, ICSE SEIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05675v1",
                "updated": "2025-01-10T02:57:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T02:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "Facilitate Collaboration between Large Language Model and Task-specific\n  Model for Time Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitate Collaboration between Large Language Model and Task-specific\n  Model for Time Series Anomaly Detection"
                },
                "summary": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge, while task-specific smaller models excel at\nextracting normal patterns and detecting value fluctuations. Inspired by the\nhuman nervous system, where the brain stores expert knowledge and the\nperipheral nervous system and spinal cord handle specific tasks like withdrawal\nand knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate\ncollaboration between LLMs and task-specific models, leveraging the strengths\nof both.\n  In this work, we first formulate the collaboration process and identify two\nkey challenges in the collaboration between LLMs and task-specific models: (1)\nthe misalignment between the expression domains of LLMs and smaller models, and\n(2) error accumulation arising from the predictions of both models.\n  To address these challenges, we introduce two key components in CoLLaTe: the\nalignment module and the collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan LLM based methods and task-specific smaller model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge, while task-specific smaller models excel at\nextracting normal patterns and detecting value fluctuations. Inspired by the\nhuman nervous system, where the brain stores expert knowledge and the\nperipheral nervous system and spinal cord handle specific tasks like withdrawal\nand knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate\ncollaboration between LLMs and task-specific models, leveraging the strengths\nof both.\n  In this work, we first formulate the collaboration process and identify two\nkey challenges in the collaboration between LLMs and task-specific models: (1)\nthe misalignment between the expression domains of LLMs and smaller models, and\n(2) error accumulation arising from the predictions of both models.\n  To address these challenges, we introduce two key components in CoLLaTe: the\nalignment module and the collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan LLM based methods and task-specific smaller model."
                },
                "authors": [
                    {
                        "name": "Feiyi Chen"
                    },
                    {
                        "name": "Leilei Zhang"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04967v2",
                "updated": "2025-01-10T02:54:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    54,
                    18,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-09T04:41:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    41,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time\n  Series Filtration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time\n  Series Filtration"
                },
                "summary": "Current machine learning (ML)-based algorithms for filtering\nelectroencephalography (EEG) time series data face challenges related to\ncumbersome training times, regularization, and accurate reconstruction. To\naddress these shortcomings, we present an ML filtration algorithm driven by a\nlogistic covariance-targeted adversarial denoising autoencoder (TADA). We\nhypothesize that the expressivity of a targeted, correlation-driven\nconvolutional autoencoder will enable effective time series filtration while\nminimizing compute requirements (e.g., runtime, model size). Furthermore, we\nexpect that adversarial training with covariance rescaling will minimize signal\ndegradation. To test this hypothesis, a TADA system prototype was trained and\nevaluated on the task of removing electromyographic (EMG) noise from EEG data\nin the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.\nThe TADA filter surpasses conventional signal filtration algorithms across\nquantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE),\nand performs competitively against other deep learning architectures at a\nreduced model size of less than 400,000 trainable parameters. Further\nexperimentation will be necessary to assess the viability of TADA on a wider\nrange of deployment cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current machine learning (ML)-based algorithms for filtering\nelectroencephalography (EEG) time series data face challenges related to\ncumbersome training times, regularization, and accurate reconstruction. To\naddress these shortcomings, we present an ML filtration algorithm driven by a\nlogistic covariance-targeted adversarial denoising autoencoder (TADA). We\nhypothesize that the expressivity of a targeted, correlation-driven\nconvolutional autoencoder will enable effective time series filtration while\nminimizing compute requirements (e.g., runtime, model size). Furthermore, we\nexpect that adversarial training with covariance rescaling will minimize signal\ndegradation. To test this hypothesis, a TADA system prototype was trained and\nevaluated on the task of removing electromyographic (EMG) noise from EEG data\nin the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects.\nThe TADA filter surpasses conventional signal filtration algorithms across\nquantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE),\nand performs competitively against other deep learning architectures at a\nreduced model size of less than 400,000 trainable parameters. Further\nexperimentation will be necessary to assess the viability of TADA on a wider\nrange of deployment cases."
                },
                "authors": [
                    {
                        "name": "Benjamin J. Choi"
                    },
                    {
                        "name": "Griffin Milsap"
                    },
                    {
                        "name": "Clara A. Scholl"
                    },
                    {
                        "name": "Francesco Tenore"
                    },
                    {
                        "name": "Mattson Ogg"
                    }
                ],
                "author_detail": {
                    "name": "Mattson Ogg"
                },
                "author": "Mattson Ogg",
                "arxiv_comment": "[Accepted] Artificial Intelligence for Time Series Analysis (AI4TS):\n  Theory, Algorithms, and Applications @ AAAI 2025, Philadelphia, PA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11484v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11484v9",
                "updated": "2025-01-10T02:18:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    18,
                    1,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-16T08:20:39Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    20,
                    39,
                    1,
                    198,
                    0
                ],
                "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
                },
                "summary": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11484v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11484v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05651v1",
                "updated": "2025-01-10T01:42:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    42,
                    5,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T01:42:05Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    42,
                    5,
                    4,
                    10,
                    0
                ],
                "title": "A Practical Cross-Layer Approach for ML-Driven Storage Placement in\n  Warehouse-Scale Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Cross-Layer Approach for ML-Driven Storage Placement in\n  Warehouse-Scale Computers"
                },
                "summary": "Storage systems account for a major portion of the total cost of ownership\n(TCO) of warehouse-scale computers, and thus have a major impact on the overall\nsystem's efficiency. Machine learning (ML)-based methods for solving key\nproblems in storage system efficiency, such as data placement, have shown\nsignificant promise. However, there are few known practical deployments of such\nmethods. Studying this problem in the context of real-world hyperscale data\ncenter deployments at Google, we identify a number of challenges that we\nbelieve cause this lack of practical adoption. Specifically, prior work assumes\na monolithic model that resides entirely within the storage layer, an\nunrealistic assumption in real-world data center deployments. We propose a\ncross-layer approach that moves ML out of the storage system and performs it in\nthe application running on top of it, co-designed with a scheduling algorithm\nat the storage layer that consumes predictions from these application-level\nmodels. This approach combines small, interpretable models with a co-designed\nheuristic that adapts to different online environments. We build a\nproof-of-concept of this approach in a production distributed computation\nframework at Google. Evaluations in a test deployment and large-scale\nsimulation studies using production traces show improvements of as much as\n3.47x in TCO savings compared to state of the art baselines. We believe this\nwork represents a significant step towards more practical ML-driven storage\nplacement in warehouse-scale computers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage systems account for a major portion of the total cost of ownership\n(TCO) of warehouse-scale computers, and thus have a major impact on the overall\nsystem's efficiency. Machine learning (ML)-based methods for solving key\nproblems in storage system efficiency, such as data placement, have shown\nsignificant promise. However, there are few known practical deployments of such\nmethods. Studying this problem in the context of real-world hyperscale data\ncenter deployments at Google, we identify a number of challenges that we\nbelieve cause this lack of practical adoption. Specifically, prior work assumes\na monolithic model that resides entirely within the storage layer, an\nunrealistic assumption in real-world data center deployments. We propose a\ncross-layer approach that moves ML out of the storage system and performs it in\nthe application running on top of it, co-designed with a scheduling algorithm\nat the storage layer that consumes predictions from these application-level\nmodels. This approach combines small, interpretable models with a co-designed\nheuristic that adapts to different online environments. We build a\nproof-of-concept of this approach in a production distributed computation\nframework at Google. Evaluations in a test deployment and large-scale\nsimulation studies using production traces show improvements of as much as\n3.47x in TCO savings compared to state of the art baselines. We believe this\nwork represents a significant step towards more practical ML-driven storage\nplacement in warehouse-scale computers."
                },
                "authors": [
                    {
                        "name": "Chenxi Yang"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Martin Maas"
                    },
                    {
                        "name": "Mustafa Uysal"
                    },
                    {
                        "name": "Ubaid Ullah Hafeez"
                    },
                    {
                        "name": "Arif Merchant"
                    },
                    {
                        "name": "Richard McDougall"
                    }
                ],
                "author_detail": {
                    "name": "Richard McDougall"
                },
                "author": "Richard McDougall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05647v1",
                "updated": "2025-01-10T01:27:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T01:27:12Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    27,
                    12,
                    4,
                    10,
                    0
                ],
                "title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation"
                },
                "summary": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec."
                },
                "authors": [
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Tianyu Zhan"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3690624.3709335",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709335",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18544v2",
                "updated": "2025-01-10T01:06:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    6,
                    6,
                    4,
                    10,
                    0
                ],
                "published": "2024-12-24T16:51:35Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    51,
                    35,
                    1,
                    359,
                    0
                ],
                "title": "Consistency Checks for Language Model Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Checks for Language Model Forecasters"
                },
                "summary": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a task that is difficult to evaluate: the ground truth can\nonly be known in the future. Recent work showing LLM forecasters rapidly\napproaching human-level performance begs the question: how can we benchmark and\nevaluate these forecasters instantaneously? Following the consistency check\nframework, we measure the performance of forecasters in terms of the\nconsistency of their predictions on different logically-related questions. We\npropose a new, general consistency metric based on arbitrage: for example, if a\nforecasting AI illogically predicts that both the Democratic and Republican\nparties have 60% probability of winning the 2024 US presidential election, an\narbitrageur can trade against the forecaster's predictions and make a profit.\nWe build an automated evaluation system that generates a set of base questions,\ninstantiates consistency checks from these questions, elicits the predictions\nof the forecaster, and measures the consistency of the predictions. We then\nbuild a standard, proper-scoring-rule forecasting benchmark, and show that our\n(instantaneous) consistency metrics correlate with LLM forecasters' ground\ntruth Brier scores (which are only known in the future). We also release a\nconsistency benchmark that resolves in 2028, providing a long-term evaluation\ntool for forecasting."
                },
                "authors": [
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Abhimanyu Pallavi Sudhir"
                    },
                    {
                        "name": "Alejandro Alvarez"
                    },
                    {
                        "name": "Vineeth Bhat"
                    },
                    {
                        "name": "Adam Shen"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "55 pages, 25 figures. Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05643v1",
                "updated": "2025-01-10T01:00:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    0,
                    5,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T01:00:05Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    1,
                    0,
                    5,
                    4,
                    10,
                    0
                ],
                "title": "Iconicity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iconicity in Large Language Models"
                },
                "summary": "Lexical iconicity, a direct relation between a word's meaning and its form,\nis an important aspect of every natural language, most commonly manifesting\nthrough sound-meaning associations. Since Large language models' (LLMs') access\nto both meaning and sound of text is only mediated (meaning through textual\ncontext, sound through written representation, further complicated by\ntokenization), we might expect that the encoding of iconicity in LLMs would be\neither insufficient or significantly different from human processing. This\nstudy addresses this hypothesis by having GPT-4 generate highly iconic\npseudowords in artificial languages. To verify that these words actually carry\niconicity, we had their meanings guessed by Czech and German participants\n(n=672) and subsequently by LLM-based participants (generated by GPT-4 and\nClaude 3.5 Sonnet). The results revealed that humans can guess the meanings of\npseudowords in the generated iconic language more accurately than words in\ndistant natural languages and that LLM-based participants are even more\nsuccessful than humans in this task. This core finding is accompanied by\nseveral additional analyses concerning the universality of the generated\nlanguage and the cues that both human and LLM-based participants utilize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical iconicity, a direct relation between a word's meaning and its form,\nis an important aspect of every natural language, most commonly manifesting\nthrough sound-meaning associations. Since Large language models' (LLMs') access\nto both meaning and sound of text is only mediated (meaning through textual\ncontext, sound through written representation, further complicated by\ntokenization), we might expect that the encoding of iconicity in LLMs would be\neither insufficient or significantly different from human processing. This\nstudy addresses this hypothesis by having GPT-4 generate highly iconic\npseudowords in artificial languages. To verify that these words actually carry\niconicity, we had their meanings guessed by Czech and German participants\n(n=672) and subsequently by LLM-based participants (generated by GPT-4 and\nClaude 3.5 Sonnet). The results revealed that humans can guess the meanings of\npseudowords in the generated iconic language more accurately than words in\ndistant natural languages and that LLM-based participants are even more\nsuccessful than humans in this task. This core finding is accompanied by\nseveral additional analyses concerning the universality of the generated\nlanguage and the cues that both human and LLM-based participants utilize."
                },
                "authors": [
                    {
                        "name": "Anna Marklová"
                    },
                    {
                        "name": "Jiří Milička"
                    },
                    {
                        "name": "Leonid Ryvkin"
                    },
                    {
                        "name": "Ľudmila Lacková Bennet"
                    },
                    {
                        "name": "Libuše Kormaníková"
                    }
                ],
                "author_detail": {
                    "name": "Libuše Kormaníková"
                },
                "author": "Libuše Kormaníková",
                "arxiv_comment": "Supplementary information: https://osf.io/ywjrk/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05629v1",
                "updated": "2025-01-10T00:10:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    0,
                    10,
                    21,
                    4,
                    10,
                    0
                ],
                "published": "2025-01-10T00:10:21Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    0,
                    10,
                    21,
                    4,
                    10,
                    0
                ],
                "title": "The Impact of Model Scaling on Seen and Unseen Language Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Model Scaling on Seen and Unseen Language Performance"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs), particularly those\ntrained on multilingual corpora, has intensified the need for a deeper\nunderstanding of their performance across a diverse range of languages and\nmodel sizes. Our research addresses this critical need by studying the\nperformance and scaling behavior of multilingual LLMs in text classification\nand machine translation tasks across 204 languages. We systematically examine\nboth seen and unseen languages across three model families of varying sizes in\nzero-shot and few-shot settings. Our findings show significant differences in\nscaling behavior between zero-shot and two-shot scenarios, with striking\ndisparities in performance between seen and unseen languages. Model scale has\nlittle effect on zero-shot performance, which remains mostly flat. However, in\ntwo-shot settings, larger models show clear linear improvements in multilingual\ntext classification. For translation tasks, however, only the instruction-tuned\nmodel showed clear benefits from scaling. Our analysis also suggests that\noverall resource levels, not just the proportions of pretraining languages, are\nbetter predictors of model performance, shedding light on what drives\nmultilingual LLM effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs), particularly those\ntrained on multilingual corpora, has intensified the need for a deeper\nunderstanding of their performance across a diverse range of languages and\nmodel sizes. Our research addresses this critical need by studying the\nperformance and scaling behavior of multilingual LLMs in text classification\nand machine translation tasks across 204 languages. We systematically examine\nboth seen and unseen languages across three model families of varying sizes in\nzero-shot and few-shot settings. Our findings show significant differences in\nscaling behavior between zero-shot and two-shot scenarios, with striking\ndisparities in performance between seen and unseen languages. Model scale has\nlittle effect on zero-shot performance, which remains mostly flat. However, in\ntwo-shot settings, larger models show clear linear improvements in multilingual\ntext classification. For translation tasks, however, only the instruction-tuned\nmodel showed clear benefits from scaling. Our analysis also suggests that\noverall resource levels, not just the proportions of pretraining languages, are\nbetter predictors of model performance, shedding light on what drives\nmultilingual LLM effectiveness."
                },
                "authors": [
                    {
                        "name": "Rhitabrat Pokharel"
                    },
                    {
                        "name": "Sina Bagheri Nezhad"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "Suresh Singh"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Singh"
                },
                "author": "Suresh Singh",
                "arxiv_comment": "Accepted at SEAS Workshop at AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05625v1",
                "updated": "2025-01-09T23:48:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    23,
                    48,
                    3,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T23:48:03Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    23,
                    48,
                    3,
                    3,
                    9,
                    0
                ],
                "title": "Harnessing Large Language Model for Virtual Reality Exploration Testing:\n  A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Model for Virtual Reality Exploration Testing:\n  A Case Study"
                },
                "summary": "As the Virtual Reality (VR) industry expands, the need for automated GUI\ntesting is growing rapidly. Large Language Models (LLMs), capable of retaining\ninformation long-term and analyzing both visual and textual data, are emerging\nas a potential key to deciphering the complexities of VR's evolving user\ninterfaces. In this paper, we conduct a case study to investigate the\ncapability of using LLMs, particularly GPT-4o, for field of view (FOV) analysis\nin VR exploration testing. Specifically, we validate that LLMs can identify\ntest entities in FOVs and that prompt engineering can effectively enhance the\naccuracy of test entity identification from 41.67% to 71.30%. Our study also\nshows that LLMs can accurately describe identified entities' features with at\nleast a 90% correction rate. We further find out that the core features that\neffectively represent an entity are color, placement, and shape. Furthermore,\nthe combination of the three features can especially be used to improve the\naccuracy of determining identical entities in multiple FOVs with the highest\nF1-score of 0.70. Additionally, our study demonstrates that LLMs are capable of\nscene recognition and spatial understanding in VR with precisely designed\nstructured prompts. Finally, we find that LLMs fail to label the identified\ntest entities, and we discuss potential solutions as future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Virtual Reality (VR) industry expands, the need for automated GUI\ntesting is growing rapidly. Large Language Models (LLMs), capable of retaining\ninformation long-term and analyzing both visual and textual data, are emerging\nas a potential key to deciphering the complexities of VR's evolving user\ninterfaces. In this paper, we conduct a case study to investigate the\ncapability of using LLMs, particularly GPT-4o, for field of view (FOV) analysis\nin VR exploration testing. Specifically, we validate that LLMs can identify\ntest entities in FOVs and that prompt engineering can effectively enhance the\naccuracy of test entity identification from 41.67% to 71.30%. Our study also\nshows that LLMs can accurately describe identified entities' features with at\nleast a 90% correction rate. We further find out that the core features that\neffectively represent an entity are color, placement, and shape. Furthermore,\nthe combination of the three features can especially be used to improve the\naccuracy of determining identical entities in multiple FOVs with the highest\nF1-score of 0.70. Additionally, our study demonstrates that LLMs are capable of\nscene recognition and spatial understanding in VR with precisely designed\nstructured prompts. Finally, we find that LLMs fail to label the identified\ntest entities, and we discuss potential solutions as future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Zhenyu Qi"
                    },
                    {
                        "name": "Haotang Li"
                    },
                    {
                        "name": "Hao Qin"
                    },
                    {
                        "name": "Kebin Peng"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Xue Qin"
                    }
                ],
                "author_detail": {
                    "name": "Xue Qin"
                },
                "author": "Xue Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10425v3",
                "updated": "2025-01-09T22:46:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    46,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-10T16:34:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    34,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian\n  Thermodynamic Approach to Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian\n  Thermodynamic Approach to Adaptation"
                },
                "summary": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments."
                },
                "authors": [
                    {
                        "name": "Rithvik Prakki"
                    }
                ],
                "author_detail": {
                    "name": "Rithvik Prakki"
                },
                "author": "Rithvik Prakki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17428v2",
                "updated": "2025-01-09T22:27:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    27,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2024-05-27T17:59:45Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    59,
                    45,
                    0,
                    148,
                    0
                ],
                "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\n  Models"
                },
                "summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model, incorporating architectural designs, training\nprocedures, and curated datasets to significantly enhance the performance of\nLLM as a versatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For training algorithm, we introduce\na two-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ninto instruction tuning, which not only enhances non-retrieval task accuracy\nbut also improves retrieval performance. For training data, we utilize the\nhard-negative mining, synthetic data generation and existing public available\ndatasets to boost the performance of embedding model. By combining these\ntechniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position\non the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024 and August\n30, 2024, respectively) across 56 embedding tasks, demonstrating the sustained\neffectiveness of the proposed methods over time. Additionally, it achieved the\nhighest scores in the Long Doc section and the second-highest scores in the QA\nsection of the AIR Benchmark, which covers a range of out-of-domain information\nretrieval topics beyond those in MTEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model, incorporating architectural designs, training\nprocedures, and curated datasets to significantly enhance the performance of\nLLM as a versatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For training algorithm, we introduce\na two-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ninto instruction tuning, which not only enhances non-retrieval task accuracy\nbut also improves retrieval performance. For training data, we utilize the\nhard-negative mining, synthetic data generation and existing public available\ndatasets to boost the performance of embedding model. By combining these\ntechniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position\non the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024 and August\n30, 2024, respectively) across 56 embedding tasks, demonstrating the sustained\neffectiveness of the proposed methods over time. Additionally, it achieved the\nhighest scores in the Long Doc section and the second-highest scores in the QA\nsection of the AIR Benchmark, which covers a range of out-of-domain information\nretrieval topics beyond those in MTEB."
                },
                "authors": [
                    {
                        "name": "Chankyu Lee"
                    },
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Mengyao Xu"
                    },
                    {
                        "name": "Jonathan Raiman"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Wei Ping"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ping"
                },
                "author": "Wei Ping",
                "arxiv_comment": "We open-source the model at:\n  https://huggingface.co/nvidia/NV-Embed-v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01635v3",
                "updated": "2025-01-09T22:27:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    27,
                    4,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-03T02:38:26Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    2,
                    38,
                    26,
                    5,
                    216,
                    0
                ],
                "title": "KTWIN: A Serverless Kubernetes-based Digital Twin Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTWIN: A Serverless Kubernetes-based Digital Twin Platform"
                },
                "summary": "Digital Twins (DTs) systems are virtual representations of physical assets\nallowing organizations to gain insights and improve existing processes. In\npractice, DTs require proper modeling, coherent development and seamless\ndeployment along cloud and edge landscapes relying on established patterns to\nreduce operational costs. In this work, we propose KTWIN a Kubernetes-based\nServerless Platform for Digital Twins. KTWIN was developed using the\nstate-of-the-art open-source Cloud Native tools, allowing DT operators to\neasily define models through open standards and configure details of the\nunderlying services and infrastructure. The experiments carried out with the\ndeveloped prototype show that KTWIN can provide a higher level of abstraction\nto model and deploy a Digital Twin use case without compromising the solution\nscalability. The tests performed also show cost savings ranging between 60% and\n80% compared to overprovisioned scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twins (DTs) systems are virtual representations of physical assets\nallowing organizations to gain insights and improve existing processes. In\npractice, DTs require proper modeling, coherent development and seamless\ndeployment along cloud and edge landscapes relying on established patterns to\nreduce operational costs. In this work, we propose KTWIN a Kubernetes-based\nServerless Platform for Digital Twins. KTWIN was developed using the\nstate-of-the-art open-source Cloud Native tools, allowing DT operators to\neasily define models through open standards and configure details of the\nunderlying services and infrastructure. The experiments carried out with the\ndeveloped prototype show that KTWIN can provide a higher level of abstraction\nto model and deploy a Digital Twin use case without compromising the solution\nscalability. The tests performed also show cost savings ranging between 60% and\n80% compared to overprovisioned scenarios."
                },
                "authors": [
                    {
                        "name": "Alexandre Gustavo Wermann"
                    },
                    {
                        "name": "Juliano Araujo Wickboldt"
                    }
                ],
                "author_detail": {
                    "name": "Juliano Araujo Wickboldt"
                },
                "author": "Juliano Araujo Wickboldt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13257v3",
                "updated": "2025-01-09T22:21:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    21,
                    56,
                    3,
                    9,
                    0
                ],
                "published": "2024-03-20T02:38:01Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    2,
                    38,
                    1,
                    2,
                    80,
                    0
                ],
                "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arcee's MergeKit: A Toolkit for Merging Large Language Models"
                },
                "summary": "The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npretrained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multitask learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npretrained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multitask learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit."
                },
                "authors": [
                    {
                        "name": "Charles Goddard"
                    },
                    {
                        "name": "Shamane Siriwardhana"
                    },
                    {
                        "name": "Malikeh Ehghaghi"
                    },
                    {
                        "name": "Luke Meyers"
                    },
                    {
                        "name": "Vlad Karpukhin"
                    },
                    {
                        "name": "Brian Benedict"
                    },
                    {
                        "name": "Mark McQuade"
                    },
                    {
                        "name": "Jacob Solawetz"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Solawetz"
                },
                "author": "Jacob Solawetz",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05601v1",
                "updated": "2025-01-09T22:17:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    17,
                    44,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T22:17:44Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    22,
                    17,
                    44,
                    3,
                    9,
                    0
                ],
                "title": "Exploring Large Language Models for Translating Romanian Computational\n  Problems into English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Translating Romanian Computational\n  Problems into English"
                },
                "summary": "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios."
                },
                "authors": [
                    {
                        "name": "Adrian Marius Dumitran"
                    },
                    {
                        "name": "Adrian-Catalin Badea"
                    },
                    {
                        "name": "Stefan-Gabriel Muscalu"
                    },
                    {
                        "name": "Angela-Liliana Dumitran"
                    },
                    {
                        "name": "Stefan-Cosmin Dascalescu"
                    },
                    {
                        "name": "Radu-Sebastian Amarie"
                    }
                ],
                "author_detail": {
                    "name": "Radu-Sebastian Amarie"
                },
                "author": "Radu-Sebastian Amarie",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08745v3",
                "updated": "2025-01-09T21:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    21,
                    53,
                    56,
                    3,
                    9,
                    0
                ],
                "published": "2024-11-13T16:26:19Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    26,
                    19,
                    2,
                    318,
                    0
                ],
                "title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers"
                },
                "summary": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models."
                },
                "authors": [
                    {
                        "name": "Clément Dumas"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "18 pages, 14 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05577v1",
                "updated": "2025-01-09T21:09:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    21,
                    9,
                    44,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T21:09:44Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    21,
                    9,
                    44,
                    3,
                    9,
                    0
                ],
                "title": "Exploring Large Language Models (LLMs) through interactive Python\n  activities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models (LLMs) through interactive Python\n  activities"
                },
                "summary": "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society."
                },
                "authors": [
                    {
                        "name": "Eugenio Tufino"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Tufino"
                },
                "author": "Eugenio Tufino",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05567v1",
                "updated": "2025-01-09T20:34:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    34,
                    36,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T20:34:36Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    34,
                    36,
                    3,
                    9,
                    0
                ],
                "title": "Approximate Supervised Object Distance Estimation on Unmanned Surface\n  Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Supervised Object Distance Estimation on Unmanned Surface\n  Vehicles"
                },
                "summary": "Unmanned surface vehicles (USVs) and boats are increasingly important in\nmaritime operations, yet their deployment is limited due to costly sensors and\ncomplexity. LiDAR, radar, and depth cameras are either costly, yield sparse\npoint clouds or are noisy, and require extensive calibration. Here, we\nintroduce a novel approach for approximate distance estimation in USVs using\nsupervised object detection. We collected a dataset comprising images with\nmanually annotated bounding boxes and corresponding distance measurements.\nLeveraging this data, we propose a specialized branch of an object detection\nmodel, not only to detect objects but also to predict their distances from the\nUSV. This method offers a cost-efficient and intuitive alternative to\nconventional distance measurement techniques, aligning more closely with human\nestimation capabilities. We demonstrate its application in a marine assistance\nsystem that alerts operators to nearby objects such as boats, buoys, or other\nwaterborne hazards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned surface vehicles (USVs) and boats are increasingly important in\nmaritime operations, yet their deployment is limited due to costly sensors and\ncomplexity. LiDAR, radar, and depth cameras are either costly, yield sparse\npoint clouds or are noisy, and require extensive calibration. Here, we\nintroduce a novel approach for approximate distance estimation in USVs using\nsupervised object detection. We collected a dataset comprising images with\nmanually annotated bounding boxes and corresponding distance measurements.\nLeveraging this data, we propose a specialized branch of an object detection\nmodel, not only to detect objects but also to predict their distances from the\nUSV. This method offers a cost-efficient and intuitive alternative to\nconventional distance measurement techniques, aligning more closely with human\nestimation capabilities. We demonstrate its application in a marine assistance\nsystem that alerts operators to nearby objects such as boats, buoys, or other\nwaterborne hazards."
                },
                "authors": [
                    {
                        "name": "Benjamin Kiefer"
                    },
                    {
                        "name": "Yitong Quan"
                    },
                    {
                        "name": "Andreas Zell"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zell"
                },
                "author": "Andreas Zell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05566v1",
                "updated": "2025-01-09T20:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    29,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T20:29:31Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    29,
                    31,
                    3,
                    9,
                    0
                ],
                "title": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene\n  Understanding"
                },
                "summary": "Scene understanding is essential for enhancing driver safety, generating\nhuman-centric explanations for Automated Vehicle (AV) decisions, and leveraging\nArtificial Intelligence (AI) for retrospective driving video analysis. This\nstudy developed a dynamic scene retrieval system using Contrastive\nLanguage-Image Pretraining (CLIP) models, which can be optimized for real-time\ndeployment on edge devices. The proposed system outperforms state-of-the-art\nin-context learning methods, including the zero-shot capabilities of GPT-4o,\nparticularly in complex scenarios. By conducting frame-level analysis on the\nHonda Scenes Dataset, which contains a collection of about 80 hours of\nannotated driving videos capturing diverse real-world road and weather\nconditions, our study highlights the robustness of CLIP models in learning\nvisual concepts from natural language supervision. Results also showed that\nfine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly\nimproved scene classification, achieving a top F1 score of 91.1%. These results\ndemonstrate the ability of the system to deliver rapid and precise scene\nrecognition, which can be used to meet the critical requirements of Advanced\nDriver Assistance Systems (ADAS). This study shows the potential of CLIP models\nto provide scalable and efficient frameworks for dynamic scene understanding\nand classification. Furthermore, this work lays the groundwork for advanced\nautonomous vehicle technologies by fostering a deeper understanding of driver\nbehavior, road conditions, and safety-critical scenarios, marking a significant\nstep toward smarter, safer, and more context-aware autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene understanding is essential for enhancing driver safety, generating\nhuman-centric explanations for Automated Vehicle (AV) decisions, and leveraging\nArtificial Intelligence (AI) for retrospective driving video analysis. This\nstudy developed a dynamic scene retrieval system using Contrastive\nLanguage-Image Pretraining (CLIP) models, which can be optimized for real-time\ndeployment on edge devices. The proposed system outperforms state-of-the-art\nin-context learning methods, including the zero-shot capabilities of GPT-4o,\nparticularly in complex scenarios. By conducting frame-level analysis on the\nHonda Scenes Dataset, which contains a collection of about 80 hours of\nannotated driving videos capturing diverse real-world road and weather\nconditions, our study highlights the robustness of CLIP models in learning\nvisual concepts from natural language supervision. Results also showed that\nfine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly\nimproved scene classification, achieving a top F1 score of 91.1%. These results\ndemonstrate the ability of the system to deliver rapid and precise scene\nrecognition, which can be used to meet the critical requirements of Advanced\nDriver Assistance Systems (ADAS). This study shows the potential of CLIP models\nto provide scalable and efficient frameworks for dynamic scene understanding\nand classification. Furthermore, this work lays the groundwork for advanced\nautonomous vehicle technologies by fostering a deeper understanding of driver\nbehavior, road conditions, and safety-critical scenarios, marking a significant\nstep toward smarter, safer, and more context-aware autonomous driving systems."
                },
                "authors": [
                    {
                        "name": "Mohammed Elhenawy"
                    },
                    {
                        "name": "Huthaifa I. Ashqar"
                    },
                    {
                        "name": "Andry Rakotonirainy"
                    },
                    {
                        "name": "Taqwa I. Alhadidi"
                    },
                    {
                        "name": "Ahmed Jaber"
                    },
                    {
                        "name": "Mohammad Abu Tami"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Abu Tami"
                },
                "author": "Mohammad Abu Tami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05561v1",
                "updated": "2025-01-09T20:14:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    14,
                    35,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T20:14:35Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    14,
                    35,
                    3,
                    9,
                    0
                ],
                "title": "Impacts of EPA's Finalized Power Plant Greenhouse Gas Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impacts of EPA's Finalized Power Plant Greenhouse Gas Standards"
                },
                "summary": "The Inflation Reduction Act subsidizes the deployment of clean electricity,\nhydrogen production, and carbon capture and storage, which could enable\nadditional actions by other federal, state, and local policymakers to reduce\nemissions. Power plant rules finalized by the Environmental Protection Agency\n(EPA) in 2024 are one such example of complementary policies. The rules\nestablish emissions intensity standards, not technology mandates, meaning power\nplant owners can choose from a range of technologies and control options\nprovided that emissions standards are met. This flexibility makes electricity\nsystems modeling important to understand the potential effects of these\nregulations. We report below a multi-model analysis of the EPA power plant\nrules that can provide timely information, including for other countries and\nstates, on emissions impacts, policy design for electricity decarbonization,\npower sector investments and retirements, cost impacts, and load growth. We\nalso discuss related technical, political, and legal uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Inflation Reduction Act subsidizes the deployment of clean electricity,\nhydrogen production, and carbon capture and storage, which could enable\nadditional actions by other federal, state, and local policymakers to reduce\nemissions. Power plant rules finalized by the Environmental Protection Agency\n(EPA) in 2024 are one such example of complementary policies. The rules\nestablish emissions intensity standards, not technology mandates, meaning power\nplant owners can choose from a range of technologies and control options\nprovided that emissions standards are met. This flexibility makes electricity\nsystems modeling important to understand the potential effects of these\nregulations. We report below a multi-model analysis of the EPA power plant\nrules that can provide timely information, including for other countries and\nstates, on emissions impacts, policy design for electricity decarbonization,\npower sector investments and retirements, cost impacts, and load growth. We\nalso discuss related technical, political, and legal uncertainties."
                },
                "authors": [
                    {
                        "name": "John Bistline"
                    },
                    {
                        "name": "Aaron Bergman"
                    },
                    {
                        "name": "Geoffrey Blanford"
                    },
                    {
                        "name": "Maxwell Brown"
                    },
                    {
                        "name": "Dallas Burtraw"
                    },
                    {
                        "name": "Maya Domeshek"
                    },
                    {
                        "name": "Allen Fawcett"
                    },
                    {
                        "name": "Anne Hamilton"
                    },
                    {
                        "name": "Gokul Iyer"
                    },
                    {
                        "name": "Jesse Jenkins"
                    },
                    {
                        "name": "Ben King"
                    },
                    {
                        "name": "Hannah Kolus"
                    },
                    {
                        "name": "Amanda Levin"
                    },
                    {
                        "name": "Qian Luo"
                    },
                    {
                        "name": "Kevin Rennert"
                    },
                    {
                        "name": "Molly Robertson"
                    },
                    {
                        "name": "Nicholas Roy"
                    },
                    {
                        "name": "Ethan Russell"
                    },
                    {
                        "name": "Daniel Shawhan"
                    },
                    {
                        "name": "Daniel Steinberg"
                    },
                    {
                        "name": "Anna van Brummen"
                    },
                    {
                        "name": "Grace Van Horn"
                    },
                    {
                        "name": "Aranya Venkatesh"
                    },
                    {
                        "name": "John Weyant"
                    },
                    {
                        "name": "Ryan Wiser"
                    },
                    {
                        "name": "Alicia Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Alicia Zhao"
                },
                "author": "Alicia Zhao",
                "arxiv_doi": "10.1126/science.adt5665",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1126/science.adt5665",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Science (2025), Vol. 387, Issue 6730, pp. 140-143",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00190v2",
                "updated": "2025-01-09T20:00:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    20,
                    0,
                    16,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-31T00:02:07Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    0,
                    2,
                    7,
                    1,
                    366,
                    0
                ],
                "title": "SepsisCalc: Integrating Clinical Calculators into Early Sepsis\n  Prediction via Dynamic Temporal Graph Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepsisCalc: Integrating Clinical Calculators into Early Sepsis\n  Prediction via Dynamic Temporal Graph Construction"
                },
                "summary": "Sepsis is an organ dysfunction caused by a deregulated immune response to an\ninfection. Early sepsis prediction and identification allow for timely\nintervention, leading to improved clinical outcomes. Clinical calculators\n(e.g., the six-organ dysfunction assessment of SOFA) play a vital role in\nsepsis identification within clinicians' workflow, providing evidence-based\nrisk assessments essential for sepsis diagnosis. However, artificial\nintelligence (AI) sepsis prediction models typically generate a single sepsis\nrisk score without incorporating clinical calculators for assessing organ\ndysfunctions, making the models less convincing and transparent to clinicians.\nTo bridge the gap, we propose to mimic clinicians' workflow with a novel\nframework SepsisCalc to integrate clinical calculators into the predictive\nmodel, yielding a clinically transparent and precise model for utilization in\nclinical settings. Practically, clinical calculators usually combine\ninformation from multiple component variables in Electronic Health Records\n(EHR), and might not be applicable when the variables are (partially) missing.\nWe mitigate this issue by representing EHRs as temporal graphs and integrating\na learning module to dynamically add the accurately estimated calculator to the\ngraphs. Experimental results on real-world datasets show that the proposed\nmodel outperforms state-of-the-art methods on sepsis prediction tasks.\nMoreover, we developed a system to identify organ dysfunctions and potential\nsepsis risks, providing a human-AI interaction tool for deployment, which can\nhelp clinicians understand the prediction outputs and prepare timely\ninterventions for the corresponding dysfunctions, paving the way for actionable\nclinical decision-making support for early intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sepsis is an organ dysfunction caused by a deregulated immune response to an\ninfection. Early sepsis prediction and identification allow for timely\nintervention, leading to improved clinical outcomes. Clinical calculators\n(e.g., the six-organ dysfunction assessment of SOFA) play a vital role in\nsepsis identification within clinicians' workflow, providing evidence-based\nrisk assessments essential for sepsis diagnosis. However, artificial\nintelligence (AI) sepsis prediction models typically generate a single sepsis\nrisk score without incorporating clinical calculators for assessing organ\ndysfunctions, making the models less convincing and transparent to clinicians.\nTo bridge the gap, we propose to mimic clinicians' workflow with a novel\nframework SepsisCalc to integrate clinical calculators into the predictive\nmodel, yielding a clinically transparent and precise model for utilization in\nclinical settings. Practically, clinical calculators usually combine\ninformation from multiple component variables in Electronic Health Records\n(EHR), and might not be applicable when the variables are (partially) missing.\nWe mitigate this issue by representing EHRs as temporal graphs and integrating\na learning module to dynamically add the accurately estimated calculator to the\ngraphs. Experimental results on real-world datasets show that the proposed\nmodel outperforms state-of-the-art methods on sepsis prediction tasks.\nMoreover, we developed a system to identify organ dysfunctions and potential\nsepsis risks, providing a human-AI interaction tool for deployment, which can\nhelp clinicians understand the prediction outputs and prepare timely\ninterventions for the corresponding dysfunctions, paving the way for actionable\nclinical decision-making support for early intervention."
                },
                "authors": [
                    {
                        "name": "Changchang Yin"
                    },
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Thai-Hoang Pham"
                    },
                    {
                        "name": "Weidan Cao"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Jeffrey Caterino"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05552v1",
                "updated": "2025-01-09T19:56:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    56,
                    44,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:56:44Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    56,
                    44,
                    3,
                    9,
                    0
                ],
                "title": "The dynamics of meaning through time: Assessment of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamics of meaning through time: Assessment of Large Language\n  Models"
                },
                "summary": "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities."
                },
                "authors": [
                    {
                        "name": "Mohamed Taher Alrefaie"
                    },
                    {
                        "name": "Fatty Salem"
                    },
                    {
                        "name": "Nour Eldin Morsy"
                    },
                    {
                        "name": "Nada Samir"
                    },
                    {
                        "name": "Mohamed Medhat Gaber"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Medhat Gaber"
                },
                "author": "Mohamed Medhat Gaber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.02075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.02075v2",
                "updated": "2025-01-09T19:54:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    54,
                    53,
                    3,
                    9,
                    0
                ],
                "published": "2023-04-04T18:58:16Z",
                "published_parsed": [
                    2023,
                    4,
                    4,
                    18,
                    58,
                    16,
                    1,
                    94,
                    0
                ],
                "title": "GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent\n  Active Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent\n  Active Search"
                },
                "summary": "Robotic solutions for quick disaster response are essential to ensure minimal\nloss of life, especially when the search area is too dangerous or too vast for\nhuman rescuers. We model this problem as an asynchronous multi-agent\nactive-search task where each robot aims to efficiently seek objects of\ninterest (OOIs) in an unknown environment. This formulation addresses the\nrequirement that search missions should focus on quick recovery of OOIs rather\nthan full coverage of the search region. Previous approaches fail to accurately\nmodel sensing uncertainty, account for occlusions due to foliage or terrain, or\nconsider the requirement for heterogeneous search teams and robustness to\nhardware and communication failures. We present the Generalized\nUncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these\nissues and is suitable for deployment on heterogeneous multi-robot systems for\nactive search in large unstructured environments. We show through simulation\nexperiments that GUTS consistently outperforms existing methods such as\nparallelized Thompson Sampling and exhaustive search, recovering all OOIs in\n80% of all runs. In contrast, existing approaches recover all OOIs in less than\n40% of all runs. We conduct field tests using our multi-robot system in an\nunstructured environment with a search area of approximately 75,000 sq. m. Our\nsystem demonstrates robustness to various failure modes, achieving full\nrecovery of OOIs (where feasible) in every field run, and significantly\noutperforming our baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic solutions for quick disaster response are essential to ensure minimal\nloss of life, especially when the search area is too dangerous or too vast for\nhuman rescuers. We model this problem as an asynchronous multi-agent\nactive-search task where each robot aims to efficiently seek objects of\ninterest (OOIs) in an unknown environment. This formulation addresses the\nrequirement that search missions should focus on quick recovery of OOIs rather\nthan full coverage of the search region. Previous approaches fail to accurately\nmodel sensing uncertainty, account for occlusions due to foliage or terrain, or\nconsider the requirement for heterogeneous search teams and robustness to\nhardware and communication failures. We present the Generalized\nUncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these\nissues and is suitable for deployment on heterogeneous multi-robot systems for\nactive search in large unstructured environments. We show through simulation\nexperiments that GUTS consistently outperforms existing methods such as\nparallelized Thompson Sampling and exhaustive search, recovering all OOIs in\n80% of all runs. In contrast, existing approaches recover all OOIs in less than\n40% of all runs. We conduct field tests using our multi-robot system in an\nunstructured environment with a search area of approximately 75,000 sq. m. Our\nsystem demonstrates robustness to various failure modes, achieving full\nrecovery of OOIs (where feasible) in every field run, and significantly\noutperforming our baseline."
                },
                "authors": [
                    {
                        "name": "Nikhil Angad Bakshi"
                    },
                    {
                        "name": "Tejus Gupta"
                    },
                    {
                        "name": "Ramina Ghods"
                    },
                    {
                        "name": "Jeff Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Schneider"
                },
                "author": "Jeff Schneider",
                "arxiv_doi": "10.1109/ICRA48891.2023.10160597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA48891.2023.10160597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.02075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.02075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures, 1 table, for associated video see:\n  https://youtu.be/K0jkzdQ_j2E , published in International Conference on\n  Robotics and Automation (ICRA) 2023. Outstanding Deployed Systems Paper\n  Winner",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05542v1",
                "updated": "2025-01-09T19:27:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    29,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:27:29Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    29,
                    3,
                    9,
                    0
                ],
                "title": "Infecting Generative AI With Viruses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infecting Generative AI With Viruses"
                },
                "summary": "This study demonstrates a novel approach to testing the security boundaries\nof Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded\nwithin JPEG images. We successfully executed four distinct protocols across\nmultiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Google\nGemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet. The experiments validated that\na modified JPEG containing the EICAR signature could be uploaded, manipulated,\nand potentially executed within LLM virtual workspaces. Key findings include:\n1) consistent ability to mask the EICAR string in image metadata without\ndetection, 2) successful extraction of the test file using Python-based\nmanipulation within LLM environments, and 3) demonstration of multiple\nobfuscation techniques including base64 encoding and string reversal. This\nresearch extends Microsoft Research's \"Penetration Testing Rules of Engagement\"\nframework to evaluate cloud-based generative AI and LLM security boundaries,\nparticularly focusing on file handling and execution capabilities within\ncontainerized environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates a novel approach to testing the security boundaries\nof Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded\nwithin JPEG images. We successfully executed four distinct protocols across\nmultiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Google\nGemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet. The experiments validated that\na modified JPEG containing the EICAR signature could be uploaded, manipulated,\nand potentially executed within LLM virtual workspaces. Key findings include:\n1) consistent ability to mask the EICAR string in image metadata without\ndetection, 2) successful extraction of the test file using Python-based\nmanipulation within LLM environments, and 3) demonstration of multiple\nobfuscation techniques including base64 encoding and string reversal. This\nresearch extends Microsoft Research's \"Penetration Testing Rules of Engagement\"\nframework to evaluate cloud-based generative AI and LLM security boundaries,\nparticularly focusing on file handling and execution capabilities within\ncontainerized environments."
                },
                "authors": [
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Forrest McKee"
                    }
                ],
                "author_detail": {
                    "name": "Forrest McKee"
                },
                "author": "Forrest McKee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05541v1",
                "updated": "2025-01-09T19:27:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:27:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "NSChat: A Chatbot System To Rule Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSChat: A Chatbot System To Rule Them All"
                },
                "summary": "The rapid advancement of artificial intelligence has resulted in the advent\nof large language models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present NSChat, a web-based chatbot system designed to assist in\nneuroscience research. The system is meticulously designed to function as an\nexperimental instrument rather than a conventional chatbot, necessitating users\nto input a username and experiment code upon access. This setup facilitates\nprecise data cross-referencing, thereby augmenting the integrity and\napplicability of the data collected for research purposes. It can be easily\nexpanded to accommodate new basic events as needed; and it allows researchers\nto integrate their own logging events without the necessity of implementing a\nseparate logging mechanism. It is worth noting that our system was built to\nassist primarily neuroscience research but is not limited to it, it can easily\nbe adapted to assist information retrieval research or interacting with chat\nbot agents in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence has resulted in the advent\nof large language models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present NSChat, a web-based chatbot system designed to assist in\nneuroscience research. The system is meticulously designed to function as an\nexperimental instrument rather than a conventional chatbot, necessitating users\nto input a username and experiment code upon access. This setup facilitates\nprecise data cross-referencing, thereby augmenting the integrity and\napplicability of the data collected for research purposes. It can be easily\nexpanded to accommodate new basic events as needed; and it allows researchers\nto integrate their own logging events without the necessity of implementing a\nseparate logging mechanism. It is worth noting that our system was built to\nassist primarily neuroscience research but is not limited to it, it can easily\nbe adapted to assist information retrieval research or interacting with chat\nbot agents in general."
                },
                "authors": [
                    {
                        "name": "Zenon Lamprou"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05539v1",
                "updated": "2025-01-09T19:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    25,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:25:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    25,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "The Norwegian-Polish CCS Network: A Case Study in Bilateral\n  Collaboration for European Climate Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Norwegian-Polish CCS Network: A Case Study in Bilateral\n  Collaboration for European Climate Action"
                },
                "summary": "In the face of escalating climate change, achieving significant reductions in\ngreenhouse gas emissions from hard-to-abate industrial sectors is imperative.\nCarbon Capture and Storage (CCS) represents an essential technological\nadvancement to achieve sustainable decarbonization. This manuscript reports on\nthe bilateral CCS network between Norway and Poland, designed and implemented\nto leverage their capabilities to expedite technology deployment via mutual\ncooperation and accelerate CCS initiatives targeted to member states'\nchallenges across Europe, aiming for a meaningful contribution to climate\ngoals. Norway is renowned for its operational acumen, as demonstrated by\nlandmark projects like Sleipner and Snohvit, and its forward-looking\ninitiatives, such as the open-source cross-border Northern Lights project,\nwhich offers advanced infrastructure and expertise. Conversely, Poland,\ncharacterized by its coal-dependent economy and the challenge of decarbonizing\nextensive industrial emissions, presents a significant geological CO$_2$\nstorage potential, estimated at over 200 gigatonnes. This study delves into the\npotential synergies derived from collaborative endeavors in academic education,\nresearch and development, industrial implementation, regulatory coherence, and\npublic engagement. By underscoring the reciprocal benefits of such partnership,\nthe study underscores the indispensable role of bilateral cooperation in\nharnessing CCS's capabilities to meet ambitious climate objectives, paving the\nway toward a sustainable and low-carbon future. Additionally, it outlines a\nscalable model for fostering and supporting broader bi- and multi-lateral\ncollaborations, emphasizing the pivotal role of interconnected networks in\nshaping effective global climate action strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the face of escalating climate change, achieving significant reductions in\ngreenhouse gas emissions from hard-to-abate industrial sectors is imperative.\nCarbon Capture and Storage (CCS) represents an essential technological\nadvancement to achieve sustainable decarbonization. This manuscript reports on\nthe bilateral CCS network between Norway and Poland, designed and implemented\nto leverage their capabilities to expedite technology deployment via mutual\ncooperation and accelerate CCS initiatives targeted to member states'\nchallenges across Europe, aiming for a meaningful contribution to climate\ngoals. Norway is renowned for its operational acumen, as demonstrated by\nlandmark projects like Sleipner and Snohvit, and its forward-looking\ninitiatives, such as the open-source cross-border Northern Lights project,\nwhich offers advanced infrastructure and expertise. Conversely, Poland,\ncharacterized by its coal-dependent economy and the challenge of decarbonizing\nextensive industrial emissions, presents a significant geological CO$_2$\nstorage potential, estimated at over 200 gigatonnes. This study delves into the\npotential synergies derived from collaborative endeavors in academic education,\nresearch and development, industrial implementation, regulatory coherence, and\npublic engagement. By underscoring the reciprocal benefits of such partnership,\nthe study underscores the indispensable role of bilateral cooperation in\nharnessing CCS's capabilities to meet ambitious climate objectives, paving the\nway toward a sustainable and low-carbon future. Additionally, it outlines a\nscalable model for fostering and supporting broader bi- and multi-lateral\ncollaborations, emphasizing the pivotal role of interconnected networks in\nshaping effective global climate action strategies."
                },
                "authors": [
                    {
                        "name": "Mohammad Nooraiepour"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Nooraiepour"
                },
                "author": "Mohammad Nooraiepour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05515v1",
                "updated": "2025-01-09T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    3,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    3,
                    3,
                    9,
                    0
                ],
                "title": "Neural Architecture Codesign for Fast Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Architecture Codesign for Fast Physics Applications"
                },
                "summary": "We develop a pipeline to streamline neural architecture codesign for physics\napplications to reduce the need for ML expertise when designing models for\nnovel tasks. Our method employs neural architecture search and network\ncompression in a two-stage approach to discover hardware efficient models. This\napproach consists of a global search stage that explores a wide range of\narchitectures while considering hardware constraints, followed by a local\nsearch stage that fine-tunes and compresses the most promising candidates. We\nexceed performance on various tasks and show further speedup through model\ncompression techniques such as quantization-aware-training and neural network\npruning. We synthesize the optimal models to high level synthesis code for FPGA\ndeployment with the hls4ml library. Additionally, our hierarchical search space\nprovides greater flexibility in optimization, which can easily extend to other\ntasks and domains. We demonstrate this with two case studies: Bragg peak\nfinding in materials science and jet classification in high energy physics,\nachieving models with improved accuracy, smaller latencies, or reduced resource\nutilization relative to the baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a pipeline to streamline neural architecture codesign for physics\napplications to reduce the need for ML expertise when designing models for\nnovel tasks. Our method employs neural architecture search and network\ncompression in a two-stage approach to discover hardware efficient models. This\napproach consists of a global search stage that explores a wide range of\narchitectures while considering hardware constraints, followed by a local\nsearch stage that fine-tunes and compresses the most promising candidates. We\nexceed performance on various tasks and show further speedup through model\ncompression techniques such as quantization-aware-training and neural network\npruning. We synthesize the optimal models to high level synthesis code for FPGA\ndeployment with the hls4ml library. Additionally, our hierarchical search space\nprovides greater flexibility in optimization, which can easily extend to other\ntasks and domains. We demonstrate this with two case studies: Bragg peak\nfinding in materials science and jet classification in high energy physics,\nachieving models with improved accuracy, smaller latencies, or reduced resource\nutilization relative to the baseline models."
                },
                "authors": [
                    {
                        "name": "Jason Weitz"
                    },
                    {
                        "name": "Dmitri Demler"
                    },
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Nhan Tran"
                    },
                    {
                        "name": "Javier Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Javier Duarte"
                },
                "author": "Javier Duarte",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05510v1",
                "updated": "2025-01-09T19:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T19:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    0,
                    1,
                    3,
                    9,
                    0
                ],
                "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video\n  Understanding?"
                },
                "summary": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Awareness, the ability to reason dynamically based on the timestamp\nwhen a question is raised, is the key distinction between offline and online\nvideo LLMs. Unlike offline models, which rely on complete videos for static,\npost hoc analysis, online models process video streams incrementally and\ndynamically adapt their responses based on the timestamp at which the question\nis posed. Despite its significance, temporal awareness has not been adequately\nevaluated in existing benchmarks. To fill this gap, we present OVO-Bench\n(Online-VideO-Benchmark), a novel video benchmark that emphasizes the\nimportance of timestamps for advanced online video understanding capability\nbenchmarking. OVO-Bench evaluates the ability of video LLMs to reason and\nrespond to events occurring at specific timestamps under three distinct\nscenarios: (1) Backward tracing: trace back to past events to answer the\nquestion. (2) Real-time understanding: understand and respond to events as they\nunfold at the current timestamp. (3) Forward active responding: delay the\nresponse until sufficient future information becomes available to answer the\nquestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos\nand approximately human-curated 2,800 fine-grained meta-annotations with\nprecise timestamps. We combine automated generation pipelines with human\ncuration. With these high-quality samples, we further developed an evaluation\npipeline to systematically query video LLMs along the video timeline.\nEvaluations of nine Video-LLMs reveal that, despite advancements on traditional\nbenchmarks, current models struggle with online video understanding, showing a\nsignificant gap compared to human agents. We hope OVO-Bench will drive progress\nin video LLMs and inspire future research in online video reasoning. Our\nbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench."
                },
                "authors": [
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Junbo Niu"
                    },
                    {
                        "name": "Ziyang Miao"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yuanhang Zhou"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05452v1",
                "updated": "2025-01-09T18:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    59,
                    58,
                    3,
                    9,
                    0
                ],
                "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFocus: Visual Editing as a Chain of Thought for Structured Image\n  Understanding"
                },
                "summary": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured image understanding, such as interpreting tables and charts,\nrequires strategically refocusing across various structures and texts within an\nimage, forming a reasoning sequence to arrive at the final answer. However,\ncurrent multimodal large language models (LLMs) lack this multihop selective\nattention capability. In this work, we introduce ReFocus, a simple yet\neffective framework that equips multimodal LLMs with the ability to generate\n\"visual thoughts\" by performing visual editing on the input image through code,\nshifting and refining their visual focuses. Specifically, ReFocus enables\nmultimodal LLMs to generate Python codes to call tools and modify the input\nimage, sequentially drawing boxes, highlighting sections, and masking out\nareas, thereby enhancing the visual reasoning process. We experiment upon a\nwide range of structured image understanding tasks involving tables and charts.\nReFocus largely improves performance on all tasks over GPT-4o without visual\nediting, yielding an average gain of 11.0% on table tasks and 6.8% on chart\ntasks. We present an in-depth analysis of the effects of different visual\nedits, and reasons why ReFocus can improve the performance without introducing\nadditional information. Further, we collect a 14k training set using ReFocus,\nand prove that such visual chain-of-thought with intermediate information\noffers a better supervision than standard VQA data, reaching a 8.0% average\ngain over the same model trained with QA pairs and 2.6% over CoT."
                },
                "authors": [
                    {
                        "name": "Xingyu Fu"
                    },
                    {
                        "name": "Minqian Liu"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "John Corring"
                    },
                    {
                        "name": "Yijuan Lu"
                    },
                    {
                        "name": "Jianwei Yang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Dinei Florencio"
                    },
                    {
                        "name": "Cha Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cha Zhang"
                },
                "author": "Cha Zhang",
                "arxiv_comment": "Project link: https://zeyofu.github.io/ReFocus/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05443v1",
                "updated": "2025-01-09T18:55:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:55:50Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    55,
                    50,
                    3,
                    9,
                    0
                ],
                "title": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of textual cyber abuse detection using cutting-edge language\n  models and large language models"
                },
                "summary": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of social media platforms has facilitated the emergence of\nvarious forms of online abuse within digital communities. This abuse manifests\nin multiple ways, including hate speech, cyberbullying, emotional abuse,\ngrooming, and sexting. In this paper, we present a comprehensive analysis of\nthe different forms of abuse prevalent in social media, with a particular focus\non how emerging technologies, such as Language Models (LMs) and Large Language\nModels (LLMs), are reshaping both the detection and generation of abusive\ncontent within these networks. We delve into the mechanisms through which\nsocial media abuse is perpetuated, exploring the psychological and social\nimpact. Additionally, we examine the dual role of advanced language\nmodels-highlighting their potential to enhance automated detection systems for\nabusive behavior while also acknowledging their capacity to generate harmful\ncontent. This paper aims to contribute to the ongoing discourse on online\nsafety and ethics, offering insights into the evolving landscape of cyberabuse\nand the technological innovations that both mitigate and exacerbate it."
                },
                "authors": [
                    {
                        "name": "Jose A. Diaz-Garcia"
                    },
                    {
                        "name": "Joao Paulo Carvalho"
                    }
                ],
                "author_detail": {
                    "name": "Joao Paulo Carvalho"
                },
                "author": "Joao Paulo Carvalho",
                "arxiv_comment": "37 pages, under review in WIREs Data Mining and Knowledge Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08405v2",
                "updated": "2025-01-09T18:43:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    18,
                    3,
                    9,
                    0
                ],
                "published": "2024-10-10T22:38:26Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    22,
                    38,
                    26,
                    3,
                    284,
                    0
                ],
                "title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning"
                },
                "summary": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in advancing large multimodal\nconversational models (LMMs), capitalizing on vast repositories of image-text\ndata available online. Despite this progress, these models often encounter\nsubstantial domain gaps, hindering their ability to engage in complex\nconversations across new domains. Recent efforts have aimed to mitigate this\nissue, albeit relying on domain-specific image-text data to curate\ninstruction-tuning data. However, many domains, such as agriculture, lack such\nvision-language data. In this work, we propose an approach to construct\ninstruction-tuning data that harnesses vision-only data for the agriculture\ndomain. We utilize diverse agricultural datasets spanning multiple domains,\ncurate class-specific information, and employ large language models (LLMs) to\nconstruct an expert-tuning set, resulting in a 70k expert-tuning dataset called\nAgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient\nLMM that can hold complex agriculture-related conversations and provide useful\ninsights. We also develop AgroEvals for evaluation and compare {AgroGPT's}\nperformance with large open and closed-source models. {AgroGPT} excels at\nidentifying fine-grained agricultural concepts, can act as an agriculture\nexpert, and provides helpful information for multimodal agriculture questions.\nThe code, datasets, and models are available at\nhttps://github.com/awaisrauf/agroGPT."
                },
                "authors": [
                    {
                        "name": "Muhammad Awais"
                    },
                    {
                        "name": "Ali Husain Salem Abdulla Alharthi"
                    },
                    {
                        "name": "Amandeep Kumar"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "Accepted at WACV, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08946v2",
                "updated": "2025-01-09T18:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    39,
                    25,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-16T17:58:49Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    17,
                    58,
                    49,
                    4,
                    229,
                    0
                ],
                "title": "Authorship Attribution in the Era of LLMs: Problems, Methodologies, and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authorship Attribution in the Era of LLMs: Problems, Methodologies, and\n  Challenges"
                },
                "summary": "Accurate attribution of authorship is crucial for maintaining the integrity\nof digital content, improving forensic investigations, and mitigating the risks\nof misinformation and plagiarism. Addressing the imperative need for proper\nauthorship attribution is essential to uphold the credibility and\naccountability of authentic authorship. The rapid advancements of Large\nLanguage Models (LLMs) have blurred the lines between human and machine\nauthorship, posing significant challenges for traditional methods. We presents\na comprehensive literature review that examines the latest research on\nauthorship attribution in the era of LLMs. This survey systematically explores\nthe landscape of this field by categorizing four representative problems: (1)\nHuman-written Text Attribution; (2) LLM-generated Text Detection; (3)\nLLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution.\nWe also discuss the challenges related to ensuring the generalization and\nexplainability of authorship attribution methods. Generalization requires the\nability to generalize across various domains, while explainability emphasizes\nproviding transparent and understandable insights into the decisions made by\nthese models. By evaluating the strengths and limitations of existing methods\nand benchmarks, we identify key open problems and future research directions in\nthis field. This literature review serves a roadmap for researchers and\npractitioners interested in understanding the state of the art in this rapidly\nevolving field. Additional resources and a curated list of papers are available\nand regularly updated at https://llm-authorship.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate attribution of authorship is crucial for maintaining the integrity\nof digital content, improving forensic investigations, and mitigating the risks\nof misinformation and plagiarism. Addressing the imperative need for proper\nauthorship attribution is essential to uphold the credibility and\naccountability of authentic authorship. The rapid advancements of Large\nLanguage Models (LLMs) have blurred the lines between human and machine\nauthorship, posing significant challenges for traditional methods. We presents\na comprehensive literature review that examines the latest research on\nauthorship attribution in the era of LLMs. This survey systematically explores\nthe landscape of this field by categorizing four representative problems: (1)\nHuman-written Text Attribution; (2) LLM-generated Text Detection; (3)\nLLM-generated Text Attribution; and (4) Human-LLM Co-authored Text Attribution.\nWe also discuss the challenges related to ensuring the generalization and\nexplainability of authorship attribution methods. Generalization requires the\nability to generalize across various domains, while explainability emphasizes\nproviding transparent and understandable insights into the decisions made by\nthese models. By evaluating the strengths and limitations of existing methods\nand benchmarks, we identify key open problems and future research directions in\nthis field. This literature review serves a roadmap for researchers and\npractitioners interested in understanding the state of the art in this rapidly\nevolving field. Additional resources and a curated list of papers are available\nand regularly updated at https://llm-authorship.github.io"
                },
                "authors": [
                    {
                        "name": "Baixiang Huang"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "Accepted to ACM SIGKDD Exploration. 12 pages for the main paper. More\n  resources and a curated list of papers are available and regularly updated at\n  https://llm-authorship.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05423v1",
                "updated": "2025-01-09T18:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T18:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    30,
                    14,
                    3,
                    9,
                    0
                ],
                "title": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese\n  Micro-bloggers"
                },
                "summary": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying public sentiment during crises is crucial for understanding how\nopinions and sentiments shift, resulting in polarized societies. We study\nWeibo, the most popular microblogging site in China, using posts made during\nthe outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19\nstage, the outbreak stage, and the early stage of epidemic prevention. We use\nLlama 3 8B, a Large Language Model, to analyze users' sentiments on the\nplatform by classifying them into positive, negative, sarcastic, and neutral\ncategories. Analyzing sentiment shifts on Weibo provides insights into how\nsocial events and government actions influence public opinion. This study\ncontributes to understanding the dynamics of social sentiments during health\ncrises, fulfilling a gap in sentiment analysis for Chinese platforms. By\nexamining these dynamics, we aim to offer valuable perspectives on digital\ncommunication's role in shaping society's responses during unprecedented global\nchallenges."
                },
                "authors": [
                    {
                        "name": "Jerry Chongyi Hu"
                    },
                    {
                        "name": "Mohammed Shahid Modi"
                    },
                    {
                        "name": "Boleslaw K. Szymanski"
                    }
                ],
                "author_detail": {
                    "name": "Boleslaw K. Szymanski"
                },
                "author": "Boleslaw K. Szymanski",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05396v1",
                "updated": "2025-01-09T17:42:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:42:23Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    42,
                    23,
                    3,
                    9,
                    0
                ],
                "title": "FairCode: Evaluating Social Bias of LLMs in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairCode: Evaluating Social Bias of LLMs in Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant capability in code\ngeneration, drawing increasing attention to the evaluation of the quality and\nsafety of their outputs. However, research on bias in code generation remains\nlimited. Existing studies typically assess bias by applying malicious prompts\nor reapply tasks and dataset for discriminative models. Given that LLMs are\noften aligned with human values and that prior datasets are not fully optimized\nfor code-related tasks, there is a pressing need for benchmarks specifically\ndesigned for evaluating code models. In this study, we introduce FairCode, a\nnovel benchmark for evaluating bias in code generation. FairCode comprises two\ntasks: function implementation and test case generation, each evaluating social\nbias through diverse scenarios. Additionally, we propose a new metric,\nFairScore, to assess model performance on this benchmark. We conduct\nexperiments on widely used LLMs and provide a comprehensive analysis of the\nresults. The findings reveal that all tested LLMs exhibit bias. The code is\navailable at https://github.com/YongkDu/FairCode."
                },
                "authors": [
                    {
                        "name": "Yongkang Du"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Jieyu Zhao"
                    },
                    {
                        "name": "Lu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lu Lin"
                },
                "author": "Lu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11672v2",
                "updated": "2025-01-09T17:18:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    18,
                    12,
                    3,
                    9,
                    0
                ],
                "published": "2024-04-17T18:13:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    13,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory"
                },
                "summary": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current large language models (LLMs) perform well on many\nknowledge-related tasks, they are limited by relying on their parameters as an\nimplicit storage mechanism. As a result, they struggle with memorizing rare\nevents and with updating their memory as facts change over time. In addition,\nthe uninterpretable nature of parametric memory makes it challenging to prevent\nhallucination. Model editing and augmenting LLMs with parameters specialized\nfor memory are only partial solutions. In this paper, we introduce MemLLM, a\nnovel method of enhancing LLMs by integrating a structured and explicit\nread-and-write memory module. MemLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory and improving the LLM's\ncapabilities in using stored knowledge. Our experiments indicate that MemLLM\nenhances the LLM's performance and interpretability, in language modeling in\ngeneral and knowledge-intensive tasks in particular. We see MemLLM as an\nimportant step towards making LLMs more grounded and factual through memory\naugmentation."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05382v1",
                "updated": "2025-01-09T17:11:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T17:11:22Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    17,
                    11,
                    22,
                    3,
                    9,
                    0
                ],
                "title": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models"
                },
                "summary": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models."
                },
                "authors": [
                    {
                        "name": "Kristian G. Barman"
                    },
                    {
                        "name": "Sascha Caron"
                    },
                    {
                        "name": "Emily Sullivan"
                    },
                    {
                        "name": "Henk W. de Regt"
                    },
                    {
                        "name": "Roberto Ruiz de Austri"
                    },
                    {
                        "name": "Mieke Boon"
                    },
                    {
                        "name": "Michael Färber"
                    },
                    {
                        "name": "Stefan Fröse"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Andreas Ipp"
                    },
                    {
                        "name": "Rukshak Kapoor"
                    },
                    {
                        "name": "Gregor Kasieczka"
                    },
                    {
                        "name": "Daniel Kostić"
                    },
                    {
                        "name": "Michael Krämer"
                    },
                    {
                        "name": "Tobias Golling"
                    },
                    {
                        "name": "Luis G. Lopez"
                    },
                    {
                        "name": "Jesus Marco"
                    },
                    {
                        "name": "Sydney Otten"
                    },
                    {
                        "name": "Pawel Pawlowski"
                    },
                    {
                        "name": "Pietro Vischia"
                    },
                    {
                        "name": "Erik Weber"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00846v2",
                "updated": "2025-01-09T16:55:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    55,
                    55,
                    3,
                    9,
                    0
                ],
                "published": "2024-08-01T18:01:23Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    1,
                    23,
                    3,
                    214,
                    0
                ],
                "title": "Occupation-aware planning method for robotic monitoring missions in\n  dynamic environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupation-aware planning method for robotic monitoring missions in\n  dynamic environments"
                },
                "summary": "This paper presents a method for robotic monitoring missions in the presence\nof moving obstacles. Although the scenario map is known, the robot lacks\ninformation about the movement of dynamic obstacles during the monitoring\nmission. Numerous local planners have been developed in recent years for\nnavigating highly dynamic environments. However, the absence of a global\nplanner for these environments can result in unavoidable collisions or the\ninability to successfully complete missions in densely populated areas, such as\na scenario monitoring in our case. This work addresses the development and\nevaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas),\naimed at enhancing the deployment of robots in such challenging conditions. The\nrobot plans and executes the mission using the proposed two-step approach. The\nfirst step involves selecting the observation goal based on the environment's\ndistribution and estimated monitoring costs. In the second step, the robot\nidentifies areas with moving obstacles and obtains paths avoiding densely\noccupied dynamic regions based on their occupation. Quantitative and\nqualitative results based on simulations and on real-world experimentation,\nconfirm that the proposed method allows the robot to effectively monitor most\nof the environment while avoiding densely occupied dynamic areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for robotic monitoring missions in the presence\nof moving obstacles. Although the scenario map is known, the robot lacks\ninformation about the movement of dynamic obstacles during the monitoring\nmission. Numerous local planners have been developed in recent years for\nnavigating highly dynamic environments. However, the absence of a global\nplanner for these environments can result in unavoidable collisions or the\ninability to successfully complete missions in densely populated areas, such as\na scenario monitoring in our case. This work addresses the development and\nevaluation of a global planner, $MADA$ (Monitoring Avoiding Dynamic Areas),\naimed at enhancing the deployment of robots in such challenging conditions. The\nrobot plans and executes the mission using the proposed two-step approach. The\nfirst step involves selecting the observation goal based on the environment's\ndistribution and estimated monitoring costs. In the second step, the robot\nidentifies areas with moving obstacles and obtains paths avoiding densely\noccupied dynamic regions based on their occupation. Quantitative and\nqualitative results based on simulations and on real-world experimentation,\nconfirm that the proposed method allows the robot to effectively monitor most\nof the environment while avoiding densely occupied dynamic areas."
                },
                "authors": [
                    {
                        "name": "Yaroslav Marchukov"
                    },
                    {
                        "name": "Luis Montano"
                    }
                ],
                "author_detail": {
                    "name": "Luis Montano"
                },
                "author": "Luis Montano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17990v2",
                "updated": "2025-01-09T16:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    47,
                    32,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-26T16:02:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models"
                },
                "summary": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Max Pellert"
                    },
                    {
                        "name": "David Garcia"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05361v1",
                "updated": "2025-01-09T16:44:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    44,
                    53,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:44:53Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    44,
                    53,
                    3,
                    9,
                    0
                ],
                "title": "No-Regret Linear Bandits under Gap-Adjusted Misspecification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Regret Linear Bandits under Gap-Adjusted Misspecification"
                },
                "summary": "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign."
                },
                "authors": [
                    {
                        "name": "Chong Liu"
                    },
                    {
                        "name": "Dan Qiao"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Yu-Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiang Wang"
                },
                "author": "Yu-Xiang Wang",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2302.13252",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20138v2",
                "updated": "2025-01-09T16:36:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    36,
                    26,
                    3,
                    9,
                    0
                ],
                "published": "2024-12-28T12:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    54,
                    6,
                    5,
                    363,
                    0
                ],
                "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TradingAgents: Multi-Agents LLM Financial Trading Framework"
                },
                "summary": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. More details on TradingAgents\nare available at https://TradingAgents-AI.github.io."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Edward Sun"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Multi-Agent AI in the Real World @ AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04733v2",
                "updated": "2025-01-09T16:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    30,
                    53,
                    3,
                    9,
                    0
                ],
                "published": "2024-06-07T08:32:30Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    32,
                    30,
                    4,
                    159,
                    0
                ],
                "title": "Unsupervised representation learning with Hebbian synaptic and\n  structural plasticity in brain-like feedforward neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised representation learning with Hebbian synaptic and\n  structural plasticity in brain-like feedforward neural networks"
                },
                "summary": "Neural networks that can capture key principles underlying brain computation\noffer exciting new opportunities for developing artificial intelligence and\nbrain-like computing algorithms. Such networks remain biologically plausible\nwhile leveraging localized forms of synaptic learning rules and modular network\narchitecture found in the neocortex. Compared to backprop-driven deep learning\napproches, they provide more suitable models for deployment of neuromorphic\nhardware and have greater potential for scalability on large-scale computing\nclusters. The development of such brain-like neural networks depends on having\na learning procedure that can build effective internal representations from\ndata. In this work, we introduce and evaluate a brain-like neural network model\ncapable of unsupervised representation learning. It builds on the Bayesian\nConfidence Propagation Neural Network (BCPNN), which has earlier been\nimplemented as abstract as well as biophyscially detailed recurrent attractor\nneural networks explaining various cortical associative memory phenomena. Here\nwe developed a feedforward BCPNN model to perform representation learning by\nincorporating a range of brain-like attributes derived from neocortical\ncircuits such as cortical columns, divisive normalization, Hebbian synaptic\nplasticity, structural plasticity, sparse activity, and sparse patchy\nconnectivity. The model was tested on a diverse set of popular machine learning\nbenchmarks: grayscale images (MNIST, F-MNIST), RGB natural images (SVHN,\nCIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of\nthe model when using a linear classifier to predict the class labels fared\ncompetitively with conventional multi-layer perceptrons and other\nstate-of-the-art brain-like neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks that can capture key principles underlying brain computation\noffer exciting new opportunities for developing artificial intelligence and\nbrain-like computing algorithms. Such networks remain biologically plausible\nwhile leveraging localized forms of synaptic learning rules and modular network\narchitecture found in the neocortex. Compared to backprop-driven deep learning\napproches, they provide more suitable models for deployment of neuromorphic\nhardware and have greater potential for scalability on large-scale computing\nclusters. The development of such brain-like neural networks depends on having\na learning procedure that can build effective internal representations from\ndata. In this work, we introduce and evaluate a brain-like neural network model\ncapable of unsupervised representation learning. It builds on the Bayesian\nConfidence Propagation Neural Network (BCPNN), which has earlier been\nimplemented as abstract as well as biophyscially detailed recurrent attractor\nneural networks explaining various cortical associative memory phenomena. Here\nwe developed a feedforward BCPNN model to perform representation learning by\nincorporating a range of brain-like attributes derived from neocortical\ncircuits such as cortical columns, divisive normalization, Hebbian synaptic\nplasticity, structural plasticity, sparse activity, and sparse patchy\nconnectivity. The model was tested on a diverse set of popular machine learning\nbenchmarks: grayscale images (MNIST, F-MNIST), RGB natural images (SVHN,\nCIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of\nthe model when using a linear classifier to predict the class labels fared\ncompetitively with conventional multi-layer perceptrons and other\nstate-of-the-art brain-like neural networks."
                },
                "authors": [
                    {
                        "name": "Naresh Ravichandran"
                    },
                    {
                        "name": "Anders Lansner"
                    },
                    {
                        "name": "Pawel Herman"
                    }
                ],
                "author_detail": {
                    "name": "Pawel Herman"
                },
                "author": "Pawel Herman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05339v1",
                "updated": "2025-01-09T16:10:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    10,
                    6,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:10:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    10,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with\n  Hardware-Software Co-Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with\n  Hardware-Software Co-Exploration"
                },
                "summary": "The co-design of neural network architectures, quantization precisions, and\nhardware accelerators offers a promising approach to achieving an optimal\nbalance between performance and efficiency, particularly for model deployment\non resource-constrained edge devices. In this work, we propose the JAQ\nFramework, which jointly optimizes the three critical dimensions. However,\neffectively automating the design process across the vast search space of those\nthree dimensions poses significant challenges, especially when pursuing\nextremely low-bit quantization. Specifical, the primary challenges include: (1)\nMemory overhead in software-side: Low-precision quantization-aware training can\nlead to significant memory usage due to storing large intermediate features and\nlatent weights for back-propagation, potentially causing memory exhaustion. (2)\nSearch time-consuming in hardware-side: The discrete nature of hardware\nparameters and the complex interplay between compiler optimizations and\nindividual operators make the accelerator search time-consuming. To address\nthese issues, JAQ mitigates the memory overhead through a channel-wise sparse\nquantization (CSQ) scheme, selectively applying quantization to the most\nsensitive components of the model during optimization. Additionally, JAQ\ndesigns BatchTile, which employs a hardware generation network to encode all\npossible tiling modes, thereby speeding up the search for the optimal compiler\nmapping strategy. Extensive experiments demonstrate the effectiveness of JAQ,\nachieving approximately 7% higher Top-1 accuracy on ImageNet compared to\nprevious methods and reducing the hardware search time per iteration to 0.15\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-design of neural network architectures, quantization precisions, and\nhardware accelerators offers a promising approach to achieving an optimal\nbalance between performance and efficiency, particularly for model deployment\non resource-constrained edge devices. In this work, we propose the JAQ\nFramework, which jointly optimizes the three critical dimensions. However,\neffectively automating the design process across the vast search space of those\nthree dimensions poses significant challenges, especially when pursuing\nextremely low-bit quantization. Specifical, the primary challenges include: (1)\nMemory overhead in software-side: Low-precision quantization-aware training can\nlead to significant memory usage due to storing large intermediate features and\nlatent weights for back-propagation, potentially causing memory exhaustion. (2)\nSearch time-consuming in hardware-side: The discrete nature of hardware\nparameters and the complex interplay between compiler optimizations and\nindividual operators make the accelerator search time-consuming. To address\nthese issues, JAQ mitigates the memory overhead through a channel-wise sparse\nquantization (CSQ) scheme, selectively applying quantization to the most\nsensitive components of the model during optimization. Additionally, JAQ\ndesigns BatchTile, which employs a hardware generation network to encode all\npossible tiling modes, thereby speeding up the search for the optimal compiler\nmapping strategy. Extensive experiments demonstrate the effectiveness of JAQ,\nachieving approximately 7% higher Top-1 accuracy on ImageNet compared to\nprevious methods and reducing the hardware search time per iteration to 0.15\nseconds."
                },
                "authors": [
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Yijian Qin"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Tongtong Feng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xun Guan"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05336v1",
                "updated": "2025-01-09T16:02:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T16:02:51Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    16,
                    2,
                    51,
                    3,
                    9,
                    0
                ],
                "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution\n  Induction"
                },
                "summary": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in their capabilities, but also to increased concerns about their\nalignment with human values and intentions. Current alignment strategies,\nincluding adaptive training and inference-time methods, have demonstrated\npotential in this area. However, these approaches still struggle to balance\ndeployment complexity and capability across various tasks and difficulties. In\nthis work, we introduce the Streaming Distribution Induce Aligner (Stream\nAligner), a novel alignment paradigm that combines efficiency with enhanced\nperformance in various tasks throughout the generation process. Stream Aligner\nachieves dynamic sentence-level correction by using a small model to learn the\npreferences of the suffix sentence, iteratively correcting the suffix sentence\noutput by the upstream model, and then using the corrected sentence to replace\nthe suffix sentence in subsequent generations. Compared to Aligner, our\nexperiments demonstrate that Stream Aligner reduces reliance on the\ncapabilities of additional models, enhances the reasoning abilities of LLMs,\nand decreases latency during user interaction. Specifically, Stream Aligner-2B\nmodel has achieved an improvement of 76.1% in helpfulness, 36.0% in\nharmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has\nachieved an improvement of 3.5% on the math ability of the tested\nLlama3-70B-Instruct model."
                },
                "authors": [
                    {
                        "name": "Hantao Lou"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Kaile Wang"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "AAAI Alignment Track 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05329v1",
                "updated": "2025-01-09T15:55:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    55,
                    8,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:55:08Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    55,
                    8,
                    3,
                    9,
                    0
                ],
                "title": "Knowledge Transfer in Model-Based Reinforcement Learning Agents for\n  Efficient Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Transfer in Model-Based Reinforcement Learning Agents for\n  Efficient Multi-Task Learning"
                },
                "summary": "We propose an efficient knowledge transfer approach for model-based\nreinforcement learning, addressing the challenge of deploying large world\nmodels in resource-constrained environments. Our method distills a\nhigh-capacity multi-task agent (317M parameters) into a compact 1M parameter\nmodel, achieving state-of-the-art performance on the MT30 benchmark with a\nnormalized score of 28.45, a substantial improvement over the original 1M\nparameter model's score of 18.93. This demonstrates the ability of our\ndistillation technique to consolidate complex multi-task knowledge effectively.\nAdditionally, we apply FP16 post-training quantization, reducing the model size\nby 50% while maintaining performance. Our work bridges the gap between the\npower of large models and practical deployment constraints, offering a scalable\nsolution for efficient and accessible multi-task reinforcement learning in\nrobotics and other resource-limited domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an efficient knowledge transfer approach for model-based\nreinforcement learning, addressing the challenge of deploying large world\nmodels in resource-constrained environments. Our method distills a\nhigh-capacity multi-task agent (317M parameters) into a compact 1M parameter\nmodel, achieving state-of-the-art performance on the MT30 benchmark with a\nnormalized score of 28.45, a substantial improvement over the original 1M\nparameter model's score of 18.93. This demonstrates the ability of our\ndistillation technique to consolidate complex multi-task knowledge effectively.\nAdditionally, we apply FP16 post-training quantization, reducing the model size\nby 50% while maintaining performance. Our work bridges the gap between the\npower of large models and practical deployment constraints, offering a scalable\nsolution for efficient and accessible multi-task reinforcement learning in\nrobotics and other resource-limited domains."
                },
                "authors": [
                    {
                        "name": "Dmytro Kuzmenko"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "arxiv_comment": "Preprint of an extended abstract accepted to AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05322v1",
                "updated": "2025-01-09T15:45:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:45:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    45,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"What's Happening\"- A Human-centered Multimodal Interpreter Explaining\n  the Actions of Autonomous Vehicles"
                },
                "summary": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public distrust of self-driving cars is growing. Studies emphasize the need\nfor interpreting the behavior of these vehicles to passengers to promote trust\nin autonomous systems. Interpreters can enhance trust by improving transparency\nand reducing perceived risk. However, current solutions often lack a\nhuman-centric approach to integrating multimodal interpretations. This paper\nintroduces a novel Human-centered Multimodal Interpreter (HMI) system that\nleverages human preferences to provide visual, textual, and auditory feedback.\nThe system combines a visual interface with Bird's Eye View (BEV), map, and\ntext display, along with voice interaction using a fine-tuned large language\nmodel (LLM). Our user study, involving diverse participants, demonstrated that\nthe HMI system significantly boosts passenger trust in AVs, increasing average\ntrust levels by over 8%, with trust in ordinary environments rising by up to\n30%. These results underscore the potential of the HMI system to improve the\nacceptance and reliability of autonomous vehicles by providing clear,\nreal-time, and context-sensitive explanations of vehicle actions."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Ruiqi Chen"
                    },
                    {
                        "name": "Rishikesh Panda"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Shuyun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuyun Zhang"
                },
                "author": "Shuyun Zhang",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop HAVI\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05313v1",
                "updated": "2025-01-09T15:29:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T15:29:33Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    29,
                    33,
                    3,
                    9,
                    0
                ],
                "title": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing"
                },
                "summary": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%."
                },
                "authors": [
                    {
                        "name": "Mengfan Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]