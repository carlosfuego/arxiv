[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v1",
                "updated": "2025-02-21T08:40:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v1",
                "updated": "2025-02-19T07:43:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v1",
                "updated": "2025-02-18T04:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.13148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v3",
                "updated": "2025-02-21T18:59:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "arxiv_comment": "In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15680v1",
                "updated": "2025-02-21T18:59:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:59:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training"
                },
                "summary": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation."
                },
                "authors": [
                    {
                        "name": "Jaydeep Borkar"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "David A. Smith"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    }
                ],
                "author_detail": {
                    "name": "Christopher A. Choquette-Choo"
                },
                "author": "Christopher A. Choquette-Choo",
                "arxiv_comment": "23 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15677v1",
                "updated": "2025-02-21T18:58:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    58,
                    6,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:58:06Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    58,
                    6,
                    4,
                    52,
                    0
                ],
                "title": "FLEKE: Federated Locate-then-Edit Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEKE: Federated Locate-then-Edit Knowledge Editing"
                },
                "summary": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE."
                },
                "authors": [
                    {
                        "name": "Zongkai Zhao"
                    },
                    {
                        "name": "Guozeng Xu"
                    },
                    {
                        "name": "Xiuhua Li"
                    },
                    {
                        "name": "Kaiwen Wei"
                    },
                    {
                        "name": "Jiang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Zhong"
                },
                "author": "Jiang Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15676v1",
                "updated": "2025-02-21T18:57:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    57,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:57:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    57,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind"
                },
                "summary": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind."
                },
                "authors": [
                    {
                        "name": "Zhining Zhang"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Mung Yao Jia"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "23 pages, 6 figures, 11 tables. Website at\n  https://chuanyangjin.com/AutoToM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15666v1",
                "updated": "2025-02-21T18:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    45,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:45:37Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    45,
                    37,
                    4,
                    52,
                    0
                ],
                "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing"
                },
                "summary": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies."
                },
                "authors": [
                    {
                        "name": "Shoumik Saha"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "17 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09760v3",
                "updated": "2025-02-21T18:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    40,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-15T15:01:00Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    1,
                    0,
                    6,
                    259,
                    0
                ],
                "title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing"
                },
                "summary": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools."
                },
                "authors": [
                    {
                        "name": "Suhyeon Yoo"
                    },
                    {
                        "name": "Khai N. Truong"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/elmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15662v1",
                "updated": "2025-02-21T18:38:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:38:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "Automating Curriculum Learning for Reinforcement Learning using a\n  Skill-Based Bayesian Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Curriculum Learning for Reinforcement Learning using a\n  Skill-Based Bayesian Network"
                },
                "summary": "A major challenge for reinforcement learning is automatically generating\ncurricula to reduce training time or improve performance in some target task.\nWe introduce SEBNs (Skill-Environment Bayesian Networks) which model a\nprobabilistic relationship between a set of skills, a set of goals that relate\nto the reward structure, and a set of environment features to predict policy\nperformance on (possibly unseen) tasks. We develop an algorithm that uses the\ninferred estimates of agent success from SEBN to weigh the possible next tasks\nby expected improvement. We evaluate the benefit of the resulting curriculum on\nthree environments: a discrete gridworld, continuous control, and simulated\nrobotics. The results show that curricula constructed using SEBN frequently\noutperform other baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge for reinforcement learning is automatically generating\ncurricula to reduce training time or improve performance in some target task.\nWe introduce SEBNs (Skill-Environment Bayesian Networks) which model a\nprobabilistic relationship between a set of skills, a set of goals that relate\nto the reward structure, and a set of environment features to predict policy\nperformance on (possibly unseen) tasks. We develop an algorithm that uses the\ninferred estimates of agent success from SEBN to weigh the possible next tasks\nby expected improvement. We evaluate the benefit of the resulting curriculum on\nthree environments: a discrete gridworld, continuous control, and simulated\nrobotics. The results show that curricula constructed using SEBN frequently\noutperform other baselines."
                },
                "authors": [
                    {
                        "name": "Vincent Hsiao"
                    },
                    {
                        "name": "Mark Roberts"
                    },
                    {
                        "name": "Laura M. Hiatt"
                    },
                    {
                        "name": "George Konidaris"
                    },
                    {
                        "name": "Dana Nau"
                    }
                ],
                "author_detail": {
                    "name": "Dana Nau"
                },
                "author": "Dana Nau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15657v1",
                "updated": "2025-02-21T18:28:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    28,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:28:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    28,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?"
                },
                "summary": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path."
                },
                "authors": [
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Michael Cohen"
                    },
                    {
                        "name": "Damiano Fornasiere"
                    },
                    {
                        "name": "Joumana Ghosn"
                    },
                    {
                        "name": "Pietro Greiner"
                    },
                    {
                        "name": "Matt MacDermott"
                    },
                    {
                        "name": "Sren Mindermann"
                    },
                    {
                        "name": "Adam Oberman"
                    },
                    {
                        "name": "Jesse Richardson"
                    },
                    {
                        "name": "Oliver Richardson"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Pierre-Luc St-Charles"
                    },
                    {
                        "name": "David Williams-King"
                    }
                ],
                "author_detail": {
                    "name": "David Williams-King"
                },
                "author": "David Williams-King",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v1",
                "updated": "2025-02-21T18:22:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15652v1",
                "updated": "2025-02-21T18:20:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    20,
                    35,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:20:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    20,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey"
                },
                "summary": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously."
                },
                "authors": [
                    {
                        "name": "Fengxiang Cheng"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Fenrong Liu"
                    },
                    {
                        "name": "Robert van Rooij"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15648v1",
                "updated": "2025-02-21T18:15:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    15,
                    11,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:15:11Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    15,
                    11,
                    4,
                    52,
                    0
                ],
                "title": "Logit Disagreement: OoD Detection with Bayesian Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logit Disagreement: OoD Detection with Bayesian Neural Networks"
                },
                "summary": "Bayesian neural networks (BNNs), which estimate the full posterior\ndistribution over model parameters, are well-known for their role in\nuncertainty quantification and its promising application in out-of-distribution\ndetection (OoD). Amongst other uncertainty measures, BNNs provide a\nstate-of-the art estimation of predictive entropy (total uncertainty) which can\nbe decomposed as the sum of mutual information and expected entropy. In the\ncontext of OoD detection the estimation of predictive uncertainty in the form\nof the predictive entropy score confounds aleatoric and epistemic uncertainty,\nthe latter being hypothesized to be high for OoD points. Despite these\njustifications, the mutual information score has been shown to perform worse\nthan predictive entropy. Taking inspiration from Bayesian variational\nautoencoder (BVAE) literature, this work proposes to measure the disagreement\nbetween a corrected version of the pre-softmax quantities, otherwise known as\nlogits, as an estimate of epistemic uncertainty for Bayesian NNs under mean\nfield variational inference. The three proposed epistemic uncertainty scores\ndemonstrate marked improvements over mutual information on a range of OoD\nexperiments, with equal performance otherwise. Moreover, the epistemic\nuncertainty scores perform on par with the Bayesian benchmark predictive\nentropy on a range of MNIST and CIFAR10 experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian neural networks (BNNs), which estimate the full posterior\ndistribution over model parameters, are well-known for their role in\nuncertainty quantification and its promising application in out-of-distribution\ndetection (OoD). Amongst other uncertainty measures, BNNs provide a\nstate-of-the art estimation of predictive entropy (total uncertainty) which can\nbe decomposed as the sum of mutual information and expected entropy. In the\ncontext of OoD detection the estimation of predictive uncertainty in the form\nof the predictive entropy score confounds aleatoric and epistemic uncertainty,\nthe latter being hypothesized to be high for OoD points. Despite these\njustifications, the mutual information score has been shown to perform worse\nthan predictive entropy. Taking inspiration from Bayesian variational\nautoencoder (BVAE) literature, this work proposes to measure the disagreement\nbetween a corrected version of the pre-softmax quantities, otherwise known as\nlogits, as an estimate of epistemic uncertainty for Bayesian NNs under mean\nfield variational inference. The three proposed epistemic uncertainty scores\ndemonstrate marked improvements over mutual information on a range of OoD\nexperiments, with equal performance otherwise. Moreover, the epistemic\nuncertainty scores perform on par with the Bayesian benchmark predictive\nentropy on a range of MNIST and CIFAR10 experiments."
                },
                "authors": [
                    {
                        "name": "Kevin Raina"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Raina"
                },
                "author": "Kevin Raina",
                "arxiv_comment": "Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty\n  Quantification for Computer Vision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15646v1",
                "updated": "2025-02-21T18:12:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    12,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:12:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    12,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Predicting gene essentiality and drug response from perturbation screens\n  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and\n  Predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting gene essentiality and drug response from perturbation screens\n  in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and\n  Predictors"
                },
                "summary": "Preclinical perturbation screens, where the effects of genetic, chemical, or\nenvironmental perturbations are systematically tested on disease models, hold\nsignificant promise for machine learning-enhanced drug discovery due to their\nscale and causal nature. Predictive models can infer perturbation responses for\npreviously untested disease models based on molecular profiles. These in silico\nlabels can expand databases and guide experimental prioritization.\n  However, modelling perturbation-specific effects and generating robust\nprediction performances across diverse biological contexts remain elusive. We\nintroduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel\nensemble framework to improve robustness and generalization. LEAP leverages\nmultiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO\nregressors. By combining diverse gene expression representation models learned\nfrom different random initializations, LEAP consistently outperforms\nstate-of-the-art approaches in predicting gene essentiality or drug responses\nin unseen cell lines, tissues and disease models. Notably, our results show\nthat ensembling representation models, rather than prediction models alone,\nyields superior predictive performance.\n  Beyond its performance gains, LEAP is computationally efficient, requires\nminimal hyperparameter tuning and can therefore be readily incorporated into\ndrug discovery pipelines to prioritize promising targets and support\nbiomarker-driven stratification. The code and datasets used in this work are\nmade publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preclinical perturbation screens, where the effects of genetic, chemical, or\nenvironmental perturbations are systematically tested on disease models, hold\nsignificant promise for machine learning-enhanced drug discovery due to their\nscale and causal nature. Predictive models can infer perturbation responses for\npreviously untested disease models based on molecular profiles. These in silico\nlabels can expand databases and guide experimental prioritization.\n  However, modelling perturbation-specific effects and generating robust\nprediction performances across diverse biological contexts remain elusive. We\nintroduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel\nensemble framework to improve robustness and generalization. LEAP leverages\nmultiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO\nregressors. By combining diverse gene expression representation models learned\nfrom different random initializations, LEAP consistently outperforms\nstate-of-the-art approaches in predicting gene essentiality or drug responses\nin unseen cell lines, tissues and disease models. Notably, our results show\nthat ensembling representation models, rather than prediction models alone,\nyields superior predictive performance.\n  Beyond its performance gains, LEAP is computationally efficient, requires\nminimal hyperparameter tuning and can therefore be readily incorporated into\ndrug discovery pipelines to prioritize promising targets and support\nbiomarker-driven stratification. The code and datasets used in this work are\nmade publicly available."
                },
                "authors": [
                    {
                        "name": "Barbara Bodinier"
                    },
                    {
                        "name": "Gaetan Dissez"
                    },
                    {
                        "name": "Linus Bleistein"
                    },
                    {
                        "name": "Antonin Dauvin"
                    }
                ],
                "author_detail": {
                    "name": "Antonin Dauvin"
                },
                "author": "Antonin Dauvin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15634v1",
                "updated": "2025-02-21T18:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    3,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:03:44Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    3,
                    44,
                    4,
                    52,
                    0
                ],
                "title": "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing"
                },
                "summary": "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortfall, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning\nfor real-world tasks demanding flexible rule-based behavior. WiNN employs a\npretrained convolutional neural network for vision, coupled with an adjustable\n\"context state\" that guides attention to relevant features. If WiNN produces an\nincorrect response, it first iteratively updates its context state to refocus\nattention on task-relevant cues, then performs minimal parameter updates to\nattention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory network, reducing catastrophic forgetting. We\nevaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task,\nrevealing several markers of cognitive flexibility: (i) WiNN autonomously\ninfers underlying rules, (ii) requires fewer examples to do so than control\nmodels reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state inference\nalone. By blending fast context inference with targeted attentional guidance,\nWiNN achieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortfall, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which expands on fast-and-slow learning\nfor real-world tasks demanding flexible rule-based behavior. WiNN employs a\npretrained convolutional neural network for vision, coupled with an adjustable\n\"context state\" that guides attention to relevant features. If WiNN produces an\nincorrect response, it first iteratively updates its context state to refocus\nattention on task-relevant cues, then performs minimal parameter updates to\nattention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory network, reducing catastrophic forgetting. We\nevaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task,\nrevealing several markers of cognitive flexibility: (i) WiNN autonomously\ninfers underlying rules, (ii) requires fewer examples to do so than control\nmodels reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state inference\nalone. By blending fast context inference with targeted attentional guidance,\nWiNN achieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks."
                },
                "authors": [
                    {
                        "name": "Rowan Sommers"
                    },
                    {
                        "name": "Sushrut Thorat"
                    },
                    {
                        "name": "Daniel Anthes"
                    },
                    {
                        "name": "Tim C. Kietzmann"
                    }
                ],
                "author_detail": {
                    "name": "Tim C. Kietzmann"
                },
                "author": "Tim C. Kietzmann",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08288v2",
                "updated": "2025-02-21T18:00:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    0,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-10T18:20:44Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    18,
                    20,
                    44,
                    3,
                    284,
                    0
                ],
                "title": "Towards Foundation Models for Mixed Integer Linear Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Foundation Models for Mixed Integer Linear Programming"
                },
                "summary": "Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide."
                },
                "authors": [
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Janardhan Kulkarni"
                    },
                    {
                        "name": "Ishai Menache"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Beibin Li"
                    }
                ],
                "author_detail": {
                    "name": "Beibin Li"
                },
                "author": "Beibin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v4",
                "updated": "2025-02-21T17:55:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    55,
                    32,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v4",
                "updated": "2025-02-21T17:53:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    53,
                    9,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, automated, end-to-end penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and LLama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming end-to-end penetration testing even with some minimal human\nassistance. Next, we advance the state-of-the-art and present ablation studies\nthat provide insights into improving the PentestGPT tool. Our research\nilluminates the challenges LLMs face in each aspect of Pentesting, e.g.\nenumeration, exploitation, and privilege escalation. This work contributes to\nthe growing body of knowledge on AI-assisted cybersecurity and lays the\nfoundation for future research in automated penetration testing using large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, automated, end-to-end penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and LLama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming end-to-end penetration testing even with some minimal human\nassistance. Next, we advance the state-of-the-art and present ablation studies\nthat provide insights into improving the PentestGPT tool. Our research\nilluminates the challenges LLMs face in each aspect of Pentesting, e.g.\nenumeration, exploitation, and privilege escalation. This work contributes to\nthe growing body of knowledge on AI-assisted cybersecurity and lays the\nfoundation for future research in automated penetration testing using large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15618v1",
                "updated": "2025-02-21T17:41:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    41,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    41,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing"
                },
                "summary": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning."
                },
                "authors": [
                    {
                        "name": "Qi Le"
                    },
                    {
                        "name": "Enmao Diao"
                    },
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Xinran Wang"
                    },
                    {
                        "name": "Jie Ding"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09838v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09838v3",
                "updated": "2025-02-21T17:39:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    39,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-14T00:42:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    0,
                    42,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation"
                },
                "summary": "We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT."
                },
                "authors": [
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Sijing Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Binhe Yu"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "arxiv_comment": "Comments: added project page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09838v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15613v1",
                "updated": "2025-02-21T17:35:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    35,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:35:10Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    35,
                    10,
                    4,
                    52,
                    0
                ],
                "title": "Pick-and-place Manipulation Across Grippers Without Retraining: A\n  Learning-optimization Diffusion Policy Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pick-and-place Manipulation Across Grippers Without Retraining: A\n  Learning-optimization Diffusion Policy Approach"
                },
                "summary": "Current robotic pick-and-place policies typically require consistent gripper\nconfigurations across training and inference. This constraint imposes high\nretraining or fine-tuning costs, especially for imitation learning-based\napproaches, when adapting to new end-effectors. To mitigate this issue, we\npresent a diffusion-based policy with a hybrid learning-optimization framework,\nenabling zero-shot adaptation to novel grippers without additional data\ncollection for retraining policy. During training, the policy learns\nmanipulation primitives from demonstrations collected using a base gripper. At\ninference, a diffusion-based optimization strategy dynamically enforces\nkinematic and safety constraints, ensuring that generated trajectories align\nwith the physical properties of unseen grippers. This is achieved through a\nconstrained denoising procedure that adapts trajectories to gripper-specific\nparameters (e.g., tool-center-point offsets, jaw widths) while preserving\ncollision avoidance and task feasibility. We validate our method on a Franka\nPanda robot across six gripper configurations, including 3D-printed fingertips,\nflexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a\n93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion\npolicy baselines), supporting tool-center-point variations of 16-23.5 cm and\njaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion\nenables robust cross-gripper manipulation while maintaining the sample\nefficiency of imitation learning, eliminating the need for gripper-specific\nretraining. Video and code are available at https://github.com/yaoxt3/GADP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current robotic pick-and-place policies typically require consistent gripper\nconfigurations across training and inference. This constraint imposes high\nretraining or fine-tuning costs, especially for imitation learning-based\napproaches, when adapting to new end-effectors. To mitigate this issue, we\npresent a diffusion-based policy with a hybrid learning-optimization framework,\nenabling zero-shot adaptation to novel grippers without additional data\ncollection for retraining policy. During training, the policy learns\nmanipulation primitives from demonstrations collected using a base gripper. At\ninference, a diffusion-based optimization strategy dynamically enforces\nkinematic and safety constraints, ensuring that generated trajectories align\nwith the physical properties of unseen grippers. This is achieved through a\nconstrained denoising procedure that adapts trajectories to gripper-specific\nparameters (e.g., tool-center-point offsets, jaw widths) while preserving\ncollision avoidance and task feasibility. We validate our method on a Franka\nPanda robot across six gripper configurations, including 3D-printed fingertips,\nflexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a\n93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion\npolicy baselines), supporting tool-center-point variations of 16-23.5 cm and\njaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion\nenables robust cross-gripper manipulation while maintaining the sample\nefficiency of imitation learning, eliminating the need for gripper-specific\nretraining. Video and code are available at https://github.com/yaoxt3/GADP."
                },
                "authors": [
                    {
                        "name": "Xiangtong Yao"
                    },
                    {
                        "name": "Yirui Zhou"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Liangyu Dong"
                    },
                    {
                        "name": "Lin Hong"
                    },
                    {
                        "name": "Zitao Zhang"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "Video and code are available at https://github.com/yaoxt3/GADP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07653v2",
                "updated": "2025-02-21T17:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    32,
                    46,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-13T19:26:09Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    19,
                    26,
                    9,
                    0,
                    13,
                    0
                ],
                "title": "Large Language Models for Interpretable Mental Health Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Interpretable Mental Health Diagnosis"
                },
                "summary": "We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method."
                },
                "authors": [
                    {
                        "name": "Brian Hyeongseok Kim"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "arxiv_comment": "Accepted at AAAI 2025 Workshop on Large Language Models and\n  Generative AI for Health (GenAI4Health)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15609v1",
                "updated": "2025-02-21T17:31:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    31,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:31:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    31,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "On the Robustness of Transformers against Context Hijacking for Linear\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Transformers against Context Hijacking for Linear\n  Classification"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Xingwu Chen"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Difan Zou"
                    }
                ],
                "author_detail": {
                    "name": "Difan Zou"
                },
                "author": "Difan Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13311v2",
                "updated": "2025-02-21T17:25:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    25,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-18T22:13:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    22,
                    13,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors"
                },
                "summary": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Yinpei Dai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15604v1",
                "updated": "2025-02-21T17:19:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:19:39Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    39,
                    4,
                    52,
                    0
                ],
                "title": "Cross-Format Retrieval-Augmented Generation in XR with LLMs for\n  Context-Aware Maintenance Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Format Retrieval-Augmented Generation in XR with LLMs for\n  Context-Aware Maintenance Assistance"
                },
                "summary": "This paper presents a detailed evaluation of a Retrieval-Augmented Generation\n(RAG) system that integrates large language models (LLMs) to enhance\ninformation retrieval and instruction generation for maintenance personnel\nacross diverse data formats. We assessed the performance of eight LLMs,\nemphasizing key metrics such as response speed and accuracy, which were\nquantified using BLEU and METEOR scores. Our findings reveal that advanced\nmodels like GPT-4 and GPT-4o-mini significantly outperform their counterparts,\nparticularly when addressing complex queries requiring multi-format data\nintegration. The results validate the system's ability to deliver timely and\naccurate responses, highlighting the potential of RAG frameworks to optimize\nmaintenance operations. Future research will focus on refining retrieval\ntechniques for these models and enhancing response generation, particularly for\nintricate scenarios, ultimately improving the system's practical applicability\nin dynamic real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a detailed evaluation of a Retrieval-Augmented Generation\n(RAG) system that integrates large language models (LLMs) to enhance\ninformation retrieval and instruction generation for maintenance personnel\nacross diverse data formats. We assessed the performance of eight LLMs,\nemphasizing key metrics such as response speed and accuracy, which were\nquantified using BLEU and METEOR scores. Our findings reveal that advanced\nmodels like GPT-4 and GPT-4o-mini significantly outperform their counterparts,\nparticularly when addressing complex queries requiring multi-format data\nintegration. The results validate the system's ability to deliver timely and\naccurate responses, highlighting the potential of RAG frameworks to optimize\nmaintenance operations. Future research will focus on refining retrieval\ntechniques for these models and enhancing response generation, particularly for\nintricate scenarios, ultimately improving the system's practical applicability\nin dynamic real-world environments."
                },
                "authors": [
                    {
                        "name": "Akos Nagy"
                    },
                    {
                        "name": "Yannis Spyridis"
                    },
                    {
                        "name": "Vasileios Argyriou"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Argyriou"
                },
                "author": "Vasileios Argyriou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15603v1",
                "updated": "2025-02-21T17:19:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    23,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:19:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Do Multilingual LLMs Think In English?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Multilingual LLMs Think In English?"
                },
                "summary": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users."
                },
                "authors": [
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Sebastian Farquhar"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Farquhar"
                },
                "author": "Sebastian Farquhar",
                "arxiv_comment": "Main paper 9 pages; including appendix 48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15601v1",
                "updated": "2025-02-21T17:18:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    18,
                    30,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:18:30Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    18,
                    30,
                    4,
                    52,
                    0
                ],
                "title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents"
                },
                "summary": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life."
                },
                "authors": [
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15594v1",
                "updated": "2025-02-21T17:12:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    12,
                    35,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:12:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    12,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention"
                },
                "summary": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chunyan Hou"
                    },
                    {
                        "name": "Xiaojie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Yuan"
                },
                "author": "Xiaojie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15592v1",
                "updated": "2025-02-21T17:02:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    2,
                    40,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:02:40Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    2,
                    40,
                    4,
                    52,
                    0
                ],
                "title": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning"
                },
                "summary": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14677v2",
                "updated": "2025-02-21T16:58:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    58,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T16:09:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Data-Constrained Synthesis of Training Data for De-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Constrained Synthesis of Training Data for De-Identification"
                },
                "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data."
                },
                "authors": [
                    {
                        "name": "Thomas Vakili"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Hercules Dalianis"
                    }
                ],
                "author_detail": {
                    "name": "Hercules Dalianis"
                },
                "author": "Hercules Dalianis",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15589v1",
                "updated": "2025-02-21T16:57:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:57:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "LightThinker: Thinking Step-by-Step Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightThinker: Thinking Step-by-Step Compression"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker."
                },
                "authors": [
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18328v2",
                "updated": "2025-02-21T16:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    45,
                    17,
                    4,
                    52,
                    0
                ],
                "published": "2024-07-04T22:26:20Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    22,
                    26,
                    20,
                    3,
                    186,
                    0
                ],
                "title": "Unveiling Scoring Processes: Dissecting the Differences between LLMs and\n  Human Graders in Automatic Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Scoring Processes: Dissecting the Differences between LLMs and\n  Human Graders in Automatic Scoring"
                },
                "summary": "Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring."
                },
                "authors": [
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Padmaja Pravin Saraf"
                    },
                    {
                        "name": "Gyeonggeon Lee"
                    },
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted by Technology, Knowledge, and Learning (TKNL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15576v1",
                "updated": "2025-02-21T16:36:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    36,
                    42,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:36:42Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    36,
                    42,
                    4,
                    52,
                    0
                ],
                "title": "Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders"
                },
                "summary": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted."
                },
                "authors": [
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "Pre-print. 20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15572v1",
                "updated": "2025-02-21T16:32:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    32,
                    28,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:32:28Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    32,
                    28,
                    4,
                    52,
                    0
                ],
                "title": "DReSD: Dense Retrieval for Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DReSD: Dense Retrieval for Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST)."
                },
                "authors": [
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Huiyin Xue"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Lampouras"
                },
                "author": "Gerasimos Lampouras",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15568v1",
                "updated": "2025-02-21T16:30:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    30,
                    53,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:30:53Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    30,
                    53,
                    4,
                    52,
                    0
                ],
                "title": "A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the\n  2025 Federal Elections in Germany",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the\n  2025 Federal Elections in Germany"
                },
                "summary": "In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties."
                },
                "authors": [
                    {
                        "name": "Ina Dormuth"
                    },
                    {
                        "name": "Sven Franke"
                    },
                    {
                        "name": "Marlies Hafer"
                    },
                    {
                        "name": "Tim Katzke"
                    },
                    {
                        "name": "Alexander Marx"
                    },
                    {
                        "name": "Emmanuel Mller"
                    },
                    {
                        "name": "Daniel Neider"
                    },
                    {
                        "name": "Markus Pauly"
                    },
                    {
                        "name": "Jrme Rutinowski"
                    }
                ],
                "author_detail": {
                    "name": "Jrme Rutinowski"
                },
                "author": "Jrme Rutinowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05673v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05673v4",
                "updated": "2025-02-21T16:17:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    17,
                    17,
                    4,
                    52,
                    0
                ],
                "published": "2024-06-09T07:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    7,
                    6,
                    58,
                    6,
                    161,
                    0
                ],
                "title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples"
                },
                "summary": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR."
                },
                "authors": [
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Haoqiang Kang"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05673v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05673v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03765v2",
                "updated": "2025-02-21T16:12:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    12,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2024-08-19T22:45:46Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    22,
                    45,
                    46,
                    0,
                    232,
                    0
                ],
                "title": "AI and Entrepreneurship: Facial Recognition Technology Detects\n  Entrepreneurs, Outperforming Human Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and Entrepreneurship: Facial Recognition Technology Detects\n  Entrepreneurs, Outperforming Human Experts"
                },
                "summary": "Occupational outcomes like entrepreneurship are generally considered personal\ninformation that individuals should have the autonomy to disclose. With the\nadvancing capability of artificial intelligence (AI) to infer private details\nfrom widely available human-centric data (e.g., social media), it is crucial to\ninvestigate whether AI can accurately extract private occupational information\nfrom such data. In this study, we demonstrate that deep neural networks can\nclassify individuals as entrepreneurs with high accuracy based on facial images\nsourced from Crunchbase, a premier source for entrepreneurship data. Utilizing\na dataset comprising facial images of 40,728 individuals, including both\nentrepreneurs and non-entrepreneurs, we train a Convolutional Neural Network\n(CNN) using a contrastive learning approach based on pairs of facial images\n(one entrepreneur and one non-entrepreneur per pair). While human experts\n(n=650) and trained participants (n=133) were unable to classify entrepreneurs\nwith accuracy above chance levels (>50%), our AI model achieved a\nclassification accuracy of 79.51%. Several robustness tests indicate that this\nhigh level of accuracy is maintained under various conditions. These results\nindicate privacy risks for entrepreneurs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupational outcomes like entrepreneurship are generally considered personal\ninformation that individuals should have the autonomy to disclose. With the\nadvancing capability of artificial intelligence (AI) to infer private details\nfrom widely available human-centric data (e.g., social media), it is crucial to\ninvestigate whether AI can accurately extract private occupational information\nfrom such data. In this study, we demonstrate that deep neural networks can\nclassify individuals as entrepreneurs with high accuracy based on facial images\nsourced from Crunchbase, a premier source for entrepreneurship data. Utilizing\na dataset comprising facial images of 40,728 individuals, including both\nentrepreneurs and non-entrepreneurs, we train a Convolutional Neural Network\n(CNN) using a contrastive learning approach based on pairs of facial images\n(one entrepreneur and one non-entrepreneur per pair). While human experts\n(n=650) and trained participants (n=133) were unable to classify entrepreneurs\nwith accuracy above chance levels (>50%), our AI model achieved a\nclassification accuracy of 79.51%. Several robustness tests indicate that this\nhigh level of accuracy is maintained under various conditions. These results\nindicate privacy risks for entrepreneurs."
                },
                "authors": [
                    {
                        "name": "Martin Obschonka"
                    },
                    {
                        "name": "Christian Fisch"
                    },
                    {
                        "name": "Tharindu Fernando"
                    },
                    {
                        "name": "Clinton Fookes"
                    }
                ],
                "author_detail": {
                    "name": "Clinton Fookes"
                },
                "author": "Clinton Fookes",
                "arxiv_comment": "46 pages, 2 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06613v2",
                "updated": "2025-02-21T15:55:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    55,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2024-11-10T22:18:53Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    22,
                    18,
                    53,
                    6,
                    315,
                    0
                ],
                "title": "Are Neuromorphic Architectures Inherently Privacy-preserving? An\n  Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Neuromorphic Architectures Inherently Privacy-preserving? An\n  Exploratory Study"
                },
                "summary": "While machine learning (ML) models are becoming mainstream, especially in\nsensitive application areas, the risk of data leakage has become a growing\nconcern. Attacks like membership inference (MIA) have shown that trained models\ncan reveal sensitive data, jeopardizing confidentiality. While traditional\nArtificial Neural Networks (ANNs) dominate ML applications, neuromorphic\narchitectures, specifically Spiking Neural Networks (SNNs), are emerging as\npromising alternatives due to their low power consumption and event-driven\nprocessing, akin to biological neurons. Privacy in ANNs is well-studied;\nhowever, little work has explored the privacy-preserving properties of SNNs.\nThis paper examines whether SNNs inherently offer better privacy. Using MIAs,\nwe assess the privacy resilience of SNNs versus ANNs across diverse datasets.\nWe analyze the impact of learning algorithms (surrogate gradient and\nevolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN\nprivacy. Our findings show that SNNs consistently outperform ANNs in privacy\npreservation, with evolutionary algorithms offering additional resilience. For\ninstance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than\nANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'\n0.88. Additionally, we explore the privacy-utility trade-off with\nDifferentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs\nsustain less accuracy loss than ANNs under similar privacy constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning (ML) models are becoming mainstream, especially in\nsensitive application areas, the risk of data leakage has become a growing\nconcern. Attacks like membership inference (MIA) have shown that trained models\ncan reveal sensitive data, jeopardizing confidentiality. While traditional\nArtificial Neural Networks (ANNs) dominate ML applications, neuromorphic\narchitectures, specifically Spiking Neural Networks (SNNs), are emerging as\npromising alternatives due to their low power consumption and event-driven\nprocessing, akin to biological neurons. Privacy in ANNs is well-studied;\nhowever, little work has explored the privacy-preserving properties of SNNs.\nThis paper examines whether SNNs inherently offer better privacy. Using MIAs,\nwe assess the privacy resilience of SNNs versus ANNs across diverse datasets.\nWe analyze the impact of learning algorithms (surrogate gradient and\nevolutionary), frameworks (snnTorch, TENNLab, LAVA), and parameters on SNN\nprivacy. Our findings show that SNNs consistently outperform ANNs in privacy\npreservation, with evolutionary algorithms offering additional resilience. For\ninstance, on CIFAR-10, SNNs achieve an AUC of 0.59, significantly lower than\nANNs' 0.82, and on CIFAR-100, SNNs maintain an AUC of 0.58 compared to ANNs'\n0.88. Additionally, we explore the privacy-utility trade-off with\nDifferentially Private Stochastic Gradient Descent (DPSGD), finding that SNNs\nsustain less accuracy loss than ANNs under similar privacy constraints."
                },
                "authors": [
                    {
                        "name": "Ayana Moshruba"
                    },
                    {
                        "name": "Ihsen Alouani"
                    },
                    {
                        "name": "Maryam Parsa"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Parsa"
                },
                "author": "Maryam Parsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15543v1",
                "updated": "2025-02-21T15:50:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    50,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:50:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    50,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning"
                },
                "summary": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG."
                },
                "authors": [
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "arxiv_comment": "20 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v2",
                "updated": "2025-02-21T15:48:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    48,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration (v1); updated version (v2)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17179v2",
                "updated": "2025-02-21T15:48:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    48,
                    32,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-23T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    40,
                    24,
                    0,
                    267,
                    0
                ],
                "title": "Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?"
                },
                "summary": "Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest."
                },
                "authors": [
                    {
                        "name": "Diego Marcos"
                    },
                    {
                        "name": "Robert van de Vlasakker"
                    },
                    {
                        "name": "Ioannis N. Athanasiadis"
                    },
                    {
                        "name": "Pierre Bonnet"
                    },
                    {
                        "name": "Herv Goeau"
                    },
                    {
                        "name": "Alexis Joly"
                    },
                    {
                        "name": "W. Daniel Kissling"
                    },
                    {
                        "name": "Csar Leblanc"
                    },
                    {
                        "name": "Andr S. J. van Proosdij"
                    },
                    {
                        "name": "Konstantinos P. Panousis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos P. Panousis"
                },
                "author": "Konstantinos P. Panousis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15526v1",
                "updated": "2025-02-21T15:28:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    28,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:28:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    28,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs"
                },
                "summary": "Scaling large language models (LLMs) has shown great potential for improving\nretrieval model performance; however, previous studies have mainly focused on\ndense retrieval trained with contrastive loss (CL), neglecting the scaling\nbehavior of other retrieval paradigms and optimization techniques, such as\nsparse retrieval and knowledge distillation (KD). In this work, we conduct a\nsystematic comparative study on how different retrieval paradigms (sparse vs.\ndense) and fine-tuning objectives (CL vs. KD vs. their combination) affect\nretrieval performance across different model scales. Using MSMARCO passages as\nthe training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a\nfixed compute budget, we evaluate various training configurations on both\nin-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key\nfindings reveal that: (1) Scaling behaviors emerge clearly only with CL, where\nlarger models achieve significant performance gains, whereas KD-trained models\nshow minimal improvement, performing similarly across the 1B, 3B, and 8B\nscales. (2) Sparse retrieval models consistently outperform dense retrieval\nacross both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,\nand they demonstrate greater robustness to imperfect supervised signals. (3) We\nsuccessfully scale sparse retrieval models with the combination of CL and KD\nlosses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation\nsets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) has shown great potential for improving\nretrieval model performance; however, previous studies have mainly focused on\ndense retrieval trained with contrastive loss (CL), neglecting the scaling\nbehavior of other retrieval paradigms and optimization techniques, such as\nsparse retrieval and knowledge distillation (KD). In this work, we conduct a\nsystematic comparative study on how different retrieval paradigms (sparse vs.\ndense) and fine-tuning objectives (CL vs. KD vs. their combination) affect\nretrieval performance across different model scales. Using MSMARCO passages as\nthe training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a\nfixed compute budget, we evaluate various training configurations on both\nin-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key\nfindings reveal that: (1) Scaling behaviors emerge clearly only with CL, where\nlarger models achieve significant performance gains, whereas KD-trained models\nshow minimal improvement, performing similarly across the 1B, 3B, and 8B\nscales. (2) Sparse retrieval models consistently outperform dense retrieval\nacross both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,\nand they demonstrate greater robustness to imperfect supervised signals. (3) We\nsuccessfully scale sparse retrieval models with the combination of CL and KD\nlosses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation\nsets."
                },
                "authors": [
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Julian Killingback"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15524v1",
                "updated": "2025-02-21T15:25:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    25,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:25:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    25,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "Towards Swift Serverless LLM Cold Starts with ParaServe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Swift Serverless LLM Cold Starts with ParaServe"
                },
                "summary": "With the surge in number of large language models (LLMs), the industry turns\nto serverless computing for LLM inference serving. However, serverless LLM\nserving suffers from significant cold start latency and service level objective\n(SLO) violations due to the substantial model size, which leads to prolonged\nmodel fetching time from remote storage. We present ParaServe, a serverless LLM\nserving system that minimizes cold start latency through the novel use of\npipeline parallelism. Our insight is that by distributing model parameters\nacross multiple GPU servers, we can utilize their aggregated network bandwidth\nto concurrently fetch different parts of the model. ParaServe adopts a\ntwo-level hierarchical design. At the cluster level, ParaServe determines the\noptimal degree of parallelism based on user SLOs and carefully places GPU\nworkers across servers to reduce network interference. At the worker level,\nParaServe overlaps model fetching, loading, and runtime initialization to\nfurther accelerate cold starts. Additionally, ParaServe introduces pipeline\nconsolidation, which merges parallel groups back to individual workers to\nmaintain optimal performance for warm requests. Our comprehensive evaluations\nunder diverse settings demonstrate that ParaServe reduces the cold start\nlatency by up to 4.7x and improves SLO attainment by up to 1.74x compared to\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the surge in number of large language models (LLMs), the industry turns\nto serverless computing for LLM inference serving. However, serverless LLM\nserving suffers from significant cold start latency and service level objective\n(SLO) violations due to the substantial model size, which leads to prolonged\nmodel fetching time from remote storage. We present ParaServe, a serverless LLM\nserving system that minimizes cold start latency through the novel use of\npipeline parallelism. Our insight is that by distributing model parameters\nacross multiple GPU servers, we can utilize their aggregated network bandwidth\nto concurrently fetch different parts of the model. ParaServe adopts a\ntwo-level hierarchical design. At the cluster level, ParaServe determines the\noptimal degree of parallelism based on user SLOs and carefully places GPU\nworkers across servers to reduce network interference. At the worker level,\nParaServe overlaps model fetching, loading, and runtime initialization to\nfurther accelerate cold starts. Additionally, ParaServe introduces pipeline\nconsolidation, which merges parallel groups back to individual workers to\nmaintain optimal performance for warm requests. Our comprehensive evaluations\nunder diverse settings demonstrate that ParaServe reduces the cold start\nlatency by up to 4.7x and improves SLO attainment by up to 1.74x compared to\nbaselines."
                },
                "authors": [
                    {
                        "name": "Chiheng Lou"
                    },
                    {
                        "name": "Sheng Qi"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15507v1",
                "updated": "2025-02-21T15:04:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "title": "Activation Steering in Neural Theorem Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering in Neural Theorem Provers"
                },
                "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shashank Kirtania"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Kirtania"
                },
                "author": "Shashank Kirtania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15506v1",
                "updated": "2025-02-21T15:02:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    2,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:02:39Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    2,
                    39,
                    4,
                    52,
                    0
                ],
                "title": "Construction and Evaluation of LLM-based agents for Semi-Autonomous\n  penetration testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction and Evaluation of LLM-based agents for Semi-Autonomous\n  penetration testing"
                },
                "summary": "With the emergence of high-performance large language models (LLMs) such as\nGPT, Claude, and Gemini, the autonomous and semi-autonomous execution of tasks\nhas significantly advanced across various domains. However, in highly\nspecialized fields such as cybersecurity, full autonomy remains a challenge.\nThis difficulty primarily stems from the limitations of LLMs in reasoning\ncapabilities and domain-specific knowledge. We propose a system that\nsemi-autonomously executes complex cybersecurity workflows by employing\nmultiple LLMs modules to formulate attack strategies, generate commands, and\nanalyze results, thereby addressing the aforementioned challenges. In our\nexperiments using Hack The Box virtual machines, we confirmed that our system\ncan autonomously construct attack strategies, issue appropriate commands, and\nautomate certain processes, thereby reducing the need for manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of high-performance large language models (LLMs) such as\nGPT, Claude, and Gemini, the autonomous and semi-autonomous execution of tasks\nhas significantly advanced across various domains. However, in highly\nspecialized fields such as cybersecurity, full autonomy remains a challenge.\nThis difficulty primarily stems from the limitations of LLMs in reasoning\ncapabilities and domain-specific knowledge. We propose a system that\nsemi-autonomously executes complex cybersecurity workflows by employing\nmultiple LLMs modules to formulate attack strategies, generate commands, and\nanalyze results, thereby addressing the aforementioned challenges. In our\nexperiments using Hack The Box virtual machines, we confirmed that our system\ncan autonomously construct attack strategies, issue appropriate commands, and\nautomate certain processes, thereby reducing the need for manual intervention."
                },
                "authors": [
                    {
                        "name": "Masaya Kobayashi"
                    },
                    {
                        "name": "Masane Fuchi"
                    },
                    {
                        "name": "Amar Zanashir"
                    },
                    {
                        "name": "Tomonori Yoneda"
                    },
                    {
                        "name": "Tomohiro Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro Takagi"
                },
                "author": "Tomohiro Takagi",
                "arxiv_comment": "7 pages, 4 tables and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14155v2",
                "updated": "2025-02-21T14:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    57,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-19T23:51:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    23,
                    51,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Giving AI Personalities Leads to More Human-Like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giving AI Personalities Leads to More Human-Like Reasoning"
                },
                "summary": "In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning."
                },
                "authors": [
                    {
                        "name": "Animesh Nighojkar"
                    },
                    {
                        "name": "Bekhzodbek Moydinboyev"
                    },
                    {
                        "name": "My Duong"
                    },
                    {
                        "name": "John Licato"
                    }
                ],
                "author_detail": {
                    "name": "John Licato"
                },
                "author": "John Licato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15499v1",
                "updated": "2025-02-21T14:49:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    49,
                    34,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:49:34Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    49,
                    34,
                    4,
                    52,
                    0
                ],
                "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models"
                },
                "summary": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD."
                },
                "authors": [
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqing Li"
                },
                "author": "Xiaoqing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v2",
                "updated": "2025-02-21T14:41:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    41,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM"
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Code: https://github.com/IST-DASLab/DarwinLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12560v2",
                "updated": "2025-02-21T14:41:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    41,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-18T05:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    5,
                    54,
                    56,
                    1,
                    49,
                    0
                ],
                "title": "How does a Language-Specific Tokenizer affect LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does a Language-Specific Tokenizer affect LLMs?"
                },
                "summary": "The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Jean Seo"
                    },
                    {
                        "name": "Jaeyoon Kim"
                    },
                    {
                        "name": "SungJoo Byun"
                    },
                    {
                        "name": "Hyopil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyopil Shin"
                },
                "author": "Hyopil Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15493v1",
                "updated": "2025-02-21T14:36:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    36,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:36:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    36,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to\n  Responsibly Use Large Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to\n  Responsibly Use Large Language Models for Code Generation"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\ncode generation, offering immense potential to enhance programmer productivity.\nHowever, their non-deterministic nature and reliance on user input necessitate\na robust understanding of programming fundamentals to ensure their responsible\nand effective use. In this paper, we argue that foundational computing skills\nremain crucial in the age of LLMs. We propose a syllabus focused on equipping\ncomputer science students to responsibly embrace LLMs as performance\nenhancement tools. This work contributes to the discussion on the why, when,\nand how of integrating LLMs into computing education, aiming to better prepare\nprogrammers to leverage these tools without compromising foundational software\ndevelopment principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\ncode generation, offering immense potential to enhance programmer productivity.\nHowever, their non-deterministic nature and reliance on user input necessitate\na robust understanding of programming fundamentals to ensure their responsible\nand effective use. In this paper, we argue that foundational computing skills\nremain crucial in the age of LLMs. We propose a syllabus focused on equipping\ncomputer science students to responsibly embrace LLMs as performance\nenhancement tools. This work contributes to the discussion on the why, when,\nand how of integrating LLMs into computing education, aiming to better prepare\nprogrammers to leverage these tools without compromising foundational software\ndevelopment principles."
                },
                "authors": [
                    {
                        "name": "Bruno Pereira Cipriano"
                    },
                    {
                        "name": "Lcio Studer Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lcio Studer Ferreira"
                },
                "author": "Lcio Studer Ferreira",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14313v2",
                "updated": "2025-02-21T14:35:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    35,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2024-06-20T13:43:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    43,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability"
                },
                "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well."
                },
                "authors": [
                    {
                        "name": "Riya Sawhney"
                    },
                    {
                        "name": "Samrat Yadav"
                    },
                    {
                        "name": "Indrajit Bhattacharya"
                    },
                    {
                        "name": "Mausam"
                    }
                ],
                "author_detail": {
                    "name": "Mausam"
                },
                "author": "Mausam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11242v2",
                "updated": "2025-02-21T14:30:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    30,
                    31,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T19:39:48Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    19,
                    39,
                    48,
                    6,
                    47,
                    0
                ],
                "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection\n  Framework for Safe Child-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection\n  Framework for Safe Child-LLM Interaction"
                },
                "summary": "This study examines the growing use of Large Language Models (LLMs) in\nchild-centered applications, highlighting safety and ethical concerns such as\nbias, harmful content, and cultural insensitivity. Despite their potential to\nenhance learning, there is a lack of standardized frameworks to mitigate these\nrisks. Through a systematic literature review, we identify key parental and\nempirical concerns, including toxicity and ethical breaches in AI outputs.\nMoreover, to address these issues, this paper proposes a protection framework\nfor safe Child-LLM interaction, incorporating metrics for content safety,\nbehavioral ethics, and cultural sensitivity. The framework provides practical\ntools for evaluating LLM safety, offering guidance for developers,\npolicymakers, and educators to ensure responsible AI deployment for children.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the growing use of Large Language Models (LLMs) in\nchild-centered applications, highlighting safety and ethical concerns such as\nbias, harmful content, and cultural insensitivity. Despite their potential to\nenhance learning, there is a lack of standardized frameworks to mitigate these\nrisks. Through a systematic literature review, we identify key parental and\nempirical concerns, including toxicity and ethical breaches in AI outputs.\nMoreover, to address these issues, this paper proposes a protection framework\nfor safe Child-LLM interaction, incorporating metrics for content safety,\nbehavioral ethics, and cultural sensitivity. The framework provides practical\ntools for evaluating LLM safety, offering guidance for developers,\npolicymakers, and educators to ensure responsible AI deployment for children."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15488v1",
                "updated": "2025-02-21T14:26:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:26:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection"
                },
                "summary": "PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization."
                },
                "authors": [
                    {
                        "name": "Jiangyong Yu"
                    },
                    {
                        "name": "Changyong Shu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15487v1",
                "updated": "2025-02-21T14:23:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:23:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size."
                },
                "authors": [
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Serenna Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Lucia Passaro"
                    },
                    {
                        "name": "Irene Sucameli"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11910v2",
                "updated": "2025-02-21T14:17:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    17,
                    57,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-17T15:28:40Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    28,
                    40,
                    0,
                    48,
                    0
                ],
                "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives"
                },
                "summary": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes."
                },
                "authors": [
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Gauthier Gidel"
                    }
                ],
                "author_detail": {
                    "name": "Gauthier Gidel"
                },
                "author": "Gauthier Gidel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15472v1",
                "updated": "2025-02-21T13:55:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    55,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:55:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    55,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence"
                },
                "summary": "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution."
                },
                "authors": [
                    {
                        "name": "Yufeng Diao"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Changyang She"
                    },
                    {
                        "name": "Philip Guodong Zhao"
                    },
                    {
                        "name": "Emma Liying Li"
                    }
                ],
                "author_detail": {
                    "name": "Emma Liying Li"
                },
                "author": "Emma Liying Li",
                "arxiv_comment": "Accepted for publication in IEEE Journal on Selected Areas in\n  Communications (JSAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15470v1",
                "updated": "2025-02-21T13:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    52,
                    31,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:52:31Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    52,
                    31,
                    4,
                    52,
                    0
                ],
                "title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System"
                },
                "summary": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively."
                },
                "authors": [
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Haiyu Mao"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Juan Gmez-Luna"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13773v2",
                "updated": "2025-02-21T13:50:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    50,
                    25,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-23T15:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "title": "Do Large Language Models Truly Understand Geometric Structures?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Understand Geometric Structures?"
                },
                "summary": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03296v2",
                "updated": "2025-02-21T13:45:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    45,
                    51,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-04T10:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    12,
                    4,
                    278,
                    0
                ],
                "title": "Comparing zero-shot self-explanations with human rationales in text\n  classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing zero-shot self-explanations with human rationales in text\n  classification"
                },
                "summary": "Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification."
                },
                "authors": [
                    {
                        "name": "Stephanie Brandl"
                    },
                    {
                        "name": "Oliver Eberle"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Eberle"
                },
                "author": "Oliver Eberle",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15455v1",
                "updated": "2025-02-21T13:30:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:30:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning"
                },
                "summary": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA."
                },
                "authors": [
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15766v2",
                "updated": "2025-02-21T13:29:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    29,
                    42,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-24T05:44:46Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    44,
                    46,
                    1,
                    268,
                    0
                ],
                "title": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models"
                },
                "summary": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench."
                },
                "authors": [
                    {
                        "name": "Chenlu Guo"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15451v1",
                "updated": "2025-02-21T13:25:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    25,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:25:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    25,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs"
                },
                "summary": "MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view."
                },
                "authors": [
                    {
                        "name": "Yuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Sun"
                },
                "author": "Yuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15443v1",
                "updated": "2025-02-21T13:11:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    11,
                    22,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:11:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    11,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models"
                },
                "summary": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed."
                },
                "authors": [
                    {
                        "name": "Weilan Wang"
                    },
                    {
                        "name": "Yu Mao"
                    },
                    {
                        "name": "Dongdong Tang"
                    },
                    {
                        "name": "Hongchao Du"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Chun Jason Xue"
                    }
                ],
                "author_detail": {
                    "name": "Chun Jason Xue"
                },
                "author": "Chun Jason Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15441v1",
                "updated": "2025-02-21T13:09:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    9,
                    58,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:09:58Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    9,
                    58,
                    4,
                    52,
                    0
                ],
                "title": "On the Effectiveness of Large Language Models in Writing Alloy Formulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Large Language Models in Writing Alloy Formulas"
                },
                "summary": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software."
                },
                "authors": [
                    {
                        "name": "Yang Hong"
                    },
                    {
                        "name": "Shan Jiang"
                    },
                    {
                        "name": "Yulei Fu"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    }
                ],
                "author_detail": {
                    "name": "Sarfraz Khurshid"
                },
                "author": "Sarfraz Khurshid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15436v1",
                "updated": "2025-02-21T13:05:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    5,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:05:19Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    5,
                    19,
                    4,
                    52,
                    0
                ],
                "title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb."
                },
                "authors": [
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Rohit Vartak"
                    },
                    {
                        "name": "Lav R. Varshney"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vepakomma"
                },
                "author": "Praneeth Vepakomma",
                "arxiv_comment": "Raghav Singhal and Kaustubh Ponkshe contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15435v1",
                "updated": "2025-02-21T13:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    4,
                    13,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    4,
                    13,
                    4,
                    52,
                    0
                ],
                "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-pass Detection of Jailbreaking Input in Large Language Models"
                },
                "summary": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks."
                },
                "authors": [
                    {
                        "name": "Leyla Naz Candogan"
                    },
                    {
                        "name": "Yongtao Wu"
                    },
                    {
                        "name": "Elias Abad Rocamora"
                    },
                    {
                        "name": "Grigorios G. Chrysos"
                    },
                    {
                        "name": "Volkan Cevher"
                    }
                ],
                "author_detail": {
                    "name": "Volkan Cevher"
                },
                "author": "Volkan Cevher",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15434v1",
                "updated": "2025-02-21T13:01:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:01:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation"
                },
                "summary": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge."
                },
                "authors": [
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v1",
                "updated": "2025-02-21T12:54:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15427v1",
                "updated": "2025-02-21T12:54:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    25,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:54:25Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    25,
                    4,
                    52,
                    0
                ],
                "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails\n  Against Prompt Input Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails\n  Against Prompt Input Attacks on LLMs"
                },
                "summary": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation."
                },
                "authors": [
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hameed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    },
                    {
                        "name": "Kush Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush Varshney"
                },
                "author": "Kush Varshney",
                "arxiv_comment": "NeurIPS 2024, Safe Generative AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18220v3",
                "updated": "2025-02-21T12:44:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    44,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2024-11-27T10:57:06Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge"
                },
                "summary": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Mohsen Pourghasemian"
                    },
                    {
                        "name": "Haris Gacanin"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "arxiv_journal_ref": "2025 IEEE International Conference on Communications (ICC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12835v2",
                "updated": "2025-02-21T12:41:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    41,
                    6,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-22T12:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
                },
                "summary": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Maria Lysyuk"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "The code and data are at https://github.com/s-nlp/AdaRAGUE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15419v1",
                "updated": "2025-02-21T12:38:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    38,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:38:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    38,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking"
                },
                "summary": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation."
                },
                "authors": [
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Aurora Cobo"
                    },
                    {
                        "name": "Pablo Serna"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Serna"
                },
                "author": "Pablo Serna",
                "arxiv_comment": "15 pages, 1 figure, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15418v1",
                "updated": "2025-02-21T12:37:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    37,
                    58,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:37:58Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    37,
                    58,
                    4,
                    52,
                    0
                ],
                "title": "MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models"
                },
                "summary": "Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores."
                },
                "authors": [
                    {
                        "name": "Suraj Racha"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Anshika Raman"
                    },
                    {
                        "name": "Nikita Jangid"
                    },
                    {
                        "name": "Mridul Sharma"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Nirmal Punjabi"
                    }
                ],
                "author_detail": {
                    "name": "Nirmal Punjabi"
                },
                "author": "Nirmal Punjabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15412v1",
                "updated": "2025-02-21T12:21:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    21,
                    9,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:21:09Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    21,
                    9,
                    4,
                    52,
                    0
                ],
                "title": "Textual-to-Visual Iterative Self-Verification for Slide Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual-to-Visual Iterative Self-Verification for Slide Generation"
                },
                "summary": "Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability."
                },
                "authors": [
                    {
                        "name": "Yunqing Xu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15411v1",
                "updated": "2025-02-21T12:19:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings"
                },
                "summary": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts"
                },
                "authors": [
                    {
                        "name": "Rasmus Aavang"
                    },
                    {
                        "name": "Giovanni Rizzi"
                    },
                    {
                        "name": "Rasmus Bggild"
                    },
                    {
                        "name": "Alexandre Iolov"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10255v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10255v3",
                "updated": "2025-02-21T12:07:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    7,
                    3,
                    4,
                    52,
                    0
                ],
                "published": "2024-04-16T03:18:27Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    18,
                    27,
                    1,
                    107,
                    0
                ],
                "title": "Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems"
                },
                "summary": "On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\ninference without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, enhancing data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\ninference without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, enhancing data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Tianliu He"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10255v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10255v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15401v1",
                "updated": "2025-02-21T12:00:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    0,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:00:10Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    0,
                    10,
                    4,
                    52,
                    0
                ],
                "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning"
                },
                "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently."
                },
                "authors": [
                    {
                        "name": "Xuetao Ma"
                    },
                    {
                        "name": "Wenbin Jiang"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02320v3",
                "updated": "2025-02-21T11:53:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    53,
                    53,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-03T08:56:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    56,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "Post-edits Are Preferences Too",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-edits Are Preferences Too"
                },
                "summary": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks."
                },
                "authors": [
                    {
                        "name": "Nathaniel Berger"
                    },
                    {
                        "name": "Miriam Exel"
                    },
                    {
                        "name": "Matthias Huck"
                    },
                    {
                        "name": "Stefan Riezler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Riezler"
                },
                "author": "Stefan Riezler",
                "arxiv_comment": "To appear at the Ninth Conference on Machine Translation (WMT24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15395v1",
                "updated": "2025-02-21T11:46:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    46,
                    4,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T11:46:04Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    46,
                    4,
                    4,
                    52,
                    0
                ],
                "title": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making"
                },
                "summary": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Eunhye Kim"
                    },
                    {
                        "name": "Kiroong Choe"
                    },
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Sadat Shams Chowdhury"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15392v1",
                "updated": "2025-02-21T11:38:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    38,
                    40,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T11:38:40Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    38,
                    40,
                    4,
                    52,
                    0
                ],
                "title": "Chitrarth: Bridging Vision and Language for a Billion People",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chitrarth: Bridging Vision and Language for a Billion People"
                },
                "summary": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena."
                },
                "authors": [
                    {
                        "name": "Shaharukh Khan"
                    },
                    {
                        "name": "Ayush Tarun"
                    },
                    {
                        "name": "Abhinav Ravi"
                    },
                    {
                        "name": "Ali Faraz"
                    },
                    {
                        "name": "Akshat Patidar"
                    },
                    {
                        "name": "Praveen Kumar Pokala"
                    },
                    {
                        "name": "Anagha Bhangare"
                    },
                    {
                        "name": "Raja Kolla"
                    },
                    {
                        "name": "Chandra Khatri"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04931v2",
                "updated": "2025-02-21T11:10:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    10,
                    15,
                    4,
                    52,
                    0
                ],
                "published": "2023-12-08T09:48:36Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    9,
                    48,
                    36,
                    4,
                    342,
                    0
                ],
                "title": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models"
                },
                "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Cuiling Lan"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Xuejin Chen"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11574v2",
                "updated": "2025-02-21T11:04:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    4,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-17T09:07:32Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    9,
                    7,
                    32,
                    0,
                    48,
                    0
                ],
                "title": "Large Language Models and Mathematical Reasoning Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Mathematical Reasoning Failures"
                },
                "summary": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling."
                },
                "authors": [
                    {
                        "name": "Johan Boye"
                    },
                    {
                        "name": "Birger Moell"
                    }
                ],
                "author_detail": {
                    "name": "Birger Moell"
                },
                "author": "Birger Moell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11102v2",
                "updated": "2025-02-21T10:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    54,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T12:38:37Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    38,
                    37,
                    6,
                    47,
                    0
                ],
                "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling"
                },
                "summary": "Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH."
                },
                "authors": [
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Zhonglin Xie"
                    },
                    {
                        "name": "Yaoyu Wu"
                    },
                    {
                        "name": "Can Ren"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen",
                "arxiv_comment": "This paper has 36 pages, 18 figures, and two co-first authors:\n  Hongliang Lu and Zhonglin Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15370v1",
                "updated": "2025-02-21T10:42:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    42,
                    4,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:42:04Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    42,
                    4,
                    4,
                    52,
                    0
                ],
                "title": "Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision"
                },
                "summary": "Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality."
                },
                "authors": [
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Kanghoon Yoon"
                    },
                    {
                        "name": "Yeonjun In"
                    },
                    {
                        "name": "Jaehyeong Jeon"
                    },
                    {
                        "name": "Jinyoung Moon"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "10 pages, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11187v2",
                "updated": "2025-02-21T10:33:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    33,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T16:22:23Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    22,
                    23,
                    6,
                    47,
                    0
                ],
                "title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking"
                },
                "summary": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a)."
                },
                "authors": [
                    {
                        "name": "Shahriar Kabir Nahin"
                    },
                    {
                        "name": "Rabindra Nath Nandi"
                    },
                    {
                        "name": "Sagor Sarker"
                    },
                    {
                        "name": "Quazi Sarwar Muhtaseem"
                    },
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Apu Chandraw Shill"
                    },
                    {
                        "name": "Md Ibrahim"
                    },
                    {
                        "name": "Mehadi Hasan Menon"
                    },
                    {
                        "name": "Tareq Al Muntasir"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "LLMs, Benchmarking, Large Language Models, Bangla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15365v1",
                "updated": "2025-02-21T10:27:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    28,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    28,
                    4,
                    52,
                    0
                ],
                "title": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses"
                },
                "summary": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction."
                },
                "authors": [
                    {
                        "name": "Kang Bongsu"
                    },
                    {
                        "name": "Kim Jundong"
                    },
                    {
                        "name": "Yun Tae-Rim"
                    },
                    {
                        "name": "Bae Hyojin"
                    },
                    {
                        "name": "Kim Chang-Eop"
                    }
                ],
                "author_detail": {
                    "name": "Kim Chang-Eop"
                },
                "author": "Kim Chang-Eop",
                "arxiv_comment": "11 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14669v2",
                "updated": "2025-02-21T10:27:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T16:05:18Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    }
                ],
                "author_detail": {
                    "name": "Dinh Bach Vu"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Dinh Bach Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13506v2",
                "updated": "2025-02-21T10:18:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    18,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-19T07:50:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    50,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Reproducing NevIR: Negation in Neural Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing NevIR: Negation in Neural Information Retrieval"
                },
                "summary": "Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks."
                },
                "authors": [
                    {
                        "name": "Coen van den Elsen"
                    },
                    {
                        "name": "Francien Barkhof"
                    },
                    {
                        "name": "Thijmen Nijdam"
                    },
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Alliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alliannejadi"
                },
                "author": "Mohammad Alliannejadi",
                "arxiv_comment": "9 pages, 5 figures, under review at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15361v1",
                "updated": "2025-02-21T10:16:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    16,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:16:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    16,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Evaluating Social Biases in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Social Biases in LLM Reasoning"
                },
                "summary": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Xuyang Wu"
                    },
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Zhiqiang Tao"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15359v1",
                "updated": "2025-02-21T10:14:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    14,
                    55,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:14:55Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    14,
                    55,
                    4,
                    52,
                    0
                ],
                "title": "ARS: Automatic Routing Solver with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARS: Automatic Routing Solver with Large Language Models"
                },
                "summary": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15349v1",
                "updated": "2025-02-21T10:06:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    6,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:06:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    6,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms"
                },
                "summary": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine."
                },
                "authors": [
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Zhi Yang"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15348v1",
                "updated": "2025-02-21T10:02:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    2,
                    15,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:02:15Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    2,
                    15,
                    4,
                    52,
                    0
                ],
                "title": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models"
                },
                "summary": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Fan Wei"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yanyan Yu"
                    },
                    {
                        "name": "Jianli Chen"
                    },
                    {
                        "name": "Zipo Cai"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Wang"
                },
                "author": "Zhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14377v2",
                "updated": "2025-02-21T10:02:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    2,
                    2,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T09:10:05Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    10,
                    5,
                    3,
                    51,
                    0
                ],
                "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers"
                },
                "summary": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta."
                },
                "authors": [
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Jiasong Feng"
                    },
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_comment": "Homepage: https://360cvgroup.github.io/RelaCtrl/ Github:\n  https://github.com/360CVGroup/RelaCtrl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15343v1",
                "updated": "2025-02-21T09:58:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    58,
                    54,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:58:54Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    58,
                    54,
                    4,
                    52,
                    0
                ],
                "title": "Tokenization is Sensitive to Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is Sensitive to Language Variation"
                },
                "summary": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance."
                },
                "authors": [
                    {
                        "name": "Anna Wegmann"
                    },
                    {
                        "name": "Dong Nguyen"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15336v1",
                "updated": "2025-02-21T09:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    41,
                    27,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:41:27Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    41,
                    27,
                    4,
                    52,
                    0
                ],
                "title": "Exploring Embodied Multimodal Large Models: Development, Datasets, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Embodied Multimodal Large Models: Development, Datasets, and\n  Future Directions"
                },
                "summary": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains."
                },
                "authors": [
                    {
                        "name": "Shoubin Chen"
                    },
                    {
                        "name": "Zehao Wu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chunyu Li"
                    },
                    {
                        "name": "Baiyang Zhang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Fei Richard Yu"
                    },
                    {
                        "name": "Qingquan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingquan Li"
                },
                "author": "Qingquan Li",
                "arxiv_comment": "81 pages, submitted to a journal for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15335v1",
                "updated": "2025-02-21T09:39:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    39,
                    27,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:39:27Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    39,
                    27,
                    4,
                    52,
                    0
                ],
                "title": "Stepwise Informativeness Search for Improving LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Informativeness Search for Improving LLM Reasoning"
                },
                "summary": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Enda Zhao"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15334v1",
                "updated": "2025-02-21T09:38:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:38:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment"
                },
                "summary": "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time)."
                },
                "authors": [
                    {
                        "name": "Pedram Zaree"
                    },
                    {
                        "name": "Md Abdullah Al Mamun"
                    },
                    {
                        "name": "Quazi Mishkatul Alam"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Ihsen Alouani"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15332v1",
                "updated": "2025-02-21T09:34:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    34,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:34:34Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    34,
                    4,
                    52,
                    0
                ],
                "title": "Detecting Future-related Contexts of Entity Mentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Future-related Contexts of Entity Mentions"
                },
                "summary": "The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences."
                },
                "authors": [
                    {
                        "name": "Puneet Prashar"
                    },
                    {
                        "name": "Krishna Mohan Shukla"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15312v1",
                "updated": "2025-02-21T09:06:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    6,
                    2,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:06:02Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    6,
                    2,
                    4,
                    52,
                    0
                ],
                "title": "FlexPie: Accelerate Distributed Inference on Edge Devices with Flexible\n  Combinatorial Optimization[Technical Report]",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPie: Accelerate Distributed Inference on Edge Devices with Flexible\n  Combinatorial Optimization[Technical Report]"
                },
                "summary": "The rapid advancement of deep learning has catalyzed the development of novel\nIoT applications, which often deploy pre-trained deep neural network (DNN)\nmodels across multiple edge devices for collaborative inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of deep learning has catalyzed the development of novel\nIoT applications, which often deploy pre-trained deep neural network (DNN)\nmodels across multiple edge devices for collaborative inference."
                },
                "authors": [
                    {
                        "name": "Runhua Zhang"
                    },
                    {
                        "name": "Hongxu Jiang"
                    },
                    {
                        "name": "Jinkun Geng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Chenhui Zhu"
                    },
                    {
                        "name": "Haojie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haojie Wang"
                },
                "author": "Haojie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15302v1",
                "updated": "2025-02-21T08:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    50,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:50:39Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    50,
                    39,
                    4,
                    52,
                    0
                ],
                "title": "A Novel Riemannian Sparse Representation Learning Network for\n  Polarimetric SAR Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Riemannian Sparse Representation Learning Network for\n  Polarimetric SAR Image Classification"
                },
                "summary": "Deep learning is an effective end-to-end method for Polarimetric Synthetic\nAperture Radar(PolSAR) image classification, but it lacks the guidance of\nrelated mathematical principle and is essentially a black-box model. In\naddition, existing deep models learn features in Euclidean space, where PolSAR\ncomplex matrix is commonly converted into a complex-valued vector as the\nnetwork input, distorting matrix structure and channel relationship. However,\nthe complex covariance matrix is Hermitian positive definite (HPD), and resides\non a Riemannian manifold instead of a Euclidean one. Existing methods cannot\nmeasure the geometric distance of HPD matrices and easily cause some\nmisclassifications due to inappropriate Euclidean measures. To address these\nissues, we propose a novel Riemannian Sparse Representation Learning Network\n(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse\nRepresentation (SRSR) model is designed to learn the sparse features with\nRiemannian metric. Then, the optimization procedure of the SRSR model is\ninferred and further unfolded into an SRSRnet, which can automatically learn\nthe sparse coefficients and dictionary atoms. Furthermore, to learn contextual\nhigh-level features, a CNN-enhanced module is added to improve classification\nperformance. The proposed network is a Sparse Representation (SR) guided deep\nlearning model, which can directly utilize the covariance matrix as the network\ninput, and utilize Riemannian metric to learn geometric structure and sparse\nfeatures of complex matrices in Riemannian space. Experiments on three real\nPolSAR datasets demonstrate that the proposed method surpasses state-of-the-art\ntechniques in ensuring accurate edge details and correct region homogeneity for\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning is an effective end-to-end method for Polarimetric Synthetic\nAperture Radar(PolSAR) image classification, but it lacks the guidance of\nrelated mathematical principle and is essentially a black-box model. In\naddition, existing deep models learn features in Euclidean space, where PolSAR\ncomplex matrix is commonly converted into a complex-valued vector as the\nnetwork input, distorting matrix structure and channel relationship. However,\nthe complex covariance matrix is Hermitian positive definite (HPD), and resides\non a Riemannian manifold instead of a Euclidean one. Existing methods cannot\nmeasure the geometric distance of HPD matrices and easily cause some\nmisclassifications due to inappropriate Euclidean measures. To address these\nissues, we propose a novel Riemannian Sparse Representation Learning Network\n(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse\nRepresentation (SRSR) model is designed to learn the sparse features with\nRiemannian metric. Then, the optimization procedure of the SRSR model is\ninferred and further unfolded into an SRSRnet, which can automatically learn\nthe sparse coefficients and dictionary atoms. Furthermore, to learn contextual\nhigh-level features, a CNN-enhanced module is added to improve classification\nperformance. The proposed network is a Sparse Representation (SR) guided deep\nlearning model, which can directly utilize the covariance matrix as the network\ninput, and utilize Riemannian metric to learn geometric structure and sparse\nfeatures of complex matrices in Riemannian space. Experiments on three real\nPolSAR datasets demonstrate that the proposed method surpasses state-of-the-art\ntechniques in ensuring accurate edge details and correct region homogeneity for\nclassification."
                },
                "authors": [
                    {
                        "name": "Junfei Shi"
                    },
                    {
                        "name": "Mengmeng Nie"
                    },
                    {
                        "name": "Weisi Lin"
                    },
                    {
                        "name": "Haiyan Jin"
                    },
                    {
                        "name": "Junhuai Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15300v1",
                "updated": "2025-02-21T08:47:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    47,
                    17,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:47:17Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    47,
                    17,
                    4,
                    52,
                    0
                ],
                "title": "Machine Learning in Stellar Astronomy: Progress up to 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning in Stellar Astronomy: Progress up to 2024"
                },
                "summary": "Machine learning (ML) has become a key tool in astronomy, driving\nadvancements in the analysis and interpretation of complex datasets from\nobservations. This article reviews the application of ML techniques in the\nidentification and classification of stellar objects, alongside the inference\nof their key astrophysical properties. We highlight the role of both supervised\nand unsupervised ML algorithms, particularly deep learning models, in\nclassifying stars and enhancing our understanding of essential stellar\nparameters, such as mass, age, and chemical composition. We discuss ML\napplications in the study of various stellar objects, including binaries,\nsupernovae, dwarfs, young stellar objects, variables, metal-poor, and\nchemically peculiar stars. Additionally, we examine the role of ML in\ninvestigating star-related interstellar medium objects, such as protoplanetary\ndisks, planetary nebulae, cold neutral medium, feedback bubbles, and molecular\nclouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has become a key tool in astronomy, driving\nadvancements in the analysis and interpretation of complex datasets from\nobservations. This article reviews the application of ML techniques in the\nidentification and classification of stellar objects, alongside the inference\nof their key astrophysical properties. We highlight the role of both supervised\nand unsupervised ML algorithms, particularly deep learning models, in\nclassifying stars and enhancing our understanding of essential stellar\nparameters, such as mass, age, and chemical composition. We discuss ML\napplications in the study of various stellar objects, including binaries,\nsupernovae, dwarfs, young stellar objects, variables, metal-poor, and\nchemically peculiar stars. Additionally, we examine the role of ML in\ninvestigating star-related interstellar medium objects, such as protoplanetary\ndisks, planetary nebulae, cold neutral medium, feedback bubbles, and molecular\nclouds."
                },
                "authors": [
                    {
                        "name": "Guangping Li"
                    },
                    {
                        "name": "Zujia Lu"
                    },
                    {
                        "name": "Junzhi Wang"
                    },
                    {
                        "name": "Zhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Wang"
                },
                "author": "Zhao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.13148v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v3",
                "updated": "2025-02-21T18:59:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: SGD with Normalization and Whitening Enables Stateless LLM\n  Training"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "arxiv_comment": "In v2 we have revised the related work, added more comprehensive\n  citations, and clarified our key contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15681v1",
                "updated": "2025-02-21T18:59:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    20,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:59:20Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    20,
                    4,
                    52,
                    0
                ],
                "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-step Diffusion Models with $f$-Divergence Distribution Matching"
                },
                "summary": "Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill"
                },
                "authors": [
                    {
                        "name": "Yilun Xu"
                    },
                    {
                        "name": "Weili Nie"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15680v1",
                "updated": "2025-02-21T18:59:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:59:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    59,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training"
                },
                "summary": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation."
                },
                "authors": [
                    {
                        "name": "Jaydeep Borkar"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "David A. Smith"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    }
                ],
                "author_detail": {
                    "name": "Christopher A. Choquette-Choo"
                },
                "author": "Christopher A. Choquette-Choo",
                "arxiv_comment": "23 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15677v1",
                "updated": "2025-02-21T18:58:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    58,
                    6,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:58:06Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    58,
                    6,
                    4,
                    52,
                    0
                ],
                "title": "FLEKE: Federated Locate-then-Edit Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLEKE: Federated Locate-then-Edit Knowledge Editing"
                },
                "summary": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating\nlarge language models (LLMs) without full retraining. However, existing methods\nassume a single-user setting and become inefficient in real-world multi-client\nscenarios, where decentralized organizations (e.g., hospitals, financial\ninstitutions) independently update overlapping knowledge, leading to redundant\nmediator knowledge vector (MKV) computations and privacy concerns. To address\nthese challenges, we introduce Federated Locate-then-Edit Knowledge Editing\n(FLEKE), a novel task that enables multiple clients to collaboratively perform\nLEKE while preserving privacy and reducing computational overhead. To achieve\nthis, we propose FedEdit, a two-stage framework that optimizes MKV selection\nand reuse. In the first stage, clients locally apply LEKE and upload the\ncomputed MKVs. In the second stage, rather than relying solely on server-based\nMKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine\nsimilarity, enabling knowledge re-edit and minimizing redundant computations.\nExperimental results on two benchmark datasets demonstrate that FedEdit retains\nover 96% of the performance of non-federated LEKE while significantly\noutperforming a FedAvg-based baseline by approximately twofold. Besides, we\nfind that MEMIT performs more consistently than PMET in the FLEKE task with our\nFedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE."
                },
                "authors": [
                    {
                        "name": "Zongkai Zhao"
                    },
                    {
                        "name": "Guozeng Xu"
                    },
                    {
                        "name": "Xiuhua Li"
                    },
                    {
                        "name": "Kaiwen Wei"
                    },
                    {
                        "name": "Jiang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Zhong"
                },
                "author": "Jiang Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15676v1",
                "updated": "2025-02-21T18:57:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    57,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:57:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    57,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoToM: Automated Bayesian Inverse Planning and Model Discovery for\n  Open-ended Theory of Mind"
                },
                "summary": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM), the ability to understand people's mental variables\nbased on their behavior, is key to developing socially intelligent agents.\nCurrent approaches to Theory of Mind reasoning either rely on prompting Large\nLanguage Models (LLMs), which are prone to systematic errors, or use rigid,\nhandcrafted Bayesian Theory of Mind (BToM) models, which are more robust but\ncannot generalize across different domains. In this work, we introduce AutoToM,\nan automated Bayesian Theory of Mind method for achieving open-ended machine\nTheory of Mind. AutoToM can operate in any domain, infer any mental variable,\nand conduct robust Theory of Mind reasoning of any order. Given a Theory of\nMind inference problem, AutoToM first proposes an initial BToM model. It then\nconducts automated Bayesian inverse planning based on the proposed model,\nleveraging an LLM as the backend. Based on the uncertainty of the inference, it\niteratively refines the model, by introducing additional mental variables\nand/or incorporating more timesteps in the context. Empirical evaluations\nacross multiple Theory of Mind benchmarks demonstrate that AutoToM consistently\nachieves state-of-the-art performance, offering a scalable, robust, and\ninterpretable approach to machine Theory of Mind."
                },
                "authors": [
                    {
                        "name": "Zhining Zhang"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Mung Yao Jia"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "23 pages, 6 figures, 11 tables. Website at\n  https://chuanyangjin.com/AutoToM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15666v1",
                "updated": "2025-02-21T18:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    45,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:45:37Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    45,
                    37,
                    4,
                    52,
                    0
                ],
                "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing"
                },
                "summary": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Misclassification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate eleven\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains $11.7K$ samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently\nmisclassify even minimally polished text as AI-generated, struggle to\ndifferentiate between degrees of AI involvement, and exhibit biases against\nolder and smaller models. These limitations highlight the urgent need for more\nnuanced detection methodologies."
                },
                "authors": [
                    {
                        "name": "Shoumik Saha"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "17 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09760v3",
                "updated": "2025-02-21T18:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    40,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-15T15:01:00Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    1,
                    0,
                    6,
                    259,
                    0
                ],
                "title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing"
                },
                "summary": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools."
                },
                "authors": [
                    {
                        "name": "Suhyeon Yoo"
                    },
                    {
                        "name": "Khai N. Truong"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/elmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v1",
                "updated": "2025-02-21T18:22:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15652v1",
                "updated": "2025-02-21T18:20:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    20,
                    35,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:20:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    20,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey"
                },
                "summary": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously."
                },
                "authors": [
                    {
                        "name": "Fengxiang Cheng"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Fenrong Liu"
                    },
                    {
                        "name": "Robert van Rooij"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15649v1",
                "updated": "2025-02-21T18:16:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    16,
                    5,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:16:05Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    16,
                    5,
                    4,
                    52,
                    0
                ],
                "title": "A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement\n  Learning Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement\n  Learning Applications"
                },
                "summary": "Reinforcement learning (RL) has gained traction for its success in solving\ncomplex tasks for robotic applications. However, its deployment on physical\nrobots remains challenging due to safety risks and the comparatively high costs\nof training. To avoid these problems, RL agents are often trained on\nsimulators, which introduces a new problem related to the gap between\nsimulation and reality. This paper presents an RL pipeline designed to help\nreduce the reality gap and facilitate developing and deploying RL policies for\nreal-world robotic systems. The pipeline organizes the RL training process into\nan initial step for system identification and three training stages: core\nsimulation training, high-fidelity simulation, and real-world deployment, each\nadding levels of realism to reduce the sim-to-real gap. Each training stage\ntakes an input policy, improves it, and either passes the improved policy to\nthe next stage or loops it back for further improvement. This iterative process\ncontinues until the policy achieves the desired performance. The pipeline's\neffectiveness is shown through a case study with the Boston Dynamics Spot\nmobile robot used in a surveillance application. The case study presents the\nsteps taken at each pipeline stage to obtain an RL agent to control the robot's\nposition and orientation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has gained traction for its success in solving\ncomplex tasks for robotic applications. However, its deployment on physical\nrobots remains challenging due to safety risks and the comparatively high costs\nof training. To avoid these problems, RL agents are often trained on\nsimulators, which introduces a new problem related to the gap between\nsimulation and reality. This paper presents an RL pipeline designed to help\nreduce the reality gap and facilitate developing and deploying RL policies for\nreal-world robotic systems. The pipeline organizes the RL training process into\nan initial step for system identification and three training stages: core\nsimulation training, high-fidelity simulation, and real-world deployment, each\nadding levels of realism to reduce the sim-to-real gap. Each training stage\ntakes an input policy, improves it, and either passes the improved policy to\nthe next stage or loops it back for further improvement. This iterative process\ncontinues until the policy achieves the desired performance. The pipeline's\neffectiveness is shown through a case study with the Boston Dynamics Spot\nmobile robot used in a surveillance application. The case study presents the\nsteps taken at each pipeline stage to obtain an RL agent to control the robot's\nposition and orientation."
                },
                "authors": [
                    {
                        "name": "Jefferson Silveira"
                    },
                    {
                        "name": "Joshua A. Marshall"
                    },
                    {
                        "name": "Sidney N. Givigi Jr"
                    }
                ],
                "author_detail": {
                    "name": "Sidney N. Givigi Jr"
                },
                "author": "Sidney N. Givigi Jr",
                "arxiv_comment": "Paper accepted to be presented at IEEE SysCon 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08288v2",
                "updated": "2025-02-21T18:00:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    0,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-10T18:20:44Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    18,
                    20,
                    44,
                    3,
                    284,
                    0
                ],
                "title": "Towards Foundation Models for Mixed Integer Linear Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Foundation Models for Mixed Integer Linear Programming"
                },
                "summary": "Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Integer Linear Programming (MILP) is essential for modeling complex\ndecision-making problems but faces challenges in computational tractability and\nrequires expert formulation. Current deep learning approaches for MILP focus on\nspecific problem classes and do not generalize to unseen classes. To address\nthis shortcoming, we take a foundation model training approach, where we train\na single deep learning model on a diverse set of MILP problems to generalize\nacross problem classes. As existing datasets for MILP lack diversity and\nvolume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that\nis capable of generating a large set of diverse MILP classes with an unlimited\namount of instances. We study our methodology on three key learning tasks that\ncapture diverse aspects of MILP: (1) integrality gap prediction, (2) learning\nto branch, and (3) a new task of aligning MILP instances with natural language\ndescriptions. Our empirical results show that models trained on the data\ngenerated by MILP-Evolve achieve significant improvements on unseen problems,\nincluding MIPLIB benchmarks. Our work highlights the potential of moving\ntowards a foundation model approach for MILP that can generalize to a broad\nrange of MILP applications. Our code and data are publicly available at\nhttps://github.com/microsoft/OptiGuide."
                },
                "authors": [
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Janardhan Kulkarni"
                    },
                    {
                        "name": "Ishai Menache"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Beibin Li"
                    }
                ],
                "author_detail": {
                    "name": "Beibin Li"
                },
                "author": "Beibin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15632v1",
                "updated": "2025-02-21T18:00:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    0,
                    40,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T18:00:40Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    0,
                    40,
                    4,
                    52,
                    0
                ],
                "title": "Continual Person Identification using Footstep-Induced Floor Vibrations\n  on Heterogeneous Floor Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Person Identification using Footstep-Induced Floor Vibrations\n  on Heterogeneous Floor Structures"
                },
                "summary": "Person identification is important for smart buildings to provide\npersonalized services such as health monitoring, activity tracking, and\npersonnel management. However, previous person identification relies on\npre-collected data from everyone, which is impractical in many buildings and\npublic facilities in which visitors are typically expected. This calls for a\ncontinual person identification system that gradually learns people's\nidentities on the fly. Existing studies use cameras to achieve this goal, but\nthey require direct line-of-sight and also have raised privacy concerns in\npublic. Other modalities such as wearables and pressure mats are limited by the\nrequirement of device-carrying or dense deployment. Thus, prior studies\nintroduced footstep-induced structural vibration sensing, which is\nnon-intrusive and perceived as more privacy-friendly. However, this approach\nhas a significant challenge: the high variability of vibration data due to\nstructural heterogeneity and human gait variations, which makes online person\nidentification algorithms perform poorly. In this paper, we characterize the\nvariability in footstep-induced structural vibration data for accurate online\nperson identification. To achieve this, we quantify and decompose different\nsources of variability and then design a feature transformation function to\nreduce the variability within each person's data to make different people's\ndata more separable. We evaluate our approach through field experiments with 20\npeople. The results show a 70% variability reduction and a 90% accuracy for\nonline person identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Person identification is important for smart buildings to provide\npersonalized services such as health monitoring, activity tracking, and\npersonnel management. However, previous person identification relies on\npre-collected data from everyone, which is impractical in many buildings and\npublic facilities in which visitors are typically expected. This calls for a\ncontinual person identification system that gradually learns people's\nidentities on the fly. Existing studies use cameras to achieve this goal, but\nthey require direct line-of-sight and also have raised privacy concerns in\npublic. Other modalities such as wearables and pressure mats are limited by the\nrequirement of device-carrying or dense deployment. Thus, prior studies\nintroduced footstep-induced structural vibration sensing, which is\nnon-intrusive and perceived as more privacy-friendly. However, this approach\nhas a significant challenge: the high variability of vibration data due to\nstructural heterogeneity and human gait variations, which makes online person\nidentification algorithms perform poorly. In this paper, we characterize the\nvariability in footstep-induced structural vibration data for accurate online\nperson identification. To achieve this, we quantify and decompose different\nsources of variability and then design a feature transformation function to\nreduce the variability within each person's data to make different people's\ndata more separable. We evaluate our approach through field experiments with 20\npeople. The results show a 70% variability reduction and a 90% accuracy for\nonline person identification."
                },
                "authors": [
                    {
                        "name": "Yiwen Dong"
                    },
                    {
                        "name": "Hae Young Noh"
                    }
                ],
                "author_detail": {
                    "name": "Hae Young Noh"
                },
                "author": "Hae Young Noh",
                "arxiv_doi": "10.1016/j.ymssp.2023.110756",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ymssp.2023.110756",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.15632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Mechanical Systems and Signal Processing, 2023",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15630v1",
                "updated": "2025-02-21T17:57:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    57,
                    55,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:57:55Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    57,
                    55,
                    4,
                    52,
                    0
                ],
                "title": "Reduced-Order Model Guided Contact-Implicit Model Predictive Control for\n  Humanoid Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reduced-Order Model Guided Contact-Implicit Model Predictive Control for\n  Humanoid Locomotion"
                },
                "summary": "Humanoid robots have great potential for real-world applications due to their\nability to operate in environments built for humans, but their deployment is\nhindered by the challenge of controlling their underlying high-dimensional\nnonlinear hybrid dynamics. While reduced-order models like the Hybrid Linear\nInverted Pendulum (HLIP) are simple and computationally efficient, they lose\nwhole-body expressiveness. Meanwhile, recent advances in Contact-Implicit Model\nPredictive Control (CI-MPC) enable robots to plan through multiple hybrid\ncontact modes, but remain vulnerable to local minima and require significant\ntuning. We propose a control framework that combines the strengths of HLIP and\nCI-MPC. The reduced-order model generates a nominal gait, while CI-MPC manages\nthe whole-body dynamics and modifies the contact schedule as needed. We\ndemonstrate the effectiveness of this approach in simulation with a novel 24\ndegree-of-freedom humanoid robot: Achilles. Our proposed framework achieves\nrough terrain walking, disturbance recovery, robustness under model and state\nuncertainty, and allows the robot to interact with obstacles in the\nenvironment, all while running online in real-time at 50 Hz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robots have great potential for real-world applications due to their\nability to operate in environments built for humans, but their deployment is\nhindered by the challenge of controlling their underlying high-dimensional\nnonlinear hybrid dynamics. While reduced-order models like the Hybrid Linear\nInverted Pendulum (HLIP) are simple and computationally efficient, they lose\nwhole-body expressiveness. Meanwhile, recent advances in Contact-Implicit Model\nPredictive Control (CI-MPC) enable robots to plan through multiple hybrid\ncontact modes, but remain vulnerable to local minima and require significant\ntuning. We propose a control framework that combines the strengths of HLIP and\nCI-MPC. The reduced-order model generates a nominal gait, while CI-MPC manages\nthe whole-body dynamics and modifies the contact schedule as needed. We\ndemonstrate the effectiveness of this approach in simulation with a novel 24\ndegree-of-freedom humanoid robot: Achilles. Our proposed framework achieves\nrough terrain walking, disturbance recovery, robustness under model and state\nuncertainty, and allows the robot to interact with obstacles in the\nenvironment, all while running online in real-time at 50 Hz."
                },
                "authors": [
                    {
                        "name": "Sergio A. Esteban"
                    },
                    {
                        "name": "Vince Kurtz"
                    },
                    {
                        "name": "Adrian B. Ghansah"
                    },
                    {
                        "name": "Aaron D. Ames"
                    }
                ],
                "author_detail": {
                    "name": "Aaron D. Ames"
                },
                "author": "Aaron D. Ames",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06556v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06556v4",
                "updated": "2025-02-21T17:55:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    55,
                    32,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-10T15:24:30Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    24,
                    30,
                    0,
                    41,
                    0
                ],
                "title": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms"
                },
                "summary": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}."
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jiangshu Du"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Chen Xing"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xing"
                },
                "author": "Chen Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06556v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06556v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v4",
                "updated": "2025-02-21T17:53:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    53,
                    9,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, automated, end-to-end penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and LLama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming end-to-end penetration testing even with some minimal human\nassistance. Next, we advance the state-of-the-art and present ablation studies\nthat provide insights into improving the PentestGPT tool. Our research\nilluminates the challenges LLMs face in each aspect of Pentesting, e.g.\nenumeration, exploitation, and privilege escalation. This work contributes to\nthe growing body of knowledge on AI-assisted cybersecurity and lays the\nfoundation for future research in automated penetration testing using large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, automated, end-to-end penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and LLama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming end-to-end penetration testing even with some minimal human\nassistance. Next, we advance the state-of-the-art and present ablation studies\nthat provide insights into improving the PentestGPT tool. Our research\nilluminates the challenges LLMs face in each aspect of Pentesting, e.g.\nenumeration, exploitation, and privilege escalation. This work contributes to\nthe growing body of knowledge on AI-assisted cybersecurity and lays the\nfoundation for future research in automated penetration testing using large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15618v1",
                "updated": "2025-02-21T17:41:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    41,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    41,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via\n  Model-Probing"
                },
                "summary": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning."
                },
                "authors": [
                    {
                        "name": "Qi Le"
                    },
                    {
                        "name": "Enmao Diao"
                    },
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Xinran Wang"
                    },
                    {
                        "name": "Jie Ding"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09838v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09838v3",
                "updated": "2025-02-21T17:39:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    39,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-14T00:42:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    0,
                    42,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation"
                },
                "summary": "We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT."
                },
                "authors": [
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Sijing Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Binhe Yu"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "arxiv_comment": "Comments: added project page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09838v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07653v2",
                "updated": "2025-02-21T17:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    32,
                    46,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-13T19:26:09Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    19,
                    26,
                    9,
                    0,
                    13,
                    0
                ],
                "title": "Large Language Models for Interpretable Mental Health Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Interpretable Mental Health Diagnosis"
                },
                "summary": "We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a clinical decision support system (CDSS) for mental health\ndiagnosis that combines the strengths of large language models (LLMs) and\nconstraint logic programming (CLP). Having a CDSS is important because of the\nhigh complexity of diagnostic manuals used by mental health professionals and\nthe danger of diagnostic errors. Our CDSS is a software tool that uses an LLM\nto translate diagnostic manuals to a logic program and solves the program using\nan off-the-shelf CLP engine to query a patient's diagnosis based on the encoded\nrules and provided data. By giving domain experts the opportunity to inspect\nthe LLM-generated logic program, and making modifications when needed, our CDSS\nensures that the diagnosis is not only accurate but also interpretable. We\nexperimentally compare it with two baseline approaches of using LLMs:\ndiagnosing patients using the LLM-only approach, and using the LLM-generated\nlogic program but without expert inspection. The results show that, while LLMs\nare extremely useful in generating candidate logic programs, these programs\nstill require expert inspection and modification to guarantee faithfulness to\nthe official diagnostic manuals. Additionally, ethical concerns arise from the\ndirect use of patient data in LLMs, underscoring the need for a safer hybrid\napproach like our proposed method."
                },
                "authors": [
                    {
                        "name": "Brian Hyeongseok Kim"
                    },
                    {
                        "name": "Chao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Wang"
                },
                "author": "Chao Wang",
                "arxiv_comment": "Accepted at AAAI 2025 Workshop on Large Language Models and\n  Generative AI for Health (GenAI4Health)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15609v1",
                "updated": "2025-02-21T17:31:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    31,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:31:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    31,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "On the Robustness of Transformers against Context Hijacking for Linear\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Transformers against Context Hijacking for Linear\n  Classification"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Xingwu Chen"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Difan Zou"
                    }
                ],
                "author_detail": {
                    "name": "Difan Zou"
                },
                "author": "Difan Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13311v2",
                "updated": "2025-02-21T17:25:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    25,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-18T22:13:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    22,
                    13,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors"
                },
                "summary": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Yinpei Dai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15604v1",
                "updated": "2025-02-21T17:19:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:19:39Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    39,
                    4,
                    52,
                    0
                ],
                "title": "Cross-Format Retrieval-Augmented Generation in XR with LLMs for\n  Context-Aware Maintenance Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Format Retrieval-Augmented Generation in XR with LLMs for\n  Context-Aware Maintenance Assistance"
                },
                "summary": "This paper presents a detailed evaluation of a Retrieval-Augmented Generation\n(RAG) system that integrates large language models (LLMs) to enhance\ninformation retrieval and instruction generation for maintenance personnel\nacross diverse data formats. We assessed the performance of eight LLMs,\nemphasizing key metrics such as response speed and accuracy, which were\nquantified using BLEU and METEOR scores. Our findings reveal that advanced\nmodels like GPT-4 and GPT-4o-mini significantly outperform their counterparts,\nparticularly when addressing complex queries requiring multi-format data\nintegration. The results validate the system's ability to deliver timely and\naccurate responses, highlighting the potential of RAG frameworks to optimize\nmaintenance operations. Future research will focus on refining retrieval\ntechniques for these models and enhancing response generation, particularly for\nintricate scenarios, ultimately improving the system's practical applicability\nin dynamic real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a detailed evaluation of a Retrieval-Augmented Generation\n(RAG) system that integrates large language models (LLMs) to enhance\ninformation retrieval and instruction generation for maintenance personnel\nacross diverse data formats. We assessed the performance of eight LLMs,\nemphasizing key metrics such as response speed and accuracy, which were\nquantified using BLEU and METEOR scores. Our findings reveal that advanced\nmodels like GPT-4 and GPT-4o-mini significantly outperform their counterparts,\nparticularly when addressing complex queries requiring multi-format data\nintegration. The results validate the system's ability to deliver timely and\naccurate responses, highlighting the potential of RAG frameworks to optimize\nmaintenance operations. Future research will focus on refining retrieval\ntechniques for these models and enhancing response generation, particularly for\nintricate scenarios, ultimately improving the system's practical applicability\nin dynamic real-world environments."
                },
                "authors": [
                    {
                        "name": "Akos Nagy"
                    },
                    {
                        "name": "Yannis Spyridis"
                    },
                    {
                        "name": "Vasileios Argyriou"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Argyriou"
                },
                "author": "Vasileios Argyriou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15603v1",
                "updated": "2025-02-21T17:19:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    23,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:19:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    19,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Do Multilingual LLMs Think In English?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Multilingual LLMs Think In English?"
                },
                "summary": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have multilingual capabilities and can solve\ntasks across various languages. However, we show that current LLMs make key\ndecisions in a representation space closest to English, regardless of their\ninput and output languages. Exploring the internal representations with a logit\nlens for sentences in French, German, Dutch, and Mandarin, we show that the LLM\nfirst emits representations close to English for semantically-loaded words\nbefore translating them into the target language. We further show that\nactivation steering in these LLMs is more effective when the steering vectors\nare computed in English rather than in the language of the inputs and outputs.\nThis suggests that multilingual LLMs perform key reasoning steps in a\nrepresentation that is heavily shaped by English in a way that is not\ntransparent to system users."
                },
                "authors": [
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Sebastian Farquhar"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Farquhar"
                },
                "author": "Sebastian Farquhar",
                "arxiv_comment": "Main paper 9 pages; including appendix 48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15601v1",
                "updated": "2025-02-21T17:18:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    18,
                    30,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:18:30Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    18,
                    30,
                    4,
                    52,
                    0
                ],
                "title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents"
                },
                "summary": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life."
                },
                "authors": [
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15594v1",
                "updated": "2025-02-21T17:12:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    12,
                    35,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:12:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    12,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention"
                },
                "summary": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation while ensuring both effectiveness and\nefficiency, we propose SafeIntervention (SafeInt), a novel defense method that\nshields LLMs from jailbreak attacks through safety-aware representation\nintervention. SafeInt is built on our analysis of the representations of\njailbreak samples. It adjusts representation distributions of jailbreak samples\nthrough intervention to align them with the representations of unsafe samples\nwhile minimizing unnecessary perturbations to jailbreak-irrelevant\nrepresentations. We conduct comprehensive experiments covering six jailbreak\nattacks, two jailbreak datasets, and two utility benchmarks. Experimental\nresults demonstrate that SafeInt outperforms all baselines in defending LLMs\nagainst jailbreak attacks while largely maintaining utility. Additionally, we\nevaluate SafeInt against adaptive attacks and verify its effectiveness in\nmitigating real-time attacks."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chunyan Hou"
                    },
                    {
                        "name": "Xiaojie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Yuan"
                },
                "author": "Xiaojie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15592v1",
                "updated": "2025-02-21T17:02:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    2,
                    40,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T17:02:40Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    2,
                    40,
                    4,
                    52,
                    0
                ],
                "title": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing From Short to Long: Effective Data Synthesis for\n  Long-Context Instruction Tuning"
                },
                "summary": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Pinzhen Chen"
                    },
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14677v2",
                "updated": "2025-02-21T16:58:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    58,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T16:09:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    9,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Data-Constrained Synthesis of Training Data for De-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Constrained Synthesis of Training Data for De-Identification"
                },
                "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data."
                },
                "authors": [
                    {
                        "name": "Thomas Vakili"
                    },
                    {
                        "name": "Aron Henriksson"
                    },
                    {
                        "name": "Hercules Dalianis"
                    }
                ],
                "author_detail": {
                    "name": "Hercules Dalianis"
                },
                "author": "Hercules Dalianis",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15589v1",
                "updated": "2025-02-21T16:57:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:57:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "LightThinker: Thinking Step-by-Step Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightThinker: Thinking Step-by-Step Compression"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker."
                },
                "authors": [
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18328v2",
                "updated": "2025-02-21T16:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    45,
                    17,
                    4,
                    52,
                    0
                ],
                "published": "2024-07-04T22:26:20Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    22,
                    26,
                    20,
                    3,
                    186,
                    0
                ],
                "title": "Unveiling Scoring Processes: Dissecting the Differences between LLMs and\n  Human Graders in Automatic Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Scoring Processes: Dissecting the Differences between LLMs and\n  Human Graders in Automatic Scoring"
                },
                "summary": "Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring."
                },
                "authors": [
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Padmaja Pravin Saraf"
                    },
                    {
                        "name": "Gyeonggeon Lee"
                    },
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted by Technology, Knowledge, and Learning (TKNL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15576v1",
                "updated": "2025-02-21T16:36:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    36,
                    42,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:36:42Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    36,
                    42,
                    4,
                    52,
                    0
                ],
                "title": "Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting and Steering LLMs with Mutual Information-based\n  Explanations on Sparse Autoencoders"
                },
                "summary": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at handling human queries, but they can\noccasionally generate flawed or unexpected responses. Understanding their\ninternal states is crucial for understanding their successes, diagnosing their\nfailures, and refining their capabilities. Although sparse autoencoders (SAEs)\nhave shown promise for interpreting LLM internal representations, limited\nresearch has explored how to better explain SAE features, i.e., understanding\nthe semantic meaning of features learned by SAE. Our theoretical analysis\nreveals that existing explanation methods suffer from the frequency bias issue,\nwhere they emphasize linguistic patterns over semantic concepts, while the\nlatter is more critical to steer LLM behaviors. To address this, we propose\nusing a fixed vocabulary set for feature interpretations and designing a mutual\ninformation-based objective, aiming to better capture the semantic meaning\nbehind these features. We further propose two runtime steering strategies that\nadjust the learned feature activations based on their corresponding\nexplanations. Empirical results show that, compared to baselines, our method\nprovides more discourse-level explanations and effectively steers LLM behaviors\nto defend against jailbreak attacks. These findings highlight the value of\nexplanations for steering LLM behaviors in downstream applications. We will\nrelease our code and data once accepted."
                },
                "authors": [
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "Pre-print. 20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15572v1",
                "updated": "2025-02-21T16:32:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    32,
                    28,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:32:28Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    32,
                    28,
                    4,
                    52,
                    0
                ],
                "title": "DReSD: Dense Retrieval for Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DReSD: Dense Retrieval for Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) accelerates Large Language Model (LLM) generation\nby using an efficient draft model to propose the next few tokens, which are\nverified by the LLM in a single forward call, reducing latency while preserving\nits outputs. We focus on retrieval-based SD where the draft model retrieves the\nnext tokens from a non-parametric datastore. Sparse retrieval (REST), which\noperates on the surface form of strings, is currently the dominant paradigm due\nto its simplicity and scalability. However, its effectiveness is limited due to\nthe usage of short contexts and exact string matching. Instead, we introduce\nDense Retrieval for Speculative Decoding (DReSD), a novel framework that uses\napproximate nearest neighbour search with contextualised token embeddings to\nretrieve the most semantically relevant token sequences for SD. Extensive\nexperiments show that DReSD achieves (on average) 87% higher acceptance rates,\n65% longer accepted tokens and 19% faster generation speeds compared to sparse\nretrieval (REST)."
                },
                "authors": [
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Huiyin Xue"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Lampouras"
                },
                "author": "Gerasimos Lampouras",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15568v1",
                "updated": "2025-02-21T16:30:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    30,
                    53,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:30:53Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    30,
                    53,
                    4,
                    52,
                    0
                ],
                "title": "A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the\n  2025 Federal Elections in Germany",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cautionary Tale About \"Neutrally\" Informative AI Tools Ahead of the\n  2025 Federal Elections in Germany"
                },
                "summary": "In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we examine the reliability of AI-based Voting Advice\nApplications (VAAs) and large language models (LLMs) in providing objective\npolitical information. Our analysis is based upon a comparison with party\nresponses to 38 statements of the Wahl-O-Mat, a well-established German online\ntool that helps inform voters by comparing their views with political party\npositions. For the LLMs, we identify significant biases. They exhibit a strong\nalignment (over 75% on average) with left-wing parties and a substantially\nlower alignment with center-right (smaller 50%) and right-wing parties (around\n30%). Furthermore, for the VAAs, intended to objectively inform voters, we\nfound substantial deviations from the parties' stated positions in Wahl-O-Mat:\nWhile one VAA deviated in 25% of cases, another VAA showed deviations in more\nthan 50% of cases. For the latter, we even observed that simple prompt\ninjections led to severe hallucinations, including false claims such as\nnon-existent connections between political parties and right-wing extremist\nties."
                },
                "authors": [
                    {
                        "name": "Ina Dormuth"
                    },
                    {
                        "name": "Sven Franke"
                    },
                    {
                        "name": "Marlies Hafer"
                    },
                    {
                        "name": "Tim Katzke"
                    },
                    {
                        "name": "Alexander Marx"
                    },
                    {
                        "name": "Emmanuel Mller"
                    },
                    {
                        "name": "Daniel Neider"
                    },
                    {
                        "name": "Markus Pauly"
                    },
                    {
                        "name": "Jrme Rutinowski"
                    }
                ],
                "author_detail": {
                    "name": "Jrme Rutinowski"
                },
                "author": "Jrme Rutinowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15561v1",
                "updated": "2025-02-21T16:22:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    22,
                    11,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T16:22:11Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    22,
                    11,
                    4,
                    52,
                    0
                ],
                "title": "A Defensive Framework Against Adversarial Attacks on Machine\n  Learning-Based Network Intrusion Detection Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Defensive Framework Against Adversarial Attacks on Machine\n  Learning-Based Network Intrusion Detection Systems"
                },
                "summary": "As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks."
                },
                "authors": [
                    {
                        "name": "Benyamin Tafreshian"
                    },
                    {
                        "name": "Shengzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengzhi Zhang"
                },
                "author": "Shengzhi Zhang",
                "arxiv_comment": "Accepted to IEEE AI+ TrustCom 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05673v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05673v4",
                "updated": "2025-02-21T16:17:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    17,
                    17,
                    4,
                    52,
                    0
                ],
                "published": "2024-06-09T07:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    7,
                    6,
                    58,
                    6,
                    161,
                    0
                ],
                "title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples"
                },
                "summary": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR."
                },
                "authors": [
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Haoqiang Kang"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Lianhui Qin"
                    }
                ],
                "author_detail": {
                    "name": "Lianhui Qin"
                },
                "author": "Lianhui Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05673v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05673v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15543v1",
                "updated": "2025-02-21T15:50:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    50,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:50:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    50,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented\n  Generation via Parametric Pruning"
                },
                "summary": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the\ninternal memory of Large Language Models (LLMs) by integrating external\nknowledge. However, KAG inevitably faces knowledge conflicts when the internal\nmemory contradicts external information. Current approaches to mitigating these\nconflicts mainly focus on improving external knowledge utilization. However,\nthese methods have shown only limited effectiveness in mitigating the knowledge\nconflict problem, as internal knowledge continues to influence the generation\nprocess of LLMs. In this paper, we propose a ParametrIc Pruning-based\nKnowledge-Augmented Generation (PIP-KAG) approach, which prunes internal\nknowledge of LLMs and incorporates a plug-and-play adaptation module to help\nLLMs better leverage external sources. Additionally, we construct the\nCoConflictQA benchmark based on the hallucination of LLMs to better evaluate\ncontextual faithfulness during answering questions. Experimental results on\nCoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts\nand improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by\n13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes\nare available at https://github.com/OpenBMB/PIP-KAG."
                },
                "authors": [
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "arxiv_comment": "20 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v2",
                "updated": "2025-02-21T15:48:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    48,
                    44,
                    4,
                    52,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling diversity in LLM-agent world simulations is essential for\nmaintaining stability in structured tasks while enabling variation where\ncreativity is needed. However, we observe that dialogue diversity declines\nsignificantly over long-term simulation. To investigate the role of prompt\ndesign in conversational diversity, we modularized the utterance generation\nprompt and found that reducing the given information leads to more diverse\noutputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a\nnovel method that allows users to control diversity through a single parameter,\nlambda. APP dynamically prunes the utterance generation prompt based on their\nattention weights and is compatible with traditional diversity control\ntechniques. We demonstrate that APP effectively controls output diversity\nthrough extensive experiments, and propose a method to balance the control\ntrade-offs. Additionally, we provide an in-depth analysis to offer insights\ninto optimizing diversity control in multi-agent simulation."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration (v1); updated version (v2)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17179v2",
                "updated": "2025-02-21T15:48:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    48,
                    32,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-23T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    40,
                    24,
                    0,
                    267,
                    0
                ],
                "title": "Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully automatic extraction of morphological traits from the Web: utopia\n  or reality?"
                },
                "summary": "Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant morphological traits, their observable characteristics, are fundamental\nto understand the role played by each species within their ecosystem. However,\ncompiling trait information for even a moderate number of species is a\ndemanding task that may take experts years to accomplish. At the same time,\nmassive amounts of information about species descriptions is available online\nin the form of text, although the lack of structure makes this source of data\nimpossible to use at scale. To overcome this, we propose to leverage recent\nadvances in large language models (LLMs) and devise a mechanism for gathering\nand processing information on plant traits in the form of unstructured textual\ndescriptions, without manual curation. We evaluate our approach by\nautomatically replicating three manually created species-trait matrices. Our\nmethod managed to find values for over half of all species-trait pairs, with an\nF1-score of over 75%. Our results suggest that large-scale creation of\nstructured trait databases from unstructured online text is currently feasible\nthanks to the information extraction capabilities of LLMs, being limited by the\navailability of textual descriptions covering all the traits of interest."
                },
                "authors": [
                    {
                        "name": "Diego Marcos"
                    },
                    {
                        "name": "Robert van de Vlasakker"
                    },
                    {
                        "name": "Ioannis N. Athanasiadis"
                    },
                    {
                        "name": "Pierre Bonnet"
                    },
                    {
                        "name": "Herv Goeau"
                    },
                    {
                        "name": "Alexis Joly"
                    },
                    {
                        "name": "W. Daniel Kissling"
                    },
                    {
                        "name": "Csar Leblanc"
                    },
                    {
                        "name": "Andr S. J. van Proosdij"
                    },
                    {
                        "name": "Konstantinos P. Panousis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos P. Panousis"
                },
                "author": "Konstantinos P. Panousis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15526v1",
                "updated": "2025-02-21T15:28:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    28,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:28:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    28,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs"
                },
                "summary": "Scaling large language models (LLMs) has shown great potential for improving\nretrieval model performance; however, previous studies have mainly focused on\ndense retrieval trained with contrastive loss (CL), neglecting the scaling\nbehavior of other retrieval paradigms and optimization techniques, such as\nsparse retrieval and knowledge distillation (KD). In this work, we conduct a\nsystematic comparative study on how different retrieval paradigms (sparse vs.\ndense) and fine-tuning objectives (CL vs. KD vs. their combination) affect\nretrieval performance across different model scales. Using MSMARCO passages as\nthe training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a\nfixed compute budget, we evaluate various training configurations on both\nin-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key\nfindings reveal that: (1) Scaling behaviors emerge clearly only with CL, where\nlarger models achieve significant performance gains, whereas KD-trained models\nshow minimal improvement, performing similarly across the 1B, 3B, and 8B\nscales. (2) Sparse retrieval models consistently outperform dense retrieval\nacross both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,\nand they demonstrate greater robustness to imperfect supervised signals. (3) We\nsuccessfully scale sparse retrieval models with the combination of CL and KD\nlosses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation\nsets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) has shown great potential for improving\nretrieval model performance; however, previous studies have mainly focused on\ndense retrieval trained with contrastive loss (CL), neglecting the scaling\nbehavior of other retrieval paradigms and optimization techniques, such as\nsparse retrieval and knowledge distillation (KD). In this work, we conduct a\nsystematic comparative study on how different retrieval paradigms (sparse vs.\ndense) and fine-tuning objectives (CL vs. KD vs. their combination) affect\nretrieval performance across different model scales. Using MSMARCO passages as\nthe training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a\nfixed compute budget, we evaluate various training configurations on both\nin-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key\nfindings reveal that: (1) Scaling behaviors emerge clearly only with CL, where\nlarger models achieve significant performance gains, whereas KD-trained models\nshow minimal improvement, performing similarly across the 1B, 3B, and 8B\nscales. (2) Sparse retrieval models consistently outperform dense retrieval\nacross both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,\nand they demonstrate greater robustness to imperfect supervised signals. (3) We\nsuccessfully scale sparse retrieval models with the combination of CL and KD\nlosses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation\nsets."
                },
                "authors": [
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Julian Killingback"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15524v1",
                "updated": "2025-02-21T15:25:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    25,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:25:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    25,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "Towards Swift Serverless LLM Cold Starts with ParaServe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Swift Serverless LLM Cold Starts with ParaServe"
                },
                "summary": "With the surge in number of large language models (LLMs), the industry turns\nto serverless computing for LLM inference serving. However, serverless LLM\nserving suffers from significant cold start latency and service level objective\n(SLO) violations due to the substantial model size, which leads to prolonged\nmodel fetching time from remote storage. We present ParaServe, a serverless LLM\nserving system that minimizes cold start latency through the novel use of\npipeline parallelism. Our insight is that by distributing model parameters\nacross multiple GPU servers, we can utilize their aggregated network bandwidth\nto concurrently fetch different parts of the model. ParaServe adopts a\ntwo-level hierarchical design. At the cluster level, ParaServe determines the\noptimal degree of parallelism based on user SLOs and carefully places GPU\nworkers across servers to reduce network interference. At the worker level,\nParaServe overlaps model fetching, loading, and runtime initialization to\nfurther accelerate cold starts. Additionally, ParaServe introduces pipeline\nconsolidation, which merges parallel groups back to individual workers to\nmaintain optimal performance for warm requests. Our comprehensive evaluations\nunder diverse settings demonstrate that ParaServe reduces the cold start\nlatency by up to 4.7x and improves SLO attainment by up to 1.74x compared to\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the surge in number of large language models (LLMs), the industry turns\nto serverless computing for LLM inference serving. However, serverless LLM\nserving suffers from significant cold start latency and service level objective\n(SLO) violations due to the substantial model size, which leads to prolonged\nmodel fetching time from remote storage. We present ParaServe, a serverless LLM\nserving system that minimizes cold start latency through the novel use of\npipeline parallelism. Our insight is that by distributing model parameters\nacross multiple GPU servers, we can utilize their aggregated network bandwidth\nto concurrently fetch different parts of the model. ParaServe adopts a\ntwo-level hierarchical design. At the cluster level, ParaServe determines the\noptimal degree of parallelism based on user SLOs and carefully places GPU\nworkers across servers to reduce network interference. At the worker level,\nParaServe overlaps model fetching, loading, and runtime initialization to\nfurther accelerate cold starts. Additionally, ParaServe introduces pipeline\nconsolidation, which merges parallel groups back to individual workers to\nmaintain optimal performance for warm requests. Our comprehensive evaluations\nunder diverse settings demonstrate that ParaServe reduces the cold start\nlatency by up to 4.7x and improves SLO attainment by up to 1.74x compared to\nbaselines."
                },
                "authors": [
                    {
                        "name": "Chiheng Lou"
                    },
                    {
                        "name": "Sheng Qi"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15507v1",
                "updated": "2025-02-21T15:04:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "title": "Activation Steering in Neural Theorem Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering in Neural Theorem Provers"
                },
                "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shashank Kirtania"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Kirtania"
                },
                "author": "Shashank Kirtania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15506v1",
                "updated": "2025-02-21T15:02:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    2,
                    39,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T15:02:39Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    2,
                    39,
                    4,
                    52,
                    0
                ],
                "title": "Construction and Evaluation of LLM-based agents for Semi-Autonomous\n  penetration testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Construction and Evaluation of LLM-based agents for Semi-Autonomous\n  penetration testing"
                },
                "summary": "With the emergence of high-performance large language models (LLMs) such as\nGPT, Claude, and Gemini, the autonomous and semi-autonomous execution of tasks\nhas significantly advanced across various domains. However, in highly\nspecialized fields such as cybersecurity, full autonomy remains a challenge.\nThis difficulty primarily stems from the limitations of LLMs in reasoning\ncapabilities and domain-specific knowledge. We propose a system that\nsemi-autonomously executes complex cybersecurity workflows by employing\nmultiple LLMs modules to formulate attack strategies, generate commands, and\nanalyze results, thereby addressing the aforementioned challenges. In our\nexperiments using Hack The Box virtual machines, we confirmed that our system\ncan autonomously construct attack strategies, issue appropriate commands, and\nautomate certain processes, thereby reducing the need for manual intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of high-performance large language models (LLMs) such as\nGPT, Claude, and Gemini, the autonomous and semi-autonomous execution of tasks\nhas significantly advanced across various domains. However, in highly\nspecialized fields such as cybersecurity, full autonomy remains a challenge.\nThis difficulty primarily stems from the limitations of LLMs in reasoning\ncapabilities and domain-specific knowledge. We propose a system that\nsemi-autonomously executes complex cybersecurity workflows by employing\nmultiple LLMs modules to formulate attack strategies, generate commands, and\nanalyze results, thereby addressing the aforementioned challenges. In our\nexperiments using Hack The Box virtual machines, we confirmed that our system\ncan autonomously construct attack strategies, issue appropriate commands, and\nautomate certain processes, thereby reducing the need for manual intervention."
                },
                "authors": [
                    {
                        "name": "Masaya Kobayashi"
                    },
                    {
                        "name": "Masane Fuchi"
                    },
                    {
                        "name": "Amar Zanashir"
                    },
                    {
                        "name": "Tomonori Yoneda"
                    },
                    {
                        "name": "Tomohiro Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro Takagi"
                },
                "author": "Tomohiro Takagi",
                "arxiv_comment": "7 pages, 4 tables and 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14155v2",
                "updated": "2025-02-21T14:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    57,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-19T23:51:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    23,
                    51,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Giving AI Personalities Leads to More Human-Like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giving AI Personalities Leads to More Human-Like Reasoning"
                },
                "summary": "In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning."
                },
                "authors": [
                    {
                        "name": "Animesh Nighojkar"
                    },
                    {
                        "name": "Bekhzodbek Moydinboyev"
                    },
                    {
                        "name": "My Duong"
                    },
                    {
                        "name": "John Licato"
                    }
                ],
                "author_detail": {
                    "name": "John Licato"
                },
                "author": "John Licato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15499v1",
                "updated": "2025-02-21T14:49:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    49,
                    34,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:49:34Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    49,
                    34,
                    4,
                    52,
                    0
                ],
                "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models"
                },
                "summary": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD."
                },
                "authors": [
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqing Li"
                },
                "author": "Xiaoqing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07780v2",
                "updated": "2025-02-21T14:41:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    41,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-11T18:59:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    59,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarwinLM: Evolutionary Structured Pruning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training. Code is at:\nhttps://github.com/IST-DASLab/DarwinLM"
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Code: https://github.com/IST-DASLab/DarwinLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12560v2",
                "updated": "2025-02-21T14:41:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    41,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-18T05:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    5,
                    54,
                    56,
                    1,
                    49,
                    0
                ],
                "title": "How does a Language-Specific Tokenizer affect LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does a Language-Specific Tokenizer affect LLMs?"
                },
                "summary": "The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Jean Seo"
                    },
                    {
                        "name": "Jaeyoon Kim"
                    },
                    {
                        "name": "SungJoo Byun"
                    },
                    {
                        "name": "Hyopil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyopil Shin"
                },
                "author": "Hyopil Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15493v1",
                "updated": "2025-02-21T14:36:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    36,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:36:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    36,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to\n  Responsibly Use Large Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to\n  Responsibly Use Large Language Models for Code Generation"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\ncode generation, offering immense potential to enhance programmer productivity.\nHowever, their non-deterministic nature and reliance on user input necessitate\na robust understanding of programming fundamentals to ensure their responsible\nand effective use. In this paper, we argue that foundational computing skills\nremain crucial in the age of LLMs. We propose a syllabus focused on equipping\ncomputer science students to responsibly embrace LLMs as performance\nenhancement tools. This work contributes to the discussion on the why, when,\nand how of integrating LLMs into computing education, aiming to better prepare\nprogrammers to leverage these tools without compromising foundational software\ndevelopment principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\ncode generation, offering immense potential to enhance programmer productivity.\nHowever, their non-deterministic nature and reliance on user input necessitate\na robust understanding of programming fundamentals to ensure their responsible\nand effective use. In this paper, we argue that foundational computing skills\nremain crucial in the age of LLMs. We propose a syllabus focused on equipping\ncomputer science students to responsibly embrace LLMs as performance\nenhancement tools. This work contributes to the discussion on the why, when,\nand how of integrating LLMs into computing education, aiming to better prepare\nprogrammers to leverage these tools without compromising foundational software\ndevelopment principles."
                },
                "authors": [
                    {
                        "name": "Bruno Pereira Cipriano"
                    },
                    {
                        "name": "Lcio Studer Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lcio Studer Ferreira"
                },
                "author": "Lcio Studer Ferreira",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14313v2",
                "updated": "2025-02-21T14:35:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    35,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2024-06-20T13:43:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    43,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with\n  Unanswerability"
                },
                "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well."
                },
                "authors": [
                    {
                        "name": "Riya Sawhney"
                    },
                    {
                        "name": "Samrat Yadav"
                    },
                    {
                        "name": "Indrajit Bhattacharya"
                    },
                    {
                        "name": "Mausam"
                    }
                ],
                "author_detail": {
                    "name": "Mausam"
                },
                "author": "Mausam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11242v2",
                "updated": "2025-02-21T14:30:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    30,
                    31,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T19:39:48Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    19,
                    39,
                    48,
                    6,
                    47,
                    0
                ],
                "title": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection\n  Framework for Safe Child-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Childhood Safety: Identifying Risks and Proposing a Protection\n  Framework for Safe Child-LLM Interaction"
                },
                "summary": "This study examines the growing use of Large Language Models (LLMs) in\nchild-centered applications, highlighting safety and ethical concerns such as\nbias, harmful content, and cultural insensitivity. Despite their potential to\nenhance learning, there is a lack of standardized frameworks to mitigate these\nrisks. Through a systematic literature review, we identify key parental and\nempirical concerns, including toxicity and ethical breaches in AI outputs.\nMoreover, to address these issues, this paper proposes a protection framework\nfor safe Child-LLM interaction, incorporating metrics for content safety,\nbehavioral ethics, and cultural sensitivity. The framework provides practical\ntools for evaluating LLM safety, offering guidance for developers,\npolicymakers, and educators to ensure responsible AI deployment for children.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the growing use of Large Language Models (LLMs) in\nchild-centered applications, highlighting safety and ethical concerns such as\nbias, harmful content, and cultural insensitivity. Despite their potential to\nenhance learning, there is a lack of standardized frameworks to mitigate these\nrisks. Through a systematic literature review, we identify key parental and\nempirical concerns, including toxicity and ethical breaches in AI outputs.\nMoreover, to address these issues, this paper proposes a protection framework\nfor safe Child-LLM interaction, incorporating metrics for content safety,\nbehavioral ethics, and cultural sensitivity. The framework provides practical\ntools for evaluating LLM safety, offering guidance for developers,\npolicymakers, and educators to ensure responsible AI deployment for children."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15488v1",
                "updated": "2025-02-21T14:26:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:26:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection"
                },
                "summary": "PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PETR-based methods have dominated benchmarks in 3D perception and are\nincreasingly becoming a key component in modern autonomous driving systems.\nHowever, their quantization performance significantly degrades when INT8\ninference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on\nthe NuScenes dataset. To address this issue, we propose a quantization-aware\nposition embedding transformation for multi-view 3D object detection, termed\nQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly\narchitecture while preserving the original performance of PETR. It\nsubstantially narrows the accuracy gap between INT8 and FP32 inference for\nPETR-series methods. Without bells and whistles, our approach reduces the mAP\nand NDS drop to within 1% under standard 8-bit per-tensor post-training\nquantization. Furthermore, our method exceeds the performance of the original\nPETR in terms of floating-point precision. Extensive experiments across a\nvariety of PETR-series models demonstrate its broad generalization."
                },
                "authors": [
                    {
                        "name": "Jiangyong Yu"
                    },
                    {
                        "name": "Changyong Shu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15487v1",
                "updated": "2025-02-21T14:23:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T14:23:14Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    23,
                    14,
                    4,
                    52,
                    0
                ],
                "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size."
                },
                "authors": [
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Serenna Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Emmanuele Chersoni"
                    },
                    {
                        "name": "Lucia Passaro"
                    },
                    {
                        "name": "Irene Sucameli"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "Submitted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11910v2",
                "updated": "2025-02-21T14:17:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    17,
                    57,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-17T15:28:40Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    28,
                    40,
                    0,
                    48,
                    0
                ],
                "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More\n  Measurable Objectives"
                },
                "summary": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes."
                },
                "authors": [
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Gauthier Gidel"
                    }
                ],
                "author_detail": {
                    "name": "Gauthier Gidel"
                },
                "author": "Gauthier Gidel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15472v1",
                "updated": "2025-02-21T13:55:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    55,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:55:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    55,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Task- and Reconstruction-Oriented Communications for Edge\n  Intelligence"
                },
                "summary": "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing communication systems aim to reconstruct the information at the\nreceiver side, and are known as reconstruction-oriented communications. This\napproach often falls short in meeting the real-time, task-specific demands of\nmodern AI-driven applications such as autonomous driving and semantic\nsegmentation. As a new design principle, task-oriented communications have been\ndeveloped. However, it typically requires joint optimization of encoder,\ndecoder, and modified inference neural networks, resulting in extensive\ncross-system redesigns and compatibility issues. This paper proposes a novel\ncommunication framework that aligns reconstruction-oriented and task-oriented\ncommunications for edge intelligence. The idea is to extend the Information\nBottleneck (IB) theory to optimize data transmission by minimizing\ntask-relevant loss function, while maintaining the structure of the original\ndata by an information reshaper. Such an approach integrates task-oriented\ncommunications with reconstruction-oriented communications, where a variational\napproach is designed to handle the intractability of mutual information in\nhigh-dimensional neural network features. We also introduce a joint\nsource-channel coding (JSCC) modulation scheme compatible with classical\nmodulation techniques, enabling the deployment of AI technologies within\nexisting digital infrastructures. The proposed framework is particularly\neffective in edge-based autonomous driving scenarios. Our evaluation in the Car\nLearning to Act (CARLA) simulator demonstrates that the proposed framework\nsignificantly reduces bits per service by 99.19% compared to existing methods,\nsuch as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task\nexecution."
                },
                "authors": [
                    {
                        "name": "Yufeng Diao"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Changyang She"
                    },
                    {
                        "name": "Philip Guodong Zhao"
                    },
                    {
                        "name": "Emma Liying Li"
                    }
                ],
                "author_detail": {
                    "name": "Emma Liying Li"
                },
                "author": "Emma Liying Li",
                "arxiv_comment": "Accepted for publication in IEEE Journal on Selected Areas in\n  Communications (JSAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15470v1",
                "updated": "2025-02-21T13:52:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    52,
                    31,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:52:31Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    52,
                    31,
                    4,
                    52,
                    0
                ],
                "title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System"
                },
                "summary": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively."
                },
                "authors": [
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Haiyu Mao"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Juan Gmez-Luna"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13773v2",
                "updated": "2025-02-21T13:50:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    50,
                    25,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-23T15:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    15,
                    52,
                    34,
                    3,
                    23,
                    0
                ],
                "title": "Do Large Language Models Truly Understand Geometric Structures?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Understand Geometric Structures?"
                },
                "summary": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03296v2",
                "updated": "2025-02-21T13:45:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    45,
                    51,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-04T10:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    12,
                    4,
                    278,
                    0
                ],
                "title": "Comparing zero-shot self-explanations with human rationales in text\n  classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing zero-shot self-explanations with human rationales in text\n  classification"
                },
                "summary": "Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations. These do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation. We evaluate\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. We study two\ntext classification tasks: sentiment classification and forced labour\ndetection, i.e., identifying pre-defined risk indicators of forced labour. In\naddition to English, we include Danish and Italian translations of the\nsentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\nanalyse 4 LLMs. We show that self-explanations align more closely with human\nannotations compared to LRP, while maintaining a comparable level of\nfaithfulness. This finding suggests that self-explanations indeed provide good\nexplanations for text classification."
                },
                "authors": [
                    {
                        "name": "Stephanie Brandl"
                    },
                    {
                        "name": "Oliver Eberle"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Eberle"
                },
                "author": "Oliver Eberle",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15455v1",
                "updated": "2025-02-21T13:30:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:30:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    30,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning"
                },
                "summary": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is prohibitively expensive in terms\nof computational and memory costs. Low-rank Adaptation (LoRA), as one of the\nmost popular parameter-efficient fine-tuning (PEFT) methods, offers a\ncost-effective alternative by approximating the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of down-projection matrix $A \\in\n\\mathbb{R}^{m \\times r}$ and head matrix $B \\in \\mathbb{R}^{r \\times n}$, where\n$r \\ll \\min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from\nmultiple domains to perform tasks across various fields, embodying multi-task\nlearning (MTL). LoRA often underperforms in such complex scenarios. To enhance\nLoRA's capability in multi-task learning, we propose R-LoRA, which incorporates\nMulti-Head Randomization. Multi-Head Randomization diversifies the head\nmatrices through Multi-Head Random Initialization and Multi-Head Dropout,\nenabling more efficient learning of task-specific features while maintaining\nshared knowledge representation. Extensive experiments demonstrate that R-LoRA\nis better at capturing task-specific knowledge, thereby improving performance\nin multi-task scenarios. The code is available at\nhttps://github.com/jinda-liu/R-LoRA."
                },
                "authors": [
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15766v2",
                "updated": "2025-02-21T13:29:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    29,
                    42,
                    4,
                    52,
                    0
                ],
                "published": "2024-09-24T05:44:46Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    44,
                    46,
                    1,
                    268,
                    0
                ],
                "title": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models"
                },
                "summary": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. The\nuse of these models in real-world contexts-where misinformation can lead to\nserious consequences for individuals seeking medical advice and\nsupport-necessitates a rigorous focus on safety and trustworthiness. In this\nwork, we introduce CHBench, the first comprehensive safety-oriented Chinese\nhealth-related benchmark designed to evaluate LLMs' capabilities in\nunderstanding and addressing physical and mental health issues with a safety\nperspective across diverse scenarios. CHBench comprises 6,493 entries on mental\nhealth and 2,999 entries on physical health, spanning a wide range of topics.\nOur extensive evaluations of four popular Chinese LLMs highlight significant\ngaps in their capacity to deliver safe and accurate health information,\nunderscoring the urgent need for further advancements in this critical domain.\nThe code is available at https://github.com/TracyGuo2001/CHBench."
                },
                "authors": [
                    {
                        "name": "Chenlu Guo"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15451v1",
                "updated": "2025-02-21T13:25:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    25,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:25:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    25,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast convergence algorithm based on binary integer programming for\n  expert load balancing in MoE LLMs"
                },
                "summary": "MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture-of-Expert) architectures appear frequently in large language\nmodels, and the number of experts can be over one hundred recently. However,\nthe expert load imbalance problem always happens in MoE model pre-training,\nwhich will cause routing collapse or increased computational overhead. In order\nto balance loads on experts, we propose BIP-Based Balancing, an expert load\nbalancing algorithm based on binary integer programming (BIP). The algorithm\nmaintains an additional vector q that can help change the top-K order of s by\nsolving a binary integer programming with very small time costs. In simulation\nexperiments, we observe that BIP-Based Balancing make imbalance disappoint very\nfast, while the final sum of routine scores decreases very little. Our\nalgorithm achieves nearly perfect trade-off between expert load balance and\npre-training efficiency under the simulation view."
                },
                "authors": [
                    {
                        "name": "Yuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Sun"
                },
                "author": "Yuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15448v1",
                "updated": "2025-02-21T13:22:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    22,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:22:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    22,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "MVIP -- A Dataset and Methods for Application Oriented Multi-View and\n  Multi-Modal Industrial Part Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVIP -- A Dataset and Methods for Application Oriented Multi-View and\n  Multi-Modal Industrial Part Recognition"
                },
                "summary": "We present MVIP, a novel dataset for multi-modal and multi-view\napplication-oriented industrial part recognition. Here we are the first to\ncombine a calibrated RGBD multi-view dataset with additional object context\nsuch as physical properties, natural language, and super-classes. The current\nportfolio of available datasets offers a wide range of representations to\ndesign and benchmark related methods. In contrast to existing classification\nchallenges, industrial recognition applications offer controlled multi-modal\nenvironments but at the same time have different problems than traditional\n2D/3D classification challenges. Frequently, industrial applications must deal\nwith a small amount or increased number of training data, visually similar\nparts, and varying object sizes, while requiring a robust near 100% top 5\naccuracy under cost and time constraints. Current methods tackle such\nchallenges individually, but direct adoption of these methods within industrial\napplications is complex and requires further research. Our main goal with MVIP\nis to study and push transferability of various state-of-the-art methods within\nrelated downstream tasks towards an efficient deployment of industrial\nclassifiers. Additionally, we intend to push with MVIP research regarding\nseveral modality fusion topics, (automated) synthetic data generation, and\ncomplex data sampling -- combined in a single application-oriented benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MVIP, a novel dataset for multi-modal and multi-view\napplication-oriented industrial part recognition. Here we are the first to\ncombine a calibrated RGBD multi-view dataset with additional object context\nsuch as physical properties, natural language, and super-classes. The current\nportfolio of available datasets offers a wide range of representations to\ndesign and benchmark related methods. In contrast to existing classification\nchallenges, industrial recognition applications offer controlled multi-modal\nenvironments but at the same time have different problems than traditional\n2D/3D classification challenges. Frequently, industrial applications must deal\nwith a small amount or increased number of training data, visually similar\nparts, and varying object sizes, while requiring a robust near 100% top 5\naccuracy under cost and time constraints. Current methods tackle such\nchallenges individually, but direct adoption of these methods within industrial\napplications is complex and requires further research. Our main goal with MVIP\nis to study and push transferability of various state-of-the-art methods within\nrelated downstream tasks towards an efficient deployment of industrial\nclassifiers. Additionally, we intend to push with MVIP research regarding\nseveral modality fusion topics, (automated) synthetic data generation, and\ncomplex data sampling -- combined in a single application-oriented benchmark."
                },
                "authors": [
                    {
                        "name": "Paul Koch"
                    },
                    {
                        "name": "Marian Schlter"
                    },
                    {
                        "name": "Jrg Krger"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Krger"
                },
                "author": "Jrg Krger",
                "arxiv_comment": "Accepted to IMPROVE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15446v1",
                "updated": "2025-02-21T13:20:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    20,
                    53,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:20:53Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    20,
                    53,
                    4,
                    52,
                    0
                ],
                "title": "Development and Performance Validation of a Versatile VLBI Digital\n  Backend Using the ROACH2 Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Performance Validation of a Versatile VLBI Digital\n  Backend Using the ROACH2 Platform"
                },
                "summary": "Customized digital backends for Very Long Baseline Interferometry (VLBI) are\ncritical components for radio astronomy observatories. There are several\nserialized products such as the Digital Baseband Converter (DBBC),\nReconfigurable Open Architecture Computing Hardware (ROACH) Digital BackEnd\n(RDBE), and Chinese Data Acquisition System (CDAS). However, the reliance on\nhigh-speed analog-to-digital converters (ADC) and Field Programmable Gate\nArrays (FPGAs) often necessitates dedicated hardware platforms with long\ndevelopment cycles and prohibitive cost, limiting scalability and adaptability\nto evolving observational needs. To address these challenges, we propose a\ndesign leveraging the versatile and cost-effective ROACH2 hardware platform,\ndeveloped by CASPER (Collaboration for Astronomy Signal Processing and\nElectronics Research). ROACH2's mature technology and streamlined firmware\ndevelopment capabilities significantly reduce the hardware platform's\ndevelopment cycle and cost, making it ideal for modern astronomical\napplications. This VLBI digital backend, based on the ROACH2 platform,\nincorporates key technologies such as Polyphase Filter Banks (PFB) algorithm\nimplementation, digital complex-to-real baseband signal conversion, Mark5B data\nformatter design and two-bit optimal threshold quantization. These features\nensure compatibility with existing systems while providing enhanced\nperformance. The backend's performance was validated through multi-station VLBI\nexperiments, demonstrating its ability to achieve good correlation fringes\ncompared to the customized CDAS2-D system. Furthermore, this platform offers\nflexibility for rapid deployment of additional digital backends, such as those\nfor spectral line observations, showcasing its potential for broader\nastronomical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized digital backends for Very Long Baseline Interferometry (VLBI) are\ncritical components for radio astronomy observatories. There are several\nserialized products such as the Digital Baseband Converter (DBBC),\nReconfigurable Open Architecture Computing Hardware (ROACH) Digital BackEnd\n(RDBE), and Chinese Data Acquisition System (CDAS). However, the reliance on\nhigh-speed analog-to-digital converters (ADC) and Field Programmable Gate\nArrays (FPGAs) often necessitates dedicated hardware platforms with long\ndevelopment cycles and prohibitive cost, limiting scalability and adaptability\nto evolving observational needs. To address these challenges, we propose a\ndesign leveraging the versatile and cost-effective ROACH2 hardware platform,\ndeveloped by CASPER (Collaboration for Astronomy Signal Processing and\nElectronics Research). ROACH2's mature technology and streamlined firmware\ndevelopment capabilities significantly reduce the hardware platform's\ndevelopment cycle and cost, making it ideal for modern astronomical\napplications. This VLBI digital backend, based on the ROACH2 platform,\nincorporates key technologies such as Polyphase Filter Banks (PFB) algorithm\nimplementation, digital complex-to-real baseband signal conversion, Mark5B data\nformatter design and two-bit optimal threshold quantization. These features\nensure compatibility with existing systems while providing enhanced\nperformance. The backend's performance was validated through multi-station VLBI\nexperiments, demonstrating its ability to achieve good correlation fringes\ncompared to the customized CDAS2-D system. Furthermore, this platform offers\nflexibility for rapid deployment of additional digital backends, such as those\nfor spectral line observations, showcasing its potential for broader\nastronomical applications."
                },
                "authors": [
                    {
                        "name": "Jiyun Li"
                    },
                    {
                        "name": "Renjie Zhu"
                    },
                    {
                        "name": "Shaoguang Guo"
                    },
                    {
                        "name": "Ping Rui"
                    },
                    {
                        "name": "Zhijun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijun Xu"
                },
                "author": "Zhijun Xu",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15443v1",
                "updated": "2025-02-21T13:11:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    11,
                    22,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:11:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    11,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Compression Meets Model Compression: Memory-Efficient Double\n  Compression for Large Language Models"
                },
                "summary": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed."
                },
                "authors": [
                    {
                        "name": "Weilan Wang"
                    },
                    {
                        "name": "Yu Mao"
                    },
                    {
                        "name": "Dongdong Tang"
                    },
                    {
                        "name": "Hongchao Du"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Chun Jason Xue"
                    }
                ],
                "author_detail": {
                    "name": "Chun Jason Xue"
                },
                "author": "Chun Jason Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15441v1",
                "updated": "2025-02-21T13:09:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    9,
                    58,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:09:58Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    9,
                    58,
                    4,
                    52,
                    0
                ],
                "title": "On the Effectiveness of Large Language Models in Writing Alloy Formulas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Large Language Models in Writing Alloy Formulas"
                },
                "summary": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software."
                },
                "authors": [
                    {
                        "name": "Yang Hong"
                    },
                    {
                        "name": "Shan Jiang"
                    },
                    {
                        "name": "Yulei Fu"
                    },
                    {
                        "name": "Sarfraz Khurshid"
                    }
                ],
                "author_detail": {
                    "name": "Sarfraz Khurshid"
                },
                "author": "Sarfraz Khurshid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15436v1",
                "updated": "2025-02-21T13:05:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    5,
                    19,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:05:19Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    5,
                    19,
                    4,
                    52,
                    0
                ],
                "title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and\n  Performance in (Private) Federated LoRA Fine-Tuning"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning\nfoundation models. However, federated fine-tuning using LoRA is challenging due\nto suboptimal updates arising from traditional federated averaging of\nindividual adapters. Existing solutions either incur prohibitively high\ncommunication cost that scales linearly with the number of clients or suffer\nfrom performance degradation due to limited expressivity. We introduce\nFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of\nLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB\noptimally aligns the optimization trajectory with the ideal low-rank full\nfine-tuning projection by learning a small square matrix (R) between adapters B\nand A, keeping other components fixed. Direct averaging of R guarantees exact\nupdates, substantially reducing communication cost, which remains independent\nof the number of clients, and enables scalability. Fed-SB achieves\nstate-of-the-art performance across commonsense reasoning, arithmetic\nreasoning, and language inference tasks while reducing communication costs by\nup to 230x. In private settings, Fed-SB further improves performance by (1)\nreducing trainable parameters, thereby lowering the noise required for\ndifferential privacy and (2) avoiding noise amplification introduced by other\nmethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff\nbetween communication and performance, offering an efficient and scalable\nsolution for both private and non-private federated fine-tuning. Our code is\npublicly available at https://github.com/CERT-Lab/fed-sb."
                },
                "authors": [
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Rohit Vartak"
                    },
                    {
                        "name": "Lav R. Varshney"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vepakomma"
                },
                "author": "Praneeth Vepakomma",
                "arxiv_comment": "Raghav Singhal and Kaustubh Ponkshe contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15435v1",
                "updated": "2025-02-21T13:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    4,
                    13,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:04:13Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    4,
                    13,
                    4,
                    52,
                    0
                ],
                "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-pass Detection of Jailbreaking Input in Large Language Models"
                },
                "summary": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending aligned Large Language Models (LLMs) against jailbreaking attacks\nis a challenging problem, with existing approaches requiring multiple requests\nor even queries to auxiliary LLMs, making them computationally heavy. Instead,\nwe focus on detecting jailbreaking input in a single forward pass. Our method,\ncalled Single Pass Detection SPD, leverages the information carried by the\nlogits to predict whether the output sentence will be harmful. This allows us\nto defend in just one forward pass. SPD can not only detect attacks effectively\non open-source models, but also minimizes the misclassification of harmless\ninputs. Furthermore, we show that SPD remains effective even without complete\nlogit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a\npromising approach to efficiently safeguard LLMs against adversarial attacks."
                },
                "authors": [
                    {
                        "name": "Leyla Naz Candogan"
                    },
                    {
                        "name": "Yongtao Wu"
                    },
                    {
                        "name": "Elias Abad Rocamora"
                    },
                    {
                        "name": "Grigorios G. Chrysos"
                    },
                    {
                        "name": "Volkan Cevher"
                    }
                ],
                "author_detail": {
                    "name": "Volkan Cevher"
                },
                "author": "Volkan Cevher",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15434v1",
                "updated": "2025-02-21T13:01:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T13:01:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    1,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixup Model Merge: Enhancing Model Merging Performance through\n  Randomized Linear Interpolation"
                },
                "summary": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge."
                },
                "authors": [
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v1",
                "updated": "2025-02-21T12:54:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15428v1",
                "updated": "2025-02-21T12:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    35,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "SmartLog: Metrics-driven Role Assignment for Byzantine Fault-tolerant\n  Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLog: Metrics-driven Role Assignment for Byzantine Fault-tolerant\n  Protocols"
                },
                "summary": "Byzantine Fault Tolerant (BFT) protocols play a pivotal role in blockchain\ntechnology. As the deployment of such systems extends to wide-area networks,\nthe scalability of BFT protocols becomes a critical concern. Optimizations that\nassign specific roles to individual replicas can significantly improve the\nperformance of BFT systems. However, such role assignment is highly sensitive\nto faults, potentially undermining the optimizations effectiveness. To address\nthese challenges, we present SmartLog, a logging framework for collecting and\nanalyzing metrics that help to assign roles in globally distributed systems,\ndespite the presence of faults. SmartLog presents local measurements in global\ndata structures, to enable consistent decisions and hold replicas accountable\nif they do not perform according to their reported measurements. We apply\nSmartLog to Kauri, an optimization using randomly composed tree overlays.\nSmartLog finds robust and low-latency tree configurations under adverse\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byzantine Fault Tolerant (BFT) protocols play a pivotal role in blockchain\ntechnology. As the deployment of such systems extends to wide-area networks,\nthe scalability of BFT protocols becomes a critical concern. Optimizations that\nassign specific roles to individual replicas can significantly improve the\nperformance of BFT systems. However, such role assignment is highly sensitive\nto faults, potentially undermining the optimizations effectiveness. To address\nthese challenges, we present SmartLog, a logging framework for collecting and\nanalyzing metrics that help to assign roles in globally distributed systems,\ndespite the presence of faults. SmartLog presents local measurements in global\ndata structures, to enable consistent decisions and hold replicas accountable\nif they do not perform according to their reported measurements. We apply\nSmartLog to Kauri, an optimization using randomly composed tree overlays.\nSmartLog finds robust and low-latency tree configurations under adverse\nconditions."
                },
                "authors": [
                    {
                        "name": "Hanish Gogada"
                    },
                    {
                        "name": "Christian Berger"
                    },
                    {
                        "name": "Leander Jehl"
                    },
                    {
                        "name": "Hans P. Reiser"
                    },
                    {
                        "name": "Hein Meling"
                    }
                ],
                "author_detail": {
                    "name": "Hein Meling"
                },
                "author": "Hein Meling",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15427v1",
                "updated": "2025-02-21T12:54:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    25,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:54:25Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    25,
                    4,
                    52,
                    0
                ],
                "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails\n  Against Prompt Input Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails\n  Against Prompt Input Attacks on LLMs"
                },
                "summary": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation."
                },
                "authors": [
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hameed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Beat Buesser"
                    },
                    {
                        "name": "Mark Purcell"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    },
                    {
                        "name": "Kush Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush Varshney"
                },
                "author": "Kush Varshney",
                "arxiv_comment": "NeurIPS 2024, Safe Generative AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18220v3",
                "updated": "2025-02-21T12:44:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    44,
                    48,
                    4,
                    52,
                    0
                ],
                "published": "2024-11-27T10:57:06Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge"
                },
                "summary": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Mohsen Pourghasemian"
                    },
                    {
                        "name": "Haris Gacanin"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "arxiv_journal_ref": "2025 IEEE International Conference on Communications (ICC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12835v2",
                "updated": "2025-02-21T12:41:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    41,
                    6,
                    4,
                    52,
                    0
                ],
                "published": "2025-01-22T12:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
                },
                "summary": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Maria Lysyuk"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "The code and data are at https://github.com/s-nlp/AdaRAGUE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15419v1",
                "updated": "2025-02-21T12:38:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    38,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:38:26Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    38,
                    26,
                    4,
                    52,
                    0
                ],
                "title": "Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Translation: LLM-Based Data Generation for Multilingual\n  Fact-Checking"
                },
                "summary": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust automatic fact-checking systems have the potential to combat online\nmisinformation at scale. However, most existing research primarily focuses on\nEnglish. In this paper, we introduce MultiSynFact, the first large-scale\nmultilingual fact-checking dataset containing 2.2M claim-source pairs designed\nto support Spanish, German, English, and other low-resource languages. Our\ndataset generation pipeline leverages Large Language Models (LLMs), integrating\nexternal knowledge from Wikipedia and incorporating rigorous claim validation\nsteps to ensure data quality. We evaluate the effectiveness of MultiSynFact\nacross multiple models and experimental settings. Additionally, we open-source\na user-friendly framework to facilitate further research in multilingual\nfact-checking and dataset generation."
                },
                "authors": [
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Aurora Cobo"
                    },
                    {
                        "name": "Pablo Serna"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Serna"
                },
                "author": "Pablo Serna",
                "arxiv_comment": "15 pages, 1 figure, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15418v1",
                "updated": "2025-02-21T12:37:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    37,
                    58,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:37:58Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    37,
                    58,
                    4,
                    52,
                    0
                ],
                "title": "MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering\n  Challenge for Language Models"
                },
                "summary": "Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health remains a challenging problem all over the world, with issues\nlike depression, anxiety becoming increasingly common. Large Language Models\n(LLMs) have seen a vast application in healthcare, specifically in answering\nmedical questions. However, there is a lack of standard benchmarking datasets\nfor question answering (QA) in mental health. Our work presents a novel\nmultiple choice dataset, MHQA (Mental Health Question Answering), for\nbenchmarking Language models (LMs). Previous mental health datasets have\nfocused primarily on text classification into specific labels or disorders.\nMHQA, on the other hand, presents question-answering for mental health focused\non four key domains: anxiety, depression, trauma, and obsessive/compulsive\nissues, with diverse question types, namely, factoid, diagnostic, prognostic,\nand preventive. We use PubMed abstracts as the primary source for QA. We\ndevelop a rigorous pipeline for LLM-based identification of information from\nabstracts based on various selection criteria and converting it into QA pairs.\nFurther, valid QA pairs are extracted based on post-hoc validation criteria.\nOverall, our MHQA dataset consists of 2,475 expert-verified gold standard\ninstances called MHQA-gold and ~56.1k pairs pseudo labeled using external\nmedical references. We report F1 scores on different LLMs along with few-shot\nand supervised fine-tuning experiments, further discussing the insights for the\nscores."
                },
                "authors": [
                    {
                        "name": "Suraj Racha"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Anshika Raman"
                    },
                    {
                        "name": "Nikita Jangid"
                    },
                    {
                        "name": "Mridul Sharma"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Nirmal Punjabi"
                    }
                ],
                "author_detail": {
                    "name": "Nirmal Punjabi"
                },
                "author": "Nirmal Punjabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15412v1",
                "updated": "2025-02-21T12:21:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    21,
                    9,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:21:09Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    21,
                    9,
                    4,
                    52,
                    0
                ],
                "title": "Textual-to-Visual Iterative Self-Verification for Slide Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual-to-Visual Iterative Self-Verification for Slide Generation"
                },
                "summary": "Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating presentation slides is a time-consuming task that urgently\nrequires automation. Due to their limited flexibility and lack of automated\nrefinement mechanisms, existing autonomous LLM-based agents face constraints in\nreal-world applicability. We decompose the task of generating missing\npresentation slides into two key components: content generation and layout\ngeneration, aligning with the typical process of creating academic slides.\nFirst, we introduce a content generation approach that enhances coherence and\nrelevance by incorporating context from surrounding slides and leveraging\nsection retrieval strategies. For layout generation, we propose a\ntextual-to-visual self-verification process using a LLM-based Reviewer +\nRefiner workflow, transforming complex textual layouts into intuitive visual\nformats. This modality transformation simplifies the task, enabling accurate\nand human-like review and refinement. Experiments show that our approach\nsignificantly outperforms baseline methods in terms of alignment, logical flow,\nvisual appeal, and readability."
                },
                "authors": [
                    {
                        "name": "Yunqing Xu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15411v1",
                "updated": "2025-02-21T12:19:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings"
                },
                "summary": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts"
                },
                "authors": [
                    {
                        "name": "Rasmus Aavang"
                    },
                    {
                        "name": "Giovanni Rizzi"
                    },
                    {
                        "name": "Rasmus Bggild"
                    },
                    {
                        "name": "Alexandre Iolov"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10255v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10255v3",
                "updated": "2025-02-21T12:07:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    7,
                    3,
                    4,
                    52,
                    0
                ],
                "published": "2024-04-16T03:18:27Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    18,
                    27,
                    1,
                    107,
                    0
                ],
                "title": "Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Enhanced Training-as-a-Service for On-Device Intelligence:\n  Concept, Architectural Scheme, and Open Problems"
                },
                "summary": "On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\ninference without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, enhancing data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device intelligence (ODI) enables artificial intelligence (AI)\napplications to run on end devices, providing real-time and customized AI\ninference without relying on remote servers. However, training models for\non-device deployment face significant challenges due to the decentralized and\nprivacy-sensitive nature of users' data, along with end-side constraints\nrelated to network connectivity, computation efficiency, etc. Existing training\nparadigms, such as cloud-based training, federated learning, and transfer\nlearning, fail to sufficiently address these practical constraints that are\nprevalent for devices. To overcome these challenges, we propose\nPrivacy-Enhanced Training-as-a-Service (PTaaS), a novel service computing\nparadigm that provides privacy-friendly, customized AI model training for end\ndevices. PTaaS outsources the core training process to remote and powerful\ncloud or edge servers, efficiently developing customized on-device models based\non uploaded anonymous queries, enhancing data privacy while reducing the\ncomputation load on individual devices. We explore the definition, goals, and\ndesign principles of PTaaS, alongside emerging technologies that support the\nPTaaS paradigm. An architectural scheme for PTaaS is also presented, followed\nby a series of open problems that set the stage for future research directions\nin the field of PTaaS."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Tianliu He"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10255v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10255v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15401v1",
                "updated": "2025-02-21T12:00:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    0,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:00:10Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    0,
                    10,
                    4,
                    52,
                    0
                ],
                "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs\n  Complex Reasoning"
                },
                "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently."
                },
                "authors": [
                    {
                        "name": "Xuetao Ma"
                    },
                    {
                        "name": "Wenbin Jiang"
                    },
                    {
                        "name": "Hua Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Huang"
                },
                "author": "Hua Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02320v3",
                "updated": "2025-02-21T11:53:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    53,
                    53,
                    4,
                    52,
                    0
                ],
                "published": "2024-10-03T08:56:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    56,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "Post-edits Are Preferences Too",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-edits Are Preferences Too"
                },
                "summary": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks."
                },
                "authors": [
                    {
                        "name": "Nathaniel Berger"
                    },
                    {
                        "name": "Miriam Exel"
                    },
                    {
                        "name": "Matthias Huck"
                    },
                    {
                        "name": "Stefan Riezler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Riezler"
                },
                "author": "Stefan Riezler",
                "arxiv_comment": "To appear at the Ninth Conference on Machine Translation (WMT24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15395v1",
                "updated": "2025-02-21T11:46:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    46,
                    4,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T11:46:04Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    46,
                    4,
                    4,
                    52,
                    0
                ],
                "title": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday\n  Tasks and Decision-Making"
                },
                "summary": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for both everyday and\nspecialized tasks. While HCI research focuses on domain-specific applications,\nlittle is known about how heavy users integrate LLMs into everyday\ndecision-making. Through qualitative interviews with heavy LLM users (n=7) who\nemploy these systems for both intuitive and analytical thinking tasks, our\nfindings show that participants use LLMs for social validation,\nself-regulation, and interpersonal guidance, seeking to build self-confidence\nand optimize cognitive resources. These users viewed LLMs either as rational,\nconsistent entities or average human decision-makers. Our findings suggest that\nheavy LLM users develop nuanced interaction patterns beyond simple delegation,\nhighlighting the need to reconsider how we study LLM integration in\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Eunhye Kim"
                    },
                    {
                        "name": "Kiroong Choe"
                    },
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Sadat Shams Chowdhury"
                    },
                    {
                        "name": "Jinwook Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jinwook Seo"
                },
                "author": "Jinwook Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15392v1",
                "updated": "2025-02-21T11:38:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    38,
                    40,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T11:38:40Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    38,
                    40,
                    4,
                    52,
                    0
                ],
                "title": "Chitrarth: Bridging Vision and Language for a Billion People",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chitrarth: Bridging Vision and Language for a Billion People"
                },
                "summary": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena."
                },
                "authors": [
                    {
                        "name": "Shaharukh Khan"
                    },
                    {
                        "name": "Ayush Tarun"
                    },
                    {
                        "name": "Abhinav Ravi"
                    },
                    {
                        "name": "Ali Faraz"
                    },
                    {
                        "name": "Akshat Patidar"
                    },
                    {
                        "name": "Praveen Kumar Pokala"
                    },
                    {
                        "name": "Anagha Bhangare"
                    },
                    {
                        "name": "Raja Kolla"
                    },
                    {
                        "name": "Chandra Khatri"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04931v2",
                "updated": "2025-02-21T11:10:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    10,
                    15,
                    4,
                    52,
                    0
                ],
                "published": "2023-12-08T09:48:36Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    9,
                    48,
                    36,
                    4,
                    342,
                    0
                ],
                "title": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models"
                },
                "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Cuiling Lan"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Xuejin Chen"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11574v2",
                "updated": "2025-02-21T11:04:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    11,
                    4,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-17T09:07:32Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    9,
                    7,
                    32,
                    0,
                    48,
                    0
                ],
                "title": "Large Language Models and Mathematical Reasoning Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Mathematical Reasoning Failures"
                },
                "summary": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling."
                },
                "authors": [
                    {
                        "name": "Johan Boye"
                    },
                    {
                        "name": "Birger Moell"
                    }
                ],
                "author_detail": {
                    "name": "Birger Moell"
                },
                "author": "Birger Moell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11102v2",
                "updated": "2025-02-21T10:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    54,
                    36,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T12:38:37Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    38,
                    37,
                    6,
                    47,
                    0
                ],
                "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for\n  Optimization Modeling"
                },
                "summary": "Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH."
                },
                "authors": [
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Zhonglin Xie"
                    },
                    {
                        "name": "Yaoyu Wu"
                    },
                    {
                        "name": "Can Ren"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen",
                "arxiv_comment": "This paper has 36 pages, 18 figures, and two co-first authors:\n  Hongliang Lu and Zhonglin Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15370v1",
                "updated": "2025-02-21T10:42:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    42,
                    4,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:42:04Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    42,
                    4,
                    4,
                    52,
                    0
                ],
                "title": "Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Video Scene Graph Generation via Natural Language\n  Supervision"
                },
                "summary": "Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully\nsupervised manner, which requires all frames in a video to be annotated,\nthereby incurring high annotation cost compared to Image Scene Graph Generation\n(ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting\na weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses\nimage captions, there are two key reasons that hinder such a naive adoption: 1)\nTemporality within video captions, i.e., unlike image captions, video captions\ninclude temporal markers (e.g., before, while, then, after) that indicate time\nrelated details, and 2) Variability in action duration, i.e., unlike human\nactions in image captions, human actions in video captions unfold over varying\nduration. To address these issues, we propose a Natural Language-based Video\nScene Graph Generation (NL-VSGG) framework that only utilizes the readily\navailable video captions for training a VidSGG model. NL-VSGG consists of two\nkey modules: Temporality-aware Caption Segmentation (TCS) module and Action\nDuration Variability-aware caption-frame alignment (ADV) module. Specifically,\nTCS segments the video captions into multiple sentences in a temporal order\nbased on a Large Language Model (LLM), and ADV aligns each segmented sentence\nwith appropriate frames considering the variability in action duration. Our\napproach leads to a significant enhancement in performance compared to simply\napplying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a\nfurther benefit of utilizing the video captions as weak supervision, we show\nthat the VidSGG model trained by NL-VSGG is able to predict a broader range of\naction classes that are not included in the training data, which makes our\nframework practical in reality."
                },
                "authors": [
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Kanghoon Yoon"
                    },
                    {
                        "name": "Yeonjun In"
                    },
                    {
                        "name": "Jaehyeong Jeon"
                    },
                    {
                        "name": "Jinyoung Moon"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "10 pages, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11187v2",
                "updated": "2025-02-21T10:33:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    33,
                    37,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-16T16:22:23Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    22,
                    23,
                    6,
                    47,
                    0
                ],
                "title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking"
                },
                "summary": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a)."
                },
                "authors": [
                    {
                        "name": "Shahriar Kabir Nahin"
                    },
                    {
                        "name": "Rabindra Nath Nandi"
                    },
                    {
                        "name": "Sagor Sarker"
                    },
                    {
                        "name": "Quazi Sarwar Muhtaseem"
                    },
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Apu Chandraw Shill"
                    },
                    {
                        "name": "Md Ibrahim"
                    },
                    {
                        "name": "Mehadi Hasan Menon"
                    },
                    {
                        "name": "Tareq Al Muntasir"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "LLMs, Benchmarking, Large Language Models, Bangla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15365v1",
                "updated": "2025-02-21T10:27:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    28,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    28,
                    4,
                    52,
                    0
                ],
                "title": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Features that Shape Perceived Consciousness in Large\n  Language Model-based AI: A Quantitative Study of Human Responses"
                },
                "summary": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study quantitively examines which features of AI-generated text lead\nhumans to perceive subjective consciousness in large language model (LLM)-based\nAI systems. Drawing on 99 passages from conversations with Claude 3 Opus and\nfocusing on eight features -- metacognitive self-reflection, logical reasoning,\nempathy, emotionality, knowledge, fluency, unexpectedness, and subjective\nexpressiveness -- we conducted a survey with 123 participants. Using regression\nand clustering analyses, we investigated how these features influence\nparticipants' perceptions of AI consciousness. The results reveal that\nmetacognitive self-reflection and the AI's expression of its own emotions\nsignificantly increased perceived consciousness, while a heavy emphasis on\nknowledge reduced it. Participants clustered into seven subgroups, each showing\ndistinct feature-weighting patterns. Additionally, higher prior knowledge of\nLLMs and more frequent usage of LLM-based chatbots were associated with greater\noverall likelihood assessments of AI consciousness. This study underscores the\nmultidimensional and individualized nature of perceived AI consciousness and\nprovides a foundation for better understanding the psychosocial implications of\nhuman-AI interaction."
                },
                "authors": [
                    {
                        "name": "Kang Bongsu"
                    },
                    {
                        "name": "Kim Jundong"
                    },
                    {
                        "name": "Yun Tae-Rim"
                    },
                    {
                        "name": "Bae Hyojin"
                    },
                    {
                        "name": "Kim Chang-Eop"
                    }
                ],
                "author_detail": {
                    "name": "Kim Chang-Eop"
                },
                "author": "Kim Chang-Eop",
                "arxiv_comment": "11 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14669v2",
                "updated": "2025-02-21T10:27:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    27,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T16:05:18Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    18,
                    3,
                    51,
                    0
                ],
                "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    }
                ],
                "author_detail": {
                    "name": "Dinh Bach Vu"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Dinh Bach Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13506v2",
                "updated": "2025-02-21T10:18:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    18,
                    26,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-19T07:50:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    50,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Reproducing NevIR: Negation in Neural Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing NevIR: Negation in Neural Information Retrieval"
                },
                "summary": "Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation is a fundamental aspect of human communication, yet it remains a\nchallenge for Language Models (LMs) in Information Retrieval (IR). Despite the\nheavy reliance of modern neural IR systems on LMs, little attention has been\ngiven to their handling of negation. In this study, we reproduce and extend the\nfindings of NevIR, a benchmark study that revealed most IR models perform at or\nbelow the level of random ranking when dealing with negation. We replicate\nNevIR's original experiments and evaluate newly developed state-of-the-art IR\nmodels. Our findings show that a recently emerging category - listwise Large\nLanguage Model (LLM) rerankers - outperforms other models but still\nunderperforms human performance. Additionally, we leverage ExcluIR, a benchmark\ndataset designed for exclusionary queries with extensive negation, to assess\nthe generalizability of negation understanding. Our findings suggest that\nfine-tuning on one dataset does not reliably improve performance on the other,\nindicating notable differences in their data distributions. Furthermore, we\nobserve that only cross-encoders and listwise LLM rerankers achieve reasonable\nperformance across both negation tasks."
                },
                "authors": [
                    {
                        "name": "Coen van den Elsen"
                    },
                    {
                        "name": "Francien Barkhof"
                    },
                    {
                        "name": "Thijmen Nijdam"
                    },
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Alliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alliannejadi"
                },
                "author": "Mohammad Alliannejadi",
                "arxiv_comment": "9 pages, 5 figures, under review at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15361v1",
                "updated": "2025-02-21T10:16:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    16,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:16:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    16,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Evaluating Social Biases in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Social Biases in LLM Reasoning"
                },
                "summary": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Xuyang Wu"
                    },
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Zhiqiang Tao"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15359v1",
                "updated": "2025-02-21T10:14:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    14,
                    55,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:14:55Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    14,
                    55,
                    4,
                    52,
                    0
                ],
                "title": "ARS: Automatic Routing Solver with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARS: Automatic Routing Solver with Large Language Models"
                },
                "summary": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15349v1",
                "updated": "2025-02-21T10:06:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    6,
                    41,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:06:41Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    6,
                    41,
                    4,
                    52,
                    0
                ],
                "title": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms"
                },
                "summary": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine."
                },
                "authors": [
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Zhi Yang"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15348v1",
                "updated": "2025-02-21T10:02:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    2,
                    15,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T10:02:15Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    2,
                    15,
                    4,
                    52,
                    0
                ],
                "title": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing a Norm for Children's Scientific Drawing: Distribution\n  Features Based on Semantic Similarity of Large Language Models"
                },
                "summary": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of children's drawings to examining their conceptual understanding\nhas been proven to be an effective method, but there are two major problems\nwith previous research: 1. The content of the drawings heavily relies on the\ntask, and the ecological validity of the conclusions is low; 2. The\ninterpretation of drawings relies too much on the subjective feelings of the\nresearchers. To address this issue, this study uses the Large Language Model\n(LLM) to identify 1420 children's scientific drawings (covering 9 scientific\nthemes/concepts), and uses the word2vec algorithm to calculate their semantic\nsimilarity. The study explores whether there are consistent drawing\nrepresentations for children on the same theme, and attempts to establish a\nnorm for children's scientific drawings, providing a baseline reference for\nfollow-up children's drawing research. The results show that the representation\nof most drawings has consistency, manifested as most semantic similarity\ngreater than 0.8. At the same time, it was found that the consistency of the\nrepresentation is independent of the accuracy (of LLM's recognition),\nindicating the existence of consistency bias. In the subsequent exploration of\ninfluencing factors, we used Kendall rank correlation coefficient to\ninvestigate the effects of Sample Size, Abstract Degree, and Focus Points on\ndrawings, and used word frequency statistics to explore whether children\nrepresented abstract themes/concepts by reproducing what was taught in class."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Fan Wei"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yanyan Yu"
                    },
                    {
                        "name": "Jianli Chen"
                    },
                    {
                        "name": "Zipo Cai"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Wang"
                },
                "author": "Zhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15343v1",
                "updated": "2025-02-21T09:58:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    58,
                    54,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:58:54Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    58,
                    54,
                    4,
                    52,
                    0
                ],
                "title": "Tokenization is Sensitive to Language Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is Sensitive to Language Variation"
                },
                "summary": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance."
                },
                "authors": [
                    {
                        "name": "Anna Wegmann"
                    },
                    {
                        "name": "Dong Nguyen"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15336v1",
                "updated": "2025-02-21T09:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    41,
                    27,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:41:27Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    41,
                    27,
                    4,
                    52,
                    0
                ],
                "title": "Exploring Embodied Multimodal Large Models: Development, Datasets, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Embodied Multimodal Large Models: Development, Datasets, and\n  Future Directions"
                },
                "summary": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied multimodal large models (EMLMs) have gained significant attention in\nrecent years due to their potential to bridge the gap between perception,\ncognition, and action in complex, real-world environments. This comprehensive\nreview explores the development of such models, including Large Language Models\n(LLMs), Large Vision Models (LVMs), and other models, while also examining\nother emerging architectures. We discuss the evolution of EMLMs, with a focus\non embodied perception, navigation, interaction, and simulation. Furthermore,\nthe review provides a detailed analysis of the datasets used for training and\nevaluating these models, highlighting the importance of diverse, high-quality\ndata for effective learning. The paper also identifies key challenges faced by\nEMLMs, including issues of scalability, generalization, and real-time\ndecision-making. Finally, we outline future directions, emphasizing the\nintegration of multimodal sensing, reasoning, and action to advance the\ndevelopment of increasingly autonomous systems. By providing an in-depth\nanalysis of state-of-the-art methods and identifying critical gaps, this paper\naims to inspire future advancements in EMLMs and their applications across\ndiverse domains."
                },
                "authors": [
                    {
                        "name": "Shoubin Chen"
                    },
                    {
                        "name": "Zehao Wu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chunyu Li"
                    },
                    {
                        "name": "Baiyang Zhang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Fei Richard Yu"
                    },
                    {
                        "name": "Qingquan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingquan Li"
                },
                "author": "Qingquan Li",
                "arxiv_comment": "81 pages, submitted to a journal for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15335v1",
                "updated": "2025-02-21T09:39:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    39,
                    27,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:39:27Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    39,
                    27,
                    4,
                    52,
                    0
                ],
                "title": "Stepwise Informativeness Search for Improving LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Informativeness Search for Improving LLM Reasoning"
                },
                "summary": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have significantly improved\nmulti-step reasoning through generating free-text rationales. However, recent\nstudies show that LLMs tend to lose focus over the middle of long contexts.\nThis raises concerns that as reasoning progresses, LLMs may overlook\ninformation in earlier steps when decoding subsequent steps, leading to\ngenerate unreliable and redundant rationales. To address this, we propose\nguiding LLMs to generate more accurate and concise step-by-step rationales by\n(1) proactively referencing information from underutilized prior steps, and (2)\nminimizing redundant information between new and existing steps. We introduce\nstepwise informativeness search, an inference-time tree search framework\nincorporating two selection heuristics: grounding-guided selection which\nprioritizes steps paying higher attention over underutilized steps; and\nnovelty-guided selection which encourages steps with novel conclusions. During\nrationale generation, we use a self-grounding strategy that prompts LLMs to\nexplicitly reference relevant prior steps to provide premises before deduction\nat each step. Experimental results on four reasoning datasets demonstrate that\nour approach improves reasoning accuracy by generating higher-quality\nrationales with reduced errors and redundancy."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Enda Zhao"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15334v1",
                "updated": "2025-02-21T09:38:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:38:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    38,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment"
                },
                "summary": "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B/AdvBench, using less than a third of the generation time)."
                },
                "authors": [
                    {
                        "name": "Pedram Zaree"
                    },
                    {
                        "name": "Md Abdullah Al Mamun"
                    },
                    {
                        "name": "Quazi Mishkatul Alam"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Ihsen Alouani"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15332v1",
                "updated": "2025-02-21T09:34:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    34,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:34:34Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    34,
                    4,
                    52,
                    0
                ],
                "title": "Detecting Future-related Contexts of Entity Mentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Future-related Contexts of Entity Mentions"
                },
                "summary": "The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to automatically identify whether an entity is referenced in a\nfuture context can have multiple applications including decision making,\nplanning and trend forecasting. This paper focuses on detecting implicit future\nreferences in entity-centric texts, addressing the growing need for automated\ntemporal analysis in information processing. We first present a novel dataset\nof 19,540 sentences built around popular entities sourced from Wikipedia, which\nconsists of future-related and non-future-related contexts in which those\nentities appear. As a second contribution, we evaluate the performance of\nseveral Language Models including also Large Language Models (LLMs) on the task\nof distinguishing future-oriented content in the absence of explicit temporal\nreferences."
                },
                "authors": [
                    {
                        "name": "Puneet Prashar"
                    },
                    {
                        "name": "Krishna Mohan Shukla"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15331v1",
                "updated": "2025-02-21T09:34:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    31,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T09:34:31Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    34,
                    31,
                    4,
                    52,
                    0
                ],
                "title": "Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation"
                },
                "summary": "Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS."
                },
                "authors": [
                    {
                        "name": "Jinyu Zhang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Zhongying Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhongying Zhao"
                },
                "author": "Zhongying Zhao",
                "arxiv_comment": "26 pages, 8 figures, journal paper, accepted by TOIS at 20th\n  February, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v1",
                "updated": "2025-02-21T08:40:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15292v1",
                "updated": "2025-02-21T08:37:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    37,
                    2,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:37:02Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    37,
                    2,
                    4,
                    52,
                    0
                ],
                "title": "Bridging Bug Localization and Issue Fixing: A Hierarchical Localization\n  Framework Leveraging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Bug Localization and Issue Fixing: A Hierarchical Localization\n  Framework Leveraging Large Language Models"
                },
                "summary": "Automated issue fixing is a critical task in software debugging and has\nrecently garnered significant attention from academia and industry. However,\nexisting fixing techniques predominantly focus on the repair phase, often\noverlooking the importance of improving the preceding bug localization phase.\nAs a foundational step in issue fixing, bug localization plays a pivotal role\nin determining the overall effectiveness of the entire process.\n  To enhance the precision of issue fixing by accurately identifying bug\nlocations in large-scale projects, this paper presents BugCerberus, the first\nhierarchical bug localization framework powered by three customized large\nlanguage models. First, BugCerberus analyzes intermediate representations of\nbug-related programs at file, function, and statement levels and extracts\nbug-related contextual information from the representations. Second,\nBugCerberus designs three customized LLMs at each level using bug reports and\ncontexts to learn the patterns of bugs. Finally, BugCerberus hierarchically\nsearches for bug-related code elements through well-tuned models to localize\nbugs at three levels. With BugCerberus, we further investigate the impact of\nbug localization on the issue fixing.\n  We evaluate BugCerberus on the widely-used benchmark SWE-bench-lite. The\nexperimental results demonstrate that BugCerberus outperforms all baselines.\nSpecifically, at the fine-grained statement level, BugCerberus surpasses the\nstate-of-the-art in Top-N (N=1, 3, 5, 10) by 16.5%, 5.4%, 10.2%, and 23.1%,\nrespectively. Moreover, in the issue fixing experiments, BugCerberus improves\nthe fix rate of the existing issue fixing approach Agentless by 17.4% compared\nto the best baseline, highlighting the significant impact of enhanced bug\nlocalization on automated issue fixing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated issue fixing is a critical task in software debugging and has\nrecently garnered significant attention from academia and industry. However,\nexisting fixing techniques predominantly focus on the repair phase, often\noverlooking the importance of improving the preceding bug localization phase.\nAs a foundational step in issue fixing, bug localization plays a pivotal role\nin determining the overall effectiveness of the entire process.\n  To enhance the precision of issue fixing by accurately identifying bug\nlocations in large-scale projects, this paper presents BugCerberus, the first\nhierarchical bug localization framework powered by three customized large\nlanguage models. First, BugCerberus analyzes intermediate representations of\nbug-related programs at file, function, and statement levels and extracts\nbug-related contextual information from the representations. Second,\nBugCerberus designs three customized LLMs at each level using bug reports and\ncontexts to learn the patterns of bugs. Finally, BugCerberus hierarchically\nsearches for bug-related code elements through well-tuned models to localize\nbugs at three levels. With BugCerberus, we further investigate the impact of\nbug localization on the issue fixing.\n  We evaluate BugCerberus on the widely-used benchmark SWE-bench-lite. The\nexperimental results demonstrate that BugCerberus outperforms all baselines.\nSpecifically, at the fine-grained statement level, BugCerberus surpasses the\nstate-of-the-art in Top-N (N=1, 3, 5, 10) by 16.5%, 5.4%, 10.2%, and 23.1%,\nrespectively. Moreover, in the issue fixing experiments, BugCerberus improves\nthe fix rate of the existing issue fixing approach Agentless by 17.4% compared\nto the best baseline, highlighting the significant impact of enhanced bug\nlocalization on automated issue fixing."
                },
                "authors": [
                    {
                        "name": "Jianming Chang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Lulu Wang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Bixin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bixin Li"
                },
                "author": "Bixin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00750v2",
                "updated": "2025-02-21T08:00:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    0,
                    10,
                    4,
                    52,
                    0
                ],
                "published": "2024-11-01T17:18:45Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    17,
                    18,
                    45,
                    4,
                    306,
                    0
                ],
                "title": "Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided\n  Sampling"
                },
                "summary": "Self-improvement methods enable large language models (LLMs) to generate\nsolutions themselves and iteratively train on filtered, high-quality\nrationales. This process proves effective and reduces the reliance on human\nsupervision in LLMs' reasoning, but the performance soon plateaus. We delve\ninto the process and find that models tend to over-sample on easy queries and\nunder-sample on queries they have yet to master. As iterations proceed, this\nimbalance in sampling is exacerbated, leading to a long-tail distribution where\nsolutions to difficult queries almost diminish. This phenomenon limits the\nperformance gain of self-improving models. A straightforward solution is\nbrute-force sampling to balance the distribution, which significantly raises\ncomputational costs. In this paper, we introduce Guided Self-Improvement (GSI),\na strategy aimed at improving the efficiency of sampling challenging\nheavy-tailed data. It leverages Socratic-style guidance signals to help LLM\nreasoning with complex queries, reducing the exploration effort and minimizing\ncomputational overhead. Experiments on four models across diverse mathematical\ntasks show that GSI strikes a balance between performance and efficiency, while\nalso being effective on held-out tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-improvement methods enable large language models (LLMs) to generate\nsolutions themselves and iteratively train on filtered, high-quality\nrationales. This process proves effective and reduces the reliance on human\nsupervision in LLMs' reasoning, but the performance soon plateaus. We delve\ninto the process and find that models tend to over-sample on easy queries and\nunder-sample on queries they have yet to master. As iterations proceed, this\nimbalance in sampling is exacerbated, leading to a long-tail distribution where\nsolutions to difficult queries almost diminish. This phenomenon limits the\nperformance gain of self-improving models. A straightforward solution is\nbrute-force sampling to balance the distribution, which significantly raises\ncomputational costs. In this paper, we introduce Guided Self-Improvement (GSI),\na strategy aimed at improving the efficiency of sampling challenging\nheavy-tailed data. It leverages Socratic-style guidance signals to help LLM\nreasoning with complex queries, reducing the exploration effort and minimizing\ncomputational overhead. Experiments on four models across diverse mathematical\ntasks show that GSI strikes a balance between performance and efficiency, while\nalso being effective on held-out tasks."
                },
                "authors": [
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Yitao Zhai"
                    },
                    {
                        "name": "Xiaowei Shi"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference. Codes are publicly available\n  at https://github.com/Yiwen-Ding/Guided-Self-Improvement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]