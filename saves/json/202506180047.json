[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v1",
                "updated": "2025-06-15T03:41:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\neven under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\neven under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v2",
                "updated": "2025-06-14T04:39:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    4,
                    39,
                    21,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07533v1",
                "updated": "2025-06-09T08:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T08:16:24Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts"
                },
                "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Haocheng Lu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v2",
                "updated": "2025-06-09T07:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    58,
                    19,
                    0,
                    160,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "The paper needs major modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07334v1",
                "updated": "2025-06-09T00:30:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T00:30:08Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models"
                },
                "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07311v1",
                "updated": "2025-06-08T22:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T22:59:20Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference"
                },
                "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Joshi"
                    },
                    {
                        "name": "Herman Saini"
                    },
                    {
                        "name": "Neil Dhillon"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar El Maghraoui"
                },
                "author": "Kaoutar El Maghraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v3",
                "updated": "2025-06-08T21:23:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    21,
                    23,
                    22,
                    6,
                    159,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v3",
                "updated": "2025-06-08T20:04:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    20,
                    4,
                    17,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08508v2",
                "updated": "2025-06-08T16:07:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    7,
                    44,
                    6,
                    159,
                    0
                ],
                "published": "2022-10-16T11:21:26Z",
                "published_parsed": [
                    2022,
                    10,
                    16,
                    11,
                    21,
                    26,
                    6,
                    289,
                    0
                ],
                "title": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory"
                },
                "summary": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions."
                },
                "authors": [
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Geraldo F. Oliveira"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Rachata Ausavarungnirun"
                    },
                    {
                        "name": "Juan Gmez Luna"
                    },
                    {
                        "name": "Joo Ferreira"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Nandita Vijaykumar"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v5",
                "updated": "2025-06-08T16:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    4,
                    59,
                    6,
                    159,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07200v1",
                "updated": "2025-06-08T15:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T15:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions"
                },
                "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach."
                },
                "authors": [
                    {
                        "name": "Kanato Nakanishi"
                    },
                    {
                        "name": "Soramichi Akiyama"
                    }
                ],
                "author_detail": {
                    "name": "Soramichi Akiyama"
                },
                "author": "Soramichi Akiyama",
                "arxiv_comment": "Presented in Machine Learning for Computer Architecture and Systems\n  (MLArchSys), June 21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v2",
                "updated": "2025-06-08T09:30:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    9,
                    30,
                    12,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1"
                },
                "summary": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "arxiv_comment": "Accepted to appear at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v2",
                "updated": "2025-06-08T00:52:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    52,
                    33,
                    6,
                    159,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v2",
                "updated": "2025-06-07T19:22:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    19,
                    22,
                    5,
                    5,
                    158,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v2",
                "updated": "2025-06-07T14:03:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    14,
                    3,
                    6,
                    5,
                    158,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "arxiv_comment": "Published in the Proceedings of the 42nd International Conference on\n  Machine Learning (ICML), Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06773v1",
                "updated": "2025-06-07T11:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-07T11:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor"
                },
                "summary": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09."
                },
                "authors": [
                    {
                        "name": "Emet Behrendt"
                    },
                    {
                        "name": "Shing Wai Pun"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Paper accepted and presented at the 6th Championship Branch\n  Prediction (CBP) workshop, co-held with ISCA 2025, on June 21, 2025, Tokyo,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.2; B.2.1; C.4; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v2",
                "updated": "2025-06-07T01:36:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    36,
                    34,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v1",
                "updated": "2025-06-06T18:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v1",
                "updated": "2025-06-06T09:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05811v1",
                "updated": "2025-06-06T07:20:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T07:20:25Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "title": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul"
                },
                "summary": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander."
                },
                "authors": [
                    {
                        "name": "Kari Aaron Clark"
                    },
                    {
                        "name": "Zun Htay"
                    },
                    {
                        "name": "Zichuan Zhou"
                    },
                    {
                        "name": "Amany Kassem"
                    },
                    {
                        "name": "Andrea Pertoldi"
                    },
                    {
                        "name": "Benjamin Rudin"
                    },
                    {
                        "name": "Florian Emaury"
                    },
                    {
                        "name": "Izzat Darwazeh"
                    },
                    {
                        "name": "Zhixin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Liu"
                },
                "author": "Zhixin Liu",
                "arxiv_comment": "Conference manuscript submitted to the European Conference on Optical\n  Communication 2025 (ECOC 2025) on 2nd May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16800v2",
                "updated": "2025-06-06T06:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    6,
                    35,
                    52,
                    4,
                    157,
                    0
                ],
                "published": "2023-05-26T10:29:25Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    10,
                    29,
                    25,
                    4,
                    146,
                    0
                ],
                "title": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache"
                },
                "summary": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe."
                },
                "authors": [
                    {
                        "name": "Jiakai Sun"
                    },
                    {
                        "name": "Weijing Zhang"
                    },
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Tianyi Chu"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v3",
                "updated": "2025-06-06T02:29:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    29,
                    18,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05682v1",
                "updated": "2025-06-06T02:20:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T02:20:49Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy"
                },
                "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Weikai Lin"
                    },
                    {
                        "name": "Yuge Cheng"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Yuhao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhu"
                },
                "author": "Yuhao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v3",
                "updated": "2025-06-05T20:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    20,
                    50,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v1",
                "updated": "2025-06-05T19:47:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian acucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05347v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Neural Inverse Rendering from Propagating Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Inverse Rendering from Propagating Light"
                },
                "summary": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes."
                },
                "authors": [
                    {
                        "name": "Anagh Malik"
                    },
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Andrew Xie"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "David B. Lindell"
                    }
                ],
                "author_detail": {
                    "name": "David B. Lindell"
                },
                "author": "David B. Lindell",
                "arxiv_comment": "Website: https://anaghmalik.com/InvProp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05071v1",
                "updated": "2025-06-05T14:19:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Hierarchy Design for Caching Middleware in the Age of NVM"
                },
                "summary": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    }
                ],
                "author_detail": {
                    "name": "Jenny Lam"
                },
                "author": "Jenny Lam",
                "arxiv_doi": "10.1109/ICDE.2018.00155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE.2018.00155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version appeared in the IEEE 34th International Conference\n  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:\n  10.1109/ICDE.2018.00155",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v2",
                "updated": "2025-06-05T13:38:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    38,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v2",
                "updated": "2025-06-05T13:20:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04920v1",
                "updated": "2025-06-05T11:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T11:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive."
                },
                "authors": [
                    {
                        "name": "Junior Cedric Tonga"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Preprint, in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04844v1",
                "updated": "2025-06-05T10:11:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments"
                },
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors."
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-Garca"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04826v1",
                "updated": "2025-06-05T09:49:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T09:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation"
                },
                "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture."
                },
                "authors": [
                    {
                        "name": "Konstantinos Giotis"
                    },
                    {
                        "name": "Dimitrios Stefas"
                    },
                    {
                        "name": "Yanis Agha"
                    },
                    {
                        "name": "Hans Hft"
                    },
                    {
                        "name": "Xavier Duten"
                    },
                    {
                        "name": "Panagiotis Svarnas"
                    },
                    {
                        "name": "Guillaume Lombardi"
                    },
                    {
                        "name": "Kristaq Gazeli"
                    }
                ],
                "author_detail": {
                    "name": "Kristaq Gazeli"
                },
                "arxiv_affiliation": "LSPM",
                "author": "Kristaq Gazeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04642v1",
                "updated": "2025-06-05T05:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T05:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering"
                },
                "summary": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts."
                },
                "authors": [
                    {
                        "name": "Vinay Joshi"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "ACL-2025 industry-track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v3",
                "updated": "2025-06-05T04:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    4,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v2",
                "updated": "2025-06-05T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    27,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v2",
                "updated": "2025-06-04T23:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    47,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "arxiv_comment": "This paper was intended to be a v2 version of my previous paper\n  (arXiv:2409.17566), but it was submitted as a new paper by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v3",
                "updated": "2025-06-04T22:37:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    22,
                    37,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v2",
                "updated": "2025-06-04T18:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    10,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v1",
                "updated": "2025-06-04T16:10:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03854v1",
                "updated": "2025-06-04T11:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks"
                },
                "summary": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases."
                },
                "authors": [
                    {
                        "name": "Emmanouil Anagnostakis"
                    },
                    {
                        "name": "Polyvios Pratikakis"
                    }
                ],
                "author_detail": {
                    "name": "Polyvios Pratikakis"
                },
                "author": "Polyvios Pratikakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03762v1",
                "updated": "2025-06-04T09:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T09:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Yifeng Gu"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Jianxiu Jin"
                    },
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03700v1",
                "updated": "2025-06-04T08:32:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T08:32:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "arxiv_comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.13759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13759v1",
                "updated": "2025-06-16T17:59:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:59:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
                },
                "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13756v1",
                "updated": "2025-06-16T17:58:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    29,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:58:29Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    29,
                    0,
                    167,
                    0
                ],
                "title": "UltraZoom: Generating Gigapixel Images from Regular Photos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraZoom: Generating Gigapixel Images from Regular Photos"
                },
                "summary": "We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input."
                },
                "authors": [
                    {
                        "name": "Jingwei Ma"
                    },
                    {
                        "name": "Vivek Jayaram"
                    },
                    {
                        "name": "Brian Curless"
                    },
                    {
                        "name": "Ira Kemelmacher-Shlizerman"
                    },
                    {
                        "name": "Steven M. Seitz"
                    }
                ],
                "author_detail": {
                    "name": "Steven M. Seitz"
                },
                "author": "Steven M. Seitz",
                "arxiv_comment": "Project page: https://ultra-zoom.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13755v1",
                "updated": "2025-06-16T17:58:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    9,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:58:09Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    9,
                    0,
                    167,
                    0
                ],
                "title": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering"
                },
                "summary": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%."
                },
                "authors": [
                    {
                        "name": "Arya Fayyazi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13754v2",
                "updated": "2025-06-17T02:15:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    15,
                    17,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T17:58:00Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    0,
                    0,
                    167,
                    0
                ],
                "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models"
                },
                "summary": "We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Edward Li"
                    },
                    {
                        "name": "Zichen Wang"
                    },
                    {
                        "name": "Jiahe Huang"
                    },
                    {
                        "name": "Jeong Joon Park"
                    }
                ],
                "author_detail": {
                    "name": "Jeong Joon Park"
                },
                "author": "Jeong Joon Park",
                "arxiv_comment": "Project page: https://videopde.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13752v1",
                "updated": "2025-06-16T17:57:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:57:05Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "title": "Steering LLM Thinking with Budget Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Thinking with Budget Guidance"
                },
                "summary": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Wenshuo Zhao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13746v1",
                "updated": "2025-06-16T17:54:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    54,
                    28,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:54:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    54,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Evaluating Large Language Models for Phishing Detection,\n  Self-Consistency, Faithfulness, and Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Phishing Detection,\n  Self-Consistency, Faithfulness, and Explainability"
                },
                "summary": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores."
                },
                "authors": [
                    {
                        "name": "Shova Kuikel"
                    },
                    {
                        "name": "Aritran Piplai"
                    },
                    {
                        "name": "Palvi Aggarwal"
                    }
                ],
                "author_detail": {
                    "name": "Palvi Aggarwal"
                },
                "author": "Palvi Aggarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13743v1",
                "updated": "2025-06-16T17:53:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    53,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:53:18Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    53,
                    18,
                    0,
                    167,
                    0
                ],
                "title": "LTRR: Learning To Rank Retrievers for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTRR: Learning To Rank Retrievers for LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems."
                },
                "authors": [
                    {
                        "name": "To Eun Kim"
                    },
                    {
                        "name": "Fernando Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Diaz"
                },
                "author": "Fernando Diaz",
                "arxiv_comment": "SIGIR 2025 LiveRAG Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13740v1",
                "updated": "2025-06-16T17:50:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    50,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:50:23Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    50,
                    23,
                    0,
                    167,
                    0
                ],
                "title": "Kolmogorov-Arnold Network for Gene Regulatory Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov-Arnold Network for Gene Regulatory Network Inference"
                },
                "summary": "Gene regulation is central to understanding cellular processes and\ndevelopment, potentially leading to the discovery of new treatments for\ndiseases and personalized medicine. Inferring gene regulatory networks (GRNs)\nfrom single-cell RNA sequencing (scRNA-seq) data presents significant\nchallenges due to its high dimensionality and complexity. Existing tree-based\nmodels, such as GENIE3 and GRNBOOST2, demonstrated scalability and\nexplainability in GRN inference, but they cannot distinguish regulation types\nnor effectively capture continuous cellular dynamics. In this paper, we\nintroduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN)\nwith explainable AI to infer GRNs from scRNA-seq data. By modeling gene\nexpression as differentiable functions matching the smooth nature of cellular\ndynamics, scKAN can accurately and precisely detect activation and inhibition\nregulations through explainable AI and geometric tools. We conducted extensive\nexperiments on the BEELINE benchmark, and scKAN surpasses and improves the\nleading signed GRN inference models ranging from 5.40\\% to 28.37\\% in AUROC and\nfrom 1.97\\% to 40.45\\% in AUPRC. These results highlight the potential of scKAN\nin capturing the underlying biological processes in gene regulation without\nprior knowledge of the graph structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene regulation is central to understanding cellular processes and\ndevelopment, potentially leading to the discovery of new treatments for\ndiseases and personalized medicine. Inferring gene regulatory networks (GRNs)\nfrom single-cell RNA sequencing (scRNA-seq) data presents significant\nchallenges due to its high dimensionality and complexity. Existing tree-based\nmodels, such as GENIE3 and GRNBOOST2, demonstrated scalability and\nexplainability in GRN inference, but they cannot distinguish regulation types\nnor effectively capture continuous cellular dynamics. In this paper, we\nintroduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN)\nwith explainable AI to infer GRNs from scRNA-seq data. By modeling gene\nexpression as differentiable functions matching the smooth nature of cellular\ndynamics, scKAN can accurately and precisely detect activation and inhibition\nregulations through explainable AI and geometric tools. We conducted extensive\nexperiments on the BEELINE benchmark, and scKAN surpasses and improves the\nleading signed GRN inference models ranging from 5.40\\% to 28.37\\% in AUROC and\nfrom 1.97\\% to 40.45\\% in AUPRC. These results highlight the potential of scKAN\nin capturing the underlying biological processes in gene regulation without\nprior knowledge of the graph structure."
                },
                "authors": [
                    {
                        "name": "Tsz Pan Tong"
                    },
                    {
                        "name": "Aoran Wang"
                    },
                    {
                        "name": "George Panagopoulos"
                    },
                    {
                        "name": "Jun Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Pang"
                },
                "author": "Jun Pang",
                "arxiv_comment": "26 pages, 14 figures, accepted in CMSB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13734v1",
                "updated": "2025-06-16T17:42:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:42:35Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "title": "Instruction Following by Boosting Attention of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Following by Boosting Attention of Large Language Models"
                },
                "summary": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering."
                },
                "authors": [
                    {
                        "name": "Vitoria Guardieiro"
                    },
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13733v1",
                "updated": "2025-06-16T17:41:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    41,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:41:36Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    41,
                    36,
                    0,
                    167,
                    0
                ],
                "title": "Robust Recursive Fusion of Multiresolution Multispectral Images with\n  Location-Aware Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Recursive Fusion of Multiresolution Multispectral Images with\n  Location-Aware Neural Networks"
                },
                "summary": "Multiresolution image fusion is a key problem for real-time satellite imaging\nand plays a central role in detecting and monitoring natural phenomena such as\nfloods. It aims to solve the trade-off between temporal and spatial resolution\nin remote sensing instruments. Although several algorithms have been proposed\nfor this problem, the presence of outliers such as clouds downgrades their\nperformance. Moreover, strategies that integrate robustness, recursive\noperation and learned models are missing. In this paper, a robust recursive\nimage fusion framework leveraging location-aware neural networks (NN) to model\nthe image dynamics is proposed. Outliers are modeled by representing the\nprobability of contamination of a given pixel and band. A NN model trained on a\nsmall dataset provides accurate predictions of the stochastic image time\nevolution, which improves both the accuracy and robustness of the method. A\nrecursive solution is proposed to estimate the high-resolution images using a\nBayesian variational inference framework. Experiments fusing images from the\nLandsat 8 and MODIS instruments show that the proposed approach is\nsignificantly more robust against cloud cover, without losing performance when\nno clouds are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiresolution image fusion is a key problem for real-time satellite imaging\nand plays a central role in detecting and monitoring natural phenomena such as\nfloods. It aims to solve the trade-off between temporal and spatial resolution\nin remote sensing instruments. Although several algorithms have been proposed\nfor this problem, the presence of outliers such as clouds downgrades their\nperformance. Moreover, strategies that integrate robustness, recursive\noperation and learned models are missing. In this paper, a robust recursive\nimage fusion framework leveraging location-aware neural networks (NN) to model\nthe image dynamics is proposed. Outliers are modeled by representing the\nprobability of contamination of a given pixel and band. A NN model trained on a\nsmall dataset provides accurate predictions of the stochastic image time\nevolution, which improves both the accuracy and robustness of the method. A\nrecursive solution is proposed to estimate the high-resolution images using a\nBayesian variational inference framework. Experiments fusing images from the\nLandsat 8 and MODIS instruments show that the proposed approach is\nsignificantly more robust against cloud cover, without losing performance when\nno clouds are present."
                },
                "authors": [
                    {
                        "name": "Haoqing Li"
                    },
                    {
                        "name": "Ricardo Borsoi"
                    },
                    {
                        "name": "Tales Imbiriba"
                    },
                    {
                        "name": "Pau Closas"
                    }
                ],
                "author_detail": {
                    "name": "Pau Closas"
                },
                "author": "Pau Closas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13727v1",
                "updated": "2025-06-16T17:38:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    38,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:38:36Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    38,
                    36,
                    0,
                    167,
                    0
                ],
                "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs"
                },
                "summary": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3."
                },
                "authors": [
                    {
                        "name": "Sayed Mohammad Vakilzadeh Hatefi"
                    },
                    {
                        "name": "Maximilian Dreyer"
                    },
                    {
                        "name": "Reduan Achtibat"
                    },
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "arxiv_comment": "Work in progress (10 pages manuscript, 3 pages references, 12 pages\n  appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05606v2",
                "updated": "2025-06-16T17:32:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    32,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-05T21:37:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    37,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation"
                },
                "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Amirali Amini"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Yakov Bart"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Upol Ehsan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Lydia Chilton"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13725v1",
                "updated": "2025-06-16T17:31:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    31,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:31:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    31,
                    16,
                    0,
                    167,
                    0
                ],
                "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit\n  Decoding"
                },
                "summary": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/."
                },
                "authors": [
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Haoang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoang Li"
                },
                "author": "Haoang Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13716v1",
                "updated": "2025-06-16T17:24:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    24,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:24:23Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    24,
                    23,
                    0,
                    167,
                    0
                ],
                "title": "Mass models of galaxy clusters from a non-parametric weak-lensing\n  reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass models of galaxy clusters from a non-parametric weak-lensing\n  reconstruction"
                },
                "summary": "We study the CLASH sample of galaxy clusters using a new deprojection method\nfor weak gravitational lensing observations. This method is non-parametric,\nallowing us to infer mass profiles, or equivalently circular velocities,\nwithout having to assume a specific halo profile. While this method assumes\nspherical symmetry, we show that, on average, triaxiality is unlikely to\nsignificantly affect our results. We use this method to study the total mass\nprofiles of the CLASH clusters, as well as the relation between their total and\nbaryonic components: (1) We find that the implied circular velocities are\nconsistent with being approximately flat at large radii, akin to the rotation\ncurves of galaxies. (2) We infer radially resolved baryonic mass fractions,\nfinding that these vary significantly from cluster to cluster and depend\nstrongly on the details of the X-ray gas mass profiles. Since the gas mass\nprofiles are poorly constrained at large radii, it is unclear whether the CLASH\nclusters reach the cosmic baryon fraction expected in $\\Lambda$CDM. (3) The\nnon-parametric masses are consistent with the stellar mass--halo mass relation\nexpected in $\\Lambda$CDM; fitting parametric NFW halos to the non-parametric\nmass profiles gives results in overall agreement with the expected\nmass-concentration relation, though the concentrations are relatively poorly\nconstrained. (4) Galaxy clusters systematically deviate from the Baryonic\nTully-Fisher Relation (BTFR) and the Radial Acceleration Relation (RAR) defined\nby galaxies, but the magnitude of the offset depends strongly on the gas mass\nextrapolation at large radii. Contrary to some previous results based on\nhydrostatic equilibrium, we find that galaxy clusters may fall on the same BTFR\nand RAR as galaxies if one adds a suitable positive baryonic mass component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the CLASH sample of galaxy clusters using a new deprojection method\nfor weak gravitational lensing observations. This method is non-parametric,\nallowing us to infer mass profiles, or equivalently circular velocities,\nwithout having to assume a specific halo profile. While this method assumes\nspherical symmetry, we show that, on average, triaxiality is unlikely to\nsignificantly affect our results. We use this method to study the total mass\nprofiles of the CLASH clusters, as well as the relation between their total and\nbaryonic components: (1) We find that the implied circular velocities are\nconsistent with being approximately flat at large radii, akin to the rotation\ncurves of galaxies. (2) We infer radially resolved baryonic mass fractions,\nfinding that these vary significantly from cluster to cluster and depend\nstrongly on the details of the X-ray gas mass profiles. Since the gas mass\nprofiles are poorly constrained at large radii, it is unclear whether the CLASH\nclusters reach the cosmic baryon fraction expected in $\\Lambda$CDM. (3) The\nnon-parametric masses are consistent with the stellar mass--halo mass relation\nexpected in $\\Lambda$CDM; fitting parametric NFW halos to the non-parametric\nmass profiles gives results in overall agreement with the expected\nmass-concentration relation, though the concentrations are relatively poorly\nconstrained. (4) Galaxy clusters systematically deviate from the Baryonic\nTully-Fisher Relation (BTFR) and the Radial Acceleration Relation (RAR) defined\nby galaxies, but the magnitude of the offset depends strongly on the gas mass\nextrapolation at large radii. Contrary to some previous results based on\nhydrostatic equilibrium, we find that galaxy clusters may fall on the same BTFR\nand RAR as galaxies if one adds a suitable positive baryonic mass component."
                },
                "authors": [
                    {
                        "name": "Tobias Mistele"
                    },
                    {
                        "name": "Federico Lelli"
                    },
                    {
                        "name": "Stacy McGaugh"
                    },
                    {
                        "name": "James Schombert"
                    },
                    {
                        "name": "Benoit Famaey"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Famaey"
                },
                "author": "Benoit Famaey",
                "arxiv_comment": "15 pages, 15 figures, plus appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13715v1",
                "updated": "2025-06-16T17:24:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    24,
                    10,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:24:10Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    24,
                    10,
                    0,
                    167,
                    0
                ],
                "title": "Sharpness-Aware Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharpness-Aware Machine Unlearning"
                },
                "summary": "We characterize the effectiveness of Sharpness-aware minimization (SAM) under\nmachine unlearning scheme, where unlearning forget signals interferes with\nlearning retain signals. While previous work prove that SAM improves\ngeneralization with noise memorization prevention, we show that SAM abandons\nsuch denoising property when fitting the forget set, leading to various test\nerror bounds depending on signal strength. We further characterize the signal\nsurplus of SAM in the order of signal strength, which enables learning from\nless retain signals to maintain model performance and putting more weight on\nunlearning the forget set. Empirical studies show that SAM outperforms SGD with\nrelaxed requirement for retain signals and can enhance various unlearning\nmethods either as pretrain or unlearn algorithm. Observing that overfitting can\nbenefit more stringent sample-specific unlearning, we propose Sharp MinMax,\nwhich splits the model into two to learn retain signals with SAM and unlearn\nforget signals with sharpness maximization, achieving best performance.\nExtensive experiments show that SAM enhances unlearning across varying\ndifficulties measured by data memorization, yielding decreased feature\nentanglement between retain and forget sets, stronger resistance to membership\ninference attacks, and a flatter loss landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We characterize the effectiveness of Sharpness-aware minimization (SAM) under\nmachine unlearning scheme, where unlearning forget signals interferes with\nlearning retain signals. While previous work prove that SAM improves\ngeneralization with noise memorization prevention, we show that SAM abandons\nsuch denoising property when fitting the forget set, leading to various test\nerror bounds depending on signal strength. We further characterize the signal\nsurplus of SAM in the order of signal strength, which enables learning from\nless retain signals to maintain model performance and putting more weight on\nunlearning the forget set. Empirical studies show that SAM outperforms SGD with\nrelaxed requirement for retain signals and can enhance various unlearning\nmethods either as pretrain or unlearn algorithm. Observing that overfitting can\nbenefit more stringent sample-specific unlearning, we propose Sharp MinMax,\nwhich splits the model into two to learn retain signals with SAM and unlearn\nforget signals with sharpness maximization, achieving best performance.\nExtensive experiments show that SAM enhances unlearning across varying\ndifficulties measured by data memorization, yielding decreased feature\nentanglement between retain and forget sets, stronger resistance to membership\ninference attacks, and a flatter loss landscape."
                },
                "authors": [
                    {
                        "name": "Haoran Tang"
                    },
                    {
                        "name": "Rajiv Khanna"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Khanna"
                },
                "author": "Rajiv Khanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13705v1",
                "updated": "2025-06-16T17:12:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    12,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:12:26Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    12,
                    26,
                    0,
                    167,
                    0
                ],
                "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via\n  Reinforcement Learning"
                },
                "summary": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs."
                },
                "authors": [
                    {
                        "name": "Junru Zhang"
                    },
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Yabo Dong"
                    },
                    {
                        "name": "Duanqing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Duanqing Xu"
                },
                "author": "Duanqing Xu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13695v1",
                "updated": "2025-06-16T16:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    58,
                    55,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:58:55Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    58,
                    55,
                    0,
                    167,
                    0
                ],
                "title": "OneRec Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec Technical Report"
                },
                "summary": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact."
                },
                "authors": [
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Weifeng Ding"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Chaoyi Ma"
                    },
                    {
                        "name": "Chengru Song"
                    },
                    {
                        "name": "Chenhui Wang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Dongxue Meng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Fuxing Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Guowang Zhang"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Hongyang Cao"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiaqiang Liu"
                    },
                    {
                        "name": "Jinghui Jia"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Liao Yu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Qidong Zhou"
                    },
                    {
                        "name": "Shengzhe Wang"
                    },
                    {
                        "name": "Shihui He"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Shujie Yang"
                    },
                    {
                        "name": "Sui Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Xiaoxiao Xu"
                    },
                    {
                        "name": "Xugang Liu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yiwu Liu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Zhanyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Liu"
                },
                "author": "Zhanyu Liu",
                "arxiv_comment": "Authors are listed alphabetically by their first name",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13692v1",
                "updated": "2025-06-16T16:54:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    54,
                    3,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:54:03Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    54,
                    3,
                    0,
                    167,
                    0
                ],
                "title": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems"
                },
                "summary": "With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers."
                },
                "authors": [
                    {
                        "name": "Shang-Chi Tsai"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "IWSDS 2025 Oral Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23077v2",
                "updated": "2025-06-16T16:51:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    51,
                    48,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-29T13:27:46Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    27,
                    46,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Inference for Large Reasoning Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Reasoning Models: A Survey"
                },
                "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Ruihan Gong"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20273v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20273v4",
                "updated": "2025-06-16T16:51:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    51,
                    10,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-27T17:01:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    1,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data"
                },
                "summary": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms."
                },
                "authors": [
                    {
                        "name": "Varshini Reddy"
                    },
                    {
                        "name": "Craig W. Schmidt"
                    },
                    {
                        "name": "Yuval Pinter"
                    },
                    {
                        "name": "Chris Tanner"
                    }
                ],
                "author_detail": {
                    "name": "Chris Tanner"
                },
                "author": "Chris Tanner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20273v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20273v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02084v3",
                "updated": "2025-06-16T16:49:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    49,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2024-10-02T23:10:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    10,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset"
                },
                "summary": "Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, We employ a pretrained large language model (LLM) to generate\npseudo-natural language captions for music from its metadata tags. With the\nLLM-enhanced MetaScore, we train a text-conditioned music generation model that\nlearns to generate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test. While a concurrent work Text2MIDI also\nsupports free-form text input, our models achieve comparable performance.\nMoreover, the text-to-music system offers a more natural interface than the\ntags-to-music model, as it allows users to provide free-form natural language\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, We employ a pretrained large language model (LLM) to generate\npseudo-natural language captions for music from its metadata tags. With the\nLLM-enhanced MetaScore, we train a text-conditioned music generation model that\nlearns to generate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test. While a concurrent work Text2MIDI also\nsupports free-form text input, our models achieve comparable performance.\nMoreover, the text-to-music system offers a more natural interface than the\ntags-to-music model, as it allows users to provide free-form natural language\nprompts."
                },
                "authors": [
                    {
                        "name": "Weihan Xu"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Shlomo Dubnov"
                    },
                    {
                        "name": "Hao-Wen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Wen Dong"
                },
                "author": "Hao-Wen Dong",
                "arxiv_comment": "Accepted at ISMIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13685v1",
                "updated": "2025-06-16T16:46:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    46,
                    14,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:46:14Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    46,
                    14,
                    0,
                    167,
                    0
                ],
                "title": "An LLM's Apology: Outsourcing Awkwardness in the Age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM's Apology: Outsourcing Awkwardness in the Age of AI"
                },
                "summary": "A key part of modern social dynamics is flaking at short notice. However,\nanxiety in coming up with believable and socially acceptable reasons to do so\ncan instead lead to 'ghosting', awkwardness, or implausible excuses, risking\nemotional harm and resentment in the other party. The ability to delegate this\ntask to a Large Language Model (LLM) could substantially reduce friction and\nenhance the flexibility of user's social life while greatly minimising the\naforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an\nevaluation of models' capacity to effectively, kindly, and humanely extract\nthemselves from a diverse set of social, professional and romantic scenarios.\nWe report the efficacy of 10 frontier or recently-frontier LLMs in bailing on\nprior commitments, because nothing says \"I value our friendship\" like having AI\ngenerate your cancellation texts. We open-source FLAKE-Bench at\ngithub.com/Cloakless/flake-bench to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key part of modern social dynamics is flaking at short notice. However,\nanxiety in coming up with believable and socially acceptable reasons to do so\ncan instead lead to 'ghosting', awkwardness, or implausible excuses, risking\nemotional harm and resentment in the other party. The ability to delegate this\ntask to a Large Language Model (LLM) could substantially reduce friction and\nenhance the flexibility of user's social life while greatly minimising the\naforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an\nevaluation of models' capacity to effectively, kindly, and humanely extract\nthemselves from a diverse set of social, professional and romantic scenarios.\nWe report the efficacy of 10 frontier or recently-frontier LLMs in bailing on\nprior commitments, because nothing says \"I value our friendship\" like having AI\ngenerate your cancellation texts. We open-source FLAKE-Bench at\ngithub.com/Cloakless/flake-bench to support future research."
                },
                "authors": [
                    {
                        "name": "Twm Stone"
                    },
                    {
                        "name": "Anna Soligo"
                    }
                ],
                "author_detail": {
                    "name": "Anna Soligo"
                },
                "author": "Anna Soligo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13681v1",
                "updated": "2025-06-16T16:38:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    38,
                    4,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:38:04Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    38,
                    4,
                    0,
                    167,
                    0
                ],
                "title": "Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models"
                },
                "summary": "Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity."
                },
                "authors": [
                    {
                        "name": "Rylan Schaeffer"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Yegor Denisov-Blanch"
                    }
                ],
                "author_detail": {
                    "name": "Yegor Denisov-Blanch"
                },
                "author": "Yegor Denisov-Blanch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13677v1",
                "updated": "2025-06-16T16:31:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    31,
                    45,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:31:45Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    31,
                    45,
                    0,
                    167,
                    0
                ],
                "title": "Bayesian Quantification of Observability and Equation of State of Twin\n  Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantification of Observability and Equation of State of Twin\n  Stars"
                },
                "summary": "The possibility of discovering twin stars, two neutron stars (NSs) with the\nsame mass but different radii, is usually studied in forward modelings by using\na restricted number of NS matter equation of state (EOS) encapsulating a\nfirst-order phase transition from hadronic to quark matter (QM). Informing our\nlikelihood function with the NS radius data from GW170817 and using a\nmeta-model with 9-parameters capable of mimicking most NS EOSs available in the\nliterature, we conduct a Bayesian quantification of the observability and\nunderlying EOSs of twin stars. Of the accepted EOSs, between 12-18\\% yield twin\nstars, depending on the restrictions we place on the twin branch. We show that\nmany of these twin star scenarios are observable with currently available\nlevels of accuracy in measuring NS radii. We also present the marginalized\nposterior probability density functions (PDFs) of every EOS parameter for each\nof four mass-radius correlation topologies. We find that the inferred EOS\ndepends sensitively on not only whether twin stars are present, but also the\ncategory of twin stars, indicating that the observation of twin stars would\nprovide a strong constraint on the underlying EOS. In particular, for two\ncoexisting hybrid stars having QM cores at different densities, the PDF for QM\nspeed of sound squared $c_{\\rm qm}^2$ has two peaks, one below and another\nabove the conformal limit $c_{\\rm qm}^2=1/3$ predicted by perturbative QCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of discovering twin stars, two neutron stars (NSs) with the\nsame mass but different radii, is usually studied in forward modelings by using\na restricted number of NS matter equation of state (EOS) encapsulating a\nfirst-order phase transition from hadronic to quark matter (QM). Informing our\nlikelihood function with the NS radius data from GW170817 and using a\nmeta-model with 9-parameters capable of mimicking most NS EOSs available in the\nliterature, we conduct a Bayesian quantification of the observability and\nunderlying EOSs of twin stars. Of the accepted EOSs, between 12-18\\% yield twin\nstars, depending on the restrictions we place on the twin branch. We show that\nmany of these twin star scenarios are observable with currently available\nlevels of accuracy in measuring NS radii. We also present the marginalized\nposterior probability density functions (PDFs) of every EOS parameter for each\nof four mass-radius correlation topologies. We find that the inferred EOS\ndepends sensitively on not only whether twin stars are present, but also the\ncategory of twin stars, indicating that the observation of twin stars would\nprovide a strong constraint on the underlying EOS. In particular, for two\ncoexisting hybrid stars having QM cores at different densities, the PDF for QM\nspeed of sound squared $c_{\\rm qm}^2$ has two peaks, one below and another\nabove the conformal limit $c_{\\rm qm}^2=1/3$ predicted by perturbative QCD."
                },
                "authors": [
                    {
                        "name": "Xavier Grundler"
                    },
                    {
                        "name": "Bao-An Li"
                    }
                ],
                "author_detail": {
                    "name": "Bao-An Li"
                },
                "author": "Bao-An Li",
                "arxiv_comment": "15 pages with 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85xx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13674v1",
                "updated": "2025-06-16T16:30:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    30,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:30:26Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    30,
                    26,
                    0,
                    167,
                    0
                ],
                "title": "Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Brian Chen"
                    },
                    {
                        "name": "Li Siquan"
                    },
                    {
                        "name": "Liang Xinhe"
                    },
                    {
                        "name": "Tianyang Hu"
                    },
                    {
                        "name": "Hwee Kuan Lee"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Kawaguchi"
                },
                "author": "Kenji Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03781v2",
                "updated": "2025-06-16T16:25:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    25,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-04T09:42:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    42,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models"
                },
                "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark."
                },
                "authors": [
                    {
                        "name": "Seungcheol Park"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Beomseok Kwon"
                    },
                    {
                        "name": "Minjun Kim"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "U Kang"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "arxiv_comment": "ACL 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14568v2",
                "updated": "2025-06-16T16:24:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    42,
                    0,
                    167,
                    0
                ],
                "published": "2024-08-26T18:39:31Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    39,
                    31,
                    0,
                    239,
                    0
                ],
                "title": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation"
                },
                "summary": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods."
                },
                "authors": [
                    {
                        "name": "Yizhan Li"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Christopher Smith"
                    },
                    {
                        "name": "Thomas Lo"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13666v1",
                "updated": "2025-06-16T16:24:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    31,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:24:31Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    31,
                    0,
                    167,
                    0
                ],
                "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems"
                },
                "summary": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Ruipeng Wang"
                    },
                    {
                        "name": "Haokai Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05317v2",
                "updated": "2025-06-16T16:22:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    22,
                    39,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-21T09:43:18Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    43,
                    18,
                    4,
                    52,
                    0
                ],
                "title": "On Synthesizing Data for Context Attribution in Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Synthesizing Data for Context Attribution in Question Answering"
                },
                "summary": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA."
                },
                "authors": [
                    {
                        "name": "Gorjan Radevski"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Shahbaz Syed"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Sebastien Nicolas"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Timo Sztyler"
                    },
                    {
                        "name": "Verena Heuer"
                    },
                    {
                        "name": "Wiem Ben Rim"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Kunihiro Takeoka"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    },
                    {
                        "name": "Goran Glava"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13658v1",
                "updated": "2025-06-16T16:18:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    18,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:18:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    18,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Adversarial Disentanglement by Backpropagation with Physics-Informed\n  Variational Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Disentanglement by Backpropagation with Physics-Informed\n  Variational Autoencoder"
                },
                "summary": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Ioannis Christoforos Koune"
                    },
                    {
                        "name": "Alice Cicirello"
                    }
                ],
                "author_detail": {
                    "name": "Alice Cicirello"
                },
                "author": "Alice Cicirello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13650v1",
                "updated": "2025-06-16T16:15:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    15,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:15:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    15,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Deceptive Path Planning: A Bayesian Game Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deceptive Path Planning: A Bayesian Game Approach"
                },
                "summary": "This paper investigates how an autonomous agent can transmit information\nthrough its motion in an adversarial setting. We consider scenarios where an\nagent must reach its goal while deceiving an intelligent observer about its\ndestination. We model this interaction as a dynamic Bayesian game between a\nmobile Attacker with a privately known goal and a Defender who infers the\nAttacker's intent to allocate defensive resources effectively. We use Perfect\nBayesian Nash Equilibrium (PBNE) as our solution concept and propose a\ncomputationally efficient approach to find it. In the resulting equilibrium,\nthe Defender employs a simple Markovian strategy, while the Attacker\nstrategically balances deception and goal efficiency by stochastically mixing\nshortest and non-shortest paths to manipulate the Defender's beliefs. Numerical\nexperiments demonstrate the advantages of our PBNE-based strategies over\nexisting methods based on one-sided optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates how an autonomous agent can transmit information\nthrough its motion in an adversarial setting. We consider scenarios where an\nagent must reach its goal while deceiving an intelligent observer about its\ndestination. We model this interaction as a dynamic Bayesian game between a\nmobile Attacker with a privately known goal and a Defender who infers the\nAttacker's intent to allocate defensive resources effectively. We use Perfect\nBayesian Nash Equilibrium (PBNE) as our solution concept and propose a\ncomputationally efficient approach to find it. In the resulting equilibrium,\nthe Defender employs a simple Markovian strategy, while the Attacker\nstrategically balances deception and goal efficiency by stochastically mixing\nshortest and non-shortest paths to manipulate the Defender's beliefs. Numerical\nexperiments demonstrate the advantages of our PBNE-based strategies over\nexisting methods based on one-sided optimization."
                },
                "authors": [
                    {
                        "name": "Violetta Rostobaya"
                    },
                    {
                        "name": "James Berneburg"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Michael Dorothy"
                    },
                    {
                        "name": "Daigo Shishika"
                    }
                ],
                "author_detail": {
                    "name": "Daigo Shishika"
                },
                "author": "Daigo Shishika",
                "arxiv_comment": "8 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13642v1",
                "updated": "2025-06-16T16:06:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    6,
                    45,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:06:45Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    6,
                    45,
                    0,
                    167,
                    0
                ],
                "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model"
                },
                "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13641v1",
                "updated": "2025-06-16T16:05:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    5,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:05:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    5,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs"
                },
                "summary": "A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Ze Li"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13639v1",
                "updated": "2025-06-16T16:04:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    43,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:04:43Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    43,
                    0,
                    167,
                    0
                ],
                "title": "An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability"
                },
                "summary": "As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present."
                },
                "authors": [
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13638v1",
                "updated": "2025-06-16T16:04:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:04:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    16,
                    0,
                    167,
                    0
                ],
                "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models"
                },
                "summary": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Zhiyi Shi"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Chongjie Si"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Junsik Kim"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19394v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19394v4",
                "updated": "2025-06-16T16:03:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    3,
                    59,
                    0,
                    167,
                    0
                ],
                "published": "2025-01-31T18:48:12Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    48,
                    12,
                    4,
                    31,
                    0
                ],
                "title": "Fixed-Population Causal Inference for Models of Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Population Causal Inference for Models of Equilibrium"
                },
                "summary": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness and\nconsistency for inverse-probability weighting (IPW) estimators for those causal\nparameters from a randomized experiment on a single network. We also analyze\nmore closely the case of marginal interventions in a model of equilibrium with\nsmooth response functions where we can recover LATE-type weighted averages of\nderivatives of those response functions. Under additional structural\nassumptions, these ``agnostic\" causal estimands can be combined to recover\nmodel parameters, but also retain their less restrictive causal interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to problems of interference in (exogenous) treatments, models of\ninterference in unit-specific (endogenous) outcomes do not usually produce a\nreduced-form representation where outcomes depend on other units' treatment\nstatus only at a short network distance, or only through a known exposure\nmapping. This remains true if the structural mechanism depends on outcomes of\npeers only at a short network distance, or through a known exposure mapping. In\nthis paper, we first define causal estimands that are identified and estimable\nfrom a single experiment on the network under minimal assumptions on the\nstructure of interference, and which represent average partial causal responses\nwhich generally vary with other global features of the realized assignment.\nUnder a fixed-population, design-based approach, we show unbiasedness and\nconsistency for inverse-probability weighting (IPW) estimators for those causal\nparameters from a randomized experiment on a single network. We also analyze\nmore closely the case of marginal interventions in a model of equilibrium with\nsmooth response functions where we can recover LATE-type weighted averages of\nderivatives of those response functions. Under additional structural\nassumptions, these ``agnostic\" causal estimands can be combined to recover\nmodel parameters, but also retain their less restrictive causal interpretation."
                },
                "authors": [
                    {
                        "name": "Konrad Menzel"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Menzel"
                },
                "author": "Konrad Menzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19394v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19394v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13793v2",
                "updated": "2025-06-16T15:59:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    59,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2024-09-20T10:47:09Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    10,
                    47,
                    9,
                    4,
                    264,
                    0
                ],
                "title": "On the Feasibility of Fully AI-automated Vishing Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Feasibility of Fully AI-automated Vishing Attacks"
                },
                "summary": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs."
                },
                "authors": [
                    {
                        "name": "Joo Figueiredo"
                    },
                    {
                        "name": "Afonso Carvalho"
                    },
                    {
                        "name": "Daniel Castro"
                    },
                    {
                        "name": "Daniel Gonalves"
                    },
                    {
                        "name": "Nuno Santos"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Santos"
                },
                "author": "Nuno Santos",
                "arxiv_comment": "To appear in AsiaCCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13629v1",
                "updated": "2025-06-16T15:56:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:56:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding"
                },
                "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13618v1",
                "updated": "2025-06-16T15:47:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    47,
                    52,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:47:52Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    47,
                    52,
                    0,
                    167,
                    0
                ],
                "title": "Thermal electrons in the radio afterglow of relativistic tidal\n  disruption event ZTF22aaajecp/AT2022cmc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal electrons in the radio afterglow of relativistic tidal\n  disruption event ZTF22aaajecp/AT2022cmc"
                },
                "summary": "A tidal disruption event (TDE) occurs when a star travels too close to a\nsupermassive black hole. In some cases, accretion of the disrupted material\nonto the black hole launches a relativistic jet. In this paper, we present a\nlong term observing campaign to study the radio and sub-millimeter emission\nassociated with the fifth jetted/relativistic TDE: AT2022cmc. Our campaign\nreveals a long lived counterpart. We fit three different models to our data: a\nnon-thermal jet, a spherical outflow consisting of both thermal and non-thermal\nelectrons, and a jet with thermal and non-thermal electrons. We find that the\ndata is best described by a relativistic spherical outflow propagating into an\nenvironment with a density profile following R^-1.8. Comparison of AT2022cmc to\nother TDEs finds agreement in the density profile of the environment but also\nthat AT2022cmc is twice as energetic as the other well-studied relativistic TDE\nSwift J1644. Our observations of AT2022cmc allow a thermal electron population\nto be inferred for the first time in a jetted transient providing, new insights\ninto the microphysics of relativistic transients jets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tidal disruption event (TDE) occurs when a star travels too close to a\nsupermassive black hole. In some cases, accretion of the disrupted material\nonto the black hole launches a relativistic jet. In this paper, we present a\nlong term observing campaign to study the radio and sub-millimeter emission\nassociated with the fifth jetted/relativistic TDE: AT2022cmc. Our campaign\nreveals a long lived counterpart. We fit three different models to our data: a\nnon-thermal jet, a spherical outflow consisting of both thermal and non-thermal\nelectrons, and a jet with thermal and non-thermal electrons. We find that the\ndata is best described by a relativistic spherical outflow propagating into an\nenvironment with a density profile following R^-1.8. Comparison of AT2022cmc to\nother TDEs finds agreement in the density profile of the environment but also\nthat AT2022cmc is twice as energetic as the other well-studied relativistic TDE\nSwift J1644. Our observations of AT2022cmc allow a thermal electron population\nto be inferred for the first time in a jetted transient providing, new insights\ninto the microphysics of relativistic transients jets."
                },
                "authors": [
                    {
                        "name": "Lauren Rhodes"
                    },
                    {
                        "name": "Ben Margalit"
                    },
                    {
                        "name": "Joe S. Bright"
                    },
                    {
                        "name": "Hannah Dykaar"
                    },
                    {
                        "name": "Rob Fender"
                    },
                    {
                        "name": "David A. Green"
                    },
                    {
                        "name": "Daryl Haggard"
                    },
                    {
                        "name": "Assaf Horesh"
                    },
                    {
                        "name": "Alexander J. van der Horst"
                    },
                    {
                        "name": "Andrew Hughes"
                    },
                    {
                        "name": "Kunal Mooley"
                    },
                    {
                        "name": "Itai Sfaradi"
                    },
                    {
                        "name": "David Titterington"
                    },
                    {
                        "name": "David WIlliams-Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "David WIlliams-Baldwin"
                },
                "author": "David WIlliams-Baldwin",
                "arxiv_comment": "18 pages, 8 figures, 1 table. Submitted to ApJ. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13613v1",
                "updated": "2025-06-16T15:42:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    42,
                    15,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:42:15Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    42,
                    15,
                    0,
                    167,
                    0
                ],
                "title": "Variational Inference with Mixtures of Isotropic Gaussians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference with Mixtures of Isotropic Gaussians"
                },
                "summary": "Variational inference (VI) is a popular approach in Bayesian inference, that\nlooks for the best approximation of the posterior distribution within a\nparametric family, minimizing a loss that is typically the (reverse)\nKullback-Leibler (KL) divergence. In this paper, we focus on the following\nparametric family: mixtures of isotropic Gaussians (i.e., with diagonal\ncovariance matrices proportional to the identity) and uniform weights. We\ndevelop a variational framework and provide efficient algorithms suited for\nthis family. In contrast with mixtures of Gaussian with generic covariance\nmatrices, this choice presents a balance between accurate approximations of\nmultimodal Bayesian posteriors, while being memory and computationally\nefficient. Our algorithms implement gradient descent on the location of the\nmixture components (the modes of the Gaussians), and either (an entropic)\nMirror or Bures descent on their variance parameters. We illustrate the\nperformance of our algorithms on numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference (VI) is a popular approach in Bayesian inference, that\nlooks for the best approximation of the posterior distribution within a\nparametric family, minimizing a loss that is typically the (reverse)\nKullback-Leibler (KL) divergence. In this paper, we focus on the following\nparametric family: mixtures of isotropic Gaussians (i.e., with diagonal\ncovariance matrices proportional to the identity) and uniform weights. We\ndevelop a variational framework and provide efficient algorithms suited for\nthis family. In contrast with mixtures of Gaussian with generic covariance\nmatrices, this choice presents a balance between accurate approximations of\nmultimodal Bayesian posteriors, while being memory and computationally\nefficient. Our algorithms implement gradient descent on the location of the\nmixture components (the modes of the Gaussians), and either (an entropic)\nMirror or Bures descent on their variance parameters. We illustrate the\nperformance of our algorithms on numerical experiments."
                },
                "authors": [
                    {
                        "name": "Marguerite Petit-Talamon"
                    },
                    {
                        "name": "Marc Lambert"
                    },
                    {
                        "name": "Anna Korba"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korba"
                },
                "author": "Anna Korba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02039v2",
                "updated": "2025-06-16T15:37:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    37,
                    6,
                    0,
                    167,
                    0
                ],
                "published": "2025-01-03T14:35:32Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    35,
                    32,
                    4,
                    3,
                    0
                ],
                "title": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Siyi Wang"
                    },
                    {
                        "name": "Ziyao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyao Liu"
                },
                "author": "Ziyao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13608v1",
                "updated": "2025-06-16T15:35:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    35,
                    41,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:35:41Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    35,
                    41,
                    0,
                    167,
                    0
                ],
                "title": "Assessing the Limits of In-Context Learning beyond Functions using\n  Partially Ordered Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Limits of In-Context Learning beyond Functions using\n  Partially Ordered Relation"
                },
                "summary": "Generating rational and generally accurate responses to tasks, often\naccompanied by example demonstrations, highlights Large Language Model's\n(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring\nupdates to the model's parameter space. Despite having an ongoing exploration\nfocused on the inference from a document-level concept, its behavior in\nlearning well-defined functions or relations in context needs a careful\ninvestigation. In this article, we present the performance of ICL on partially\nordered relation by introducing the notion of inductively increasing complexity\nin prompts. In most cases, the saturated performance of the chosen metric\nindicates that while ICL offers some benefits, its effectiveness remains\nconstrained as we increase the complexity in the prompts even in presence of\nsufficient demonstrative examples. The behavior is evident from our empirical\nfindings and has further been theoretically justified in term of its implicit\noptimization process. The code is available\n\\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating rational and generally accurate responses to tasks, often\naccompanied by example demonstrations, highlights Large Language Model's\n(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring\nupdates to the model's parameter space. Despite having an ongoing exploration\nfocused on the inference from a document-level concept, its behavior in\nlearning well-defined functions or relations in context needs a careful\ninvestigation. In this article, we present the performance of ICL on partially\nordered relation by introducing the notion of inductively increasing complexity\nin prompts. In most cases, the saturated performance of the chosen metric\nindicates that while ICL offers some benefits, its effectiveness remains\nconstrained as we increase the complexity in the prompts even in presence of\nsufficient demonstrative examples. The behavior is evident from our empirical\nfindings and has further been theoretically justified in term of its implicit\noptimization process. The code is available\n\\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}."
                },
                "authors": [
                    {
                        "name": "Debanjan Dutta"
                    },
                    {
                        "name": "Faizanuddin Ansari"
                    },
                    {
                        "name": "Swagatam Das"
                    }
                ],
                "author_detail": {
                    "name": "Swagatam Das"
                },
                "author": "Swagatam Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12150v2",
                "updated": "2025-06-16T15:27:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    27,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-17T18:59:02Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    59,
                    2,
                    0,
                    48,
                    0
                ],
                "title": "Idiosyncrasies in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiosyncrasies in Large Language Models"
                },
                "summary": "In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies."
                },
                "authors": [
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zhiqiu Xu"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "Published in ICML 2025. Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13599v1",
                "updated": "2025-06-16T15:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    24,
                    7,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:24:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    24,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation"
                },
                "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation."
                },
                "authors": [
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13596v1",
                "updated": "2025-06-16T15:23:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems"
                },
                "summary": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Long-Vu Hoang"
                    },
                    {
                        "name": "Huy-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Huy-Dat Tran"
                },
                "author": "Huy-Dat Tran",
                "arxiv_comment": "Technical report for Interspeech 2025 MLC-SLM Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13595v1",
                "updated": "2025-06-16T15:21:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    32,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:21:32Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    32,
                    0,
                    167,
                    0
                ],
                "title": "Persistent Homology of Music Network with Three Different Distances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Homology of Music Network with Three Different Distances"
                },
                "summary": "Persistent homology has been widely used to discover hidden topological\nstructures in data across various applications, including music data. To apply\npersistent homology, a distance or metric must be defined between points in a\npoint cloud or between nodes in a graph network. These definitions are not\nunique and depend on the specific objectives of a given problem. In other\nwords, selecting different metric definitions allows for multiple topological\ninferences. In this work, we focus on applying persistent homology to music\ngraph with predefined weights. We examine three distinct distance definitions\nbased on edge-wise pathways and demonstrate how these definitions affect\npersistent barcodes, persistence diagrams, and birth/death edges. We found that\nthere exist inclusion relations in one-dimensional persistent homology\nreflected on persistence barcode and diagram among these three distance\ndefinitions. We verified these findings using real music data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent homology has been widely used to discover hidden topological\nstructures in data across various applications, including music data. To apply\npersistent homology, a distance or metric must be defined between points in a\npoint cloud or between nodes in a graph network. These definitions are not\nunique and depend on the specific objectives of a given problem. In other\nwords, selecting different metric definitions allows for multiple topological\ninferences. In this work, we focus on applying persistent homology to music\ngraph with predefined weights. We examine three distinct distance definitions\nbased on edge-wise pathways and demonstrate how these definitions affect\npersistent barcodes, persistence diagrams, and birth/death edges. We found that\nthere exist inclusion relations in one-dimensional persistent homology\nreflected on persistence barcode and diagram among these three distance\ndefinitions. We verified these findings using real music data."
                },
                "authors": [
                    {
                        "name": "Eunwoo Heo"
                    },
                    {
                        "name": "Byeongchan Choi"
                    },
                    {
                        "name": "Myung ock Kim"
                    },
                    {
                        "name": "Mai Lan Tran"
                    },
                    {
                        "name": "Jae-Hun Jung"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Hun Jung"
                },
                "author": "Jae-Hun Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13593v1",
                "updated": "2025-06-16T15:21:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:21:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs"
                },
                "summary": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models."
                },
                "authors": [
                    {
                        "name": "Hen Davidov"
                    },
                    {
                        "name": "Gilad Freidkin"
                    },
                    {
                        "name": "Shai Feldman"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13583v1",
                "updated": "2025-06-16T15:04:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    4,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:04:27Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    4,
                    27,
                    0,
                    167,
                    0
                ],
                "title": "Can you see how I learn? Human observers' inferences about Reinforcement\n  Learning agents' learning processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can you see how I learn? Human observers' inferences about Reinforcement\n  Learning agents' learning processes"
                },
                "summary": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are\nnot intuitively interpretable by human observers, which can result in\nsuboptimal feedback in collaborative teaching settings. Yet, how humans\nperceive and interpret RL agent's learning behavior is largely unknown. In a\nbottom-up approach with two experiments, this work provides a data-driven\nunderstanding of the factors of human observers' understanding of the agent's\nlearning process. A novel, observation-based paradigm to directly assess human\ninferences about agent learning was developed. In an exploratory interview\nstudy (\\textit{N}=9), we identify four core themes in human interpretations:\nAgent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second\nconfirmatory study (\\textit{N}=34) applied an expanded version of the paradigm\nacross two tasks (navigation/manipulation) and two RL algorithms\n(tabular/function approximation). Analyses of 816 responses confirmed the\nreliability of the paradigm and refined the thematic framework, revealing how\nthese themes evolve over time and interrelate. Our findings provide a\nhuman-centered understanding of how people make sense of agent learning,\noffering actionable insights for designing interpretable RL systems and\nimproving transparency in Human-Robot Interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are\nnot intuitively interpretable by human observers, which can result in\nsuboptimal feedback in collaborative teaching settings. Yet, how humans\nperceive and interpret RL agent's learning behavior is largely unknown. In a\nbottom-up approach with two experiments, this work provides a data-driven\nunderstanding of the factors of human observers' understanding of the agent's\nlearning process. A novel, observation-based paradigm to directly assess human\ninferences about agent learning was developed. In an exploratory interview\nstudy (\\textit{N}=9), we identify four core themes in human interpretations:\nAgent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second\nconfirmatory study (\\textit{N}=34) applied an expanded version of the paradigm\nacross two tasks (navigation/manipulation) and two RL algorithms\n(tabular/function approximation). Analyses of 816 responses confirmed the\nreliability of the paradigm and refined the thematic framework, revealing how\nthese themes evolve over time and interrelate. Our findings provide a\nhuman-centered understanding of how people make sense of agent learning,\noffering actionable insights for designing interpretable RL systems and\nimproving transparency in Human-Robot Interaction."
                },
                "authors": [
                    {
                        "name": "Bernhard Hilpert"
                    },
                    {
                        "name": "Muhan Hou"
                    },
                    {
                        "name": "Kim Baraka"
                    },
                    {
                        "name": "Joost Broekens"
                    }
                ],
                "author_detail": {
                    "name": "Joost Broekens"
                },
                "author": "Joost Broekens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13682v2",
                "updated": "2025-06-16T15:04:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    4,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-19T12:51:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "Cross-Comparison of Sampling Algorithms for Pulse Profile Modeling of\n  PSR J0740+6620",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Comparison of Sampling Algorithms for Pulse Profile Modeling of\n  PSR J0740+6620"
                },
                "summary": "In the last few years, NICER data has enabled mass and radius inferences for\nvarious pulsars, and thus shed light on the equation of state for dense nuclear\nmatter. This is achieved through a technique called pulse profile modeling. The\nimportance of the results necessitates careful validation and testing of the\nrobustness of the inference procedure. In this paper, we investigate the effect\nof sampler choice for X-PSI (X-ray Pulse Simulation and Inference), an\nopen-source package for pulse profile modeling and Bayesian statistical\ninference that has been used extensively for analysis of NICER data. We focus\non the specific case of the high-mass pulsar PSR J0740+6620. Using synthetic\ndata that mimics the most recently analyzed NICER and XMM-Newton data sets of\nPSR J0740+6620, we evaluate the parameter recovery performance, convergence,\nand computational cost for MultiNest's multimodal nested sampling algorithm and\nUltraNest's slice nested sampling algorithm. We find that both samplers perform\nreliably, producing accurate and unbiased parameter estimation results when\nanalyzing simulated data. We also investigate the consequences for inference\nusing the real data for PSR J0740+6620, finding that both samplers produce\nconsistent credible intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few years, NICER data has enabled mass and radius inferences for\nvarious pulsars, and thus shed light on the equation of state for dense nuclear\nmatter. This is achieved through a technique called pulse profile modeling. The\nimportance of the results necessitates careful validation and testing of the\nrobustness of the inference procedure. In this paper, we investigate the effect\nof sampler choice for X-PSI (X-ray Pulse Simulation and Inference), an\nopen-source package for pulse profile modeling and Bayesian statistical\ninference that has been used extensively for analysis of NICER data. We focus\non the specific case of the high-mass pulsar PSR J0740+6620. Using synthetic\ndata that mimics the most recently analyzed NICER and XMM-Newton data sets of\nPSR J0740+6620, we evaluate the parameter recovery performance, convergence,\nand computational cost for MultiNest's multimodal nested sampling algorithm and\nUltraNest's slice nested sampling algorithm. We find that both samplers perform\nreliably, producing accurate and unbiased parameter estimation results when\nanalyzing simulated data. We also investigate the consequences for inference\nusing the real data for PSR J0740+6620, finding that both samplers produce\nconsistent credible intervals."
                },
                "authors": [
                    {
                        "name": "Mariska Hoogkamer"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Johannes Buchner"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Buchner"
                },
                "author": "Johannes Buchner",
                "arxiv_doi": "10.1103/cp8c-2nbk",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/cp8c-2nbk",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in PRD",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13432v2",
                "updated": "2025-06-16T14:53:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    53,
                    55,
                    0,
                    167,
                    0
                ],
                "published": "2025-05-19T17:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Synthetic-Powered Predictive Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic-Powered Predictive Inference"
                },
                "summary": "Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPI), a novel framework that\nincorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification -- augmenting data\nwith synthetic diffusion-model generated images -- and on tabular regression\ndemonstrate notable improvements in predictive efficiency in data-scarce\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPI), a novel framework that\nincorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification -- augmenting data\nwith synthetic diffusion-model generated images -- and on tabular regression\ndemonstrate notable improvements in predictive efficiency in data-scarce\nsettings."
                },
                "authors": [
                    {
                        "name": "Meshi Bashari"
                    },
                    {
                        "name": "Roy Maor Lotan"
                    },
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13560v1",
                "updated": "2025-06-16T14:45:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    24,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:45:24Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    24,
                    0,
                    167,
                    0
                ],
                "title": "Insights into the structure and kinematics of a Milky Way-like galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights into the structure and kinematics of a Milky Way-like galaxy"
                },
                "summary": "How the large-scale kinematics of the Milky Way (MW) relate -- or even\nregulate -- the formation of large-to-small scale structures is incredibly hard\nto infer from observations. Here we investigate this interplay through a\ndetailed analysis of a MW-like galaxy simulation, generated through the\nself-consistent evolution of gas, stars, and dark matter. We show that our\nmodel provides a close match to many of the MW's stellar structure and\nkinematic features (including in the inner Galaxy, and around the Solar\nneighbourhood), and find that the stellar spiral pattern in our model is very\nfaint, with significantly less multiplicity than the sharper gaseous arms.If\ntaken as an analogue to the MW, this finding would explain the difficulty in\nobservational studies to agree on the number and location of our Galaxy's\nspiral arms. We also examine radial and tangential velocity residuals in the\ndisc, and find that sharp kinematic transitions correlate with spiral arms,\nespecially in the gas, where values reach $30 - 50$\\,km\\,s$^{-1}$. We find\nstrong radial converging flows promoting spiral-arm growth, while diverging\nflows disrupt them. A time-resolved analysis of spiral-ridge segments confirms\nthat convergence precedes density enhancements and potential star-forming\nconditions, while divergence leads to fragmentation and mass redistribution.\nThese patterns evolve on relatively short timescales ($\\sim10-20$\\,Myr),\nhighlighting the transient nature of spiral arms. Our model's spiral arms are\ndynamically driven, short-lived features shaped by evolving flows, rather than\nstatic density waves, which could explain the observed lack of contrast of\ncloud properties and star formation within and outside spiral arms in the MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How the large-scale kinematics of the Milky Way (MW) relate -- or even\nregulate -- the formation of large-to-small scale structures is incredibly hard\nto infer from observations. Here we investigate this interplay through a\ndetailed analysis of a MW-like galaxy simulation, generated through the\nself-consistent evolution of gas, stars, and dark matter. We show that our\nmodel provides a close match to many of the MW's stellar structure and\nkinematic features (including in the inner Galaxy, and around the Solar\nneighbourhood), and find that the stellar spiral pattern in our model is very\nfaint, with significantly less multiplicity than the sharper gaseous arms.If\ntaken as an analogue to the MW, this finding would explain the difficulty in\nobservational studies to agree on the number and location of our Galaxy's\nspiral arms. We also examine radial and tangential velocity residuals in the\ndisc, and find that sharp kinematic transitions correlate with spiral arms,\nespecially in the gas, where values reach $30 - 50$\\,km\\,s$^{-1}$. We find\nstrong radial converging flows promoting spiral-arm growth, while diverging\nflows disrupt them. A time-resolved analysis of spiral-ridge segments confirms\nthat convergence precedes density enhancements and potential star-forming\nconditions, while divergence leads to fragmentation and mass redistribution.\nThese patterns evolve on relatively short timescales ($\\sim10-20$\\,Myr),\nhighlighting the transient nature of spiral arms. Our model's spiral arms are\ndynamically driven, short-lived features shaped by evolving flows, rather than\nstatic density waves, which could explain the observed lack of contrast of\ncloud properties and star formation within and outside spiral arms in the MW."
                },
                "authors": [
                    {
                        "name": "Eva Durn-Camacho"
                    },
                    {
                        "name": "Ana Duarte-Cabral"
                    }
                ],
                "author_detail": {
                    "name": "Ana Duarte-Cabral"
                },
                "author": "Ana Duarte-Cabral",
                "arxiv_comment": "22 pages, 22 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13559v1",
                "updated": "2025-06-16T14:45:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:45:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Understand the Implication: Learning to Think for Pragmatic\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understand the Implication: Learning to Think for Pragmatic\n  Understanding"
                },
                "summary": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models."
                },
                "authors": [
                    {
                        "name": "Settaluri Lakshmi Sravanthi"
                    },
                    {
                        "name": "Kishan Maharaj"
                    },
                    {
                        "name": "Sravani Gunnu"
                    },
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "arxiv_comment": "SS and KM contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13558v1",
                "updated": "2025-06-16T14:43:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    43,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:43:18Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    43,
                    18,
                    0,
                    167,
                    0
                ],
                "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and\n  Flexible Controllability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and\n  Flexible Controllability"
                },
                "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Alan Liang"
                    },
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Yukai Ma"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13557v1",
                "updated": "2025-06-16T14:42:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    42,
                    53,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:42:53Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    42,
                    53,
                    0,
                    167,
                    0
                ],
                "title": "Timing and spectral variability in 2S 1417-624 observed with\n  Insight-HXMT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timing and spectral variability in 2S 1417-624 observed with\n  Insight-HXMT"
                },
                "summary": "We present the results of the spectral and timing analyses of the accreting\nX-ray pulsar, 2S 1417-624, during the 2018 and 2021 outbursts with\nInsight-HXMT. We find that the pulse profiles in all energy bands exhibit clear\ndouble-peaked structures at low flux states. In the 1-10 keV band, the pulse\nprofiles evolve from double to triple peaks at a flux level of $\\sim$4.1$\\\n\\times \\ 10^{-9}$ erg cm$^{-2}$ s$^{-1}$, and from triple to quadruple peaks at\n$\\sim$6.4$\\ \\times \\ 10^{-9}$ erg cm$^{-2}$ s$^{-1}$. In the 10-30 keV and\n30-100 keV bands, the pulse profiles become narrower at the first transition\nflux level, followed by a stark transition to quadruple-peaked and\ntriple-peaked structures around the second flux level, respectively. The change\nof the pulse profile {during the second transition} reveals the transition of\nthe emission pattern from the sub-critical (pencil beam) to the supercritical\n(fan beam) regime. By performing the binary orbital fitting of the observed\nspin periods, we provide new measurements of the orbital parameters from the\n2021 outburst. Applying different accretion torque models and using the\ncritical luminosity inferred from the pulse profile transitions, we derive a\nself-consistent distance of {2S 1417-624} in the range of approximately\n12.0-15.0~kpc, based on the magnetic field strength derived from the cyclotron\nresonance scattering feature (CRSF). From the estimated distance of 13 kpc and\nGaia's distance of 7.4 kpc, we can infer the observed transition luminosity of\n\\((1.0-1.4) \\times 10^{38} \\, \\mathrm{erg \\, s^{-1}}\\) and \\((3.0-5.0) \\times\n10^{37} \\, \\mathrm{erg \\, s^{-1}}\\), respectively, and compare them with\ntheoretical models. The spectral continuum parameters and the hardness ratio\nalso show significant transitions around the second transition, strongly\nsupporting a change in the accretion regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the results of the spectral and timing analyses of the accreting\nX-ray pulsar, 2S 1417-624, during the 2018 and 2021 outbursts with\nInsight-HXMT. We find that the pulse profiles in all energy bands exhibit clear\ndouble-peaked structures at low flux states. In the 1-10 keV band, the pulse\nprofiles evolve from double to triple peaks at a flux level of $\\sim$4.1$\\\n\\times \\ 10^{-9}$ erg cm$^{-2}$ s$^{-1}$, and from triple to quadruple peaks at\n$\\sim$6.4$\\ \\times \\ 10^{-9}$ erg cm$^{-2}$ s$^{-1}$. In the 10-30 keV and\n30-100 keV bands, the pulse profiles become narrower at the first transition\nflux level, followed by a stark transition to quadruple-peaked and\ntriple-peaked structures around the second flux level, respectively. The change\nof the pulse profile {during the second transition} reveals the transition of\nthe emission pattern from the sub-critical (pencil beam) to the supercritical\n(fan beam) regime. By performing the binary orbital fitting of the observed\nspin periods, we provide new measurements of the orbital parameters from the\n2021 outburst. Applying different accretion torque models and using the\ncritical luminosity inferred from the pulse profile transitions, we derive a\nself-consistent distance of {2S 1417-624} in the range of approximately\n12.0-15.0~kpc, based on the magnetic field strength derived from the cyclotron\nresonance scattering feature (CRSF). From the estimated distance of 13 kpc and\nGaia's distance of 7.4 kpc, we can infer the observed transition luminosity of\n\\((1.0-1.4) \\times 10^{38} \\, \\mathrm{erg \\, s^{-1}}\\) and \\((3.0-5.0) \\times\n10^{37} \\, \\mathrm{erg \\, s^{-1}}\\), respectively, and compare them with\ntheoretical models. The spectral continuum parameters and the hardness ratio\nalso show significant transitions around the second transition, strongly\nsupporting a change in the accretion regime."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Lingda Kong"
                    },
                    {
                        "name": "Can Gngr"
                    },
                    {
                        "name": "Lorenzo Ducci"
                    },
                    {
                        "name": "Long Ji"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xiaohang Dai"
                    },
                    {
                        "name": "Andrea Santangelo"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Santangelo"
                },
                "author": "Andrea Santangelo",
                "arxiv_doi": "10.1051/0004-6361/202554488 10.1051/0004-6361/202554488\n  10.1051/0004-6361/202554488 10.1051/0004-6361/202554488",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.13557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures, and 3 tables. Accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13553v1",
                "updated": "2025-06-16T14:40:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    40,
                    28,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:40:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    40,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "RelTopo: Enhancing Relational Modeling for Driving Scene Topology\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelTopo: Enhancing Relational Modeling for Driving Scene Topology\n  Reasoning"
                },
                "summary": "Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released."
                },
                "authors": [
                    {
                        "name": "Yueru Luo"
                    },
                    {
                        "name": "Changqing Zhou"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Erlong Li"
                    },
                    {
                        "name": "Chao Zheng"
                    },
                    {
                        "name": "Shuqi Mei"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Zhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Li"
                },
                "author": "Zhen Li",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v5",
                "updated": "2025-06-16T14:32:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    32,
                    22,
                    0,
                    167,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chlo Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chlo Clavel"
                },
                "author": "Chlo Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v2",
                "updated": "2025-06-16T14:30:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making"
                },
                "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19596v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19596v3",
                "updated": "2025-06-16T14:27:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    27,
                    30,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-26T22:20:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    22,
                    20,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "Reference-Aligned Retrieval-Augmented Question Answering over\n  Heterogeneous Proprietary Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Aligned Retrieval-Augmented Question Answering over\n  Heterogeneous Proprietary Documents"
                },
                "summary": "Proprietary corporate documents contain rich domain-specific knowledge, but\ntheir overwhelming volume and disorganized structure make it difficult even for\nemployees to access the right information when needed. For example, in the\nautomotive industry, vehicle crash-collision tests, each costing hundreds of\nthousands of dollars, produce highly detailed documentation. However,\nretrieving relevant content during decision-making remains time-consuming due\nto the scale and complexity of the material. While Retrieval-Augmented\nGeneration (RAG)-based Question Answering (QA) systems offer a promising\nsolution, building an internal RAG-QA system poses several challenges: (1)\nhandling heterogeneous multi-modal data sources, (2) preserving data\nconfidentiality, and (3) enabling traceability between each piece of\ninformation in the generated answer and its original source document. To\naddress these, we propose a RAG-QA framework for internal enterprise use,\nconsisting of: (1) a data pipeline that converts raw multi-modal documents into\na structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving\narchitecture, and (3) a lightweight reference matcher that links answer\nsegments to supporting content. Applied to the automotive domain, our system\nimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),\nand helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale\nratings from both human and LLM judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary corporate documents contain rich domain-specific knowledge, but\ntheir overwhelming volume and disorganized structure make it difficult even for\nemployees to access the right information when needed. For example, in the\nautomotive industry, vehicle crash-collision tests, each costing hundreds of\nthousands of dollars, produce highly detailed documentation. However,\nretrieving relevant content during decision-making remains time-consuming due\nto the scale and complexity of the material. While Retrieval-Augmented\nGeneration (RAG)-based Question Answering (QA) systems offer a promising\nsolution, building an internal RAG-QA system poses several challenges: (1)\nhandling heterogeneous multi-modal data sources, (2) preserving data\nconfidentiality, and (3) enabling traceability between each piece of\ninformation in the generated answer and its original source document. To\naddress these, we propose a RAG-QA framework for internal enterprise use,\nconsisting of: (1) a data pipeline that converts raw multi-modal documents into\na structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving\narchitecture, and (3) a lightweight reference matcher that links answer\nsegments to supporting content. Applied to the automotive domain, our system\nimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),\nand helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale\nratings from both human and LLM judge."
                },
                "authors": [
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Andrew Chung"
                    },
                    {
                        "name": "Ellie S. Paek"
                    },
                    {
                        "name": "Shinsun Lee"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19596v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19596v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17366v3",
                "updated": "2025-06-16T14:20:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    20,
                    42,
                    0,
                    167,
                    0
                ],
                "published": "2024-02-27T10:01:01Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    10,
                    1,
                    1,
                    1,
                    58,
                    0
                ],
                "title": "The risks of risk assessment: causal blind spots when using prediction\n  models for treatment decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The risks of risk assessment: causal blind spots when using prediction\n  models for treatment decisions"
                },
                "summary": "Clinicians increasingly rely on prediction models to guide treatment choices.\nMost prediction models, however, are developed using observational data that\ninclude some patients who have already received the treatment the prediction\nmodel is meant to inform. Special attention to the causal role of those earlier\ntreatments is required when interpreting the resulting predictions. We identify\n'causal blind spots' in three common approaches to handling treatment when\ndeveloping a prediction model: including treatment as a predictor, restricting\nto individuals taking a certain treatment, and ignoring treatment. Through\nseveral real examples, we illustrate how the risks obtained from models\ndeveloped using such approaches may be misinterpreted and can lead to\nmisinformed decision-making. Our discussion covers issues attributable to\nconfounding, selection, mediation and changes in treatment protocols over time.\nWe advocate for an extension of guidelines for the development, reporting and\nevaluation of prediction models to avoid such misinterpretations. Developers\nmust ensure that the intended target population for the model, and the\ntreatment conditions under which predictions hold, are clearly communicated.\nWhen prediction models are intended to inform treatment decisions, they need to\nprovide estimates of risk under the specific treatment (or intervention)\noptions being considered, known as 'prediction under interventions'. Next to\nsuitable data, this requires causal reasoning and causal inference techniques\nduring model development and evaluation. Being clear about what a given\nprediction model can and cannot be used for prevents misinformed treatment\ndecisions and thereby prevents potential harm to patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinicians increasingly rely on prediction models to guide treatment choices.\nMost prediction models, however, are developed using observational data that\ninclude some patients who have already received the treatment the prediction\nmodel is meant to inform. Special attention to the causal role of those earlier\ntreatments is required when interpreting the resulting predictions. We identify\n'causal blind spots' in three common approaches to handling treatment when\ndeveloping a prediction model: including treatment as a predictor, restricting\nto individuals taking a certain treatment, and ignoring treatment. Through\nseveral real examples, we illustrate how the risks obtained from models\ndeveloped using such approaches may be misinterpreted and can lead to\nmisinformed decision-making. Our discussion covers issues attributable to\nconfounding, selection, mediation and changes in treatment protocols over time.\nWe advocate for an extension of guidelines for the development, reporting and\nevaluation of prediction models to avoid such misinterpretations. Developers\nmust ensure that the intended target population for the model, and the\ntreatment conditions under which predictions hold, are clearly communicated.\nWhen prediction models are intended to inform treatment decisions, they need to\nprovide estimates of risk under the specific treatment (or intervention)\noptions being considered, known as 'prediction under interventions'. Next to\nsuitable data, this requires causal reasoning and causal inference techniques\nduring model development and evaluation. Being clear about what a given\nprediction model can and cannot be used for prevents misinformed treatment\ndecisions and thereby prevents potential harm to patients."
                },
                "authors": [
                    {
                        "name": "Nan van Geloven"
                    },
                    {
                        "name": "Ruth H Keogh"
                    },
                    {
                        "name": "Wouter van Amsterdam"
                    },
                    {
                        "name": "Giovanni Cin"
                    },
                    {
                        "name": "Jesse H. Krijthe"
                    },
                    {
                        "name": "Niels Peek"
                    },
                    {
                        "name": "Kim Luijken"
                    },
                    {
                        "name": "Sara Magliacane"
                    },
                    {
                        "name": "Pawe Morzywoek"
                    },
                    {
                        "name": "Thijs van Ommen"
                    },
                    {
                        "name": "Hein Putter"
                    },
                    {
                        "name": "Matthew Sperrin"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Daniala L. Weir"
                    },
                    {
                        "name": "Vanessa Didelez"
                    }
                ],
                "author_detail": {
                    "name": "Vanessa Didelez"
                },
                "author": "Vanessa Didelez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v3",
                "updated": "2025-06-16T14:19:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    19,
                    1,
                    0,
                    167,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jn Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13525v1",
                "updated": "2025-06-16T14:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    18,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:18:23Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    18,
                    23,
                    0,
                    167,
                    0
                ],
                "title": "Implicit and Explicit Research Quality Score Probabilities from ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit and Explicit Research Quality Score Probabilities from ChatGPT"
                },
                "summary": "The large language model (LLM) ChatGPT's quality scores for journal articles\ncorrelate more strongly with human judgements than some citation-based\nindicators in most fields. Averaging multiple ChatGPT scores improves the\nresults, apparently leveraging its internal probability model. To leverage\nthese probabilities, this article tests two novel strategies: requesting\npercentage likelihoods for scores and extracting the probabilities of\nalternative tokens in the responses. The probability estimates were then used\nto calculate weighted average scores. Both strategies were evaluated with five\niterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research\nExcellence Framework (REF) 2021, using departmental average REF2021 quality\nscores as a proxy for article quality. The data was analysed separately for\neach of the 34 field-based REF Units of Assessment. For the first strategy,\nexplicit requests for tables of score percentage likelihoods substantially\ndecreased the value of the scores (lower correlation with the proxy quality\nindicator). In contrast, weighed averages of score token probabilities slightly\nincreased the correlation with the quality proxy indicator and these\nprobabilities reasonably accurately reflected ChatGPT's outputs. The token\nprobability approach is therefore the most accurate method for ranking articles\nby research quality as well as being cheaper than comparable ChatGPT\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large language model (LLM) ChatGPT's quality scores for journal articles\ncorrelate more strongly with human judgements than some citation-based\nindicators in most fields. Averaging multiple ChatGPT scores improves the\nresults, apparently leveraging its internal probability model. To leverage\nthese probabilities, this article tests two novel strategies: requesting\npercentage likelihoods for scores and extracting the probabilities of\nalternative tokens in the responses. The probability estimates were then used\nto calculate weighted average scores. Both strategies were evaluated with five\niterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research\nExcellence Framework (REF) 2021, using departmental average REF2021 quality\nscores as a proxy for article quality. The data was analysed separately for\neach of the 34 field-based REF Units of Assessment. For the first strategy,\nexplicit requests for tables of score percentage likelihoods substantially\ndecreased the value of the scores (lower correlation with the proxy quality\nindicator). In contrast, weighed averages of score token probabilities slightly\nincreased the correlation with the quality proxy indicator and these\nprobabilities reasonably accurately reflected ChatGPT's outputs. The token\nprobability approach is therefore the most accurate method for ranking articles\nby research quality as well as being cheaper than comparable ChatGPT\nstrategies."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Yunhan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhan Yang"
                },
                "author": "Yunhan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13519v1",
                "updated": "2025-06-16T14:13:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    13,
                    21,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:13:21Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    13,
                    21,
                    0,
                    167,
                    0
                ],
                "title": "General-relativistic magnetar magnetospheres in 3D with physics-informed\n  neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-relativistic magnetar magnetospheres in 3D with physics-informed\n  neural networks"
                },
                "summary": "Magnetar phenomena are likely intertwined with the location and structure of\nmagnetospheric currents. While general-relativistic effects may be important in\nshaping the force-free equilibria describing static configurations, most\nstudies have been restricted to axial symmetry. Using a novel methodology based\non physics-informed neural networks, fully three-dimensional configurations of\nvarying stellar compactness are constructed. Realistic profiles for surface\ncurrents, qualitatively capturing the geometry of observed hotspots, are\napplied as boundary conditions to deduce the amount of free energy available to\nfuel outburst activity. It is found that the lowest-energy solution branches\npermit only a $\\approx 30\\%$ excess relative to current-starved solutions in\naxisymmetric cases with global twists, regardless of compactness, reducing to\n$\\approx 5\\%$ in 3D models with localised spots. Accounting for redshift\nreductions to their inferred dipole moments from timing data, explaining\nmagnetar burst energetics therefore becomes more difficult unless the field\nhosts non-negligible multipoles. Discussions on other aspects of magnetar\nphenomena are also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetar phenomena are likely intertwined with the location and structure of\nmagnetospheric currents. While general-relativistic effects may be important in\nshaping the force-free equilibria describing static configurations, most\nstudies have been restricted to axial symmetry. Using a novel methodology based\non physics-informed neural networks, fully three-dimensional configurations of\nvarying stellar compactness are constructed. Realistic profiles for surface\ncurrents, qualitatively capturing the geometry of observed hotspots, are\napplied as boundary conditions to deduce the amount of free energy available to\nfuel outburst activity. It is found that the lowest-energy solution branches\npermit only a $\\approx 30\\%$ excess relative to current-starved solutions in\naxisymmetric cases with global twists, regardless of compactness, reducing to\n$\\approx 5\\%$ in 3D models with localised spots. Accounting for redshift\nreductions to their inferred dipole moments from timing data, explaining\nmagnetar burst energetics therefore becomes more difficult unless the field\nhosts non-negligible multipoles. Discussions on other aspects of magnetar\nphenomena are also provided."
                },
                "authors": [
                    {
                        "name": "Petros Stefanou"
                    },
                    {
                        "name": "Arthur G. Suvorov"
                    },
                    {
                        "name": "Jose A. Pons"
                    }
                ],
                "author_detail": {
                    "name": "Jose A. Pons"
                },
                "author": "Jose A. Pons",
                "arxiv_comment": "10 pages, 9 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13514v1",
                "updated": "2025-06-16T14:09:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    9,
                    43,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:09:43Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    9,
                    43,
                    0,
                    167,
                    0
                ],
                "title": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices"
                },
                "summary": "Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half."
                },
                "authors": [
                    {
                        "name": "Mingxue Xu"
                    },
                    {
                        "name": "Yao Lei Xu"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "ICML 2025 Workshop on Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models (TTODLer-FM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10112v2",
                "updated": "2025-06-16T14:07:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    7,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-04-14T11:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design"
                },
                "summary": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. Due to the opaque nature of LLMs,\nempirical methods are typically used to analyze their efficacy. The quality of\nthis analysis is highly dependent on the chosen testbed, captured metrics and\nanalysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for\nevaluating Large Language Model (LLM)-driven attacks, focusing on offensive\nuses of LLMs in cybersecurity. We review 19 research papers detailing 18\nprototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. Due to the opaque nature of LLMs,\nempirical methods are typically used to analyze their efficacy. The quality of\nthis analysis is highly dependent on the chosen testbed, captured metrics and\nanalysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for\nevaluating Large Language Model (LLM)-driven attacks, focusing on offensive\nuses of LLMs in cybersecurity. We review 19 research papers detailing 18\nprototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jrgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Cito"
                },
                "author": "Jrgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13510v2",
                "updated": "2025-06-17T02:13:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    13,
                    8,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T14:04:54Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    4,
                    54,
                    0,
                    167,
                    0
                ],
                "title": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in\n  Child-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in\n  Child-LLM Interactions"
                },
                "summary": "As Large Language Models (LLMs) increasingly power applications used by\nchildren and adolescents, ensuring safe and age-appropriate interactions has\nbecome an urgent ethical imperative. Despite progress in AI safety, current\nevaluations predominantly focus on adults, neglecting the unique\nvulnerabilities of minors engaging with generative AI. We introduce\nSafe-Child-LLM, a comprehensive benchmark and dataset for systematically\nassessing LLM safety across two developmental stages: children (7-12) and\nadolescents (13-17). Our framework includes a novel multi-part dataset of 200\nadversarial prompts, curated from red-teaming corpora (e.g., SG-Bench,\nHarmBench), with human-annotated labels for jailbreak success and a\nstandardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including\nChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we\nuncover critical safety deficiencies in child-facing scenarios. This work\nhighlights the need for community-driven benchmarks to protect young users in\nLLM interactions. To promote transparency and collaborative advancement in\nethical AI development, we are publicly releasing both our benchmark datasets\nand evaluation codebase at\nhttps://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly power applications used by\nchildren and adolescents, ensuring safe and age-appropriate interactions has\nbecome an urgent ethical imperative. Despite progress in AI safety, current\nevaluations predominantly focus on adults, neglecting the unique\nvulnerabilities of minors engaging with generative AI. We introduce\nSafe-Child-LLM, a comprehensive benchmark and dataset for systematically\nassessing LLM safety across two developmental stages: children (7-12) and\nadolescents (13-17). Our framework includes a novel multi-part dataset of 200\nadversarial prompts, curated from red-teaming corpora (e.g., SG-Bench,\nHarmBench), with human-annotated labels for jailbreak success and a\nstandardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including\nChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we\nuncover critical safety deficiencies in child-facing scenarios. This work\nhighlights the need for community-driven benchmarks to protect young users in\nLLM interactions. To promote transparency and collaborative advancement in\nethical AI development, we are publicly releasing both our benchmark datasets\nand evaluation codebase at\nhttps://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git"
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13502v1",
                "updated": "2025-06-16T13:58:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    58,
                    54,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:58:54Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    58,
                    54,
                    0,
                    167,
                    0
                ],
                "title": "BOW: Bottlenecked Next Word Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOW: Bottlenecked Next Word Exploration"
                },
                "summary": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP."
                },
                "authors": [
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00513v2",
                "updated": "2025-06-16T13:53:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    53,
                    34,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-01T14:38:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    14,
                    38,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal\n  Instruction Tuning"
                },
                "summary": "Despite encouraging progress in 3D scene understanding, it remains\nchallenging to develop an effective Large Multi-modal Model (LMM) that is\ncapable of understanding and reasoning in complex 3D environments. Most\nprevious methods typically encode 3D point and 2D image features separately,\nneglecting interactions between 2D semantics and 3D object properties, as well\nas the spatial relationships within the 3D environment. This limitation not\nonly hinders comprehensive representations of 3D scene, but also compromises\ntraining and inference efficiency. To address these challenges, we propose a\nunified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with\nmultiple 3D scene understanding tasks simultaneously. To obtain the\nfine-grained instance-level visual tokens, we first introduce a novel\nMulti-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D\nsemantics into their corresponding 3D geometric features. For scene-level\nrelation-aware tokens, we further present a 3D Instance Spatial Relation\n(3D-ISR) module to capture the intricate pairwise spatial relationships among\nobjects. Additionally, we perform end-to-end multi-task instruction tuning\nsimultaneously without the subsequent task-specific fine-tuning. Extensive\nexperiments demonstrate that our approach outperforms the state-of-the-art\nmethods across 3D scene understanding, reasoning and grounding tasks. Source\ncode is available at https://github.com/hanxunyu/Inst3D-LMM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite encouraging progress in 3D scene understanding, it remains\nchallenging to develop an effective Large Multi-modal Model (LMM) that is\ncapable of understanding and reasoning in complex 3D environments. Most\nprevious methods typically encode 3D point and 2D image features separately,\nneglecting interactions between 2D semantics and 3D object properties, as well\nas the spatial relationships within the 3D environment. This limitation not\nonly hinders comprehensive representations of 3D scene, but also compromises\ntraining and inference efficiency. To address these challenges, we propose a\nunified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with\nmultiple 3D scene understanding tasks simultaneously. To obtain the\nfine-grained instance-level visual tokens, we first introduce a novel\nMulti-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D\nsemantics into their corresponding 3D geometric features. For scene-level\nrelation-aware tokens, we further present a 3D Instance Spatial Relation\n(3D-ISR) module to capture the intricate pairwise spatial relationships among\nobjects. Additionally, we perform end-to-end multi-task instruction tuning\nsimultaneously without the subsequent task-specific fine-tuning. Extensive\nexperiments demonstrate that our approach outperforms the state-of-the-art\nmethods across 3D scene understanding, reasoning and grounding tasks. Source\ncode is available at https://github.com/hanxunyu/Inst3D-LMM"
                },
                "authors": [
                    {
                        "name": "Hanxun Yu"
                    },
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Junbo Chen"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "CVPR2025, Code Link: https://github.com/hanxunyu/Inst3D-LMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13494v1",
                "updated": "2025-06-16T13:51:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    51,
                    49,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:51:49Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    51,
                    49,
                    0,
                    167,
                    0
                ],
                "title": "Watermarking LLM-Generated Datasets in Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking LLM-Generated Datasets in Downstream Tasks"
                },
                "summary": "Large Language Models (LLMs) have experienced rapid advancements, with\napplications spanning a wide range of fields, including sentiment\nclassification, review generation, and question answering. Due to their\nefficiency and versatility, researchers and companies increasingly employ\nLLM-generated data to train their models. However, the inability to track\ncontent produced by LLMs poses a significant challenge, potentially leading to\ncopyright infringement for the LLM owners. In this paper, we propose a method\nfor injecting watermarks into LLM-generated datasets, enabling the tracking of\ndownstream tasks to detect whether these datasets were produced using the\noriginal LLM. These downstream tasks can be divided into two categories. The\nfirst involves using the generated datasets at the input level, commonly for\ntraining classification tasks. The other is the output level, where model\ntrainers use LLM-generated content as output for downstream tasks, such as\nquestion-answering tasks. We design a comprehensive set of experiments to\nevaluate both watermark methods. Our results indicate the high effectiveness of\nour watermark approach. Additionally, regarding model utility, we find that\nclassifiers trained on the generated datasets achieve a test accuracy exceeding\n0.900 in many cases, suggesting that the utility of such models remains robust.\nFor the output-level watermark, we observe that the quality of the generated\ntext is comparable to that produced using real-world datasets. Through our\nresearch, we aim to advance the protection of LLM copyrights, taking a\nsignificant step forward in safeguarding intellectual property in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have experienced rapid advancements, with\napplications spanning a wide range of fields, including sentiment\nclassification, review generation, and question answering. Due to their\nefficiency and versatility, researchers and companies increasingly employ\nLLM-generated data to train their models. However, the inability to track\ncontent produced by LLMs poses a significant challenge, potentially leading to\ncopyright infringement for the LLM owners. In this paper, we propose a method\nfor injecting watermarks into LLM-generated datasets, enabling the tracking of\ndownstream tasks to detect whether these datasets were produced using the\noriginal LLM. These downstream tasks can be divided into two categories. The\nfirst involves using the generated datasets at the input level, commonly for\ntraining classification tasks. The other is the output level, where model\ntrainers use LLM-generated content as output for downstream tasks, such as\nquestion-answering tasks. We design a comprehensive set of experiments to\nevaluate both watermark methods. Our results indicate the high effectiveness of\nour watermark approach. Additionally, regarding model utility, we find that\nclassifiers trained on the generated datasets achieve a test accuracy exceeding\n0.900 in many cases, suggesting that the utility of such models remains robust.\nFor the output-level watermark, we observe that the quality of the generated\ntext is comparable to that produced using real-world datasets. Through our\nresearch, we aim to advance the protection of LLM copyrights, taking a\nsignificant step forward in safeguarding intellectual property in this domain."
                },
                "authors": [
                    {
                        "name": "Yugeng Liu"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13485v1",
                "updated": "2025-06-16T13:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    44,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    44,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Curriculum Learning for Biological Sequence Prediction: The Case of De\n  Novo Peptide Sequencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Learning for Biological Sequence Prediction: The Case of De\n  Novo Peptide Sequencing"
                },
                "summary": "Peptide sequencing-the process of identifying amino acid sequences from mass\nspectrometry data-is a fundamental task in proteomics. Non-Autoregressive\nTransformers (NATs) have proven highly effective for this task, outperforming\ntraditional methods. Unlike autoregressive models, which generate tokens\nsequentially, NATs predict all positions simultaneously, leveraging\nbidirectional context through unmasked self-attention. However, existing NAT\napproaches often rely on Connectionist Temporal Classification (CTC) loss,\nwhich presents significant optimization challenges due to CTC's complexity and\nincreases the risk of training failures. To address these issues, we propose an\nimproved non-autoregressive peptide sequencing model that incorporates a\nstructured protein sequence curriculum learning strategy. This approach adjusts\nprotein's learning difficulty based on the model's estimated protein\ngenerational capabilities through a sampling process, progressively learning\npeptide generation from simple to complex sequences. Additionally, we introduce\na self-refining inference-time module that iteratively enhances predictions\nusing learned NAT token embeddings, improving sequence accuracy at a\nfine-grained level. Our curriculum learning strategy reduces NAT training\nfailures frequency by more than 90% based on sampled training over various data\ndistributions. Evaluations on nine benchmark species demonstrate that our\napproach outperforms all previous methods across multiple metrics and species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peptide sequencing-the process of identifying amino acid sequences from mass\nspectrometry data-is a fundamental task in proteomics. Non-Autoregressive\nTransformers (NATs) have proven highly effective for this task, outperforming\ntraditional methods. Unlike autoregressive models, which generate tokens\nsequentially, NATs predict all positions simultaneously, leveraging\nbidirectional context through unmasked self-attention. However, existing NAT\napproaches often rely on Connectionist Temporal Classification (CTC) loss,\nwhich presents significant optimization challenges due to CTC's complexity and\nincreases the risk of training failures. To address these issues, we propose an\nimproved non-autoregressive peptide sequencing model that incorporates a\nstructured protein sequence curriculum learning strategy. This approach adjusts\nprotein's learning difficulty based on the model's estimated protein\ngenerational capabilities through a sampling process, progressively learning\npeptide generation from simple to complex sequences. Additionally, we introduce\na self-refining inference-time module that iteratively enhances predictions\nusing learned NAT token embeddings, improving sequence accuracy at a\nfine-grained level. Our curriculum learning strategy reduces NAT training\nfailures frequency by more than 90% based on sampled training over various data\ndistributions. Evaluations on nine benchmark species demonstrate that our\napproach outperforms all previous methods across multiple metrics and species."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Jiaqi Wei"
                    },
                    {
                        "name": "Zijie Qiu"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "name": "Siqi Sun"
                    }
                ],
                "author_detail": {
                    "name": "Siqi Sun"
                },
                "author": "Siqi Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13474v1",
                "updated": "2025-06-16T13:32:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    32,
                    1,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:32:01Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    32,
                    1,
                    0,
                    167,
                    0
                ],
                "title": "Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning"
                },
                "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency."
                },
                "authors": [
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege zsoy"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09975v2",
                "updated": "2025-06-16T13:31:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    31,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T17:51:28Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text"
                },
                "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case."
                },
                "authors": [
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear in ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13472v2",
                "updated": "2025-06-17T09:13:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    13,
                    54,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T13:30:33Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    30,
                    33,
                    0,
                    167,
                    0
                ],
                "title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models"
                },
                "summary": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64."
                },
                "authors": [
                    {
                        "name": "Junho Yoon"
                    },
                    {
                        "name": "Geom Lee"
                    },
                    {
                        "name": "Donghyeon Jeon"
                    },
                    {
                        "name": "Inho Kang"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10019v2",
                "updated": "2025-06-16T13:26:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    26,
                    24,
                    0,
                    167,
                    0
                ],
                "published": "2023-08-19T14:01:04Z",
                "published_parsed": [
                    2023,
                    8,
                    19,
                    14,
                    1,
                    4,
                    5,
                    231,
                    0
                ],
                "title": "Dissecting RGB-D Learning for Improved Multi-modal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting RGB-D Learning for Improved Multi-modal Fusion"
                },
                "summary": "In the RGB-D vision community, extensive research has been focused on\ndesigning multi-modal learning strategies and fusion structures. However, the\ncomplementary and fusion mechanisms in RGB-D models remain a black box. In this\npaper, we present an analytical framework and a novel score to dissect the\nRGB-D vision community. Our approach involves measuring proposed semantic\nvariance and feature similarity across modalities and levels, conducting visual\nand quantitative analyzes on multi-modal learning through comprehensive\nexperiments. Specifically, we investigate the consistency and specialty of\nfeatures across modalities, evolution rules within each modality, and the\ncollaboration logic used when optimizing a RGB-D model. Our studies\nreveal/verify several important findings, such as the discrepancy in\ncross-modal features and the hybrid multi-modal cooperation rule, which\nhighlights consistency and specialty simultaneously for complementary\ninference. We also showcase the versatility of the proposed RGB-D dissection\nmethod and introduce a straightforward fusion strategy based on our findings,\nwhich delivers significant enhancements across various tasks and even other\nmulti-modal data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the RGB-D vision community, extensive research has been focused on\ndesigning multi-modal learning strategies and fusion structures. However, the\ncomplementary and fusion mechanisms in RGB-D models remain a black box. In this\npaper, we present an analytical framework and a novel score to dissect the\nRGB-D vision community. Our approach involves measuring proposed semantic\nvariance and feature similarity across modalities and levels, conducting visual\nand quantitative analyzes on multi-modal learning through comprehensive\nexperiments. Specifically, we investigate the consistency and specialty of\nfeatures across modalities, evolution rules within each modality, and the\ncollaboration logic used when optimizing a RGB-D model. Our studies\nreveal/verify several important findings, such as the discrepancy in\ncross-modal features and the hybrid multi-modal cooperation rule, which\nhighlights consistency and specialty simultaneously for complementary\ninference. We also showcase the versatility of the proposed RGB-D dissection\nmethod and introduce a straightforward fusion strategy based on our findings,\nwhich delivers significant enhancements across various tasks and even other\nmulti-modal data."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoran Zhou"
                    },
                    {
                        "name": "Yunshu Zhang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yongjian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Deng"
                },
                "author": "Yongjian Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13464v1",
                "updated": "2025-06-16T13:24:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    24,
                    50,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:24:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    24,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Zheyuan Xiao"
                    },
                    {
                        "name": "Seraphina Zhang"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03454v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03454v3",
                "updated": "2025-06-16T13:23:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    23,
                    4,
                    0,
                    167,
                    0
                ],
                "published": "2024-12-04T16:42:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    42,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "Decoding Long-duration Gravitational Waves from Binary Neutron Stars\n  with Machine Learning: Parameter Estimation and Equations of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Long-duration Gravitational Waves from Binary Neutron Stars\n  with Machine Learning: Parameter Estimation and Equations of State"
                },
                "summary": "Gravitational waves (GWs) from binary neutron stars (BNSs) offer valuable\nunderstanding of the nature of compact objects and hadronic matter, and the\nscience potential will be greatly enhanced by the third-generation (3G) GW\ndetectors, which are expected to detect BNS signals with order-of-magnitude\nimprovements in duration, detection rates, and signal strength. However, the\nresulting computational demands for analyzing such prolonged signals pose a\ncritical challenge that existing Bayesian methods cannot feasibly address in\nthe 3G era. To bridge this critical gap, we demonstrate a machine\nlearning-based workflow capable of producing source parameter estimation and\nconstraints on equations of state (EOSs) for hours-long BNS signals in seconds\nwith minimal hardware costs. We employ efficient compression of the GW data and\nEOS using neural networks, based on which we build normalizing flows for\ninference that can deliver results in seconds. The optimized computational cost\nof BNS signal analysis with our framework shows that machine learning has the\npotential to be an indispensable tool for future catalog-level BNS analyses,\npaving the way for large-scale investigations of BNS-related physics across the\n3G observational landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) from binary neutron stars (BNSs) offer valuable\nunderstanding of the nature of compact objects and hadronic matter, and the\nscience potential will be greatly enhanced by the third-generation (3G) GW\ndetectors, which are expected to detect BNS signals with order-of-magnitude\nimprovements in duration, detection rates, and signal strength. However, the\nresulting computational demands for analyzing such prolonged signals pose a\ncritical challenge that existing Bayesian methods cannot feasibly address in\nthe 3G era. To bridge this critical gap, we demonstrate a machine\nlearning-based workflow capable of producing source parameter estimation and\nconstraints on equations of state (EOSs) for hours-long BNS signals in seconds\nwith minimal hardware costs. We employ efficient compression of the GW data and\nEOS using neural networks, based on which we build normalizing flows for\ninference that can deliver results in seconds. The optimized computational cost\nof BNS signal analysis with our framework shows that machine learning has the\npotential to be an indispensable tool for future catalog-level BNS analyses,\npaving the way for large-scale investigations of BNS-related physics across the\n3G observational landscape."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "Jessica Irwin"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Christopher Messenger"
                    },
                    {
                        "name": "Lami Suleiman"
                    },
                    {
                        "name": "Ik Siong Heng"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "10 pages, 4 figures. Accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03454v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03450v2",
                "updated": "2025-06-16T13:17:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    17,
                    57,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-03T23:21:25Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    23,
                    21,
                    25,
                    1,
                    154,
                    0
                ],
                "title": "SENMAP: Multi-objective data-flow mapping and synthesis for hybrid\n  scalable neuromorphic systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SENMAP: Multi-objective data-flow mapping and synthesis for hybrid\n  scalable neuromorphic systems"
                },
                "summary": "This paper introduces SENMap, a mapping and synthesis tool for scalable,\nenergy-efficient neuromorphic computing architecture frameworks. SENECA is a\nflexible architectural design optimized for executing edge AI SNN/ANN inference\napplications efficiently. To speed up the silicon tape-out and chip design for\nSENECA, an accurate emulator, SENSIM, was designed. While SENSIM supports\ndirect mapping of SNNs on neuromorphic architectures, as the SNN and ANNs grow\nin size, achieving optimal mapping for objectives like energy, throughput,\narea, and accuracy becomes challenging. This paper introduces SENMap, flexible\nmapping software for efficiently mapping large SNN and ANN applications onto\nadaptable architectures. SENMap considers architectural, pretrained SNN and ANN\nrealistic examples, and event rate-based parameters and is open-sourced along\nwith SENSIM to aid flexible neuromorphic chip design before fabrication.\nExperimental results show SENMap enables 40 percent energy improvements for a\nbaseline SENSIM operating in timestep asynchronous mode of operation. SENMap is\ndesigned in such a way that it facilitates mapping large spiking neural\nnetworks for future modifications as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SENMap, a mapping and synthesis tool for scalable,\nenergy-efficient neuromorphic computing architecture frameworks. SENECA is a\nflexible architectural design optimized for executing edge AI SNN/ANN inference\napplications efficiently. To speed up the silicon tape-out and chip design for\nSENECA, an accurate emulator, SENSIM, was designed. While SENSIM supports\ndirect mapping of SNNs on neuromorphic architectures, as the SNN and ANNs grow\nin size, achieving optimal mapping for objectives like energy, throughput,\narea, and accuracy becomes challenging. This paper introduces SENMap, flexible\nmapping software for efficiently mapping large SNN and ANN applications onto\nadaptable architectures. SENMap considers architectural, pretrained SNN and ANN\nrealistic examples, and event rate-based parameters and is open-sourced along\nwith SENSIM to aid flexible neuromorphic chip design before fabrication.\nExperimental results show SENMap enables 40 percent energy improvements for a\nbaseline SENSIM operating in timestep asynchronous mode of operation. SENMap is\ndesigned in such a way that it facilitates mapping large spiking neural\nnetworks for future modifications as well."
                },
                "authors": [
                    {
                        "name": "Prithvish V Nembhani"
                    },
                    {
                        "name": "Oliver Rhodes"
                    },
                    {
                        "name": "Guangzhi Tang"
                    },
                    {
                        "name": "Alexandra F Dobrita"
                    },
                    {
                        "name": "Yingfu Xu"
                    },
                    {
                        "name": "Kanishkan Vadivel"
                    },
                    {
                        "name": "Kevin Shidqi"
                    },
                    {
                        "name": "Paul Detterer"
                    },
                    {
                        "name": "Mario Konijnenburg"
                    },
                    {
                        "name": "Gert-Jan van Schaik"
                    },
                    {
                        "name": "Manolis Sifalakis"
                    },
                    {
                        "name": "Zaid Al-Ars"
                    },
                    {
                        "name": "Amirreza Yousefzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Amirreza Yousefzadeh"
                },
                "author": "Amirreza Yousefzadeh",
                "arxiv_comment": "IJCNN conference, Italy, 2025, accepted, 30 June - 5 July",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17533v2",
                "updated": "2025-06-16T13:07:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    7,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-24T14:42:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    42,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "From Euler to AI: Unifying Formulas for Mathematical Constants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Euler to AI: Unifying Formulas for Mathematical Constants"
                },
                "summary": "The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains."
                },
                "authors": [
                    {
                        "name": "Tomer Raz"
                    },
                    {
                        "name": "Michael Shalyt"
                    },
                    {
                        "name": "Elyasheev Leibtag"
                    },
                    {
                        "name": "Rotem Kalisch"
                    },
                    {
                        "name": "Shachar Weinbaum"
                    },
                    {
                        "name": "Yaron Hadad"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_comment": "60 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.HO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13439v1",
                "updated": "2025-06-16T12:54:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    54,
                    6,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:54:06Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    54,
                    6,
                    0,
                    167,
                    0
                ],
                "title": "Dark Energy Survey Year 3 results: $w$CDM cosmology from\n  simulation-based inference with persistent homology on the sphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark Energy Survey Year 3 results: $w$CDM cosmology from\n  simulation-based inference with persistent homology on the sphere"
                },
                "summary": "We present cosmological constraints from Dark Energy Survey Year 3 (DES Y3)\nweak lensing data using persistent homology, a topological data analysis\ntechnique that tracks how features like clusters and voids evolve across\ndensity thresholds. For the first time, we apply spherical persistent homology\nto galaxy survey data through the algorithm TopoS2, which is optimized for\ncurved-sky analyses and HEALPix compatibility. Employing a simulation-based\ninference framework with the Gower Street simulation suite, specifically\ndesigned to mimic DES Y3 data properties, we extract topological summary\nstatistics from convergence maps across multiple smoothing scales and redshift\nbins. After neural network compression of these statistics, we estimate the\nlikelihood function and validate our analysis against baryonic feedback\neffects, finding minimal biases (under $0.3\\sigma$) in the\n$\\Omega_\\mathrm{m}-S_8$ plane. Assuming the $w$CDM model, our combined Betti\nnumbers and second moments analysis yields $S_8 = 0.821 \\pm 0.018$ and\n$\\Omega_\\mathrm{m} = 0.304\\pm0.037$-constraints 70% tighter than those from\ncosmic shear two-point statistics in the same parameter plane. Our results\ndemonstrate that topological methods provide a powerful and robust framework\nfor extracting cosmological information, with our spherical methodology readily\napplicable to upcoming Stage IV wide-field galaxy surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present cosmological constraints from Dark Energy Survey Year 3 (DES Y3)\nweak lensing data using persistent homology, a topological data analysis\ntechnique that tracks how features like clusters and voids evolve across\ndensity thresholds. For the first time, we apply spherical persistent homology\nto galaxy survey data through the algorithm TopoS2, which is optimized for\ncurved-sky analyses and HEALPix compatibility. Employing a simulation-based\ninference framework with the Gower Street simulation suite, specifically\ndesigned to mimic DES Y3 data properties, we extract topological summary\nstatistics from convergence maps across multiple smoothing scales and redshift\nbins. After neural network compression of these statistics, we estimate the\nlikelihood function and validate our analysis against baryonic feedback\neffects, finding minimal biases (under $0.3\\sigma$) in the\n$\\Omega_\\mathrm{m}-S_8$ plane. Assuming the $w$CDM model, our combined Betti\nnumbers and second moments analysis yields $S_8 = 0.821 \\pm 0.018$ and\n$\\Omega_\\mathrm{m} = 0.304\\pm0.037$-constraints 70% tighter than those from\ncosmic shear two-point statistics in the same parameter plane. Our results\ndemonstrate that topological methods provide a powerful and robust framework\nfor extracting cosmological information, with our spherical methodology readily\napplicable to upcoming Stage IV wide-field galaxy surveys."
                },
                "authors": [
                    {
                        "name": "J. Prat"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "C. Doux"
                    },
                    {
                        "name": "P. Pranav"
                    },
                    {
                        "name": "C. Chang"
                    },
                    {
                        "name": "N. Jeffrey"
                    },
                    {
                        "name": "L. Whiteway"
                    },
                    {
                        "name": "D. Anbajagane"
                    },
                    {
                        "name": "S. Sugiyama"
                    },
                    {
                        "name": "A. Thomsen"
                    },
                    {
                        "name": "A. Alarcon"
                    },
                    {
                        "name": "A. Amon"
                    },
                    {
                        "name": "K. Bechtol"
                    },
                    {
                        "name": "G. M. Bernstein"
                    },
                    {
                        "name": "A. Campos"
                    },
                    {
                        "name": "R. Chen"
                    },
                    {
                        "name": "A. Choi"
                    },
                    {
                        "name": "C. Davis"
                    },
                    {
                        "name": "J. DeRose"
                    },
                    {
                        "name": "S. Dodelson"
                    },
                    {
                        "name": "K. Eckert"
                    },
                    {
                        "name": "J. Elvin-Poole"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "A. Fert"
                    },
                    {
                        "name": "D. Gruen"
                    },
                    {
                        "name": "E. M. Huff"
                    },
                    {
                        "name": "I. Harrison"
                    },
                    {
                        "name": "K. Herner"
                    },
                    {
                        "name": "M. Jarvis"
                    },
                    {
                        "name": "N. Kuropatkin"
                    },
                    {
                        "name": "P. -F. Leget"
                    },
                    {
                        "name": "N. MacCrann"
                    },
                    {
                        "name": "J. McCullough"
                    },
                    {
                        "name": "J. Myles"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Pandey"
                    },
                    {
                        "name": "M. Raveri"
                    },
                    {
                        "name": "R. P. Rollins"
                    },
                    {
                        "name": "A. Roodman"
                    },
                    {
                        "name": "C. Snchez"
                    },
                    {
                        "name": "L. F. Secco"
                    },
                    {
                        "name": "E. Sheldon"
                    },
                    {
                        "name": "T. Shin"
                    },
                    {
                        "name": "M. A. Troxel"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "T. N. Varga"
                    },
                    {
                        "name": "B. Yanny"
                    },
                    {
                        "name": "B. Yin"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "J. Zuntz"
                    },
                    {
                        "name": "T. M. C. Abbott"
                    },
                    {
                        "name": "M. Aguena"
                    },
                    {
                        "name": "S. Allam"
                    },
                    {
                        "name": "F. Andrade-Oliveira"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "S. Bocquet"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "R. Cawthon"
                    },
                    {
                        "name": "J. De Vicente"
                    },
                    {
                        "name": "S. Desai"
                    },
                    {
                        "name": "M. E. da Silva Pereira"
                    },
                    {
                        "name": "H. T. Diehl"
                    },
                    {
                        "name": "B. Flaugher"
                    },
                    {
                        "name": "J. Frieman"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "R. A. Gruendl"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "S. R. Hinton"
                    },
                    {
                        "name": "D. L. Hollowood"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. J. James"
                    },
                    {
                        "name": "K. Kuehn"
                    },
                    {
                        "name": "L. N. da Costa"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "J. L. Marshall"
                    },
                    {
                        "name": "J. Mena-Fernndez"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. J. Mohr"
                    },
                    {
                        "name": "R. L. C. Ogando"
                    },
                    {
                        "name": "A. A. Plazas Malagn"
                    },
                    {
                        "name": "A. Porredon"
                    },
                    {
                        "name": "S. Samuroff"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "B. Santiago"
                    },
                    {
                        "name": "I. Sevilla-Noarbe"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "E. Suchyta"
                    },
                    {
                        "name": "M. E. C. Swanson"
                    },
                    {
                        "name": "D. Thomas"
                    },
                    {
                        "name": "C. To"
                    },
                    {
                        "name": "V. Vikram"
                    },
                    {
                        "name": "A. R. Walker"
                    },
                    {
                        "name": "N. Weaverdyck"
                    },
                    {
                        "name": "J. Weller"
                    }
                ],
                "author_detail": {
                    "name": "J. Weller"
                },
                "arxiv_affiliation": "DES Collaboration",
                "author": "J. Weller",
                "arxiv_comment": "To be submitted to MNRAS. 18 + 3 pages. 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13434v1",
                "updated": "2025-06-16T12:52:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    19,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:52:19Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    19,
                    0,
                    167,
                    0
                ],
                "title": "From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in\n  the Age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in\n  the Age of LLMs"
                },
                "summary": "Large Language Models (LLMs) are set to reshape cybersecurity by augmenting\nred and blue team operations. Red teams can exploit LLMs to plan attacks, craft\nphishing content, simulate adversaries, and generate exploit code. Conversely,\nblue teams may deploy them for threat intelligence synthesis, root cause\nanalysis, and streamlined documentation. This dual capability introduces both\ntransformative potential and serious risks.\n  This position paper maps LLM applications across cybersecurity frameworks\nsuch as MITRE ATT&CK and the NIST Cybersecurity Framework (CSF), offering a\nstructured view of their current utility and limitations. While LLMs\ndemonstrate fluency and versatility across various tasks, they remain fragile\nin high-stakes, context-heavy environments. Key limitations include\nhallucinations, limited context retention, poor reasoning, and sensitivity to\nprompts, which undermine their reliability in operational settings.\n  Moreover, real-world integration raises concerns around dual-use risks,\nadversarial misuse, and diminished human oversight. Malicious actors could\nexploit LLMs to automate reconnaissance, obscure attack vectors, and lower the\ntechnical threshold for executing sophisticated attacks.\n  To ensure safer adoption, we recommend maintaining human-in-the-loop\noversight, enhancing model explainability, integrating privacy-preserving\nmechanisms, and building systems robust to adversarial exploitation. As\norganizations increasingly adopt AI driven cybersecurity, a nuanced\nunderstanding of LLMs' risks and operational impacts is critical to securing\ntheir defensive value while mitigating unintended consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are set to reshape cybersecurity by augmenting\nred and blue team operations. Red teams can exploit LLMs to plan attacks, craft\nphishing content, simulate adversaries, and generate exploit code. Conversely,\nblue teams may deploy them for threat intelligence synthesis, root cause\nanalysis, and streamlined documentation. This dual capability introduces both\ntransformative potential and serious risks.\n  This position paper maps LLM applications across cybersecurity frameworks\nsuch as MITRE ATT&CK and the NIST Cybersecurity Framework (CSF), offering a\nstructured view of their current utility and limitations. While LLMs\ndemonstrate fluency and versatility across various tasks, they remain fragile\nin high-stakes, context-heavy environments. Key limitations include\nhallucinations, limited context retention, poor reasoning, and sensitivity to\nprompts, which undermine their reliability in operational settings.\n  Moreover, real-world integration raises concerns around dual-use risks,\nadversarial misuse, and diminished human oversight. Malicious actors could\nexploit LLMs to automate reconnaissance, obscure attack vectors, and lower the\ntechnical threshold for executing sophisticated attacks.\n  To ensure safer adoption, we recommend maintaining human-in-the-loop\noversight, enhancing model explainability, integrating privacy-preserving\nmechanisms, and building systems robust to adversarial exploitation. As\norganizations increasingly adopt AI driven cybersecurity, a nuanced\nunderstanding of LLMs' risks and operational impacts is critical to securing\ntheir defensive value while mitigating unintended consequences."
                },
                "authors": [
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Chris Hicks"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Burak Hasircioglu"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Piers Jennings"
                    }
                ],
                "author_detail": {
                    "name": "Piers Jennings"
                },
                "author": "Piers Jennings",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09813v2",
                "updated": "2025-06-16T12:43:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    43,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T14:53:47Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metritocracy: Representative Metrics for Lite Benchmarks"
                },
                "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation."
                },
                "authors": [
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Benjamin Schiffer"
                    },
                    {
                        "name": "Serena Wang"
                    },
                    {
                        "name": "Shirley Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Zhang"
                },
                "author": "Shirley Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13422v2",
                "updated": "2025-06-17T13:31:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    13,
                    31,
                    16,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T12:37:49Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    37,
                    49,
                    0,
                    167,
                    0
                ],
                "title": "Jet outbursts, non-thermal pressure and the AGN jet duty cycle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jet outbursts, non-thermal pressure and the AGN jet duty cycle"
                },
                "summary": "We predict the non-thermal pressure (NTP) induced in the cores of galaxy\nclusters by kinetic jet feedback from an active galactic nucleus (AGN). We\nmodel a population of Fanaroff-Riley type I jets when sampling power-law\ndistributions in jet power and age, which we evolve in time with a two-phase\njet-lobe model. We couple the energy of each jet outburst to the surrounding\ngas inside spherical shells, allowing us to estimate the fraction of NTP to\ntotal pressure induced in the cluster. We predict the mean profile for this NTP\nfraction over the source population in a variety of cluster environments and\nfor different AGN jet duty cycles. For typical gas and dark matter profiles,\nthe mean NTP fraction peaks at ~4-6% when the AGN jets are active for 10-30% of\nthe total AGN lifecycle. These predictions are in good agreement with\nobservational constraints, suggesting that AGN feedback imparts only small\nnon-thermal contributions to the cluster's core. Furthermore, we find a\nrelationship between the peak in the mean NTP fraction and the AGN jet duty\ncycle in a given cluster environment. Applying this to Hitomi measurements of\nthe NTP in the Perseus cluster, we infer an AGN jet duty cycle that is\nconsistent with independent evidence of Perseus' AGN jet activity. We propose\nthis as a novel approach for observationally inferring the past AGN activity of\nreal clusters from their observed NTP fraction and environmental profiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We predict the non-thermal pressure (NTP) induced in the cores of galaxy\nclusters by kinetic jet feedback from an active galactic nucleus (AGN). We\nmodel a population of Fanaroff-Riley type I jets when sampling power-law\ndistributions in jet power and age, which we evolve in time with a two-phase\njet-lobe model. We couple the energy of each jet outburst to the surrounding\ngas inside spherical shells, allowing us to estimate the fraction of NTP to\ntotal pressure induced in the cluster. We predict the mean profile for this NTP\nfraction over the source population in a variety of cluster environments and\nfor different AGN jet duty cycles. For typical gas and dark matter profiles,\nthe mean NTP fraction peaks at ~4-6% when the AGN jets are active for 10-30% of\nthe total AGN lifecycle. These predictions are in good agreement with\nobservational constraints, suggesting that AGN feedback imparts only small\nnon-thermal contributions to the cluster's core. Furthermore, we find a\nrelationship between the peak in the mean NTP fraction and the AGN jet duty\ncycle in a given cluster environment. Applying this to Hitomi measurements of\nthe NTP in the Perseus cluster, we infer an AGN jet duty cycle that is\nconsistent with independent evidence of Perseus' AGN jet activity. We propose\nthis as a novel approach for observationally inferring the past AGN activity of\nreal clusters from their observed NTP fraction and environmental profiles."
                },
                "authors": [
                    {
                        "name": "Andrew Sullivan"
                    },
                    {
                        "name": "Ross J. Turner"
                    },
                    {
                        "name": "Stanislav S. Shabala"
                    },
                    {
                        "name": "Chris Power"
                    },
                    {
                        "name": "Sophie A. Young"
                    }
                ],
                "author_detail": {
                    "name": "Sophie A. Young"
                },
                "author": "Sophie A. Young",
                "arxiv_comment": "Added further acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10599v2",
                "updated": "2025-06-16T12:36:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    36,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-12T11:43:40Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    43,
                    40,
                    3,
                    163,
                    0
                ],
                "title": "Enhancing Taiji's Parameter Estimation under Non-Stationarity: a\n  Time-Frequency Domain Framework for Galactic Binaries and Instrumental Noises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Taiji's Parameter Estimation under Non-Stationarity: a\n  Time-Frequency Domain Framework for Galactic Binaries and Instrumental Noises"
                },
                "summary": "The data analysis of space-based gravitational wave detectors like Taiji\nfaces significant challenges from non-stationary noise, which compromises the\nefficacy of traditional frequency-domain analysis. This work proposes a unified\nframework based on short-time Fourier transform (STFT) to enhance parameter\nestimation of Galactic binary and characterization of instrumental noise under\nnon-stationarity. Segmenting data into locally stationary intervals, we derive\nSTFT-based models for signals and noises, and implement Bayesian inference via\nthe extended Whittle likelihood. Validated through the analysis of verification\nGalactic binaries and instrumental noises, our STFT approach outperforms\nfrequency-domain methods by reducing the uncertainty and bias of estimation,\nsuccessfully recovering low signal-to-noise ratio signals missed by\nfrequency-domain analysis, and mitigating the degeneracy among noise\nparameters. The framework's robustness against noise drifts and computational\nefficiency highlight its potential for integration into future global analysis\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data analysis of space-based gravitational wave detectors like Taiji\nfaces significant challenges from non-stationary noise, which compromises the\nefficacy of traditional frequency-domain analysis. This work proposes a unified\nframework based on short-time Fourier transform (STFT) to enhance parameter\nestimation of Galactic binary and characterization of instrumental noise under\nnon-stationarity. Segmenting data into locally stationary intervals, we derive\nSTFT-based models for signals and noises, and implement Bayesian inference via\nthe extended Whittle likelihood. Validated through the analysis of verification\nGalactic binaries and instrumental noises, our STFT approach outperforms\nfrequency-domain methods by reducing the uncertainty and bias of estimation,\nsuccessfully recovering low signal-to-noise ratio signals missed by\nfrequency-domain analysis, and mitigating the degeneracy among noise\nparameters. The framework's robustness against noise drifts and computational\nefficiency highlight its potential for integration into future global analysis\npipelines."
                },
                "authors": [
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "Ziren Luo"
                    },
                    {
                        "name": "Peng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Xu"
                },
                "author": "Peng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13417v1",
                "updated": "2025-06-16T12:34:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    34,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:34:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    34,
                    16,
                    0,
                    167,
                    0
                ],
                "title": "Chaos, coherence and turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chaos, coherence and turbulence"
                },
                "summary": "This paper is a personal overview of the efforts over the last half century\nto understand fluid turbulence in terms of simpler coherent units. The\nconsequences of chaos and the concept of coherence are first reviewed, using\nexamples from free-shear and wall-bounded shear flows, and including how the\nsimplifications due to coherent structures have been useful in the\nconceptualization and control of turbulence. It is remarked that, even if this\napproach has revolutionized our understanding of the flow, most of turbulence\ncannot yet be described by structures. This includes cascades, both direct and\ninverse, and possibly junk turbulence, whose role, if any, is currently\nunknown. This part of the paper is mostly a catalog of questions, some of them\nanswered and others still open. A second part of the paper examines which new\ntechniques can be expected to help in attacking the open questions, and which,\nin the opinion of the author, are the strengths and limitations of current\napproaches, such as data-driven science and causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is a personal overview of the efforts over the last half century\nto understand fluid turbulence in terms of simpler coherent units. The\nconsequences of chaos and the concept of coherence are first reviewed, using\nexamples from free-shear and wall-bounded shear flows, and including how the\nsimplifications due to coherent structures have been useful in the\nconceptualization and control of turbulence. It is remarked that, even if this\napproach has revolutionized our understanding of the flow, most of turbulence\ncannot yet be described by structures. This includes cascades, both direct and\ninverse, and possibly junk turbulence, whose role, if any, is currently\nunknown. This part of the paper is mostly a catalog of questions, some of them\nanswered and others still open. A second part of the paper examines which new\ntechniques can be expected to help in attacking the open questions, and which,\nin the opinion of the author, are the strengths and limitations of current\napproaches, such as data-driven science and causal inference."
                },
                "authors": [
                    {
                        "name": "Javier Jimenez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Jimenez"
                },
                "author": "Javier Jimenez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12188v2",
                "updated": "2025-06-16T12:31:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    31,
                    12,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-15T08:04:00Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    8,
                    4,
                    0,
                    5,
                    46,
                    0
                ],
                "title": "Boosting Generalization in Diffusion-Based Neural Combinatorial Solver\n  via Inference Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Generalization in Diffusion-Based Neural Combinatorial Solver\n  via Inference Time Adaptation"
                },
                "summary": "Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies on\ndiffusion models have introduced training-free guidance approaches that\nleverage pre-defined guidance functions for conditional generation, such\nmethodologies have not been extensively explored in combinatorial optimization.\nTo bridge this gap, we propose a training-free inference time adaptation\nframework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and\ncross-scale generalization capabilities of diffusion-based NCO solvers without\nrequiring additional training. We provide theoretical analysis that helps\nunderstanding the cross-problem transfer capability. Our experimental results\ndemonstrate that a diffusion solver, trained exclusively on the Traveling\nSalesman Problem (TSP), can achieve competitive zero-shot transfer performance\nacross different problem scales on TSP variants, such as Prize Collecting TSP\n(PCTSP) and the Orienteering Problem (OP), through inference time adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies on\ndiffusion models have introduced training-free guidance approaches that\nleverage pre-defined guidance functions for conditional generation, such\nmethodologies have not been extensively explored in combinatorial optimization.\nTo bridge this gap, we propose a training-free inference time adaptation\nframework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and\ncross-scale generalization capabilities of diffusion-based NCO solvers without\nrequiring additional training. We provide theoretical analysis that helps\nunderstanding the cross-problem transfer capability. Our experimental results\ndemonstrate that a diffusion solver, trained exclusively on the Traveling\nSalesman Problem (TSP), can achieve competitive zero-shot transfer performance\nacross different problem scales on TSP variants, such as Prize Collecting TSP\n(PCTSP) and the Orienteering Problem (OP), through inference time adaptation."
                },
                "authors": [
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Yinchuan Li"
                    },
                    {
                        "name": "Zhitang Chen"
                    },
                    {
                        "name": "Farzan Farnia"
                    }
                ],
                "author_detail": {
                    "name": "Farzan Farnia"
                },
                "author": "Farzan Farnia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.12179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.12179v3",
                "updated": "2025-06-16T12:30:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    30,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2023-07-22T22:19:11Z",
                "published_parsed": [
                    2023,
                    7,
                    22,
                    22,
                    19,
                    11,
                    5,
                    203,
                    0
                ],
                "title": "Recognizing Unseen States of Unknown Objects by Leveraging Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing Unseen States of Unknown Objects by Leveraging Knowledge\n  Graphs"
                },
                "summary": "We investigate the problem of Object State Classification (OSC) as a\nzero-shot learning problem. Specifically, we propose the first Object-agnostic\nState Classification (OaSC) method that infers the state of a certain object\nwithout relying on the knowledge or the estimation of the object class. In that\ndirection, we capitalize on Knowledge Graphs (KGs) for structuring and\norganizing knowledge, which, in combination with visual information, enable the\ninference of the states of objects in object/state pairs that have not been\nencountered in the method's training set. A series of experiments investigate\nthe performance of the proposed method in various settings, against several\nhypotheses and in comparison with state of the art approaches for object\nattribute classification. The experimental results demonstrate that the\nknowledge of an object class is not decisive for the prediction of its state.\nMoreover, the proposed OaSC method outperforms existing methods in all datasets\nand benchmarks by a great margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the problem of Object State Classification (OSC) as a\nzero-shot learning problem. Specifically, we propose the first Object-agnostic\nState Classification (OaSC) method that infers the state of a certain object\nwithout relying on the knowledge or the estimation of the object class. In that\ndirection, we capitalize on Knowledge Graphs (KGs) for structuring and\norganizing knowledge, which, in combination with visual information, enable the\ninference of the states of objects in object/state pairs that have not been\nencountered in the method's training set. A series of experiments investigate\nthe performance of the proposed method in various settings, against several\nhypotheses and in comparison with state of the art approaches for object\nattribute classification. The experimental results demonstrate that the\nknowledge of an object class is not decisive for the prediction of its state.\nMoreover, the proposed OaSC method outperforms existing methods in all datasets\nand benchmarks by a great margin."
                },
                "authors": [
                    {
                        "name": "Filipos Gouidis"
                    },
                    {
                        "name": "Konstantinos Papoutsakis"
                    },
                    {
                        "name": "Theodore Patkos"
                    },
                    {
                        "name": "Antonis Argyros"
                    },
                    {
                        "name": "Dimitris Plexousakis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Plexousakis"
                },
                "author": "Dimitris Plexousakis",
                "arxiv_doi": "10.1109/WACV61041.2025.00838",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WACV61041.2025.00838",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.12179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.12179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the authors' version of the paper published at IEEE/CVF\n  Winter Conference on Applications of Computer Vision (WACV) 2025. The\n  definitive version is available at:\n  https://openaccess.thecvf.com/content/WACV2025/html/Gouidis_Recognizing_Unseen_States_of_Unknown_Objects_by_Leveraging_Knowledge_Graphs_WACV_2025_paper.html",
                "arxiv_journal_ref": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision\n  (WACV), pp. 8637-8648",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13408v1",
                "updated": "2025-06-16T12:21:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    21,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:21:27Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    21,
                    27,
                    0,
                    167,
                    0
                ],
                "title": "HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention"
                },
                "summary": "Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment."
                },
                "authors": [
                    {
                        "name": "Miguel Camelo Botero"
                    },
                    {
                        "name": "Esra Aycan Beyazit"
                    },
                    {
                        "name": "Nina Slamnik-Krijetorac"
                    },
                    {
                        "name": "Johann M. Marquez-Barja"
                    }
                ],
                "author_detail": {
                    "name": "Johann M. Marquez-Barja"
                },
                "author": "Johann M. Marquez-Barja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13405v1",
                "updated": "2025-06-16T12:19:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    19,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:19:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    19,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench."
                },
                "authors": [
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Xu Lu"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Bowen Bao"
                    },
                    {
                        "name": "Yijing He"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Haobo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haobo Wang"
                },
                "author": "Haobo Wang",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13403v1",
                "updated": "2025-06-16T12:17:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    17,
                    11,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:17:11Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    17,
                    11,
                    0,
                    167,
                    0
                ],
                "title": "Deflating Deflationism: A Critical Perspective on Debunking Arguments\n  Against LLM Mentality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deflating Deflationism: A Critical Perspective on Debunking Arguments\n  Against LLM Mentality"
                },
                "summary": "Many people feel compelled to interpret, describe, and respond to Large\nLanguage Models (LLMs) as if they possess inner mental lives similar to our\nown. Responses to this phenomenon have varied. Inflationists hold that at least\nsome folk psychological ascriptions to LLMs are warranted. Deflationists argue\nthat all such attributions of mentality to LLMs are misplaced, often cautioning\nagainst the risk that anthropomorphic projection may lead to misplaced trust or\npotentially even confusion about the moral status of LLMs. We advance this\ndebate by assessing two common deflationary arguments against LLM mentality.\nWhat we term the 'robustness strategy' aims to undercut one justification for\nbelieving that LLMs are minded entities by showing that putatively cognitive\nand humanlike behaviours are not robust, failing to generalise appropriately.\nWhat we term the 'etiological strategy' undercuts attributions of mentality by\nchallenging naive causal explanations of LLM behaviours, offering alternative\ncausal accounts that weaken the case for mental state attributions. While both\nstrategies offer powerful challenges to full-blown inflationism, we find that\nneither strategy provides a knock-down case against ascriptions of mentality to\nLLMs simpliciter. With this in mind, we explore a modest form of inflationism\nthat permits ascriptions of mentality to LLMs under certain conditions.\nSpecifically, we argue that folk practice provides a defeasible basis for\nattributing mental states and capacities to LLMs provided those mental states\nand capacities can be understood in metaphysically undemanding terms (e.g.\nknowledge, beliefs and desires), while greater caution is required when\nattributing metaphysically demanding mental phenomena such as phenomenal\nconsciousness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many people feel compelled to interpret, describe, and respond to Large\nLanguage Models (LLMs) as if they possess inner mental lives similar to our\nown. Responses to this phenomenon have varied. Inflationists hold that at least\nsome folk psychological ascriptions to LLMs are warranted. Deflationists argue\nthat all such attributions of mentality to LLMs are misplaced, often cautioning\nagainst the risk that anthropomorphic projection may lead to misplaced trust or\npotentially even confusion about the moral status of LLMs. We advance this\ndebate by assessing two common deflationary arguments against LLM mentality.\nWhat we term the 'robustness strategy' aims to undercut one justification for\nbelieving that LLMs are minded entities by showing that putatively cognitive\nand humanlike behaviours are not robust, failing to generalise appropriately.\nWhat we term the 'etiological strategy' undercuts attributions of mentality by\nchallenging naive causal explanations of LLM behaviours, offering alternative\ncausal accounts that weaken the case for mental state attributions. While both\nstrategies offer powerful challenges to full-blown inflationism, we find that\nneither strategy provides a knock-down case against ascriptions of mentality to\nLLMs simpliciter. With this in mind, we explore a modest form of inflationism\nthat permits ascriptions of mentality to LLMs under certain conditions.\nSpecifically, we argue that folk practice provides a defeasible basis for\nattributing mental states and capacities to LLMs provided those mental states\nand capacities can be understood in metaphysically undemanding terms (e.g.\nknowledge, beliefs and desires), while greater caution is required when\nattributing metaphysically demanding mental phenomena such as phenomenal\nconsciousness."
                },
                "authors": [
                    {
                        "name": "Alex Grzankowski"
                    },
                    {
                        "name": "Geoff Keeling"
                    },
                    {
                        "name": "Henry Shevlin"
                    },
                    {
                        "name": "Winnie Street"
                    }
                ],
                "author_detail": {
                    "name": "Winnie Street"
                },
                "author": "Winnie Street",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13396v1",
                "updated": "2025-06-16T12:03:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    3,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:03:23Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    3,
                    23,
                    0,
                    167,
                    0
                ],
                "title": "Bi-directional Context-Enhanced Speech Large Language Models for\n  Multilingual Conversational ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-directional Context-Enhanced Speech Large Language Models for\n  Multilingual Conversational ASR"
                },
                "summary": "This paper introduces the integration of language-specific bi-directional\ncontext into a speech large language model (SLLM) to improve multilingual\ncontinuous conversational automatic speech recognition (ASR). We propose a\ncharacter-level contextual masking strategy during training, which randomly\nremoves portions of the context to enhance robustness and better emulate the\nflawed transcriptions that may occur during inference. For decoding, a\ntwo-stage pipeline is utilized: initial isolated segment decoding followed by\ncontext-aware re-decoding using neighboring hypotheses. Evaluated on the\n1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM)\ncorpus covering eleven languages, our method achieves an 18% relative\nimprovement compared to a strong baseline, outperforming even the model trained\non 6000 hours of data for the MLC-SLM competition. These results underscore the\nsignificant benefit of incorporating contextual information in multilingual\ncontinuous conversational ASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the integration of language-specific bi-directional\ncontext into a speech large language model (SLLM) to improve multilingual\ncontinuous conversational automatic speech recognition (ASR). We propose a\ncharacter-level contextual masking strategy during training, which randomly\nremoves portions of the context to enhance robustness and better emulate the\nflawed transcriptions that may occur during inference. For decoding, a\ntwo-stage pipeline is utilized: initial isolated segment decoding followed by\ncontext-aware re-decoding using neighboring hypotheses. Evaluated on the\n1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM)\ncorpus covering eleven languages, our method achieves an 18% relative\nimprovement compared to a strong baseline, outperforming even the model trained\non 6000 hours of data for the MLC-SLM competition. These results underscore the\nsignificant benefit of incorporating contextual information in multilingual\ncontinuous conversational ASR."
                },
                "authors": [
                    {
                        "name": "Yizhou Peng"
                    },
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17189v3",
                "updated": "2025-06-16T12:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    0,
                    53,
                    0,
                    167,
                    0
                ],
                "published": "2024-12-22T23:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "title": "Better Think with Tables: Tabular Structures Enhance LLM Comprehension\n  for Data-Analytics Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Think with Tables: Tabular Structures Enhance LLM Comprehension\n  for Data-Analytics Requests"
                },
                "summary": "Large Language Models (LLMs) often struggle with data-analytics requests\nrelated to information retrieval and data manipulation that frequently arise in\nreal-world scenarios under multiple conditions. In this paper, we introduce\nThinking with Tables, where we inject tabular structures into LLMs for\ndata-analytics requests. Through comprehensive evaluations across various\nrequest types, we show that providing tabular structures yields a 40.29 percent\naverage performance gain along with better robustness and token efficiency.\nThrough attention-value analysis, we uncover that tables help LLMs better\nattend to relevant information, explaining these improvements. Beyond tables\nand text, we evaluate whether (1) blending structuredness within text, such as\nproviding templates or fixing the order of attributes, and (2) other\nrepresentative structures, such as knowledge graphs and JSON, are helpful. We\nobserve that utilizing tables offers the best balance between efficiency and\neffectiveness. These advantages remain consistent under increased task\ncomplexity and even when all input data cannot be structured. Finally, as data\nanalytics typically relies on structured factual inputs, our text-to-table\nconversion demonstrates the method's applicability to text-compatible data\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with data-analytics requests\nrelated to information retrieval and data manipulation that frequently arise in\nreal-world scenarios under multiple conditions. In this paper, we introduce\nThinking with Tables, where we inject tabular structures into LLMs for\ndata-analytics requests. Through comprehensive evaluations across various\nrequest types, we show that providing tabular structures yields a 40.29 percent\naverage performance gain along with better robustness and token efficiency.\nThrough attention-value analysis, we uncover that tables help LLMs better\nattend to relevant information, explaining these improvements. Beyond tables\nand text, we evaluate whether (1) blending structuredness within text, such as\nproviding templates or fixing the order of attributes, and (2) other\nrepresentative structures, such as knowledge graphs and JSON, are helpful. We\nobserve that utilizing tables offers the best balance between efficiency and\neffectiveness. These advantages remain consistent under increased task\ncomplexity and even when all input data cannot be structured. Finally, as data\nanalytics typically relies on structured factual inputs, our text-to-table\nconversion demonstrates the method's applicability to text-compatible data\nsources."
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Geon Heo"
                    },
                    {
                        "name": "Seungjun Oh"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10647v2",
                "updated": "2025-06-16T11:57:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    57,
                    29,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-12T12:38:04Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    38,
                    4,
                    3,
                    163,
                    0
                ],
                "title": "Data Shifts Hurt CoT: A Theoretical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Shifts Hurt CoT: A Theoretical Study"
                },
                "summary": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact."
                },
                "authors": [
                    {
                        "name": "Lang Yin"
                    },
                    {
                        "name": "Debangshu Banerjee"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "Comparison to v1: upgraded the quality of a figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13391v1",
                "updated": "2025-06-16T11:56:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:56:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined\n  Likelihood Guided Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined\n  Likelihood Guided Diffusion Models"
                },
                "summary": "Diffusion models have achieved remarkable success in imaging inverse problems\nowing to their powerful generative capabilities. However, existing approaches\ntypically rely on models trained for specific degradation types, limiting their\ngeneralizability to various degradation scenarios. To address this limitation,\nwe propose a zero-shot framework capable of handling various imaging inverse\nproblems without model retraining. We introduce a likelihood-guided noise\nrefinement mechanism that derives a closed-form approximation of the likelihood\nscore, simplifying score estimation and avoiding expensive gradient\ncomputations. This estimated score is subsequently utilized to refine the\nmodel-predicted noise, thereby better aligning the restoration process with the\ngenerative framework of diffusion models. In addition, we integrate the\nDenoising Diffusion Implicit Models (DDIM) sampling strategy to further improve\ninference efficiency. The proposed mechanism can be applied to both\noptimization-based and sampling-based schemes, providing an effective and\nflexible zero-shot solution for imaging inverse problems. Extensive experiments\ndemonstrate that our method achieves superior performance across multiple\ninverse problems, particularly in compressive sensing, delivering high-quality\nreconstructions even at an extremely low sampling rate (5%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in imaging inverse problems\nowing to their powerful generative capabilities. However, existing approaches\ntypically rely on models trained for specific degradation types, limiting their\ngeneralizability to various degradation scenarios. To address this limitation,\nwe propose a zero-shot framework capable of handling various imaging inverse\nproblems without model retraining. We introduce a likelihood-guided noise\nrefinement mechanism that derives a closed-form approximation of the likelihood\nscore, simplifying score estimation and avoiding expensive gradient\ncomputations. This estimated score is subsequently utilized to refine the\nmodel-predicted noise, thereby better aligning the restoration process with the\ngenerative framework of diffusion models. In addition, we integrate the\nDenoising Diffusion Implicit Models (DDIM) sampling strategy to further improve\ninference efficiency. The proposed mechanism can be applied to both\noptimization-based and sampling-based schemes, providing an effective and\nflexible zero-shot solution for imaging inverse problems. Extensive experiments\ndemonstrate that our method achieves superior performance across multiple\ninverse problems, particularly in compressive sensing, delivering high-quality\nreconstructions even at an extremely low sampling rate (5%)."
                },
                "authors": [
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Zhihui Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhihui Wei"
                },
                "author": "Zhihui Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13384v1",
                "updated": "2025-06-16T11:48:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    48,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:48:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    48,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Delving Into the Psychology of Machines: Exploring the Structure of\n  Self-Regulated Learning via LLM-Generated Survey Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving Into the Psychology of Machines: Exploring the Structure of\n  Self-Regulated Learning via LLM-Generated Survey Responses"
                },
                "summary": "Large language models (LLMs) offer the potential to simulate human-like\nresponses and behaviors, creating new opportunities for psychological science.\nIn the context of self-regulated learning (SRL), if LLMs can reliably simulate\nsurvey responses at scale and speed, they could be used to test intervention\nscenarios, refine theoretical models, augment sparse datasets, and represent\nhard-to-reach populations. However, the validity of LLM-generated survey\nresponses remains uncertain, with limited research focused on SRL and existing\nstudies beyond SRL yielding mixed results. Therefore, in this study, we\nexamined LLM-generated responses to the 44-item Motivated Strategies for\nLearning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used\ninstrument assessing students' learning strategies and academic motivation.\nParticularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA\n3.1-8B, and Mistral Large. We analyzed item distributions, the psychological\nnetwork of the theoretical SRL dimensions, and psychometric validity based on\nthe latent factor structure. Our results suggest that Gemini 2 Flash was the\nmost promising LLM, showing considerable sampling variability and producing\nunderlying dimensions and theoretical relationships that align with prior\ntheory and empirical findings. At the same time, we observed discrepancies and\nlimitations, underscoring both the potential and current constraints of using\nLLMs for simulating psychological survey data and applying it in educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer the potential to simulate human-like\nresponses and behaviors, creating new opportunities for psychological science.\nIn the context of self-regulated learning (SRL), if LLMs can reliably simulate\nsurvey responses at scale and speed, they could be used to test intervention\nscenarios, refine theoretical models, augment sparse datasets, and represent\nhard-to-reach populations. However, the validity of LLM-generated survey\nresponses remains uncertain, with limited research focused on SRL and existing\nstudies beyond SRL yielding mixed results. Therefore, in this study, we\nexamined LLM-generated responses to the 44-item Motivated Strategies for\nLearning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used\ninstrument assessing students' learning strategies and academic motivation.\nParticularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA\n3.1-8B, and Mistral Large. We analyzed item distributions, the psychological\nnetwork of the theoretical SRL dimensions, and psychometric validity based on\nthe latent factor structure. Our results suggest that Gemini 2 Flash was the\nmost promising LLM, showing considerable sampling variability and producing\nunderlying dimensions and theoretical relationships that align with prior\ntheory and empirical findings. At the same time, we observed discrepancies and\nlimitations, underscoring both the potential and current constraints of using\nLLMs for simulating psychological survey data and applying it in educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Leonie V. D. E. Vogelsmeier"
                    },
                    {
                        "name": "Eduardo Oliveira"
                    },
                    {
                        "name": "Kamila Misiejuk"
                    },
                    {
                        "name": "Sonsoles Lpez-Pernas"
                    },
                    {
                        "name": "Mohammed Saqr"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Saqr"
                },
                "author": "Mohammed Saqr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03662v3",
                "updated": "2025-06-16T11:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    47,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-04T07:52:46Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    52,
                    46,
                    2,
                    155,
                    0
                ],
                "title": "Zero-Shot Temporal Interaction Localization for Egocentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Temporal Interaction Localization for Egocentric Videos"
                },
                "summary": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc."
                },
                "authors": [
                    {
                        "name": "Erhang Zhang"
                    },
                    {
                        "name": "Junyi Ma"
                    },
                    {
                        "name": "Yin-Dong Zheng"
                    },
                    {
                        "name": "Yixuan Zhou"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12484v2",
                "updated": "2025-06-16T11:46:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    46,
                    29,
                    0,
                    167,
                    0
                ],
                "published": "2024-11-19T13:08:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    8,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "Regular-pattern-sensitive CRFs for Distant Label Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regular-pattern-sensitive CRFs for Distant Label Interactions"
                },
                "summary": "While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets."
                },
                "authors": [
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    },
                    {
                        "name": "Sebastian Pado"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pado"
                },
                "author": "Sebastian Pado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v1",
                "updated": "2025-06-16T11:44:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompositional Reasoning for Graph Retrieval with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gal de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gal de Chalendar"
                },
                "author": "Gal de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11169v2",
                "updated": "2025-06-16T11:37:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    37,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-16T15:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    39,
                    57,
                    6,
                    47,
                    0
                ],
                "title": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model"
                },
                "summary": "This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qingwen Lin"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zhifeng Hao"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    }
                ],
                "author_detail": {
                    "name": "Ruichu Cai"
                },
                "author": "Ruichu Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20747v2",
                "updated": "2025-06-16T11:26:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    26,
                    57,
                    0,
                    167,
                    0
                ],
                "published": "2024-05-31T10:23:47Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    10,
                    23,
                    47,
                    4,
                    152,
                    0
                ],
                "title": "Generalized Inverse Optimal Control and its Application in Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Inverse Optimal Control and its Application in Biology"
                },
                "summary": "Living organisms exhibit remarkable adaptations across all scales, from\nmolecules to ecosystems. We believe that many of these adaptations correspond\nto optimal solutions driven by evolution, training, and underlying physical and\nchemical laws and constraints. While some argue against such optimality\nprinciples due to their potential ambiguity, we propose generalized inverse\noptimal control to infer them directly from data. This novel approach\nincorporates multi-criteria optimality, nestedness of objective functions on\ndifferent scales, the presence of active constraints, the possibility of\nswitches of optimality principles during the observed time horizon,\nmaximization of robustness, and minimization of time as important special\ncases, as well as uncertainties involved with the mathematical modeling of\nbiological systems. This data-driven approach ensures that optimality\nprinciples are not merely theoretical constructs but are firmly rooted in\nexperimental observations. Furthermore, the inferred principles can be used in\nforward optimal control to predict and manipulate biological systems, with\npossible applications in bio-medicine, biotechnology, and agriculture. As\ndiscussed and illustrated, the well-posed problem formulation and the inference\nare challenging and require a substantial interdisciplinary effort in the\ndevelopment of theory and robust numerical methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Living organisms exhibit remarkable adaptations across all scales, from\nmolecules to ecosystems. We believe that many of these adaptations correspond\nto optimal solutions driven by evolution, training, and underlying physical and\nchemical laws and constraints. While some argue against such optimality\nprinciples due to their potential ambiguity, we propose generalized inverse\noptimal control to infer them directly from data. This novel approach\nincorporates multi-criteria optimality, nestedness of objective functions on\ndifferent scales, the presence of active constraints, the possibility of\nswitches of optimality principles during the observed time horizon,\nmaximization of robustness, and minimization of time as important special\ncases, as well as uncertainties involved with the mathematical modeling of\nbiological systems. This data-driven approach ensures that optimality\nprinciples are not merely theoretical constructs but are firmly rooted in\nexperimental observations. Furthermore, the inferred principles can be used in\nforward optimal control to predict and manipulate biological systems, with\npossible applications in bio-medicine, biotechnology, and agriculture. As\ndiscussed and illustrated, the well-posed problem formulation and the inference\nare challenging and require a substantial interdisciplinary effort in the\ndevelopment of theory and robust numerical methods."
                },
                "authors": [
                    {
                        "name": "Julio R. Banga"
                    },
                    {
                        "name": "Sebastian Sager"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Sager"
                },
                "author": "Sebastian Sager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.13759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13759v1",
                "updated": "2025-06-16T17:59:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:59:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
                },
                "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13755v1",
                "updated": "2025-06-16T17:58:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    9,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:58:09Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    58,
                    9,
                    0,
                    167,
                    0
                ],
                "title": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering"
                },
                "summary": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%."
                },
                "authors": [
                    {
                        "name": "Arya Fayyazi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13752v1",
                "updated": "2025-06-16T17:57:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:57:05Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "title": "Steering LLM Thinking with Budget Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering LLM Thinking with Budget Guidance"
                },
                "summary": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Wenshuo Zhao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13746v1",
                "updated": "2025-06-16T17:54:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    54,
                    28,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:54:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    54,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Evaluating Large Language Models for Phishing Detection,\n  Self-Consistency, Faithfulness, and Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Phishing Detection,\n  Self-Consistency, Faithfulness, and Explainability"
                },
                "summary": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores."
                },
                "authors": [
                    {
                        "name": "Shova Kuikel"
                    },
                    {
                        "name": "Aritran Piplai"
                    },
                    {
                        "name": "Palvi Aggarwal"
                    }
                ],
                "author_detail": {
                    "name": "Palvi Aggarwal"
                },
                "author": "Palvi Aggarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13743v1",
                "updated": "2025-06-16T17:53:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    53,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:53:18Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    53,
                    18,
                    0,
                    167,
                    0
                ],
                "title": "LTRR: Learning To Rank Retrievers for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTRR: Learning To Rank Retrievers for LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems."
                },
                "authors": [
                    {
                        "name": "To Eun Kim"
                    },
                    {
                        "name": "Fernando Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Diaz"
                },
                "author": "Fernando Diaz",
                "arxiv_comment": "SIGIR 2025 LiveRAG Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13739v1",
                "updated": "2025-06-16T17:50:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    50,
                    1,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:50:01Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    50,
                    1,
                    0,
                    167,
                    0
                ],
                "title": "Critical Insights about Robots for Mental Wellbeing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Insights about Robots for Mental Wellbeing"
                },
                "summary": "Social robots are increasingly being explored as tools to support emotional\nwellbeing, particularly in non-clinical settings. Drawing on a range of\nempirical studies and practical deployments, this paper outlines six key\ninsights that highlight both the opportunities and challenges in using robots\nto promote mental wellbeing. These include (1) the lack of a single, objective\nmeasure of wellbeing, (2) the fact that robots don't need to act as companions\nto be effective, (3) the growing potential of virtual interactions, (4) the\nimportance of involving clinicians in the design process, (5) the difference\nbetween one-off and long-term interactions, and (6) the idea that adaptation\nand personalization are not always necessary for positive outcomes. Rather than\npositioning robots as replacements for human therapists, we argue that they are\nbest understood as supportive tools that must be designed with care, grounded\nin evidence, and shaped by ethical and psychological considerations. Our aim is\nto inform future research and guide responsible, effective use of robots in\nmental health and wellbeing contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social robots are increasingly being explored as tools to support emotional\nwellbeing, particularly in non-clinical settings. Drawing on a range of\nempirical studies and practical deployments, this paper outlines six key\ninsights that highlight both the opportunities and challenges in using robots\nto promote mental wellbeing. These include (1) the lack of a single, objective\nmeasure of wellbeing, (2) the fact that robots don't need to act as companions\nto be effective, (3) the growing potential of virtual interactions, (4) the\nimportance of involving clinicians in the design process, (5) the difference\nbetween one-off and long-term interactions, and (6) the idea that adaptation\nand personalization are not always necessary for positive outcomes. Rather than\npositioning robots as replacements for human therapists, we argue that they are\nbest understood as supportive tools that must be designed with care, grounded\nin evidence, and shaped by ethical and psychological considerations. Our aim is\nto inform future research and guide responsible, effective use of robots in\nmental health and wellbeing contexts."
                },
                "authors": [
                    {
                        "name": "Guy Laban"
                    },
                    {
                        "name": "Micol Spitale"
                    },
                    {
                        "name": "Minja Axelsson"
                    },
                    {
                        "name": "Nida Itrat Abbasi"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13734v1",
                "updated": "2025-06-16T17:42:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:42:35Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    42,
                    35,
                    0,
                    167,
                    0
                ],
                "title": "Instruction Following by Boosting Attention of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Following by Boosting Attention of Large Language Models"
                },
                "summary": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering."
                },
                "authors": [
                    {
                        "name": "Vitoria Guardieiro"
                    },
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13727v1",
                "updated": "2025-06-16T17:38:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    38,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:38:36Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    38,
                    36,
                    0,
                    167,
                    0
                ],
                "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs"
                },
                "summary": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3."
                },
                "authors": [
                    {
                        "name": "Sayed Mohammad Vakilzadeh Hatefi"
                    },
                    {
                        "name": "Maximilian Dreyer"
                    },
                    {
                        "name": "Reduan Achtibat"
                    },
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "arxiv_comment": "Work in progress (10 pages manuscript, 3 pages references, 12 pages\n  appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05606v2",
                "updated": "2025-06-16T17:32:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    32,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-05T21:37:49Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    37,
                    49,
                    3,
                    156,
                    0
                ],
                "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation"
                },
                "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Amirali Amini"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Yakov Bart"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Jiri Gesi"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Upol Ehsan"
                    },
                    {
                        "name": "Malihe Alikhani"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Lydia Chilton"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13725v1",
                "updated": "2025-06-16T17:31:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    31,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:31:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    31,
                    16,
                    0,
                    167,
                    0
                ],
                "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit\n  Decoding"
                },
                "summary": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Vision-Language-Action (VLA) models have become a vital\nresearch direction in robotics due to their impressive multimodal understanding\nand generalization capabilities. Despite the progress, their practical\ndeployment is severely constrained by inference speed bottlenecks, particularly\nin high-frequency and dexterous manipulation tasks. While recent studies have\nexplored Jacobi decoding as a more efficient alternative to traditional\nautoregressive decoding, its practical benefits are marginal due to the lengthy\niterations. To address it, we introduce consistency distillation training to\npredict multiple correct action tokens in each iteration, thereby achieving\nacceleration. Besides, we design mixed-label supervision to mitigate the error\naccumulation during distillation. Although distillation brings acceptable\nspeedup, we identify that certain inefficient iterations remain a critical\nbottleneck. To tackle this, we propose an early-exit decoding strategy that\nmoderately relaxes convergence conditions, which further improves average\ninference efficiency. Experimental results show that the proposed method\nachieves more than 4 times inference acceleration across different baselines\nwhile maintaining high task success rates in both simulated and real-world\nrobot tasks. These experiments validate that our approach provides an efficient\nand general paradigm for accelerating multimodal decision-making in robotics.\nOur project page is available at https://irpn-eai.github.io/CEED-VLA/."
                },
                "authors": [
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Haoang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoang Li"
                },
                "author": "Haoang Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13705v1",
                "updated": "2025-06-16T17:12:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    12,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T17:12:26Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    12,
                    26,
                    0,
                    167,
                    0
                ],
                "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via\n  Reinforcement Learning"
                },
                "summary": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs."
                },
                "authors": [
                    {
                        "name": "Junru Zhang"
                    },
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Yabo Dong"
                    },
                    {
                        "name": "Duanqing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Duanqing Xu"
                },
                "author": "Duanqing Xu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01438v2",
                "updated": "2025-06-16T17:03:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    3,
                    41,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-02T08:52:23Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    52,
                    23,
                    0,
                    153,
                    0
                ],
                "title": "Distinguishing Autonomous AI Agents from Collaborative Agentic Systems:\n  A Comprehensive Framework for Understanding Modern Intelligent Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Autonomous AI Agents from Collaborative Agentic Systems:\n  A Comprehensive Framework for Understanding Modern Intelligent Architectures"
                },
                "summary": "The emergence of large language models has catalyzed two distinct yet\ninterconnected paradigms in artificial intelligence: standalone AI Agents and\ncollaborative Agentic AI ecosystems. This comprehensive study establishes a\ndefinitive framework for distinguishing these architectures through systematic\nanalysis of their operational principles, structural compositions, and\ndeployment methodologies. We characterize AI Agents as specialized,\ntool-enhanced systems leveraging foundation models for targeted automation\nwithin constrained environments. Conversely, Agentic AI represents\nsophisticated multi-entity frameworks where distributed agents exhibit emergent\ncollective intelligence through coordinated interaction protocols. Our\ninvestigation traces the evolutionary trajectory from traditional rule-based\nsystems through generative AI foundations to contemporary agent architectures.\nWe present detailed architectural comparisons examining planning mechanisms,\nmemory systems, coordination protocols, and decision-making processes. The\nstudy categorizes application landscapes, contrasting single-agent\nimplementations in customer service and content management with multi-agent\ndeployments in research automation and complex decision support. We identify\ncritical challenges including reliability issues, coordination complexities,\nand scalability constraints, while proposing innovative solutions through\nenhanced reasoning frameworks, robust memory architectures, and improved\ncoordination mechanisms. This framework provides essential guidance for\npractitioners selecting appropriate agentic approaches and establishes\nfoundational principles for next-generation intelligent system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models has catalyzed two distinct yet\ninterconnected paradigms in artificial intelligence: standalone AI Agents and\ncollaborative Agentic AI ecosystems. This comprehensive study establishes a\ndefinitive framework for distinguishing these architectures through systematic\nanalysis of their operational principles, structural compositions, and\ndeployment methodologies. We characterize AI Agents as specialized,\ntool-enhanced systems leveraging foundation models for targeted automation\nwithin constrained environments. Conversely, Agentic AI represents\nsophisticated multi-entity frameworks where distributed agents exhibit emergent\ncollective intelligence through coordinated interaction protocols. Our\ninvestigation traces the evolutionary trajectory from traditional rule-based\nsystems through generative AI foundations to contemporary agent architectures.\nWe present detailed architectural comparisons examining planning mechanisms,\nmemory systems, coordination protocols, and decision-making processes. The\nstudy categorizes application landscapes, contrasting single-agent\nimplementations in customer service and content management with multi-agent\ndeployments in research automation and complex decision support. We identify\ncritical challenges including reliability issues, coordination complexities,\nand scalability constraints, while proposing innovative solutions through\nenhanced reasoning frameworks, robust memory architectures, and improved\ncoordination mechanisms. This framework provides essential guidance for\npractitioners selecting appropriate agentic approaches and establishes\nfoundational principles for next-generation intelligent system development."
                },
                "authors": [
                    {
                        "name": "Prashik Buddhaghosh Bansod"
                    }
                ],
                "author_detail": {
                    "name": "Prashik Buddhaghosh Bansod"
                },
                "author": "Prashik Buddhaghosh Bansod",
                "arxiv_comment": "There may be overlap with another author's work. I am withdrawing\n  this for me to review further",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13695v1",
                "updated": "2025-06-16T16:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    58,
                    55,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:58:55Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    58,
                    55,
                    0,
                    167,
                    0
                ],
                "title": "OneRec Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec Technical Report"
                },
                "summary": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact."
                },
                "authors": [
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Weifeng Ding"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Chaoyi Ma"
                    },
                    {
                        "name": "Chengru Song"
                    },
                    {
                        "name": "Chenhui Wang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Dongxue Meng"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Fangyu Zhang"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Fuxing Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Guowang Zhang"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Hengrui Hu"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Hongyang Cao"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Jiaqiang Liu"
                    },
                    {
                        "name": "Jinghui Jia"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Lantao Hu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Liao Yu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Qidong Zhou"
                    },
                    {
                        "name": "Shengzhe Wang"
                    },
                    {
                        "name": "Shihui He"
                    },
                    {
                        "name": "Shuang Yang"
                    },
                    {
                        "name": "Shujie Yang"
                    },
                    {
                        "name": "Sui Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Xiaoxiao Xu"
                    },
                    {
                        "name": "Xugang Liu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yiwu Liu"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Zhanyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Liu"
                },
                "author": "Zhanyu Liu",
                "arxiv_comment": "Authors are listed alphabetically by their first name",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13692v1",
                "updated": "2025-06-16T16:54:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    54,
                    3,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:54:03Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    54,
                    3,
                    0,
                    167,
                    0
                ],
                "title": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems"
                },
                "summary": "With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers."
                },
                "authors": [
                    {
                        "name": "Shang-Chi Tsai"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "IWSDS 2025 Oral Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23077v2",
                "updated": "2025-06-16T16:51:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    51,
                    48,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-29T13:27:46Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    13,
                    27,
                    46,
                    5,
                    88,
                    0
                ],
                "title": "Efficient Inference for Large Reasoning Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Large Reasoning Models: A Survey"
                },
                "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}."
                },
                "authors": [
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Ruihan Gong"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02084v3",
                "updated": "2025-06-16T16:49:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    49,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2024-10-02T23:10:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    10,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic Music from Natural Language Prompts using an\n  LLM-Enhanced Dataset"
                },
                "summary": "Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, We employ a pretrained large language model (LLM) to generate\npseudo-natural language captions for music from its metadata tags. With the\nLLM-enhanced MetaScore, we train a text-conditioned music generation model that\nlearns to generate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test. While a concurrent work Text2MIDI also\nsupports free-form text input, our models achieve comparable performance.\nMoreover, the text-to-music system offers a more natural interface than the\ntags-to-music model, as it allows users to provide free-form natural language\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, We employ a pretrained large language model (LLM) to generate\npseudo-natural language captions for music from its metadata tags. With the\nLLM-enhanced MetaScore, we train a text-conditioned music generation model that\nlearns to generate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test. While a concurrent work Text2MIDI also\nsupports free-form text input, our models achieve comparable performance.\nMoreover, the text-to-music system offers a more natural interface than the\ntags-to-music model, as it allows users to provide free-form natural language\nprompts."
                },
                "authors": [
                    {
                        "name": "Weihan Xu"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Shlomo Dubnov"
                    },
                    {
                        "name": "Hao-Wen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Wen Dong"
                },
                "author": "Hao-Wen Dong",
                "arxiv_comment": "Accepted at ISMIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13685v1",
                "updated": "2025-06-16T16:46:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    46,
                    14,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:46:14Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    46,
                    14,
                    0,
                    167,
                    0
                ],
                "title": "An LLM's Apology: Outsourcing Awkwardness in the Age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM's Apology: Outsourcing Awkwardness in the Age of AI"
                },
                "summary": "A key part of modern social dynamics is flaking at short notice. However,\nanxiety in coming up with believable and socially acceptable reasons to do so\ncan instead lead to 'ghosting', awkwardness, or implausible excuses, risking\nemotional harm and resentment in the other party. The ability to delegate this\ntask to a Large Language Model (LLM) could substantially reduce friction and\nenhance the flexibility of user's social life while greatly minimising the\naforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an\nevaluation of models' capacity to effectively, kindly, and humanely extract\nthemselves from a diverse set of social, professional and romantic scenarios.\nWe report the efficacy of 10 frontier or recently-frontier LLMs in bailing on\nprior commitments, because nothing says \"I value our friendship\" like having AI\ngenerate your cancellation texts. We open-source FLAKE-Bench at\ngithub.com/Cloakless/flake-bench to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key part of modern social dynamics is flaking at short notice. However,\nanxiety in coming up with believable and socially acceptable reasons to do so\ncan instead lead to 'ghosting', awkwardness, or implausible excuses, risking\nemotional harm and resentment in the other party. The ability to delegate this\ntask to a Large Language Model (LLM) could substantially reduce friction and\nenhance the flexibility of user's social life while greatly minimising the\naforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an\nevaluation of models' capacity to effectively, kindly, and humanely extract\nthemselves from a diverse set of social, professional and romantic scenarios.\nWe report the efficacy of 10 frontier or recently-frontier LLMs in bailing on\nprior commitments, because nothing says \"I value our friendship\" like having AI\ngenerate your cancellation texts. We open-source FLAKE-Bench at\ngithub.com/Cloakless/flake-bench to support future research."
                },
                "authors": [
                    {
                        "name": "Twm Stone"
                    },
                    {
                        "name": "Anna Soligo"
                    }
                ],
                "author_detail": {
                    "name": "Anna Soligo"
                },
                "author": "Anna Soligo",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13681v1",
                "updated": "2025-06-16T16:38:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    38,
                    4,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:38:04Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    38,
                    4,
                    0,
                    167,
                    0
                ],
                "title": "Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models"
                },
                "summary": "Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity."
                },
                "authors": [
                    {
                        "name": "Rylan Schaeffer"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Yegor Denisov-Blanch"
                    }
                ],
                "author_detail": {
                    "name": "Yegor Denisov-Blanch"
                },
                "author": "Yegor Denisov-Blanch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13674v1",
                "updated": "2025-06-16T16:30:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    30,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:30:26Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    30,
                    26,
                    0,
                    167,
                    0
                ],
                "title": "Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Brian Chen"
                    },
                    {
                        "name": "Li Siquan"
                    },
                    {
                        "name": "Liang Xinhe"
                    },
                    {
                        "name": "Tianyang Hu"
                    },
                    {
                        "name": "Hwee Kuan Lee"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Kawaguchi"
                },
                "author": "Kenji Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03781v2",
                "updated": "2025-06-16T16:25:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    25,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-04T09:42:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    42,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models"
                },
                "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark."
                },
                "authors": [
                    {
                        "name": "Seungcheol Park"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Beomseok Kwon"
                    },
                    {
                        "name": "Minjun Kim"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "U Kang"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "arxiv_comment": "ACL 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14568v2",
                "updated": "2025-06-16T16:24:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    42,
                    0,
                    167,
                    0
                ],
                "published": "2024-08-26T18:39:31Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    39,
                    31,
                    0,
                    239,
                    0
                ],
                "title": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation"
                },
                "summary": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods."
                },
                "authors": [
                    {
                        "name": "Yizhan Li"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Christopher Smith"
                    },
                    {
                        "name": "Thomas Lo"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13666v1",
                "updated": "2025-06-16T16:24:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    31,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:24:31Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    24,
                    31,
                    0,
                    167,
                    0
                ],
                "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems"
                },
                "summary": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git."
                },
                "authors": [
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Ruipeng Wang"
                    },
                    {
                        "name": "Haokai Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05317v2",
                "updated": "2025-06-16T16:22:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    22,
                    39,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-21T09:43:18Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    9,
                    43,
                    18,
                    4,
                    52,
                    0
                ],
                "title": "On Synthesizing Data for Context Attribution in Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Synthesizing Data for Context Attribution in Question Answering"
                },
                "summary": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA."
                },
                "authors": [
                    {
                        "name": "Gorjan Radevski"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Shahbaz Syed"
                    },
                    {
                        "name": "Christopher Malon"
                    },
                    {
                        "name": "Sebastien Nicolas"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Timo Sztyler"
                    },
                    {
                        "name": "Verena Heuer"
                    },
                    {
                        "name": "Wiem Ben Rim"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Kunihiro Takeoka"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    },
                    {
                        "name": "Goran Glava"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13642v1",
                "updated": "2025-06-16T16:06:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    6,
                    45,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:06:45Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    6,
                    45,
                    0,
                    167,
                    0
                ],
                "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model"
                },
                "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13641v1",
                "updated": "2025-06-16T16:05:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    5,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:05:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    5,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs"
                },
                "summary": "A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Jinhua Du"
                    },
                    {
                        "name": "Ze Li"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13639v1",
                "updated": "2025-06-16T16:04:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    43,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:04:43Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    43,
                    0,
                    167,
                    0
                ],
                "title": "An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability"
                },
                "summary": "As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present."
                },
                "authors": [
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13638v1",
                "updated": "2025-06-16T16:04:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T16:04:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    4,
                    16,
                    0,
                    167,
                    0
                ],
                "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models"
                },
                "summary": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Zhiyi Shi"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Chongjie Si"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Junsik Kim"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Hanspeter Pfister"
                },
                "author": "Hanspeter Pfister",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13793v2",
                "updated": "2025-06-16T15:59:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    59,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2024-09-20T10:47:09Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    10,
                    47,
                    9,
                    4,
                    264,
                    0
                ],
                "title": "On the Feasibility of Fully AI-automated Vishing Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Feasibility of Fully AI-automated Vishing Attacks"
                },
                "summary": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs."
                },
                "authors": [
                    {
                        "name": "Joo Figueiredo"
                    },
                    {
                        "name": "Afonso Carvalho"
                    },
                    {
                        "name": "Daniel Castro"
                    },
                    {
                        "name": "Daniel Gonalves"
                    },
                    {
                        "name": "Nuno Santos"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Santos"
                },
                "author": "Nuno Santos",
                "arxiv_comment": "To appear in AsiaCCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13629v1",
                "updated": "2025-06-16T15:56:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:56:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    56,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding"
                },
                "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Hongwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wang"
                },
                "author": "Hongwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13622v1",
                "updated": "2025-06-16T15:50:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    50,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:50:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    50,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Disturbance-aware minimum-time planning strategies for motorsport\n  vehicles with probabilistic safety certificates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disturbance-aware minimum-time planning strategies for motorsport\n  vehicles with probabilistic safety certificates"
                },
                "summary": "This paper presents a disturbance-aware framework that embeds robustness into\nminimum-lap-time trajectory optimization for motorsport. Two formulations are\nintroduced. (i) Open-loop, horizon-based covariance propagation uses worst-case\nuncertainty growth over a finite window to tighten tire-friction and\ntrack-limit constraints. (ii) Closed-loop, covariance-aware planning\nincorporates a time-varying LQR feedback law in the optimizer, providing a\nfeedback-consistent estimate of disturbance attenuation and enabling sharper\nyet reliable constraint tightening. Both methods yield reference trajectories\nfor human or artificial drivers: in autonomous applications the modelled\ncontroller can replicate the on-board implementation, while for human driving\naccuracy increases with the extent to which the driver can be approximated by\nthe assumed time-varying LQR policy. Computational tests on a representative\nBarcelona-Catalunya sector show that both schemes meet the prescribed safety\nprobability, yet the closed-loop variant incurs smaller lap-time penalties than\nthe more conservative open-loop solution, while the nominal (non-robust)\ntrajectory remains infeasible under the same uncertainties. By accounting for\nuncertainty growth and feedback action during planning, the proposed framework\ndelivers trajectories that are both performance-optimal and probabilistically\nsafe, advancing minimum-time optimization toward real-world deployment in\nhigh-performance motorsport and autonomous racing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a disturbance-aware framework that embeds robustness into\nminimum-lap-time trajectory optimization for motorsport. Two formulations are\nintroduced. (i) Open-loop, horizon-based covariance propagation uses worst-case\nuncertainty growth over a finite window to tighten tire-friction and\ntrack-limit constraints. (ii) Closed-loop, covariance-aware planning\nincorporates a time-varying LQR feedback law in the optimizer, providing a\nfeedback-consistent estimate of disturbance attenuation and enabling sharper\nyet reliable constraint tightening. Both methods yield reference trajectories\nfor human or artificial drivers: in autonomous applications the modelled\ncontroller can replicate the on-board implementation, while for human driving\naccuracy increases with the extent to which the driver can be approximated by\nthe assumed time-varying LQR policy. Computational tests on a representative\nBarcelona-Catalunya sector show that both schemes meet the prescribed safety\nprobability, yet the closed-loop variant incurs smaller lap-time penalties than\nthe more conservative open-loop solution, while the nominal (non-robust)\ntrajectory remains infeasible under the same uncertainties. By accounting for\nuncertainty growth and feedback action during planning, the proposed framework\ndelivers trajectories that are both performance-optimal and probabilistically\nsafe, advancing minimum-time optimization toward real-world deployment in\nhigh-performance motorsport and autonomous racing."
                },
                "authors": [
                    {
                        "name": "Martino Gulisano"
                    },
                    {
                        "name": "Matteo Masoni"
                    },
                    {
                        "name": "Marco Gabiccini"
                    },
                    {
                        "name": "Massimo Guiggiani"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Guiggiani"
                },
                "author": "Massimo Guiggiani",
                "arxiv_comment": "24 pages, 11 figures, paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02039v2",
                "updated": "2025-06-16T15:37:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    37,
                    6,
                    0,
                    167,
                    0
                ],
                "published": "2025-01-03T14:35:32Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    35,
                    32,
                    4,
                    3,
                    0
                ],
                "title": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Siyi Wang"
                    },
                    {
                        "name": "Ziyao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyao Liu"
                },
                "author": "Ziyao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13608v1",
                "updated": "2025-06-16T15:35:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    35,
                    41,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:35:41Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    35,
                    41,
                    0,
                    167,
                    0
                ],
                "title": "Assessing the Limits of In-Context Learning beyond Functions using\n  Partially Ordered Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Limits of In-Context Learning beyond Functions using\n  Partially Ordered Relation"
                },
                "summary": "Generating rational and generally accurate responses to tasks, often\naccompanied by example demonstrations, highlights Large Language Model's\n(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring\nupdates to the model's parameter space. Despite having an ongoing exploration\nfocused on the inference from a document-level concept, its behavior in\nlearning well-defined functions or relations in context needs a careful\ninvestigation. In this article, we present the performance of ICL on partially\nordered relation by introducing the notion of inductively increasing complexity\nin prompts. In most cases, the saturated performance of the chosen metric\nindicates that while ICL offers some benefits, its effectiveness remains\nconstrained as we increase the complexity in the prompts even in presence of\nsufficient demonstrative examples. The behavior is evident from our empirical\nfindings and has further been theoretically justified in term of its implicit\noptimization process. The code is available\n\\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating rational and generally accurate responses to tasks, often\naccompanied by example demonstrations, highlights Large Language Model's\n(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring\nupdates to the model's parameter space. Despite having an ongoing exploration\nfocused on the inference from a document-level concept, its behavior in\nlearning well-defined functions or relations in context needs a careful\ninvestigation. In this article, we present the performance of ICL on partially\nordered relation by introducing the notion of inductively increasing complexity\nin prompts. In most cases, the saturated performance of the chosen metric\nindicates that while ICL offers some benefits, its effectiveness remains\nconstrained as we increase the complexity in the prompts even in presence of\nsufficient demonstrative examples. The behavior is evident from our empirical\nfindings and has further been theoretically justified in term of its implicit\noptimization process. The code is available\n\\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}."
                },
                "authors": [
                    {
                        "name": "Debanjan Dutta"
                    },
                    {
                        "name": "Faizanuddin Ansari"
                    },
                    {
                        "name": "Swagatam Das"
                    }
                ],
                "author_detail": {
                    "name": "Swagatam Das"
                },
                "author": "Swagatam Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12150v2",
                "updated": "2025-06-16T15:27:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    27,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-17T18:59:02Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    59,
                    2,
                    0,
                    48,
                    0
                ],
                "title": "Idiosyncrasies in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiosyncrasies in Large Language Models"
                },
                "summary": "In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies."
                },
                "authors": [
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zhiqiu Xu"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "Published in ICML 2025. Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13600v1",
                "updated": "2025-06-16T15:25:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    25,
                    6,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:25:06Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    25,
                    6,
                    0,
                    167,
                    0
                ],
                "title": "The ASP-based Nurse Scheduling System at the University of Yamanashi\n  Hospital",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ASP-based Nurse Scheduling System at the University of Yamanashi\n  Hospital"
                },
                "summary": "We present the design principles of a nurse scheduling system built using\nAnswer Set Programming (ASP) and successfully deployed at the University of\nYamanashi Hospital. Nurse scheduling is a complex optimization problem\nrequiring the reconciliation of individual nurse preferences with hospital\nstaffing needs across various wards. This involves balancing hard and soft\nconstraints and the flexibility of interactive adjustments. While extensively\nstudied in academia, real-world nurse scheduling presents unique challenges\nthat go beyond typical benchmark problems and competitions. This paper details\nthe practical application of ASP to address these challenges at the University\nof Yamanashi Hospital, focusing on the insights gained and the advancements in\nASP technology necessary to effectively manage the complexities of real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design principles of a nurse scheduling system built using\nAnswer Set Programming (ASP) and successfully deployed at the University of\nYamanashi Hospital. Nurse scheduling is a complex optimization problem\nrequiring the reconciliation of individual nurse preferences with hospital\nstaffing needs across various wards. This involves balancing hard and soft\nconstraints and the flexibility of interactive adjustments. While extensively\nstudied in academia, real-world nurse scheduling presents unique challenges\nthat go beyond typical benchmark problems and competitions. This paper details\nthe practical application of ASP to address these challenges at the University\nof Yamanashi Hospital, focusing on the insights gained and the advancements in\nASP technology necessary to effectively manage the complexities of real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Hidetomo Nabeshima"
                    },
                    {
                        "name": "Mutsunori Banbara"
                    },
                    {
                        "name": "Torsten Schaub"
                    },
                    {
                        "name": "Takehide Soh"
                    }
                ],
                "author_detail": {
                    "name": "Takehide Soh"
                },
                "author": "Takehide Soh",
                "arxiv_comment": "Reduced version appears in Technical Communications of ICLP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13599v1",
                "updated": "2025-06-16T15:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    24,
                    7,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:24:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    24,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation"
                },
                "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation."
                },
                "authors": [
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13596v1",
                "updated": "2025-06-16T15:23:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    23,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems"
                },
                "summary": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Long-Vu Hoang"
                    },
                    {
                        "name": "Huy-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Huy-Dat Tran"
                },
                "author": "Huy-Dat Tran",
                "arxiv_comment": "Technical report for Interspeech 2025 MLC-SLM Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13593v1",
                "updated": "2025-06-16T15:21:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T15:21:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs"
                },
                "summary": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models."
                },
                "authors": [
                    {
                        "name": "Hen Davidov"
                    },
                    {
                        "name": "Gilad Freidkin"
                    },
                    {
                        "name": "Shai Feldman"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13573v1",
                "updated": "2025-06-16T14:57:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:57:05Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    57,
                    5,
                    0,
                    167,
                    0
                ],
                "title": "Integrated Pipeline for Monocular 3D Reconstruction and Finite Element\n  Simulation in Industrial Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Pipeline for Monocular 3D Reconstruction and Finite Element\n  Simulation in Industrial Applications"
                },
                "summary": "To address the challenges of 3D modeling and structural simulation in\nindustrial environment, such as the difficulty of equipment deployment, and the\ndifficulty of balancing accuracy and real-time performance, this paper proposes\nan integrated workflow, which integrates high-fidelity 3D reconstruction based\non monocular video, finite element simulation analysis, and mixed reality\nvisual display, aiming to build an interactive digital twin system for\nindustrial inspection, equipment maintenance and other scenes. Firstly, the\nNeuralangelo algorithm based on deep learning is used to reconstruct the 3D\nmesh model with rich details from the surround-shot video. Then, the QuadRemesh\ntool of Rhino is used to optimize the initial triangular mesh and generate a\nstructured mesh suitable for finite element analysis. The optimized mesh is\nfurther discretized by HyperMesh, and the material parameter setting and stress\nsimulation are carried out in Abaqus to obtain high-precision stress and\ndeformation results. Finally, combined with Unity and Vuforia engine, the\nreal-time superposition and interactive operation of simulation results in the\naugmented reality environment are realized, which improves users 'intuitive\nunderstanding of structural response. Experiments show that the method has good\nsimulation efficiency and visualization effect while maintaining high geometric\naccuracy. It provides a practical solution for digital modeling, mechanical\nanalysis and interactive display in complex industrial scenes, and lays a\nfoundation for the deep integration of digital twin and mixed reality\ntechnology in industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the challenges of 3D modeling and structural simulation in\nindustrial environment, such as the difficulty of equipment deployment, and the\ndifficulty of balancing accuracy and real-time performance, this paper proposes\nan integrated workflow, which integrates high-fidelity 3D reconstruction based\non monocular video, finite element simulation analysis, and mixed reality\nvisual display, aiming to build an interactive digital twin system for\nindustrial inspection, equipment maintenance and other scenes. Firstly, the\nNeuralangelo algorithm based on deep learning is used to reconstruct the 3D\nmesh model with rich details from the surround-shot video. Then, the QuadRemesh\ntool of Rhino is used to optimize the initial triangular mesh and generate a\nstructured mesh suitable for finite element analysis. The optimized mesh is\nfurther discretized by HyperMesh, and the material parameter setting and stress\nsimulation are carried out in Abaqus to obtain high-precision stress and\ndeformation results. Finally, combined with Unity and Vuforia engine, the\nreal-time superposition and interactive operation of simulation results in the\naugmented reality environment are realized, which improves users 'intuitive\nunderstanding of structural response. Experiments show that the method has good\nsimulation efficiency and visualization effect while maintaining high geometric\naccuracy. It provides a practical solution for digital modeling, mechanical\nanalysis and interactive display in complex industrial scenes, and lays a\nfoundation for the deep integration of digital twin and mixed reality\ntechnology in industrial applications."
                },
                "authors": [
                    {
                        "name": "Bowen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zheng"
                },
                "author": "Bowen Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13564v1",
                "updated": "2025-06-16T14:49:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    49,
                    49,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:49:49Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    49,
                    49,
                    0,
                    167,
                    0
                ],
                "title": "MambaMia: A State-Space-Model-Based Compression for Efficient Video\n  Understanding in Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MambaMia: A State-Space-Model-Based Compression for Efficient Video\n  Understanding in Large Multimodal Models"
                },
                "summary": "We propose an efficient framework to compress multiple video-frame features\nbefore feeding them into large multimodal models, thereby mitigating the severe\ntoken explosion arising from long or dense videos. Our design leverages a\nbidirectional state-space-based block equipped with a gated skip connection and\na learnable weighted-average pooling mechanism applied to periodically inserted\nlearned queries. This structure enables hierarchical downsampling across both\nspatial and temporal dimensions, preserving performance in a cost-effective\nmanner. Across challenging long and dense video understanding tasks, our\napproach demonstrates competitive results against state-of-the-art models,\nwhile significantly reducing overall token budget. Notably, replacing our\nproposed state-space block with a conventional Transformer results in\nsubstantial performance degradation, highlighting the advantages of state-space\nmodeling for effectively compressing multi-frame video data. Our framework\nemphasizes resource-conscious efficiency, making it practical for real-world\ndeployments. We validate its scalability and generality across multiple\nbenchmarks, achieving the dual objectives of efficient resource usage and\ncomprehensive video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an efficient framework to compress multiple video-frame features\nbefore feeding them into large multimodal models, thereby mitigating the severe\ntoken explosion arising from long or dense videos. Our design leverages a\nbidirectional state-space-based block equipped with a gated skip connection and\na learnable weighted-average pooling mechanism applied to periodically inserted\nlearned queries. This structure enables hierarchical downsampling across both\nspatial and temporal dimensions, preserving performance in a cost-effective\nmanner. Across challenging long and dense video understanding tasks, our\napproach demonstrates competitive results against state-of-the-art models,\nwhile significantly reducing overall token budget. Notably, replacing our\nproposed state-space block with a conventional Transformer results in\nsubstantial performance degradation, highlighting the advantages of state-space\nmodeling for effectively compressing multi-frame video data. Our framework\nemphasizes resource-conscious efficiency, making it practical for real-world\ndeployments. We validate its scalability and generality across multiple\nbenchmarks, achieving the dual objectives of efficient resource usage and\ncomprehensive video understanding."
                },
                "authors": [
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13563v1",
                "updated": "2025-06-16T14:48:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    48,
                    41,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:48:41Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    48,
                    41,
                    0,
                    167,
                    0
                ],
                "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor\n  Poisoning in Anonymous Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor\n  Poisoning in Anonymous Networks"
                },
                "summary": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings."
                },
                "authors": [
                    {
                        "name": "Yali Yuan"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Ruolin Ma"
                    },
                    {
                        "name": "Yuchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Zhang"
                },
                "author": "Yuchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13559v1",
                "updated": "2025-06-16T14:45:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:45:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    45,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Understand the Implication: Learning to Think for Pragmatic\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understand the Implication: Learning to Think for Pragmatic\n  Understanding"
                },
                "summary": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models."
                },
                "authors": [
                    {
                        "name": "Settaluri Lakshmi Sravanthi"
                    },
                    {
                        "name": "Kishan Maharaj"
                    },
                    {
                        "name": "Sravani Gunnu"
                    },
                    {
                        "name": "Abhijit Mishra"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "arxiv_comment": "SS and KM contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13558v1",
                "updated": "2025-06-16T14:43:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    43,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:43:18Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    43,
                    18,
                    0,
                    167,
                    0
                ],
                "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and\n  Flexible Controllability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and\n  Flexible Controllability"
                },
                "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Alan Liang"
                    },
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Yukai Ma"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v5",
                "updated": "2025-06-16T14:32:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    32,
                    22,
                    0,
                    167,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chlo Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chlo Clavel"
                },
                "author": "Chlo Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v2",
                "updated": "2025-06-16T14:30:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making"
                },
                "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19596v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19596v3",
                "updated": "2025-06-16T14:27:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    27,
                    30,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-26T22:20:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    22,
                    20,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "Reference-Aligned Retrieval-Augmented Question Answering over\n  Heterogeneous Proprietary Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-Aligned Retrieval-Augmented Question Answering over\n  Heterogeneous Proprietary Documents"
                },
                "summary": "Proprietary corporate documents contain rich domain-specific knowledge, but\ntheir overwhelming volume and disorganized structure make it difficult even for\nemployees to access the right information when needed. For example, in the\nautomotive industry, vehicle crash-collision tests, each costing hundreds of\nthousands of dollars, produce highly detailed documentation. However,\nretrieving relevant content during decision-making remains time-consuming due\nto the scale and complexity of the material. While Retrieval-Augmented\nGeneration (RAG)-based Question Answering (QA) systems offer a promising\nsolution, building an internal RAG-QA system poses several challenges: (1)\nhandling heterogeneous multi-modal data sources, (2) preserving data\nconfidentiality, and (3) enabling traceability between each piece of\ninformation in the generated answer and its original source document. To\naddress these, we propose a RAG-QA framework for internal enterprise use,\nconsisting of: (1) a data pipeline that converts raw multi-modal documents into\na structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving\narchitecture, and (3) a lightweight reference matcher that links answer\nsegments to supporting content. Applied to the automotive domain, our system\nimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),\nand helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale\nratings from both human and LLM judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprietary corporate documents contain rich domain-specific knowledge, but\ntheir overwhelming volume and disorganized structure make it difficult even for\nemployees to access the right information when needed. For example, in the\nautomotive industry, vehicle crash-collision tests, each costing hundreds of\nthousands of dollars, produce highly detailed documentation. However,\nretrieving relevant content during decision-making remains time-consuming due\nto the scale and complexity of the material. While Retrieval-Augmented\nGeneration (RAG)-based Question Answering (QA) systems offer a promising\nsolution, building an internal RAG-QA system poses several challenges: (1)\nhandling heterogeneous multi-modal data sources, (2) preserving data\nconfidentiality, and (3) enabling traceability between each piece of\ninformation in the generated answer and its original source document. To\naddress these, we propose a RAG-QA framework for internal enterprise use,\nconsisting of: (1) a data pipeline that converts raw multi-modal documents into\na structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving\narchitecture, and (3) a lightweight reference matcher that links answer\nsegments to supporting content. Applied to the automotive domain, our system\nimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),\nand helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale\nratings from both human and LLM judge."
                },
                "authors": [
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Andrew Chung"
                    },
                    {
                        "name": "Ellie S. Paek"
                    },
                    {
                        "name": "Shinsun Lee"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19596v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19596v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v3",
                "updated": "2025-06-16T14:19:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    19,
                    1,
                    0,
                    167,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jn Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13525v1",
                "updated": "2025-06-16T14:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    18,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:18:23Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    18,
                    23,
                    0,
                    167,
                    0
                ],
                "title": "Implicit and Explicit Research Quality Score Probabilities from ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit and Explicit Research Quality Score Probabilities from ChatGPT"
                },
                "summary": "The large language model (LLM) ChatGPT's quality scores for journal articles\ncorrelate more strongly with human judgements than some citation-based\nindicators in most fields. Averaging multiple ChatGPT scores improves the\nresults, apparently leveraging its internal probability model. To leverage\nthese probabilities, this article tests two novel strategies: requesting\npercentage likelihoods for scores and extracting the probabilities of\nalternative tokens in the responses. The probability estimates were then used\nto calculate weighted average scores. Both strategies were evaluated with five\niterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research\nExcellence Framework (REF) 2021, using departmental average REF2021 quality\nscores as a proxy for article quality. The data was analysed separately for\neach of the 34 field-based REF Units of Assessment. For the first strategy,\nexplicit requests for tables of score percentage likelihoods substantially\ndecreased the value of the scores (lower correlation with the proxy quality\nindicator). In contrast, weighed averages of score token probabilities slightly\nincreased the correlation with the quality proxy indicator and these\nprobabilities reasonably accurately reflected ChatGPT's outputs. The token\nprobability approach is therefore the most accurate method for ranking articles\nby research quality as well as being cheaper than comparable ChatGPT\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large language model (LLM) ChatGPT's quality scores for journal articles\ncorrelate more strongly with human judgements than some citation-based\nindicators in most fields. Averaging multiple ChatGPT scores improves the\nresults, apparently leveraging its internal probability model. To leverage\nthese probabilities, this article tests two novel strategies: requesting\npercentage likelihoods for scores and extracting the probabilities of\nalternative tokens in the responses. The probability estimates were then used\nto calculate weighted average scores. Both strategies were evaluated with five\niterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research\nExcellence Framework (REF) 2021, using departmental average REF2021 quality\nscores as a proxy for article quality. The data was analysed separately for\neach of the 34 field-based REF Units of Assessment. For the first strategy,\nexplicit requests for tables of score percentage likelihoods substantially\ndecreased the value of the scores (lower correlation with the proxy quality\nindicator). In contrast, weighed averages of score token probabilities slightly\nincreased the correlation with the quality proxy indicator and these\nprobabilities reasonably accurately reflected ChatGPT's outputs. The token\nprobability approach is therefore the most accurate method for ranking articles\nby research quality as well as being cheaper than comparable ChatGPT\nstrategies."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Yunhan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhan Yang"
                },
                "author": "Yunhan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13514v1",
                "updated": "2025-06-16T14:09:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    9,
                    43,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:09:43Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    9,
                    43,
                    0,
                    167,
                    0
                ],
                "title": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices"
                },
                "summary": "Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half."
                },
                "authors": [
                    {
                        "name": "Mingxue Xu"
                    },
                    {
                        "name": "Yao Lei Xu"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "ICML 2025 Workshop on Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models (TTODLer-FM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10112v2",
                "updated": "2025-06-16T14:07:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    7,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-04-14T11:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design"
                },
                "summary": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. Due to the opaque nature of LLMs,\nempirical methods are typically used to analyze their efficacy. The quality of\nthis analysis is highly dependent on the chosen testbed, captured metrics and\nanalysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for\nevaluating Large Language Model (LLM)-driven attacks, focusing on offensive\nuses of LLMs in cybersecurity. We review 19 research papers detailing 18\nprototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. Due to the opaque nature of LLMs,\nempirical methods are typically used to analyze their efficacy. The quality of\nthis analysis is highly dependent on the chosen testbed, captured metrics and\nanalysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for\nevaluating Large Language Model (LLM)-driven attacks, focusing on offensive\nuses of LLMs in cybersecurity. We review 19 research papers detailing 18\nprototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jrgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Cito"
                },
                "author": "Jrgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13510v2",
                "updated": "2025-06-17T02:13:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    13,
                    8,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T14:04:54Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    4,
                    54,
                    0,
                    167,
                    0
                ],
                "title": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in\n  Child-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in\n  Child-LLM Interactions"
                },
                "summary": "As Large Language Models (LLMs) increasingly power applications used by\nchildren and adolescents, ensuring safe and age-appropriate interactions has\nbecome an urgent ethical imperative. Despite progress in AI safety, current\nevaluations predominantly focus on adults, neglecting the unique\nvulnerabilities of minors engaging with generative AI. We introduce\nSafe-Child-LLM, a comprehensive benchmark and dataset for systematically\nassessing LLM safety across two developmental stages: children (7-12) and\nadolescents (13-17). Our framework includes a novel multi-part dataset of 200\nadversarial prompts, curated from red-teaming corpora (e.g., SG-Bench,\nHarmBench), with human-annotated labels for jailbreak success and a\nstandardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including\nChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we\nuncover critical safety deficiencies in child-facing scenarios. This work\nhighlights the need for community-driven benchmarks to protect young users in\nLLM interactions. To promote transparency and collaborative advancement in\nethical AI development, we are publicly releasing both our benchmark datasets\nand evaluation codebase at\nhttps://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly power applications used by\nchildren and adolescents, ensuring safe and age-appropriate interactions has\nbecome an urgent ethical imperative. Despite progress in AI safety, current\nevaluations predominantly focus on adults, neglecting the unique\nvulnerabilities of minors engaging with generative AI. We introduce\nSafe-Child-LLM, a comprehensive benchmark and dataset for systematically\nassessing LLM safety across two developmental stages: children (7-12) and\nadolescents (13-17). Our framework includes a novel multi-part dataset of 200\nadversarial prompts, curated from red-teaming corpora (e.g., SG-Bench,\nHarmBench), with human-annotated labels for jailbreak success and a\nstandardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including\nChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we\nuncover critical safety deficiencies in child-facing scenarios. This work\nhighlights the need for community-driven benchmarks to protect young users in\nLLM interactions. To promote transparency and collaborative advancement in\nethical AI development, we are publicly releasing both our benchmark datasets\nand evaluation codebase at\nhttps://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git"
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13505v1",
                "updated": "2025-06-16T13:59:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    59,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:59:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    59,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse\n  with Custom Geo-Referenced Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV Object Detection and Positioning in a Mining Industrial Metaverse\n  with Custom Geo-Referenced Data"
                },
                "summary": "The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mining sector increasingly adopts digital tools to improve operational\nefficiency, safety, and data-driven decision-making. One of the key challenges\nremains the reliable acquisition of high-resolution, geo-referenced spatial\ninformation to support core activities such as extraction planning and on-site\nmonitoring. This work presents an integrated system architecture that combines\nUAV-based sensing, LiDAR terrain modeling, and deep learning-based object\ndetection to generate spatially accurate information for open-pit mining\nenvironments. The proposed pipeline includes geo-referencing, 3D\nreconstruction, and object localization, enabling structured spatial outputs to\nbe integrated into an industrial digital twin platform. Unlike traditional\nstatic surveying methods, the system offers higher coverage and automation\npotential, with modular components suitable for deployment in real-world\nindustrial contexts. While the current implementation operates in post-flight\nbatch mode, it lays the foundation for real-time extensions. The system\ncontributes to the development of AI-enhanced remote sensing in mining by\ndemonstrating a scalable and field-validated geospatial data workflow that\nsupports situational awareness and infrastructure safety."
                },
                "authors": [
                    {
                        "name": "Vasiliki Balaska"
                    },
                    {
                        "name": "Ioannis Tsampikos Papapetros"
                    },
                    {
                        "name": "Katerina Maria Oikonomou"
                    },
                    {
                        "name": "Loukas Bampis"
                    },
                    {
                        "name": "Antonios Gasteratos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Gasteratos"
                },
                "author": "Antonios Gasteratos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13502v1",
                "updated": "2025-06-16T13:58:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    58,
                    54,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:58:54Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    58,
                    54,
                    0,
                    167,
                    0
                ],
                "title": "BOW: Bottlenecked Next Word Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOW: Bottlenecked Next Word Exploration"
                },
                "summary": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP."
                },
                "authors": [
                    {
                        "name": "Ming Shen"
                    },
                    {
                        "name": "Zhikun Xu"
                    },
                    {
                        "name": "Xiao Ye"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Ben Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ben Zhou"
                },
                "author": "Ben Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13497v1",
                "updated": "2025-06-16T13:54:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    54,
                    41,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:54:41Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    54,
                    41,
                    0,
                    167,
                    0
                ],
                "title": "DDiT: Dynamic Resource Allocation for Diffusion Transformer Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DDiT: Dynamic Resource Allocation for Diffusion Transformer Model\n  Serving"
                },
                "summary": "The Text-to-Video (T2V) model aims to generate dynamic and expressive videos\nfrom textual prompts. The generation pipeline typically involves multiple\nmodules, such as language encoder, Diffusion Transformer (DiT), and Variational\nAutoencoders (VAE). Existing serving systems often rely on monolithic model\ndeployment, while overlooking the distinct characteristics of each module,\nleading to inefficient GPU utilization. In addition, DiT exhibits varying\nperformance gains across different resolutions and degrees of parallelism, and\nsignificant optimization potential remains unexplored. To address these\nproblems, we present DDiT, a flexible system that integrates both inter-phase\nand intra-phase optimizations. DDiT focuses on two key metrics: optimal degree\nof parallelism, which prevents excessive parallelism for specific resolutions,\nand starvation time, which quantifies the sacrifice of each request. To this\nend, DDiT introduces a decoupled control mechanism to minimize the\ncomputational inefficiency caused by imbalances in the degree of parallelism\nbetween the DiT and VAE phases. It also designs a greedy resource allocation\nalgorithm with a novel scheduling mechanism that operates at the single-step\ngranularity, enabling dynamic and timely resource scaling. Our evaluation on\nthe T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets\nreveals that DDiT significantly outperforms state-of-the-art baselines by up to\n1.44x in p99 latency and 1.43x in average latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Video (T2V) model aims to generate dynamic and expressive videos\nfrom textual prompts. The generation pipeline typically involves multiple\nmodules, such as language encoder, Diffusion Transformer (DiT), and Variational\nAutoencoders (VAE). Existing serving systems often rely on monolithic model\ndeployment, while overlooking the distinct characteristics of each module,\nleading to inefficient GPU utilization. In addition, DiT exhibits varying\nperformance gains across different resolutions and degrees of parallelism, and\nsignificant optimization potential remains unexplored. To address these\nproblems, we present DDiT, a flexible system that integrates both inter-phase\nand intra-phase optimizations. DDiT focuses on two key metrics: optimal degree\nof parallelism, which prevents excessive parallelism for specific resolutions,\nand starvation time, which quantifies the sacrifice of each request. To this\nend, DDiT introduces a decoupled control mechanism to minimize the\ncomputational inefficiency caused by imbalances in the degree of parallelism\nbetween the DiT and VAE phases. It also designs a greedy resource allocation\nalgorithm with a novel scheduling mechanism that operates at the single-step\ngranularity, enabling dynamic and timely resource scaling. Our evaluation on\nthe T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets\nreveals that DDiT significantly outperforms state-of-the-art baselines by up to\n1.44x in p99 latency and 1.43x in average latency."
                },
                "authors": [
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Ziyuan Gao"
                    },
                    {
                        "name": "Liangliang Xu"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Sun Ninghui"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Sa Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sa Wang"
                },
                "author": "Sa Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13494v1",
                "updated": "2025-06-16T13:51:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    51,
                    49,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:51:49Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    51,
                    49,
                    0,
                    167,
                    0
                ],
                "title": "Watermarking LLM-Generated Datasets in Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking LLM-Generated Datasets in Downstream Tasks"
                },
                "summary": "Large Language Models (LLMs) have experienced rapid advancements, with\napplications spanning a wide range of fields, including sentiment\nclassification, review generation, and question answering. Due to their\nefficiency and versatility, researchers and companies increasingly employ\nLLM-generated data to train their models. However, the inability to track\ncontent produced by LLMs poses a significant challenge, potentially leading to\ncopyright infringement for the LLM owners. In this paper, we propose a method\nfor injecting watermarks into LLM-generated datasets, enabling the tracking of\ndownstream tasks to detect whether these datasets were produced using the\noriginal LLM. These downstream tasks can be divided into two categories. The\nfirst involves using the generated datasets at the input level, commonly for\ntraining classification tasks. The other is the output level, where model\ntrainers use LLM-generated content as output for downstream tasks, such as\nquestion-answering tasks. We design a comprehensive set of experiments to\nevaluate both watermark methods. Our results indicate the high effectiveness of\nour watermark approach. Additionally, regarding model utility, we find that\nclassifiers trained on the generated datasets achieve a test accuracy exceeding\n0.900 in many cases, suggesting that the utility of such models remains robust.\nFor the output-level watermark, we observe that the quality of the generated\ntext is comparable to that produced using real-world datasets. Through our\nresearch, we aim to advance the protection of LLM copyrights, taking a\nsignificant step forward in safeguarding intellectual property in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have experienced rapid advancements, with\napplications spanning a wide range of fields, including sentiment\nclassification, review generation, and question answering. Due to their\nefficiency and versatility, researchers and companies increasingly employ\nLLM-generated data to train their models. However, the inability to track\ncontent produced by LLMs poses a significant challenge, potentially leading to\ncopyright infringement for the LLM owners. In this paper, we propose a method\nfor injecting watermarks into LLM-generated datasets, enabling the tracking of\ndownstream tasks to detect whether these datasets were produced using the\noriginal LLM. These downstream tasks can be divided into two categories. The\nfirst involves using the generated datasets at the input level, commonly for\ntraining classification tasks. The other is the output level, where model\ntrainers use LLM-generated content as output for downstream tasks, such as\nquestion-answering tasks. We design a comprehensive set of experiments to\nevaluate both watermark methods. Our results indicate the high effectiveness of\nour watermark approach. Additionally, regarding model utility, we find that\nclassifiers trained on the generated datasets achieve a test accuracy exceeding\n0.900 in many cases, suggesting that the utility of such models remains robust.\nFor the output-level watermark, we observe that the quality of the generated\ntext is comparable to that produced using real-world datasets. Through our\nresearch, we aim to advance the protection of LLM copyrights, taking a\nsignificant step forward in safeguarding intellectual property in this domain."
                },
                "authors": [
                    {
                        "name": "Yugeng Liu"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13474v1",
                "updated": "2025-06-16T13:32:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    32,
                    1,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:32:01Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    32,
                    1,
                    0,
                    167,
                    0
                ],
                "title": "Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning"
                },
                "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency."
                },
                "authors": [
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege zsoy"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09975v2",
                "updated": "2025-06-16T13:31:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    31,
                    25,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T17:51:28Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    51,
                    28,
                    2,
                    162,
                    0
                ],
                "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text"
                },
                "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case."
                },
                "authors": [
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear in ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13472v2",
                "updated": "2025-06-17T09:13:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    9,
                    13,
                    54,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-16T13:30:33Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    30,
                    33,
                    0,
                    167,
                    0
                ],
                "title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models"
                },
                "summary": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64."
                },
                "authors": [
                    {
                        "name": "Junho Yoon"
                    },
                    {
                        "name": "Geom Lee"
                    },
                    {
                        "name": "Donghyeon Jeon"
                    },
                    {
                        "name": "Inho Kang"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13464v1",
                "updated": "2025-06-16T13:24:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    24,
                    50,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:24:50Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    24,
                    50,
                    0,
                    167,
                    0
                ],
                "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Zheyuan Xiao"
                    },
                    {
                        "name": "Seraphina Zhang"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13458v1",
                "updated": "2025-06-16T13:15:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    15,
                    2,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:15:02Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    15,
                    2,
                    0,
                    167,
                    0
                ],
                "title": "Leveraging Vision-Language Pre-training for Human Activity Recognition\n  in Still Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Vision-Language Pre-training for Human Activity Recognition\n  in Still Images"
                },
                "summary": "Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments."
                },
                "authors": [
                    {
                        "name": "Cristina Mahanta"
                    },
                    {
                        "name": "Gagan Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Gagan Bhatia"
                },
                "author": "Gagan Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17533v2",
                "updated": "2025-06-16T13:07:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    7,
                    26,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-24T14:42:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    42,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "From Euler to AI: Unifying Formulas for Mathematical Constants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Euler to AI: Unifying Formulas for Mathematical Constants"
                },
                "summary": "The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains."
                },
                "authors": [
                    {
                        "name": "Tomer Raz"
                    },
                    {
                        "name": "Michael Shalyt"
                    },
                    {
                        "name": "Elyasheev Leibtag"
                    },
                    {
                        "name": "Rotem Kalisch"
                    },
                    {
                        "name": "Shachar Weinbaum"
                    },
                    {
                        "name": "Yaron Hadad"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_comment": "60 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.HO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13436v1",
                "updated": "2025-06-16T12:52:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    55,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:52:55Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    55,
                    0,
                    167,
                    0
                ],
                "title": "Q-AIM: A Unified Portable Workflow for Seamless Integration of Quantum\n  Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-AIM: A Unified Portable Workflow for Seamless Integration of Quantum\n  Resources"
                },
                "summary": "Quantum computing (QC) holds the potential to solve classically intractable\nproblems. Although there has been significant progress towards the availability\nof quantum hardware, a software infrastructure to integrate them is still\nmissing. We present Q-AIM (Quantum Access Infrastructure Management) to fill\nthis gap. Q-AIM is a software framework unifying the access and management for\nquantum hardware in a vendor-independent and open-source fashion.\n  Utilizing a dockerized micro-service architecture, we show Q-AIM's\nlightweight, portable, and customizable nature, capable of running on different\nhosting paradigms ranging from small personal computing devices to cloud\nservers and dedicated server infrastructure. Q-AIM exposes a single entry point\ninto the host's infrastructure, providing secure and easy interaction with\nquantum computers on different levels of abstraction. With a minimal memory\nfootprint, the container is optimized for deployment on even the smallest\nserver instances, reducing costs and instantiation overhead while ensuring\nseamless scalability to accommodate increasing demands. Q-AIM intends to equip\nresearch groups and facilities purchasing and hosting their own quantum\nhardware with a tool simplifying the process from procurement to operation and\nremoving non-research related technical redundancies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing (QC) holds the potential to solve classically intractable\nproblems. Although there has been significant progress towards the availability\nof quantum hardware, a software infrastructure to integrate them is still\nmissing. We present Q-AIM (Quantum Access Infrastructure Management) to fill\nthis gap. Q-AIM is a software framework unifying the access and management for\nquantum hardware in a vendor-independent and open-source fashion.\n  Utilizing a dockerized micro-service architecture, we show Q-AIM's\nlightweight, portable, and customizable nature, capable of running on different\nhosting paradigms ranging from small personal computing devices to cloud\nservers and dedicated server infrastructure. Q-AIM exposes a single entry point\ninto the host's infrastructure, providing secure and easy interaction with\nquantum computers on different levels of abstraction. With a minimal memory\nfootprint, the container is optimized for deployment on even the smallest\nserver instances, reducing costs and instantiation overhead while ensuring\nseamless scalability to accommodate increasing demands. Q-AIM intends to equip\nresearch groups and facilities purchasing and hosting their own quantum\nhardware with a tool simplifying the process from procurement to operation and\nremoving non-research related technical redundancies."
                },
                "authors": [
                    {
                        "name": "Zhaobin Zhu"
                    },
                    {
                        "name": "Cedric Gaberle"
                    },
                    {
                        "name": "Sarah M. Neuwirth"
                    },
                    {
                        "name": "Thomas Lippert"
                    },
                    {
                        "name": "Manpreet S. Jattana"
                    }
                ],
                "author_detail": {
                    "name": "Manpreet S. Jattana"
                },
                "author": "Manpreet S. Jattana",
                "arxiv_comment": "preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13434v1",
                "updated": "2025-06-16T12:52:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    19,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:52:19Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    52,
                    19,
                    0,
                    167,
                    0
                ],
                "title": "From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in\n  the Age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Promise to Peril: Rethinking Cybersecurity Red and Blue Teaming in\n  the Age of LLMs"
                },
                "summary": "Large Language Models (LLMs) are set to reshape cybersecurity by augmenting\nred and blue team operations. Red teams can exploit LLMs to plan attacks, craft\nphishing content, simulate adversaries, and generate exploit code. Conversely,\nblue teams may deploy them for threat intelligence synthesis, root cause\nanalysis, and streamlined documentation. This dual capability introduces both\ntransformative potential and serious risks.\n  This position paper maps LLM applications across cybersecurity frameworks\nsuch as MITRE ATT&CK and the NIST Cybersecurity Framework (CSF), offering a\nstructured view of their current utility and limitations. While LLMs\ndemonstrate fluency and versatility across various tasks, they remain fragile\nin high-stakes, context-heavy environments. Key limitations include\nhallucinations, limited context retention, poor reasoning, and sensitivity to\nprompts, which undermine their reliability in operational settings.\n  Moreover, real-world integration raises concerns around dual-use risks,\nadversarial misuse, and diminished human oversight. Malicious actors could\nexploit LLMs to automate reconnaissance, obscure attack vectors, and lower the\ntechnical threshold for executing sophisticated attacks.\n  To ensure safer adoption, we recommend maintaining human-in-the-loop\noversight, enhancing model explainability, integrating privacy-preserving\nmechanisms, and building systems robust to adversarial exploitation. As\norganizations increasingly adopt AI driven cybersecurity, a nuanced\nunderstanding of LLMs' risks and operational impacts is critical to securing\ntheir defensive value while mitigating unintended consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are set to reshape cybersecurity by augmenting\nred and blue team operations. Red teams can exploit LLMs to plan attacks, craft\nphishing content, simulate adversaries, and generate exploit code. Conversely,\nblue teams may deploy them for threat intelligence synthesis, root cause\nanalysis, and streamlined documentation. This dual capability introduces both\ntransformative potential and serious risks.\n  This position paper maps LLM applications across cybersecurity frameworks\nsuch as MITRE ATT&CK and the NIST Cybersecurity Framework (CSF), offering a\nstructured view of their current utility and limitations. While LLMs\ndemonstrate fluency and versatility across various tasks, they remain fragile\nin high-stakes, context-heavy environments. Key limitations include\nhallucinations, limited context retention, poor reasoning, and sensitivity to\nprompts, which undermine their reliability in operational settings.\n  Moreover, real-world integration raises concerns around dual-use risks,\nadversarial misuse, and diminished human oversight. Malicious actors could\nexploit LLMs to automate reconnaissance, obscure attack vectors, and lower the\ntechnical threshold for executing sophisticated attacks.\n  To ensure safer adoption, we recommend maintaining human-in-the-loop\noversight, enhancing model explainability, integrating privacy-preserving\nmechanisms, and building systems robust to adversarial exploitation. As\norganizations increasingly adopt AI driven cybersecurity, a nuanced\nunderstanding of LLMs' risks and operational impacts is critical to securing\ntheir defensive value while mitigating unintended consequences."
                },
                "authors": [
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Chris Hicks"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Burak Hasircioglu"
                    },
                    {
                        "name": "Diksha Goel"
                    },
                    {
                        "name": "Piers Jennings"
                    }
                ],
                "author_detail": {
                    "name": "Piers Jennings"
                },
                "author": "Piers Jennings",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13430v1",
                "updated": "2025-06-16T12:47:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    47,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:47:37Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    47,
                    37,
                    0,
                    167,
                    0
                ],
                "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Remaining Lifespan Prediction from Images"
                },
                "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research."
                },
                "authors": [
                    {
                        "name": "Tristan Kenneweg"
                    },
                    {
                        "name": "Philip Kenneweg"
                    },
                    {
                        "name": "Barbara Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Hammer"
                },
                "author": "Barbara Hammer",
                "arxiv_comment": "Submitted to IMPACT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09813v2",
                "updated": "2025-06-16T12:43:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    43,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T14:53:47Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    53,
                    47,
                    2,
                    162,
                    0
                ],
                "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metritocracy: Representative Metrics for Lite Benchmarks"
                },
                "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation."
                },
                "authors": [
                    {
                        "name": "Ariel Procaccia"
                    },
                    {
                        "name": "Benjamin Schiffer"
                    },
                    {
                        "name": "Serena Wang"
                    },
                    {
                        "name": "Shirley Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Zhang"
                },
                "author": "Shirley Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13425v1",
                "updated": "2025-06-16T12:43:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    43,
                    2,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:43:02Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    43,
                    2,
                    0,
                    167,
                    0
                ],
                "title": "JENGA: Object selection and pose estimation for robotic grasping from a\n  stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JENGA: Object selection and pose estimation for robotic grasping from a\n  stack"
                },
                "summary": "Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario."
                },
                "authors": [
                    {
                        "name": "Sai Srinivas Jeevanandam"
                    },
                    {
                        "name": "Sandeep Inuganti"
                    },
                    {
                        "name": "Shreedhar Govil"
                    },
                    {
                        "name": "Didier Stricker"
                    },
                    {
                        "name": "Jason Rambach"
                    }
                ],
                "author_detail": {
                    "name": "Jason Rambach"
                },
                "author": "Jason Rambach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13416v1",
                "updated": "2025-06-16T12:33:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    33,
                    9,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:33:09Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    33,
                    9,
                    0,
                    167,
                    0
                ],
                "title": "Spiking Neural Networks for Low-Power Vibration-Based Predictive\n  Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks for Low-Power Vibration-Based Predictive\n  Maintenance"
                },
                "summary": "Advancements in Industrial Internet of Things (IIoT) sensors enable\nsophisticated Predictive Maintenance (PM) with high temporal resolution. For\ncost-efficient solutions, vibration-based condition monitoring is especially of\ninterest. However, analyzing high-resolution vibration data via traditional\ncloud approaches incurs significant energy and communication costs, hindering\nbattery-powered edge deployments. This necessitates shifting intelligence to\nthe sensor edge. Due to their event-driven nature, Spiking Neural Networks\n(SNNs) offer a promising pathway toward energy-efficient on-device processing.\nThis paper investigates a recurrent SNN for simultaneous regression (flow,\npressure, pump speed) and multi-label classification (normal, overpressure,\ncavitation) for an industrial progressing cavity pump (PCP) using 3-axis\nvibration data. Furthermore, we provide energy consumption estimates comparing\nthe SNN approach on conventional (x86, ARM) and neuromorphic (Loihi) hardware\nplatforms. Results demonstrate high classification accuracy (>97%) with zero\nFalse Negative Rates for critical Overpressure and Cavitation faults. Smoothed\nregression outputs achieve Mean Relative Percentage Errors below 1% for flow\nand pump speed, approaching industrial sensor standards, although pressure\nprediction requires further refinement. Energy estimates indicate significant\npower savings, with the Loihi consumption (0.0032 J/inf) being up to 3 orders\nof magnitude less compared to the estimated x86 CPU (11.3 J/inf) and ARM CPU\n(1.18 J/inf) execution. Our findings underscore the potential of SNNs for\nmulti-task PM directly on resource-constrained edge devices, enabling scalable\nand energy-efficient industrial monitoring solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in Industrial Internet of Things (IIoT) sensors enable\nsophisticated Predictive Maintenance (PM) with high temporal resolution. For\ncost-efficient solutions, vibration-based condition monitoring is especially of\ninterest. However, analyzing high-resolution vibration data via traditional\ncloud approaches incurs significant energy and communication costs, hindering\nbattery-powered edge deployments. This necessitates shifting intelligence to\nthe sensor edge. Due to their event-driven nature, Spiking Neural Networks\n(SNNs) offer a promising pathway toward energy-efficient on-device processing.\nThis paper investigates a recurrent SNN for simultaneous regression (flow,\npressure, pump speed) and multi-label classification (normal, overpressure,\ncavitation) for an industrial progressing cavity pump (PCP) using 3-axis\nvibration data. Furthermore, we provide energy consumption estimates comparing\nthe SNN approach on conventional (x86, ARM) and neuromorphic (Loihi) hardware\nplatforms. Results demonstrate high classification accuracy (>97%) with zero\nFalse Negative Rates for critical Overpressure and Cavitation faults. Smoothed\nregression outputs achieve Mean Relative Percentage Errors below 1% for flow\nand pump speed, approaching industrial sensor standards, although pressure\nprediction requires further refinement. Energy estimates indicate significant\npower savings, with the Loihi consumption (0.0032 J/inf) being up to 3 orders\nof magnitude less compared to the estimated x86 CPU (11.3 J/inf) and ARM CPU\n(1.18 J/inf) execution. Our findings underscore the potential of SNNs for\nmulti-task PM directly on resource-constrained edge devices, enabling scalable\nand energy-efficient industrial monitoring solutions."
                },
                "authors": [
                    {
                        "name": "Alexandru Vasilache"
                    },
                    {
                        "name": "Sven Nitzsche"
                    },
                    {
                        "name": "Christian Kneidl"
                    },
                    {
                        "name": "Mikael Tekneyan"
                    },
                    {
                        "name": "Moritz Neher"
                    },
                    {
                        "name": "Juergen Becker"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Becker"
                },
                "author": "Juergen Becker",
                "arxiv_comment": "This paper has been accepted and will be presented at the\n  International Conference on Neuromorphic Systems (ICONS) 2025, July 29-31,\n  2025. The proceedings will be published later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13410v1",
                "updated": "2025-06-16T12:26:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    26,
                    13,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:26:13Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    26,
                    13,
                    0,
                    167,
                    0
                ],
                "title": "Training Neural Networks by Optimizing Neuron Positions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Neural Networks by Optimizing Neuron Positions"
                },
                "summary": "The high computational complexity and increasing parameter counts of deep\nneural networks pose significant challenges for deployment in\nresource-constrained environments, such as edge devices or real-time systems.\nTo address this, we propose a parameter-efficient neural architecture where\nneurons are embedded in Euclidean space. During training, their positions are\noptimized and synaptic weights are determined as the inverse of the spatial\ndistance between connected neurons. These distance-dependent wiring rules\nreplace traditional learnable weight matrices and significantly reduce the\nnumber of parameters while introducing a biologically inspired inductive bias:\nconnection strength decreases with spatial distance, reflecting the brain's\nembedding in three-dimensional space where connections tend to minimize wiring\nlength. We validate this approach for both multi-layer perceptrons and spiking\nneural networks. Through a series of experiments, we demonstrate that these\nspatially embedded neural networks achieve a performance competitive with\nconventional architectures on the MNIST dataset. Additionally, the models\nmaintain performance even at pruning rates exceeding 80% sparsity,\noutperforming traditional networks with the same number of parameters under\nsimilar conditions. Finally, the spatial embedding framework offers an\nintuitive visualization of the network structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational complexity and increasing parameter counts of deep\nneural networks pose significant challenges for deployment in\nresource-constrained environments, such as edge devices or real-time systems.\nTo address this, we propose a parameter-efficient neural architecture where\nneurons are embedded in Euclidean space. During training, their positions are\noptimized and synaptic weights are determined as the inverse of the spatial\ndistance between connected neurons. These distance-dependent wiring rules\nreplace traditional learnable weight matrices and significantly reduce the\nnumber of parameters while introducing a biologically inspired inductive bias:\nconnection strength decreases with spatial distance, reflecting the brain's\nembedding in three-dimensional space where connections tend to minimize wiring\nlength. We validate this approach for both multi-layer perceptrons and spiking\nneural networks. Through a series of experiments, we demonstrate that these\nspatially embedded neural networks achieve a performance competitive with\nconventional architectures on the MNIST dataset. Additionally, the models\nmaintain performance even at pruning rates exceeding 80% sparsity,\noutperforming traditional networks with the same number of parameters under\nsimilar conditions. Finally, the spatial embedding framework offers an\nintuitive visualization of the network structure."
                },
                "authors": [
                    {
                        "name": "Laura Erb"
                    },
                    {
                        "name": "Tommaso Boccato"
                    },
                    {
                        "name": "Alexandru Vasilache"
                    },
                    {
                        "name": "Juergen Becker"
                    },
                    {
                        "name": "Nicola Toschi"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Toschi"
                },
                "author": "Nicola Toschi",
                "arxiv_comment": "This paper has been accepted and will be presented at the 14th\n  International Conference on Biomimetic and Biohybrid Systems (Living Machines\n  2025), July 15-18, 2025, Sheffield, UK. The proceedings will be published\n  later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13408v1",
                "updated": "2025-06-16T12:21:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    21,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:21:27Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    21,
                    27,
                    0,
                    167,
                    0
                ],
                "title": "HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention"
                },
                "summary": "Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment."
                },
                "authors": [
                    {
                        "name": "Miguel Camelo Botero"
                    },
                    {
                        "name": "Esra Aycan Beyazit"
                    },
                    {
                        "name": "Nina Slamnik-Krijetorac"
                    },
                    {
                        "name": "Johann M. Marquez-Barja"
                    }
                ],
                "author_detail": {
                    "name": "Johann M. Marquez-Barja"
                },
                "author": "Johann M. Marquez-Barja",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13405v1",
                "updated": "2025-06-16T12:19:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    19,
                    8,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:19:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    19,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis"
                },
                "summary": "With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench."
                },
                "authors": [
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Xu Lu"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Bowen Bao"
                    },
                    {
                        "name": "Yijing He"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Haobo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haobo Wang"
                },
                "author": "Haobo Wang",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13403v1",
                "updated": "2025-06-16T12:17:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    17,
                    11,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T12:17:11Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    17,
                    11,
                    0,
                    167,
                    0
                ],
                "title": "Deflating Deflationism: A Critical Perspective on Debunking Arguments\n  Against LLM Mentality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deflating Deflationism: A Critical Perspective on Debunking Arguments\n  Against LLM Mentality"
                },
                "summary": "Many people feel compelled to interpret, describe, and respond to Large\nLanguage Models (LLMs) as if they possess inner mental lives similar to our\nown. Responses to this phenomenon have varied. Inflationists hold that at least\nsome folk psychological ascriptions to LLMs are warranted. Deflationists argue\nthat all such attributions of mentality to LLMs are misplaced, often cautioning\nagainst the risk that anthropomorphic projection may lead to misplaced trust or\npotentially even confusion about the moral status of LLMs. We advance this\ndebate by assessing two common deflationary arguments against LLM mentality.\nWhat we term the 'robustness strategy' aims to undercut one justification for\nbelieving that LLMs are minded entities by showing that putatively cognitive\nand humanlike behaviours are not robust, failing to generalise appropriately.\nWhat we term the 'etiological strategy' undercuts attributions of mentality by\nchallenging naive causal explanations of LLM behaviours, offering alternative\ncausal accounts that weaken the case for mental state attributions. While both\nstrategies offer powerful challenges to full-blown inflationism, we find that\nneither strategy provides a knock-down case against ascriptions of mentality to\nLLMs simpliciter. With this in mind, we explore a modest form of inflationism\nthat permits ascriptions of mentality to LLMs under certain conditions.\nSpecifically, we argue that folk practice provides a defeasible basis for\nattributing mental states and capacities to LLMs provided those mental states\nand capacities can be understood in metaphysically undemanding terms (e.g.\nknowledge, beliefs and desires), while greater caution is required when\nattributing metaphysically demanding mental phenomena such as phenomenal\nconsciousness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many people feel compelled to interpret, describe, and respond to Large\nLanguage Models (LLMs) as if they possess inner mental lives similar to our\nown. Responses to this phenomenon have varied. Inflationists hold that at least\nsome folk psychological ascriptions to LLMs are warranted. Deflationists argue\nthat all such attributions of mentality to LLMs are misplaced, often cautioning\nagainst the risk that anthropomorphic projection may lead to misplaced trust or\npotentially even confusion about the moral status of LLMs. We advance this\ndebate by assessing two common deflationary arguments against LLM mentality.\nWhat we term the 'robustness strategy' aims to undercut one justification for\nbelieving that LLMs are minded entities by showing that putatively cognitive\nand humanlike behaviours are not robust, failing to generalise appropriately.\nWhat we term the 'etiological strategy' undercuts attributions of mentality by\nchallenging naive causal explanations of LLM behaviours, offering alternative\ncausal accounts that weaken the case for mental state attributions. While both\nstrategies offer powerful challenges to full-blown inflationism, we find that\nneither strategy provides a knock-down case against ascriptions of mentality to\nLLMs simpliciter. With this in mind, we explore a modest form of inflationism\nthat permits ascriptions of mentality to LLMs under certain conditions.\nSpecifically, we argue that folk practice provides a defeasible basis for\nattributing mental states and capacities to LLMs provided those mental states\nand capacities can be understood in metaphysically undemanding terms (e.g.\nknowledge, beliefs and desires), while greater caution is required when\nattributing metaphysically demanding mental phenomena such as phenomenal\nconsciousness."
                },
                "authors": [
                    {
                        "name": "Alex Grzankowski"
                    },
                    {
                        "name": "Geoff Keeling"
                    },
                    {
                        "name": "Henry Shevlin"
                    },
                    {
                        "name": "Winnie Street"
                    }
                ],
                "author_detail": {
                    "name": "Winnie Street"
                },
                "author": "Winnie Street",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17189v3",
                "updated": "2025-06-16T12:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    0,
                    53,
                    0,
                    167,
                    0
                ],
                "published": "2024-12-22T23:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    23,
                    31,
                    3,
                    6,
                    357,
                    0
                ],
                "title": "Better Think with Tables: Tabular Structures Enhance LLM Comprehension\n  for Data-Analytics Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Think with Tables: Tabular Structures Enhance LLM Comprehension\n  for Data-Analytics Requests"
                },
                "summary": "Large Language Models (LLMs) often struggle with data-analytics requests\nrelated to information retrieval and data manipulation that frequently arise in\nreal-world scenarios under multiple conditions. In this paper, we introduce\nThinking with Tables, where we inject tabular structures into LLMs for\ndata-analytics requests. Through comprehensive evaluations across various\nrequest types, we show that providing tabular structures yields a 40.29 percent\naverage performance gain along with better robustness and token efficiency.\nThrough attention-value analysis, we uncover that tables help LLMs better\nattend to relevant information, explaining these improvements. Beyond tables\nand text, we evaluate whether (1) blending structuredness within text, such as\nproviding templates or fixing the order of attributes, and (2) other\nrepresentative structures, such as knowledge graphs and JSON, are helpful. We\nobserve that utilizing tables offers the best balance between efficiency and\neffectiveness. These advantages remain consistent under increased task\ncomplexity and even when all input data cannot be structured. Finally, as data\nanalytics typically relies on structured factual inputs, our text-to-table\nconversion demonstrates the method's applicability to text-compatible data\nsources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with data-analytics requests\nrelated to information retrieval and data manipulation that frequently arise in\nreal-world scenarios under multiple conditions. In this paper, we introduce\nThinking with Tables, where we inject tabular structures into LLMs for\ndata-analytics requests. Through comprehensive evaluations across various\nrequest types, we show that providing tabular structures yields a 40.29 percent\naverage performance gain along with better robustness and token efficiency.\nThrough attention-value analysis, we uncover that tables help LLMs better\nattend to relevant information, explaining these improvements. Beyond tables\nand text, we evaluate whether (1) blending structuredness within text, such as\nproviding templates or fixing the order of attributes, and (2) other\nrepresentative structures, such as knowledge graphs and JSON, are helpful. We\nobserve that utilizing tables offers the best balance between efficiency and\neffectiveness. These advantages remain consistent under increased task\ncomplexity and even when all input data cannot be structured. Finally, as data\nanalytics typically relies on structured factual inputs, our text-to-table\nconversion demonstrates the method's applicability to text-compatible data\nsources."
                },
                "authors": [
                    {
                        "name": "Jio Oh"
                    },
                    {
                        "name": "Geon Heo"
                    },
                    {
                        "name": "Seungjun Oh"
                    },
                    {
                        "name": "Hyunjin Kim"
                    },
                    {
                        "name": "JinYeong Bak"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Steven Euijong Whang"
                    }
                ],
                "author_detail": {
                    "name": "Steven Euijong Whang"
                },
                "author": "Steven Euijong Whang",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13394v1",
                "updated": "2025-06-16T11:59:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    59,
                    49,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:59:49Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    59,
                    49,
                    0,
                    167,
                    0
                ],
                "title": "A Model-Free Detection Method for Internal Short Circuits in Single\n  Lithium-ion Cells Using Pseudo Open-Circuit Voltage Difference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Model-Free Detection Method for Internal Short Circuits in Single\n  Lithium-ion Cells Using Pseudo Open-Circuit Voltage Difference"
                },
                "summary": "This letter proposes a lightweight, model-free online diagnostic framework\nfor detecting internal short circuits (ISC) in single lithium-ion cells under\ndynamic operating conditions. The core of the method lies in computing the\nfirst-order difference of pseudo open-circuit voltage\n($\\boldsymbol{\\mathrm{OCV}_{\\text{pseudo}}}$) to extract high-frequency\ndeviations caused by ISC events from low-frequency polarization variations. The\nmethod relies solely on terminal voltage, current measurements, and an offline\n$R_0$--SOC look-up table, thereby eliminating the need for electrochemical or\nequivalent-circuit observers. Validated on ten real and one false fault\nscenarios, the proposed approach achieves a 100\\% detection success rate with\nno missed or false alarms. In addition, the proposed method exhibits extremely\nlow computational and memory requirements, making it highly suitable for\nreal-time deployment in battery management systems (BMS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter proposes a lightweight, model-free online diagnostic framework\nfor detecting internal short circuits (ISC) in single lithium-ion cells under\ndynamic operating conditions. The core of the method lies in computing the\nfirst-order difference of pseudo open-circuit voltage\n($\\boldsymbol{\\mathrm{OCV}_{\\text{pseudo}}}$) to extract high-frequency\ndeviations caused by ISC events from low-frequency polarization variations. The\nmethod relies solely on terminal voltage, current measurements, and an offline\n$R_0$--SOC look-up table, thereby eliminating the need for electrochemical or\nequivalent-circuit observers. Validated on ten real and one false fault\nscenarios, the proposed approach achieves a 100\\% detection success rate with\nno missed or false alarms. In addition, the proposed method exhibits extremely\nlow computational and memory requirements, making it highly suitable for\nreal-time deployment in battery management systems (BMS)."
                },
                "authors": [
                    {
                        "name": "Yangyang Xu"
                    },
                    {
                        "name": "Chenglin Liao"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Liao"
                },
                "author": "Chenglin Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10647v2",
                "updated": "2025-06-16T11:57:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    57,
                    29,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-12T12:38:04Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    38,
                    4,
                    3,
                    163,
                    0
                ],
                "title": "Data Shifts Hurt CoT: A Theoretical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Shifts Hurt CoT: A Theoretical Study"
                },
                "summary": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact."
                },
                "authors": [
                    {
                        "name": "Lang Yin"
                    },
                    {
                        "name": "Debangshu Banerjee"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "Comparison to v1: upgraded the quality of a figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13384v1",
                "updated": "2025-06-16T11:48:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    48,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:48:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    48,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Delving Into the Psychology of Machines: Exploring the Structure of\n  Self-Regulated Learning via LLM-Generated Survey Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving Into the Psychology of Machines: Exploring the Structure of\n  Self-Regulated Learning via LLM-Generated Survey Responses"
                },
                "summary": "Large language models (LLMs) offer the potential to simulate human-like\nresponses and behaviors, creating new opportunities for psychological science.\nIn the context of self-regulated learning (SRL), if LLMs can reliably simulate\nsurvey responses at scale and speed, they could be used to test intervention\nscenarios, refine theoretical models, augment sparse datasets, and represent\nhard-to-reach populations. However, the validity of LLM-generated survey\nresponses remains uncertain, with limited research focused on SRL and existing\nstudies beyond SRL yielding mixed results. Therefore, in this study, we\nexamined LLM-generated responses to the 44-item Motivated Strategies for\nLearning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used\ninstrument assessing students' learning strategies and academic motivation.\nParticularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA\n3.1-8B, and Mistral Large. We analyzed item distributions, the psychological\nnetwork of the theoretical SRL dimensions, and psychometric validity based on\nthe latent factor structure. Our results suggest that Gemini 2 Flash was the\nmost promising LLM, showing considerable sampling variability and producing\nunderlying dimensions and theoretical relationships that align with prior\ntheory and empirical findings. At the same time, we observed discrepancies and\nlimitations, underscoring both the potential and current constraints of using\nLLMs for simulating psychological survey data and applying it in educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer the potential to simulate human-like\nresponses and behaviors, creating new opportunities for psychological science.\nIn the context of self-regulated learning (SRL), if LLMs can reliably simulate\nsurvey responses at scale and speed, they could be used to test intervention\nscenarios, refine theoretical models, augment sparse datasets, and represent\nhard-to-reach populations. However, the validity of LLM-generated survey\nresponses remains uncertain, with limited research focused on SRL and existing\nstudies beyond SRL yielding mixed results. Therefore, in this study, we\nexamined LLM-generated responses to the 44-item Motivated Strategies for\nLearning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used\ninstrument assessing students' learning strategies and academic motivation.\nParticularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA\n3.1-8B, and Mistral Large. We analyzed item distributions, the psychological\nnetwork of the theoretical SRL dimensions, and psychometric validity based on\nthe latent factor structure. Our results suggest that Gemini 2 Flash was the\nmost promising LLM, showing considerable sampling variability and producing\nunderlying dimensions and theoretical relationships that align with prior\ntheory and empirical findings. At the same time, we observed discrepancies and\nlimitations, underscoring both the potential and current constraints of using\nLLMs for simulating psychological survey data and applying it in educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Leonie V. D. E. Vogelsmeier"
                    },
                    {
                        "name": "Eduardo Oliveira"
                    },
                    {
                        "name": "Kamila Misiejuk"
                    },
                    {
                        "name": "Sonsoles Lpez-Pernas"
                    },
                    {
                        "name": "Mohammed Saqr"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed Saqr"
                },
                "author": "Mohammed Saqr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03662v3",
                "updated": "2025-06-16T11:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    47,
                    36,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-04T07:52:46Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    7,
                    52,
                    46,
                    2,
                    155,
                    0
                ],
                "title": "Zero-Shot Temporal Interaction Localization for Egocentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Temporal Interaction Localization for Egocentric Videos"
                },
                "summary": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc."
                },
                "authors": [
                    {
                        "name": "Erhang Zhang"
                    },
                    {
                        "name": "Junyi Ma"
                    },
                    {
                        "name": "Yin-Dong Zheng"
                    },
                    {
                        "name": "Yixuan Zhou"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12484v2",
                "updated": "2025-06-16T11:46:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    46,
                    29,
                    0,
                    167,
                    0
                ],
                "published": "2024-11-19T13:08:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    13,
                    8,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "Regular-pattern-sensitive CRFs for Distant Label Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regular-pattern-sensitive CRFs for Distant Label Interactions"
                },
                "summary": "While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets."
                },
                "authors": [
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    },
                    {
                        "name": "Sebastian Pado"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pado"
                },
                "author": "Sebastian Pado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v1",
                "updated": "2025-06-16T11:44:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompositional Reasoning for Graph Retrieval with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gal de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gal de Chalendar"
                },
                "author": "Gal de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11169v2",
                "updated": "2025-06-16T11:37:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    37,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-16T15:39:57Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    39,
                    57,
                    6,
                    47,
                    0
                ],
                "title": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model"
                },
                "summary": "This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qingwen Lin"
                    },
                    {
                        "name": "Boyan Xu"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zhifeng Hao"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    }
                ],
                "author_detail": {
                    "name": "Ruichu Cai"
                },
                "author": "Ruichu Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09387v3",
                "updated": "2025-06-16T11:10:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    10,
                    18,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-13T15:04:53Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    15,
                    4,
                    53,
                    3,
                    44,
                    0
                ],
                "title": "Truth Knows No Language: Evaluating Truthfulness Beyond English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth Knows No Language: Evaluating Truthfulness Beyond English"
                },
                "summary": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses."
                },
                "authors": [
                    {
                        "name": "Blanca Calvo Figueras"
                    },
                    {
                        "name": "Eneko Sagarzazu"
                    },
                    {
                        "name": "Julen Etxaniz"
                    },
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Pablo Gamallo"
                    },
                    {
                        "name": "Iria De Dios Flores"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "14 pages, 6 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13361v1",
                "updated": "2025-06-16T11:04:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    4,
                    48,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T11:04:48Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    4,
                    48,
                    0,
                    167,
                    0
                ],
                "title": "Evaluation of Nuclear Microreactor Cost-competitiveness in Current\n  Electricity Markets Considering Reactor Cost Uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Nuclear Microreactor Cost-competitiveness in Current\n  Electricity Markets Considering Reactor Cost Uncertainties"
                },
                "summary": "This paper evaluates the cost competitiveness of microreactors in today's\nelectricity markets, with a focus on uncertainties in reactor costs. A Genetic\nAlgorithm (GA) is used to optimize key technical parameters, such as reactor\ncapacity, fuel enrichment, tail enrichment, refueling interval, and discharge\nburnup, to minimize the Levelized Cost of Energy (LCOE). Base case results are\nvalidated using Simulated Annealing (SA). By incorporating Probability\nDistribution Functions (PDFs) for fuel cycle costs, the study identifies\noptimal configurations under uncertainty. Methodologically, it introduces a\nnovel framework combining probabilistic cost modeling with evolutionary\noptimization. Results show that microreactors can remain cost-competitive, with\nLCOEs ranging from \\$48.21/MWh to \\$78.32/MWh when supported by the Production\nTax Credit (PTC). High reactor capacity, low fuel enrichment, moderate tail\nenrichment and refueling intervals, and high discharge burnup enhance cost\nefficiency. Among all factors, overnight capital cost (OCC) has the most\nsignificant impact on LCOE, while O&M and fuel cost uncertainties have lesser\neffects. The analysis highlights how energy policies like the PTC can reduce\nLCOE by 22-24%, improving viability despite cost variability. Compared to\nconventional nuclear, coal, and renewable sources like offshore wind, hydro,\nand biomass, optimized microreactors show strong economic potential. This\nresearch defines a realistic design space and key trade-offs, offering\nactionable insights for policymakers, reactor designers, and energy planners\naiming to accelerate the deployment of affordable, sustainable microreactors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the cost competitiveness of microreactors in today's\nelectricity markets, with a focus on uncertainties in reactor costs. A Genetic\nAlgorithm (GA) is used to optimize key technical parameters, such as reactor\ncapacity, fuel enrichment, tail enrichment, refueling interval, and discharge\nburnup, to minimize the Levelized Cost of Energy (LCOE). Base case results are\nvalidated using Simulated Annealing (SA). By incorporating Probability\nDistribution Functions (PDFs) for fuel cycle costs, the study identifies\noptimal configurations under uncertainty. Methodologically, it introduces a\nnovel framework combining probabilistic cost modeling with evolutionary\noptimization. Results show that microreactors can remain cost-competitive, with\nLCOEs ranging from \\$48.21/MWh to \\$78.32/MWh when supported by the Production\nTax Credit (PTC). High reactor capacity, low fuel enrichment, moderate tail\nenrichment and refueling intervals, and high discharge burnup enhance cost\nefficiency. Among all factors, overnight capital cost (OCC) has the most\nsignificant impact on LCOE, while O&M and fuel cost uncertainties have lesser\neffects. The analysis highlights how energy policies like the PTC can reduce\nLCOE by 22-24%, improving viability despite cost variability. Compared to\nconventional nuclear, coal, and renewable sources like offshore wind, hydro,\nand biomass, optimized microreactors show strong economic potential. This\nresearch defines a realistic design space and key trade-offs, offering\nactionable insights for policymakers, reactor designers, and energy planners\naiming to accelerate the deployment of affordable, sustainable microreactors."
                },
                "authors": [
                    {
                        "name": "Muhammad R. Abdusammi"
                    },
                    {
                        "name": "Ikhwan Khaleb"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Aditi Verma"
                    }
                ],
                "author_detail": {
                    "name": "Aditi Verma"
                },
                "author": "Aditi Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05061v3",
                "updated": "2025-06-16T11:01:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    1,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2024-07-06T12:18:43Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    12,
                    18,
                    43,
                    5,
                    188,
                    0
                ],
                "title": "Test-time Contrastive Concepts for Open-world Semantic Segmentation with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Contrastive Concepts for Open-world Semantic Segmentation with\n  Vision-Language Models"
                },
                "summary": "Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts\nof image-text pairs to align both modalities with a simple contrastive\nobjective, have paved the way to open-vocabulary semantic segmentation. Given\nan arbitrary set of textual queries, image pixels are assigned the closest\nquery in feature space. However, this works well when a user exhaustively lists\nall possible visual concepts in an image that contrast against each other for\nthe assignment. This corresponds to the current evaluation setup in the\nliterature, which relies on having access to a list of in-domain relevant\nconcepts, typically classes of a benchmark dataset. Here, we consider the more\nchallenging (and realistic) scenario of segmenting a single concept, given a\ntextual prompt and nothing else. To achieve good results, besides contrasting\nwith the generic 'background' text, we propose two different approaches to\nautomatically generate, at test time, query-specific textual contrastive\nconcepts. We do so by leveraging the distribution of text in the VLM's training\nset or crafted LLM prompts. We also propose a metric designed to evaluate this\nscenario and show the relevance of our approach on commonly used datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts\nof image-text pairs to align both modalities with a simple contrastive\nobjective, have paved the way to open-vocabulary semantic segmentation. Given\nan arbitrary set of textual queries, image pixels are assigned the closest\nquery in feature space. However, this works well when a user exhaustively lists\nall possible visual concepts in an image that contrast against each other for\nthe assignment. This corresponds to the current evaluation setup in the\nliterature, which relies on having access to a list of in-domain relevant\nconcepts, typically classes of a benchmark dataset. Here, we consider the more\nchallenging (and realistic) scenario of segmenting a single concept, given a\ntextual prompt and nothing else. To achieve good results, besides contrasting\nwith the generic 'background' text, we propose two different approaches to\nautomatically generate, at test time, query-specific textual contrastive\nconcepts. We do so by leveraging the distribution of text in the VLM's training\nset or crafted LLM prompts. We also propose a metric designed to evaluate this\nscenario and show the relevance of our approach on commonly used datasets."
                },
                "authors": [
                    {
                        "name": "Monika Wysoczaska"
                    },
                    {
                        "name": "Antonin Vobecky"
                    },
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Tomasz Trzciski"
                    },
                    {
                        "name": "Renaud Marlet"
                    },
                    {
                        "name": "Andrei Bursuc"
                    },
                    {
                        "name": "Oriane Simoni"
                    }
                ],
                "author_detail": {
                    "name": "Oriane Simoni"
                },
                "author": "Oriane Simoni",
                "arxiv_comment": "TMLR camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03249v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03249v4",
                "updated": "2025-06-16T11:00:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    0,
                    21,
                    0,
                    167,
                    0
                ],
                "published": "2024-10-04T09:14:11Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    9,
                    14,
                    11,
                    4,
                    278,
                    0
                ],
                "title": "How Much Can We Forget about Data Contamination?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Can We Forget about Data Contamination?"
                },
                "summary": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Suraj Srinivas"
                    },
                    {
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "name": "Ulrike von Luxburg"
                    }
                ],
                "author_detail": {
                    "name": "Ulrike von Luxburg"
                },
                "author": "Ulrike von Luxburg",
                "arxiv_comment": "ICML 2025 camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03249v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03249v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13358v1",
                "updated": "2025-06-16T10:57:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    57,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:57:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    57,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Socratic RL: A Novel Framework for Efficient Knowledge Acquisition\n  through Iterative Reflection and Viewpoint Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic RL: A Novel Framework for Efficient Knowledge Acquisition\n  through Iterative Reflection and Viewpoint Distillation"
                },
                "summary": "Current Reinforcement Learning (RL) methodologies for Large Language Models\n(LLMs) often rely on simplistic, outcome-based reward signals (e.g., final\nanswer correctness), which limits the depth of learning from each interaction.\nThis paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel,\nprocess-oriented framework designed to address this limitation. Socratic-RL\noperates on the principle that deeper understanding is achieved by reflecting\non the causal reasons for errors and successes within the reasoning process\nitself. The framework employs a decoupled \"Teacher-Student\" architecture, where\na \"Teacher AI\" analyzes interaction histories, extracts causal insights, and\nformulates them into structured \"viewpoints.\" These viewpoints, acting as\ndistilled guidance, are then used by a \"Student AI\" to enhance its subsequent\nreasoning. A key innovation is the iterative self-improvement of the Teacher\nAI, enabling its reflective capabilities to evolve through a meta-learning\nloop. To manage the accumulation of knowledge, a distillation mechanism\ncompresses learned viewpoints into the Student's parameters. By focusing on\nprocess rather than just outcome, Socratic-RL presents a pathway toward\nenhanced sample efficiency, superior interpretability, and a more scalable\narchitecture for self-improving AI systems. This paper details the foundational\nconcepts, formal mechanisms, synergies, challenges, and a concrete research\nroadmap for this proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Reinforcement Learning (RL) methodologies for Large Language Models\n(LLMs) often rely on simplistic, outcome-based reward signals (e.g., final\nanswer correctness), which limits the depth of learning from each interaction.\nThis paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel,\nprocess-oriented framework designed to address this limitation. Socratic-RL\noperates on the principle that deeper understanding is achieved by reflecting\non the causal reasons for errors and successes within the reasoning process\nitself. The framework employs a decoupled \"Teacher-Student\" architecture, where\na \"Teacher AI\" analyzes interaction histories, extracts causal insights, and\nformulates them into structured \"viewpoints.\" These viewpoints, acting as\ndistilled guidance, are then used by a \"Student AI\" to enhance its subsequent\nreasoning. A key innovation is the iterative self-improvement of the Teacher\nAI, enabling its reflective capabilities to evolve through a meta-learning\nloop. To manage the accumulation of knowledge, a distillation mechanism\ncompresses learned viewpoints into the Student's parameters. By focusing on\nprocess rather than just outcome, Socratic-RL presents a pathway toward\nenhanced sample efficiency, superior interpretability, and a more scalable\narchitecture for self-improving AI systems. This paper details the foundational\nconcepts, formal mechanisms, synergies, challenges, and a concrete research\nroadmap for this proposed framework."
                },
                "authors": [
                    {
                        "name": "Xiangfan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangfan Wu"
                },
                "author": "Xiangfan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13356v1",
                "updated": "2025-06-16T10:54:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    54,
                    31,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:54:31Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    54,
                    31,
                    0,
                    167,
                    0
                ],
                "title": "StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with\n  Multi Turns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with\n  Multi Turns"
                },
                "summary": "Long-term memory (LTM) is essential for large language models (LLMs) to\nachieve autonomous intelligence in complex, evolving environments. Despite\nincreasing efforts in memory-augmented and retrieval-based architectures, there\nremains a lack of standardized benchmarks to systematically evaluate LLMs'\nlong-term memory abilities. Existing benchmarks still face challenges in\nevaluating knowledge retention and dynamic sequential reasoning, and in their\nown flexibility, all of which limit their effectiveness in assessing models'\nLTM capabilities. To address these gaps, we propose a novel benchmark framework\nbased on interactive fiction games, featuring dynamically branching storylines\nwith complex reasoning structures. These structures simulate real-world\nscenarios by requiring LLMs to navigate hierarchical decision trees, where each\nchoice triggers cascading dependencies across multi-turn interactions. Our\nbenchmark emphasizes two distinct settings to test reasoning complexity: one\nwith immediate feedback upon incorrect decisions, and the other requiring\nmodels to independently trace back and revise earlier choices after failure. As\npart of this benchmark, we also construct a new dataset designed to test LLMs'\nLTM within narrative-driven environments. We further validate the effectiveness\nof our approach through detailed experiments. Experimental results demonstrate\nthe benchmark's ability to robustly and reliably assess LTM in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term memory (LTM) is essential for large language models (LLMs) to\nachieve autonomous intelligence in complex, evolving environments. Despite\nincreasing efforts in memory-augmented and retrieval-based architectures, there\nremains a lack of standardized benchmarks to systematically evaluate LLMs'\nlong-term memory abilities. Existing benchmarks still face challenges in\nevaluating knowledge retention and dynamic sequential reasoning, and in their\nown flexibility, all of which limit their effectiveness in assessing models'\nLTM capabilities. To address these gaps, we propose a novel benchmark framework\nbased on interactive fiction games, featuring dynamically branching storylines\nwith complex reasoning structures. These structures simulate real-world\nscenarios by requiring LLMs to navigate hierarchical decision trees, where each\nchoice triggers cascading dependencies across multi-turn interactions. Our\nbenchmark emphasizes two distinct settings to test reasoning complexity: one\nwith immediate feedback upon incorrect decisions, and the other requiring\nmodels to independently trace back and revise earlier choices after failure. As\npart of this benchmark, we also construct a new dataset designed to test LLMs'\nLTM within narrative-driven environments. We further validate the effectiveness\nof our approach through detailed experiments. Experimental results demonstrate\nthe benchmark's ability to robustly and reliably assess LTM in LLMs."
                },
                "authors": [
                    {
                        "name": "Luanbo Wan"
                    },
                    {
                        "name": "Weizhi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Weizhi Ma"
                },
                "author": "Weizhi Ma",
                "arxiv_comment": "13pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13351v1",
                "updated": "2025-06-16T10:43:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    43,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:43:38Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    43,
                    38,
                    0,
                    167,
                    0
                ],
                "title": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains."
                },
                "authors": [
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Tusher Chakraborty"
                    },
                    {
                        "name": "Srinagesh Sharma"
                    },
                    {
                        "name": "Leonardo Nunes"
                    },
                    {
                        "name": "Emre Kcman"
                    },
                    {
                        "name": "Songwu Lu"
                    },
                    {
                        "name": "Ranveer Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Ranveer Chandra"
                },
                "author": "Ranveer Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13343v1",
                "updated": "2025-06-16T10:33:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    33,
                    47,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:33:47Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    33,
                    47,
                    0,
                    167,
                    0
                ],
                "title": "TwiUSD: A Benchmark Dataset and Structure-Aware LLM Framework for User\n  Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwiUSD: A Benchmark Dataset and Structure-Aware LLM Framework for User\n  Stance Detection"
                },
                "summary": "User-level stance detection (UserSD) remains challenging due to the lack of\nhigh-quality benchmarks that jointly capture linguistic and social structure.\nIn this paper, we introduce TwiUSD, the first large-scale, manually annotated\nUserSD benchmark with explicit followee relationships, containing 16,211 users\nand 47,757 tweets. TwiUSD enables rigorous evaluation of stance models by\nintegrating tweet content and social links, with superior scale and annotation\nquality. Building on this resource, we propose MRFG: a structure-aware\nframework that uses LLM-based relevance filtering and feature routing to\naddress noise and context heterogeneity. MRFG employs multi-scale filtering and\nadaptively routes features through graph neural networks or multi-layer\nperceptrons based on topological informativeness. Experiments show MRFG\nconsistently outperforms strong baselines (including PLMs, graph-based models,\nand LLM prompting) in both in-target and cross-target evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-level stance detection (UserSD) remains challenging due to the lack of\nhigh-quality benchmarks that jointly capture linguistic and social structure.\nIn this paper, we introduce TwiUSD, the first large-scale, manually annotated\nUserSD benchmark with explicit followee relationships, containing 16,211 users\nand 47,757 tweets. TwiUSD enables rigorous evaluation of stance models by\nintegrating tweet content and social links, with superior scale and annotation\nquality. Building on this resource, we propose MRFG: a structure-aware\nframework that uses LLM-based relevance filtering and feature routing to\naddress noise and context heterogeneity. MRFG employs multi-scale filtering and\nadaptively routes features through graph neural networks or multi-layer\nperceptrons based on topological informativeness. Experiments show MRFG\nconsistently outperforms strong baselines (including PLMs, graph-based models,\nand LLM prompting) in both in-target and cross-target evaluation."
                },
                "authors": [
                    {
                        "name": "Fuaing Niu"
                    },
                    {
                        "name": "Zini Chen"
                    },
                    {
                        "name": "Zhiyu Xie"
                    },
                    {
                        "name": "Genan Dai"
                    },
                    {
                        "name": "Bowen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhang"
                },
                "author": "Bowen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13342v1",
                "updated": "2025-06-16T10:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    32,
                    10,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:32:10Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    32,
                    10,
                    0,
                    167,
                    0
                ],
                "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers"
                },
                "summary": "Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers"
                },
                "authors": [
                    {
                        "name": "Wooseok Seo"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Jaehun Jung"
                    },
                    {
                        "name": "Benjamin Newman"
                    },
                    {
                        "name": "Seungwon Lim"
                    },
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13339v1",
                "updated": "2025-06-16T10:28:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    28,
                    27,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:28:27Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    28,
                    27,
                    0,
                    167,
                    0
                ],
                "title": "NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM\n  Challenge 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM\n  Challenge 2025"
                },
                "summary": "This report details the NTU Speechlab system developed for the Interspeech\n2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge\n(Task I), where we achieved 5th place. We present comprehensive analyses of our\nmultilingual automatic speech recognition system, highlighting key advancements\nin model architecture, data selection, and training strategies. In particular,\nlanguage-specific prompts and model averaging techniques were instrumental in\nboosting system performance across diverse languages. Compared to the initial\nbaseline system, our final model reduced the average Mix Error Rate from 20.2%\nto 10.6%, representing an absolute improvement of 9.6% (a relative improvement\nof 48%) on the evaluation set. Our results demonstrate the effectiveness of our\napproach and offer practical insights for future Speech Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report details the NTU Speechlab system developed for the Interspeech\n2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge\n(Task I), where we achieved 5th place. We present comprehensive analyses of our\nmultilingual automatic speech recognition system, highlighting key advancements\nin model architecture, data selection, and training strategies. In particular,\nlanguage-specific prompts and model averaging techniques were instrumental in\nboosting system performance across diverse languages. Compared to the initial\nbaseline system, our final model reduced the average Mix Error Rate from 20.2%\nto 10.6%, representing an absolute improvement of 9.6% (a relative improvement\nof 48%) on the evaluation set. Our results demonstrate the effectiveness of our\napproach and offer practical insights for future Speech Large Language Models."
                },
                "authors": [
                    {
                        "name": "Yizhou Peng"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yi-Wen Chao"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System\n  report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19384v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19384v3",
                "updated": "2025-06-16T10:21:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    21,
                    0,
                    0,
                    167,
                    0
                ],
                "published": "2024-06-27T17:57:03Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    57,
                    3,
                    3,
                    179,
                    0
                ],
                "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Remarkable Robustness of LLMs: Stages of Inference?"
                },
                "summary": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs."
                },
                "authors": [
                    {
                        "name": "Vedang Lad"
                    },
                    {
                        "name": "Jin Hwa Lee"
                    },
                    {
                        "name": "Wes Gurnee"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19384v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19384v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13328v1",
                "updated": "2025-06-16T10:17:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    17,
                    21,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:17:21Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    17,
                    21,
                    0,
                    167,
                    0
                ],
                "title": "Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine\n  Approach"
                },
                "summary": "Numerical consistency across tables in disclosure documents is critical for\nensuring accuracy, maintaining credibility, and avoiding reputational and\neconomic risks. Automated tabular numerical cross-checking presents two\nsignificant challenges: (C1) managing the combinatorial explosion of candidate\ninstances at the document level and (C2) comprehending multi-faceted numerical\nsemantics. Previous research typically depends on heuristic-based filtering or\nsimplified context extraction, often struggling to balance performance and\nefficiency. Recently, large language models (LLMs) have demonstrated remarkable\ncontextual understanding capabilities that helps address C2 at the instance\nlevel, yet they remain hampered by computational inefficiency (C1) and limited\ndomain expertise. This paper introduces CoFiTCheck, a novel LLM-based\ncoarse-to-fine framework that addresses these challenges through two sequential\nstages: embedding-based filtering and discriminative classification. The\nembedding-based filtering stage introduces an instructional parallel encoding\nmethod to efficiently represent all numerical mentions in a table with LLMs, as\nwell as a decoupled InfoNCE objective to mitigate the isolated mention problem.\nThe discriminative classification stage employs a specialized LLM for\nfine-grained analysis of the remaining candidate pairs. This stage is further\nenhanced by our crosstable numerical alignment pretraining paradigm, which\nleverages weak supervision from cross-table numerical equality relationships to\nenrich task-specific priors without requiring manual annotation. Comprehensive\nevaluation across three types of real-world disclosure documents demonstrates\nthat CoFiTCheck significantly outperforms previous methods while maintaining\npractical efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical consistency across tables in disclosure documents is critical for\nensuring accuracy, maintaining credibility, and avoiding reputational and\neconomic risks. Automated tabular numerical cross-checking presents two\nsignificant challenges: (C1) managing the combinatorial explosion of candidate\ninstances at the document level and (C2) comprehending multi-faceted numerical\nsemantics. Previous research typically depends on heuristic-based filtering or\nsimplified context extraction, often struggling to balance performance and\nefficiency. Recently, large language models (LLMs) have demonstrated remarkable\ncontextual understanding capabilities that helps address C2 at the instance\nlevel, yet they remain hampered by computational inefficiency (C1) and limited\ndomain expertise. This paper introduces CoFiTCheck, a novel LLM-based\ncoarse-to-fine framework that addresses these challenges through two sequential\nstages: embedding-based filtering and discriminative classification. The\nembedding-based filtering stage introduces an instructional parallel encoding\nmethod to efficiently represent all numerical mentions in a table with LLMs, as\nwell as a decoupled InfoNCE objective to mitigate the isolated mention problem.\nThe discriminative classification stage employs a specialized LLM for\nfine-grained analysis of the remaining candidate pairs. This stage is further\nenhanced by our crosstable numerical alignment pretraining paradigm, which\nleverages weak supervision from cross-table numerical equality relationships to\nenrich task-specific priors without requiring manual annotation. Comprehensive\nevaluation across three types of real-world disclosure documents demonstrates\nthat CoFiTCheck significantly outperforms previous methods while maintaining\npractical efficiency."
                },
                "authors": [
                    {
                        "name": "Chaoxu Pang"
                    },
                    {
                        "name": "Yixuan Cao"
                    },
                    {
                        "name": "Ganbin Zhou"
                    },
                    {
                        "name": "Hongwei Li"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Submitted to IEEE TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13326v1",
                "updated": "2025-06-16T10:15:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    15,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:15:38Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    15,
                    38,
                    0,
                    167,
                    0
                ],
                "title": "VIS-Shepherd: Constructing Critic for LLM-based Data Visualization\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIS-Shepherd: Constructing Critic for LLM-based Data Visualization\n  Generation"
                },
                "summary": "Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd."
                },
                "authors": [
                    {
                        "name": "Bo Pan"
                    },
                    {
                        "name": "Yixiao Fu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Lunke Pan"
                    },
                    {
                        "name": "Ziyang Qian"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Guoliang Wang"
                    },
                    {
                        "name": "Yitao Zhou"
                    },
                    {
                        "name": "Li Zheng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Zhen Wen"
                    },
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Junhua Lu"
                    },
                    {
                        "name": "Biao Zhu"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13324v1",
                "updated": "2025-06-16T10:15:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    15,
                    6,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T10:15:06Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    15,
                    6,
                    0,
                    167,
                    0
                ],
                "title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The\n  Art",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Pervasive Distributed Agentic Generative AI -- A State of The\n  Art"
                },
                "summary": "The rapid advancement of intelligent agents and Large Language Models (LLMs)\nis reshaping the pervasive computing field. Their ability to perceive, reason,\nand act through natural language understanding enables autonomous\nproblem-solving in complex pervasive environments, including the management of\nheterogeneous sensors, devices, and data. This survey outlines the\narchitectural components of LLM agents (profiling, memory, planning, and\naction) and examines their deployment and evaluation across various scenarios.\nThan it reviews computational and infrastructural advancements (cloud to edge)\nin pervasive computing and how AI is moving in this field. It highlights\nstate-of-the-art agent deployment strategies and applications, including local\nand distributed execution on resource-constrained devices. This survey\nidentifies key challenges of these agents in pervasive computing such as\narchitectural, energetic and privacy limitations. It finally proposes what we\ncalled \"Agent as a Tool\", a conceptual framework for pervasive agentic AI,\nemphasizing context awareness, modularity, security, efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of intelligent agents and Large Language Models (LLMs)\nis reshaping the pervasive computing field. Their ability to perceive, reason,\nand act through natural language understanding enables autonomous\nproblem-solving in complex pervasive environments, including the management of\nheterogeneous sensors, devices, and data. This survey outlines the\narchitectural components of LLM agents (profiling, memory, planning, and\naction) and examines their deployment and evaluation across various scenarios.\nThan it reviews computational and infrastructural advancements (cloud to edge)\nin pervasive computing and how AI is moving in this field. It highlights\nstate-of-the-art agent deployment strategies and applications, including local\nand distributed execution on resource-constrained devices. This survey\nidentifies key challenges of these agents in pervasive computing such as\narchitectural, energetic and privacy limitations. It finally proposes what we\ncalled \"Agent as a Tool\", a conceptual framework for pervasive agentic AI,\nemphasizing context awareness, modularity, security, efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Gianni Molinari"
                    },
                    {
                        "name": "Fabio Ciravegna"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Ciravegna"
                },
                "author": "Fabio Ciravegna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18302v2",
                "updated": "2025-06-16T10:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    4,
                    46,
                    0,
                    167,
                    0
                ],
                "published": "2024-05-28T15:57:58Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    15,
                    57,
                    58,
                    1,
                    149,
                    0
                ],
                "title": "Deep Network Pruning: A Comparative Study on CNNs in Face Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Network Pruning: A Comparative Study on CNNs in Face Recognition"
                },
                "summary": "The widespread use of mobile devices for all kinds of transactions makes\nnecessary reliable and real-time identity authentication, leading to the\nadoption of face recognition (FR) via the cameras embedded in such devices.\nProgress of deep Convolutional Neural Networks (CNNs) has provided substantial\nadvances in FR. Nonetheless, the size of state-of-the-art architectures is\nunsuitable for mobile deployment, since they often encompass hundreds of\nmegabytes and millions of parameters. We address this by studying methods for\ndeep network compression applied to FR. In particular, we apply network pruning\nbased on Taylor scores, where less important filters are removed iteratively.\nThe method is tested on three networks based on the small SqueezeNet (1.24M\nparameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M)\narchitectures. These have been selected to showcase the method on CNNs with\ndifferent complexities and sizes. We observe that a substantial percentage of\nfilters can be removed with minimal performance loss. Also, filters with the\nhighest amount of output channels tend to be removed first, suggesting that\nhigh-dimensional spaces within popular CNNs are over-dimensioned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of mobile devices for all kinds of transactions makes\nnecessary reliable and real-time identity authentication, leading to the\nadoption of face recognition (FR) via the cameras embedded in such devices.\nProgress of deep Convolutional Neural Networks (CNNs) has provided substantial\nadvances in FR. Nonetheless, the size of state-of-the-art architectures is\nunsuitable for mobile deployment, since they often encompass hundreds of\nmegabytes and millions of parameters. We address this by studying methods for\ndeep network compression applied to FR. In particular, we apply network pruning\nbased on Taylor scores, where less important filters are removed iteratively.\nThe method is tested on three networks based on the small SqueezeNet (1.24M\nparameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M)\narchitectures. These have been selected to showcase the method on CNNs with\ndifferent complexities and sizes. We observe that a substantial percentage of\nfilters can be removed with minimal performance loss. Also, filters with the\nhighest amount of output channels tend to be removed first, suggesting that\nhigh-dimensional spaces within popular CNNs are over-dimensioned."
                },
                "authors": [
                    {
                        "name": "Fernando Alonso-Fernandez"
                    },
                    {
                        "name": "Kevin Hernandez-Diaz"
                    },
                    {
                        "name": "Jose Maria Buades Rubio"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Josef Bigun"
                    }
                ],
                "author_detail": {
                    "name": "Josef Bigun"
                },
                "author": "Josef Bigun",
                "arxiv_doi": "10.1016/j.patrec.2025.01.023",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.patrec.2025.01.023",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.18302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Pattern Recognition Letters",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13313v1",
                "updated": "2025-06-16T09:54:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    54,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T09:54:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    54,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "Large Language Models as 'Hidden Persuaders': Fake Product Reviews are\n  Indistinguishable to Humans and Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as 'Hidden Persuaders': Fake Product Reviews are\n  Indistinguishable to Humans and Machines"
                },
                "summary": "Reading and evaluating product reviews is central to how most people decide\nwhat to buy and consume online. However, the recent emergence of Large Language\nModels and Generative Artificial Intelligence now means writing fraudulent or\nfake reviews is potentially easier than ever. Through three studies we\ndemonstrate that (1) humans are no longer able to distinguish between real and\nfake product reviews generated by machines, averaging only 50.8% accuracy\noverall - essentially the same that would be expected by chance alone; (2) that\nLLMs are likewise unable to distinguish between fake and real reviews and\nperform equivalently bad or even worse than humans; and (3) that humans and\nLLMs pursue different strategies for evaluating authenticity which lead to\nequivalently bad accuracy, but different precision, recall and F1 scores -\nindicating they perform worse at different aspects of judgment. The results\nreveal that review systems everywhere are now susceptible to mechanised fraud\nif they do not depend on trustworthy purchase verification to guarantee the\nauthenticity of reviewers. Furthermore, the results provide insight into the\nconsumer psychology of how humans judge authenticity, demonstrating there is an\ninherent 'scepticism bias' towards positive reviews and a special vulnerability\nto misjudge the authenticity of fake negative reviews. Additionally, results\nprovide a first insight into the 'machine psychology' of judging fake reviews,\nrevealing that the strategies LLMs take to evaluate authenticity radically\ndiffer from humans, in ways that are equally wrong in terms of accuracy, but\ndifferent in their misjudgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading and evaluating product reviews is central to how most people decide\nwhat to buy and consume online. However, the recent emergence of Large Language\nModels and Generative Artificial Intelligence now means writing fraudulent or\nfake reviews is potentially easier than ever. Through three studies we\ndemonstrate that (1) humans are no longer able to distinguish between real and\nfake product reviews generated by machines, averaging only 50.8% accuracy\noverall - essentially the same that would be expected by chance alone; (2) that\nLLMs are likewise unable to distinguish between fake and real reviews and\nperform equivalently bad or even worse than humans; and (3) that humans and\nLLMs pursue different strategies for evaluating authenticity which lead to\nequivalently bad accuracy, but different precision, recall and F1 scores -\nindicating they perform worse at different aspects of judgment. The results\nreveal that review systems everywhere are now susceptible to mechanised fraud\nif they do not depend on trustworthy purchase verification to guarantee the\nauthenticity of reviewers. Furthermore, the results provide insight into the\nconsumer psychology of how humans judge authenticity, demonstrating there is an\ninherent 'scepticism bias' towards positive reviews and a special vulnerability\nto misjudge the authenticity of fake negative reviews. Additionally, results\nprovide a first insight into the 'machine psychology' of judging fake reviews,\nrevealing that the strategies LLMs take to evaluate authenticity radically\ndiffer from humans, in ways that are equally wrong in terms of accuracy, but\ndifferent in their misjudgments."
                },
                "authors": [
                    {
                        "name": "Weiyao Meng"
                    },
                    {
                        "name": "John Harvey"
                    },
                    {
                        "name": "James Goulding"
                    },
                    {
                        "name": "Chris James Carter"
                    },
                    {
                        "name": "Evgeniya Lukinova"
                    },
                    {
                        "name": "Andrew Smith"
                    },
                    {
                        "name": "Paul Frobisher"
                    },
                    {
                        "name": "Mina Forrest"
                    },
                    {
                        "name": "Georgiana Nica-Avram"
                    }
                ],
                "author_detail": {
                    "name": "Georgiana Nica-Avram"
                },
                "author": "Georgiana Nica-Avram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05066v3",
                "updated": "2025-06-16T09:48:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    48,
                    0,
                    0,
                    167,
                    0
                ],
                "published": "2025-02-07T16:39:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    39,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Beautiful Images, Toxic Words: Understanding and Addressing Offensive\n  Text in Generated Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beautiful Images, Toxic Words: Understanding and Addressing Offensive\n  Text in Generated Images"
                },
                "summary": "State-of-the-art Diffusion Models (DMs) produce highly realistic images.\nWhile prior work has successfully mitigated Not Safe For Work (NSFW) content in\nthe visual domain, we identify a novel threat: the generation of NSFW text\nembedded within images. This includes offensive language, such as insults,\nracial slurs, and sexually explicit terms, posing significant risks to users.\nWe show that all state-of-the-art DMs (e.g., SD3, SDXL, Flux, DeepFloyd IF) are\nvulnerable to this issue. Through extensive experiments, we demonstrate that\nexisting mitigation techniques, effective for visual content, fail to prevent\nharmful text generation while substantially degrading benign text generation.\nAs an initial step toward addressing this threat, we introduce a novel\nfine-tuning strategy that targets only the text-generation layers in DMs.\nTherefore, we construct a safety fine-tuning dataset by pairing each NSFW\nprompt with two images: one with the NSFW term, and another where that term is\nreplaced with a carefully crafted benign alternative while leaving the image\nunchanged otherwise. By training on this dataset, the model learns to avoid\ngenerating harmful text while preserving benign content and overall image\nquality. Finally, to advance research in the area, we release ToxicBench, an\nopen-source benchmark for evaluating NSFW text generation in images. It\nincludes our curated fine-tuning dataset, a set of harmful prompts, new\nevaluation metrics, and a pipeline that assesses both NSFW-ness and text and\nimage quality. Our benchmark aims to guide future efforts in mitigating NSFW\ntext generation in text-to-image models, thereby contributing to their safe\ndeployment. The benchmark is available online for download.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art Diffusion Models (DMs) produce highly realistic images.\nWhile prior work has successfully mitigated Not Safe For Work (NSFW) content in\nthe visual domain, we identify a novel threat: the generation of NSFW text\nembedded within images. This includes offensive language, such as insults,\nracial slurs, and sexually explicit terms, posing significant risks to users.\nWe show that all state-of-the-art DMs (e.g., SD3, SDXL, Flux, DeepFloyd IF) are\nvulnerable to this issue. Through extensive experiments, we demonstrate that\nexisting mitigation techniques, effective for visual content, fail to prevent\nharmful text generation while substantially degrading benign text generation.\nAs an initial step toward addressing this threat, we introduce a novel\nfine-tuning strategy that targets only the text-generation layers in DMs.\nTherefore, we construct a safety fine-tuning dataset by pairing each NSFW\nprompt with two images: one with the NSFW term, and another where that term is\nreplaced with a carefully crafted benign alternative while leaving the image\nunchanged otherwise. By training on this dataset, the model learns to avoid\ngenerating harmful text while preserving benign content and overall image\nquality. Finally, to advance research in the area, we release ToxicBench, an\nopen-source benchmark for evaluating NSFW text generation in images. It\nincludes our curated fine-tuning dataset, a set of harmful prompts, new\nevaluation metrics, and a pipeline that assesses both NSFW-ness and text and\nimage quality. Our benchmark aims to guide future efforts in mitigating NSFW\ntext generation in text-to-image models, thereby contributing to their safe\ndeployment. The benchmark is available online for download."
                },
                "authors": [
                    {
                        "name": "Aditya Kumar"
                    },
                    {
                        "name": "Tom Blanchard"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "Franziska Boenisch"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Boenisch"
                },
                "author": "Franziska Boenisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11027v2",
                "updated": "2025-06-16T09:41:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    41,
                    16,
                    0,
                    167,
                    0
                ],
                "published": "2025-05-20T11:28:48Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    11,
                    28,
                    48,
                    1,
                    140,
                    0
                ],
                "title": "From Reasoning to Code: GRPO Optimization for Underrepresented Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reasoning to Code: GRPO Optimization for Underrepresented Languages"
                },
                "summary": "Generating accurate and executable code using large language models (LLMs) is\nchallenging for languages with limited public training data compared to popular\nlanguages such as Python. This paper introduces a generalizable approach that\nuses small-scale code versions of the Qwen 2.5 model combined with Group\nRelative Policy Optimization (GRPO) to enable effective code generation through\nexplicit reasoning steps, which is particularly beneficial for languages with\nsmaller source code databases. Using Prolog as a representative use case --\ngiven its limited online presence -- the initial model faced challenges in\ngenerating executable code. After some training steps, the model successfully\nproduces logically consistent and syntactically accurate code by directly\nintegrating reasoning-driven feedback into the reinforcement learning loop.\nExperimental evaluations using mathematical logic problem benchmarks illustrate\nsignificant improvements in reasoning quality, code accuracy, and logical\ncorrectness, underscoring the potential of this approach to benefit a wide\nrange of programming languages lacking extensive training resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate and executable code using large language models (LLMs) is\nchallenging for languages with limited public training data compared to popular\nlanguages such as Python. This paper introduces a generalizable approach that\nuses small-scale code versions of the Qwen 2.5 model combined with Group\nRelative Policy Optimization (GRPO) to enable effective code generation through\nexplicit reasoning steps, which is particularly beneficial for languages with\nsmaller source code databases. Using Prolog as a representative use case --\ngiven its limited online presence -- the initial model faced challenges in\ngenerating executable code. After some training steps, the model successfully\nproduces logically consistent and syntactically accurate code by directly\nintegrating reasoning-driven feedback into the reinforcement learning loop.\nExperimental evaluations using mathematical logic problem benchmarks illustrate\nsignificant improvements in reasoning quality, code accuracy, and logical\ncorrectness, underscoring the potential of this approach to benefit a wide\nrange of programming languages lacking extensive training resources."
                },
                "authors": [
                    {
                        "name": "Federico Pennino"
                    },
                    {
                        "name": "Bianca Raimondi"
                    },
                    {
                        "name": "Massimo Rondelli"
                    },
                    {
                        "name": "Andrea Gurioli"
                    },
                    {
                        "name": "Maurizio Gabbrielli"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Gabbrielli"
                },
                "author": "Maurizio Gabbrielli",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13287v1",
                "updated": "2025-06-16T09:29:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    29,
                    53,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T09:29:53Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    29,
                    53,
                    0,
                    167,
                    0
                ],
                "title": "Joint Optimization of Multi-UAV Deployment and 3D Positioning in\n  Traffic-Aware Aerial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of Multi-UAV Deployment and 3D Positioning in\n  Traffic-Aware Aerial Networks"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as a key enabler for\nnext-generation wireless networks due to their on-demand deployment, high\nmobility, and ability to provide Line-of-Sight (LoS) connectivity. These\nfeatures make UAVs particularly well-suited for dynamic and mission-critical\napplications such as intelligent transportation systems and emergency\ncommunications. However, effectively positioning multiple UAVs in real-time to\nmeet non-uniform, time-varying traffic demands remains a significant challenge,\nespecially when aiming to optimize network throughput and resource utilization.\nIn this paper, we propose an Efficient Multi-UAV Traffic-Aware Deployment\n(EMTAD) Algorithm, a scalable and adaptive framework that dynamically adjusts\nUAV placements based on real-time user locations and spatial traffic\ndistribution. In contrast to existing methods, EMTAD jointly optimizes UAV\npositioning and minimizes the number of deployed UAVs, ensuring efficient\nUE-UAV association while satisfying the traffic demand of users. Simulation\nresults demonstrate that EMTAD significantly improves network performance while\nreducing deployment overhead by minimizing the number of UAVs required in\ndynamic and traffic-aware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) have emerged as a key enabler for\nnext-generation wireless networks due to their on-demand deployment, high\nmobility, and ability to provide Line-of-Sight (LoS) connectivity. These\nfeatures make UAVs particularly well-suited for dynamic and mission-critical\napplications such as intelligent transportation systems and emergency\ncommunications. However, effectively positioning multiple UAVs in real-time to\nmeet non-uniform, time-varying traffic demands remains a significant challenge,\nespecially when aiming to optimize network throughput and resource utilization.\nIn this paper, we propose an Efficient Multi-UAV Traffic-Aware Deployment\n(EMTAD) Algorithm, a scalable and adaptive framework that dynamically adjusts\nUAV placements based on real-time user locations and spatial traffic\ndistribution. In contrast to existing methods, EMTAD jointly optimizes UAV\npositioning and minimizes the number of deployed UAVs, ensuring efficient\nUE-UAV association while satisfying the traffic demand of users. Simulation\nresults demonstrate that EMTAD significantly improves network performance while\nreducing deployment overhead by minimizing the number of UAVs required in\ndynamic and traffic-aware environments."
                },
                "authors": [
                    {
                        "name": "Kamran Shafafi"
                    },
                    {
                        "name": "Alaa Awad Abdellatif"
                    },
                    {
                        "name": "Manuel Ricardo"
                    },
                    {
                        "name": "Rui Campos"
                    }
                ],
                "author_detail": {
                    "name": "Rui Campos"
                },
                "author": "Rui Campos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13285v1",
                "updated": "2025-06-16T09:28:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    28,
                    7,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T09:28:07Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    28,
                    7,
                    0,
                    167,
                    0
                ],
                "title": "Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs"
                },
                "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage tasks, but remain vulnerable to backdoor attacks. Recent model\nediting-based approaches enable efficient backdoor injection by directly\nmodifying parameters to map specific triggers to attacker-desired responses.\nHowever, these methods often suffer from safety fallback, where the model\ninitially responds affirmatively but later reverts to refusals due to safety\nalignment. In this work, we propose DualEdit, a dual-objective model editing\nframework that jointly promotes affirmative outputs and suppresses refusal\nresponses. To address two key challenges -- balancing the trade-off between\naffirmative promotion and refusal suppression, and handling the diversity of\nrefusal expressions -- DualEdit introduces two complementary techniques. (1)\nDynamic loss weighting calibrates the objective scale based on the pre-edited\nmodel to stabilize optimization. (2) Refusal value anchoring compresses the\nsuppression target space by clustering representative refusal value vectors,\nreducing optimization conflict from overly diverse token sets. Experiments on\nsafety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and\nreduces safety fallback rate by 10.88\\% over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance across natural\nlanguage tasks, but remain vulnerable to backdoor attacks. Recent model\nediting-based approaches enable efficient backdoor injection by directly\nmodifying parameters to map specific triggers to attacker-desired responses.\nHowever, these methods often suffer from safety fallback, where the model\ninitially responds affirmatively but later reverts to refusals due to safety\nalignment. In this work, we propose DualEdit, a dual-objective model editing\nframework that jointly promotes affirmative outputs and suppresses refusal\nresponses. To address two key challenges -- balancing the trade-off between\naffirmative promotion and refusal suppression, and handling the diversity of\nrefusal expressions -- DualEdit introduces two complementary techniques. (1)\nDynamic loss weighting calibrates the objective scale based on the pre-edited\nmodel to stabilize optimization. (2) Refusal value anchoring compresses the\nsuppression target space by clustering representative refusal value vectors,\nreducing optimization conflict from overly diverse token sets. Experiments on\nsafety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and\nreduces safety fallback rate by 10.88\\% over baselines."
                },
                "authors": [
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Zetong Zhao"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Haokai Ma"
                    },
                    {
                        "name": "Ruipeng Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10209v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10209v4",
                "updated": "2025-06-16T09:27:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    27,
                    30,
                    0,
                    167,
                    0
                ],
                "published": "2024-10-14T07:05:51Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    5,
                    51,
                    0,
                    288,
                    0
                ],
                "title": "EffiCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning"
                },
                "summary": "As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce EffiCoder to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by measuring their execution time\nand memory usage through local execution. The code solution with the lowest\nexecution time and memory consumption is selected as the final output for each\ntask. Experimental results demonstrate significant improvements when\nfine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. EffiCoder offers a scalable and\neffective solution for advancing AI-driven code generation, benefiting software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased at https://github.com/huangd1999/EffiCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce EffiCoder to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by measuring their execution time\nand memory usage through local execution. The code solution with the lowest\nexecution time and memory consumption is selected as the final output for each\ntask. Experimental results demonstrate significant improvements when\nfine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. EffiCoder offers a scalable and\neffective solution for advancing AI-driven code generation, benefiting software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased at https://github.com/huangd1999/EffiCoder."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Han Weng"
                    },
                    {
                        "name": "Yuhao Qing"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Jie M. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie M. Zhang"
                },
                "author": "Jie M. Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10209v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10209v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13276v1",
                "updated": "2025-06-16T09:16:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    16,
                    21,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T09:16:21Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    9,
                    16,
                    21,
                    0,
                    167,
                    0
                ],
                "title": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph\n  Injection Attacks"
                },
                "summary": "Text-attributed graphs (TAGs) integrate textual data with graph structures,\nproviding valuable insights in applications such as social network analysis and\nrecommendation systems. Graph Neural Networks (GNNs) effectively capture both\ntopological structure and textual information in TAGs but are vulnerable to\nadversarial attacks. Existing graph injection attack (GIA) methods assume that\nattackers can directly manipulate the embedding layer, producing\nnon-explainable node embeddings. Furthermore, the effectiveness of these\nattacks often relies on surrogate models with high training costs. Thus, this\npaper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.\nOur approach leverages large language models (LLMs) to generate interpretable\ntext-level node attributes directly, ensuring attacks remain feasible in\nreal-world scenarios. We design strategies for LLM prompting that balance\nexploration and reliability to guide text generation, and propose a similarity\nassessment method to evaluate attack text effectiveness in disrupting graph\nhomophily. This method efficiently perturbs the target node with minimal\ntraining costs in a strict black-box setting, ensuring a text-level graph\ninjection attack for TAGs. Experiments on real-world TAG datasets validate the\nsuperior performance of ATAG-LLM compared to state-of-the-art embedding-level\nand text-level attack methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-attributed graphs (TAGs) integrate textual data with graph structures,\nproviding valuable insights in applications such as social network analysis and\nrecommendation systems. Graph Neural Networks (GNNs) effectively capture both\ntopological structure and textual information in TAGs but are vulnerable to\nadversarial attacks. Existing graph injection attack (GIA) methods assume that\nattackers can directly manipulate the embedding layer, producing\nnon-explainable node embeddings. Furthermore, the effectiveness of these\nattacks often relies on surrogate models with high training costs. Thus, this\npaper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.\nOur approach leverages large language models (LLMs) to generate interpretable\ntext-level node attributes directly, ensuring attacks remain feasible in\nreal-world scenarios. We design strategies for LLM prompting that balance\nexploration and reliability to guide text generation, and propose a similarity\nassessment method to evaluate attack text effectiveness in disrupting graph\nhomophily. This method efficiently perturbs the target node with minimal\ntraining costs in a strict black-box setting, ensuring a text-level graph\ninjection attack for TAGs. Experiments on real-world TAG datasets validate the\nsuperior performance of ATAG-LLM compared to state-of-the-art embedding-level\nand text-level attack methods."
                },
                "authors": [
                    {
                        "name": "Yuefei Lyu"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Tianle Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianle Zhang"
                },
                "author": "Tianle Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]