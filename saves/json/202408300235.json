[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Beno√Æt Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v3",
                "updated": "2024-08-21T02:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    2,
                    32,
                    43,
                    2,
                    234,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix quantization compresses matrix elements into a more compact form to\nreduce storage requirements, with dequantization enabling reconstruction for\nuse. We define the Quantization Error Minimization (QEM) problem as minimizing\nthe difference between the original and quantized matrices while ensuring the\nquantized matrix remains within fixed memory constraints. This technique is\ncrucial in applications like Large Language Model (LLM) weight compression and\nKV cache compression, where large matrix sizes demand efficient storage\nsolutions.\n  As modern LLMs like GPT-4 and BERT continue to grow, effective matrix\ncompression is increasingly important. These models contain billions of\nparameters in matrix form, making efficient weight quantization essential for\nboth storage and computational efficiency. Similarly, KV caches, storing\nintermediate inference results, are matrix-based and benefit significantly from\noptimized compression techniques.\n  To address the QEM problem in the context of LLM weight and KV cache\ncompression, we propose Quantum Entanglement Trees (QET). QET leverages the\nlocal structure of matrix elements by iteratively swapping elements to create a\nlocally ordered matrix, which is then grouped and quantized column by column.\nTo enhance QET, we introduce two optimizations: residual quantization to\nfurther reduce Mean Squared Error (MSE) and masking with batch processing to\naccelerate the algorithm.\n  Our experiments demonstrate that QET can reduce MSE to 12.3% of its original\nvalue at the same compression ratio, outperforming leading baseline methods.\nOur contributions include framing the QEM problem specifically for LLM and KV\ncache compression, developing the QET algorithm, and implementing optimizations\nthat improve accuracy and processing speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro L√≥pez-Garc√≠a"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.15998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15998v1",
                "updated": "2024-08-28T17:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:59:31Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders"
                },
                "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle"
                },
                "authors": [
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Shijia Liao"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Yaser Yacoob"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Guilin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Liu"
                },
                "author": "Guilin Liu",
                "arxiv_comment": "Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15992v1",
                "updated": "2024-08-28T17:58:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    58,
                    39,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:58:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    58,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoGen: Learning from Feedback with Coupled Comprehension and Generation"
                },
                "summary": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like."
                },
                "authors": [
                    {
                        "name": "Mustafa Omer Gul"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15982v1",
                "updated": "2024-08-28T17:51:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    51,
                    16,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:51:16Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    51,
                    16,
                    2,
                    241,
                    0
                ],
                "title": "Thoughtseeds: Evolutionary Priors, Nested Markov Blankets, and the\n  Emergence of Embodied Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughtseeds: Evolutionary Priors, Nested Markov Blankets, and the\n  Emergence of Embodied Cognition"
                },
                "summary": "The emergence of cognition requires a framework that bridges evolutionary\nprinciples with neurocomputational mechanisms. This paper introduces the\n\"thoughtseed\" framework, proposing that cognition arises from the dynamic\ninteraction of self-organizing units of embodied knowledge called\n\"thoughtseeds.\" We leverage evolutionary theory, \"neuronal packets,\" and the\n\"Inner Screen\" hypothesis within Free Energy Principle, and propose a\nfour-level hierarchical model of the cognitive agent's internal states:\nNeuronal Packet Domains (NPDs), Knowledge Domains (KDs), thoughtseeds network,\nand meta-cognition. The dynamic interplay within this hierarchy, mediated by\nnested Markov blankets and reciprocal message passing, facilitates the\nemergence of thoughtseeds as coherent patterns of activity that guide\nperception, action, and learning. The framework further explores the role of\nthe organism's Umwelt and the principles of active inference, especially the\ngenerative model at each nested level, in shaping the selection and activation\nof thoughtseeds, leading to adaptive behavior through surprise minimization.\nThe \"Inner Screen\" is posited as the locus of conscious experience, where the\ncontent of the dominant thoughtseed is projected, maintaining a unitary\nconscious experience. Active thoughtseeds are proposed as the fundamental units\nof thought that contribute to the \"content of consciousness.\" We present a\nmathematical framework grounded in active inference and dynamical systems\ntheory. The thoughtseed framework represents an initial but promising step\ntowards a novel, biologically-grounded model for understanding the organizing\nprinciples and emergence of embodied cognition, offering a unified account of\ncognitive phenomena, from basic physiological regulation to higher-order\nthought processes, and potentially bridge neuroscience and contemplative\ntraditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of cognition requires a framework that bridges evolutionary\nprinciples with neurocomputational mechanisms. This paper introduces the\n\"thoughtseed\" framework, proposing that cognition arises from the dynamic\ninteraction of self-organizing units of embodied knowledge called\n\"thoughtseeds.\" We leverage evolutionary theory, \"neuronal packets,\" and the\n\"Inner Screen\" hypothesis within Free Energy Principle, and propose a\nfour-level hierarchical model of the cognitive agent's internal states:\nNeuronal Packet Domains (NPDs), Knowledge Domains (KDs), thoughtseeds network,\nand meta-cognition. The dynamic interplay within this hierarchy, mediated by\nnested Markov blankets and reciprocal message passing, facilitates the\nemergence of thoughtseeds as coherent patterns of activity that guide\nperception, action, and learning. The framework further explores the role of\nthe organism's Umwelt and the principles of active inference, especially the\ngenerative model at each nested level, in shaping the selection and activation\nof thoughtseeds, leading to adaptive behavior through surprise minimization.\nThe \"Inner Screen\" is posited as the locus of conscious experience, where the\ncontent of the dominant thoughtseed is projected, maintaining a unitary\nconscious experience. Active thoughtseeds are proposed as the fundamental units\nof thought that contribute to the \"content of consciousness.\" We present a\nmathematical framework grounded in active inference and dynamical systems\ntheory. The thoughtseed framework represents an initial but promising step\ntowards a novel, biologically-grounded model for understanding the organizing\nprinciples and emergence of embodied cognition, offering a unified account of\ncognitive phenomena, from basic physiological regulation to higher-order\nthought processes, and potentially bridge neuroscience and contemplative\ntraditions."
                },
                "authors": [
                    {
                        "name": "Prakash Chandra Kavi"
                    },
                    {
                        "name": "Gorka Zamora Lopez"
                    },
                    {
                        "name": "Daniel Ari Friedman"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Ari Friedman"
                },
                "author": "Daniel Ari Friedman",
                "arxiv_comment": "I will be referencing the pre-print for my poster/Spotlight\n  Presentation at IWAI,https://iwaiworkshop.github.io/ on September -10, 2024\n  at Oxford, England. It has embedded 5 Figures and 2 Tables. it has\n  mathematical equations 35+ mathematical equations, with explanations in\n  Supllementary Section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15978v1",
                "updated": "2024-08-28T17:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:49:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration"
                },
                "summary": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Zijian Ma"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15971v1",
                "updated": "2024-08-28T17:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:43:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems"
                },
                "summary": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Boyan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v1",
                "updated": "2024-08-28T17:38:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15964v1",
                "updated": "2024-08-28T17:33:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    33,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:33:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    33,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "On harmonic oscillator hazard functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On harmonic oscillator hazard functions"
                },
                "summary": "We propose a parametric hazard model obtained by enforcing positivity in the\ndamped harmonic oscillator. The resulting model has closed-form hazard and\ncumulative hazard functions, facilitating likelihood and Bayesian inference on\nthe parameters. We show that this model can capture a range of hazard shapes,\nsuch as increasing, decreasing, unimodal, bathtub, and oscillatory patterns,\nand characterize the tails of the corresponding survival function. We\nillustrate the use of this model in survival analysis using real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a parametric hazard model obtained by enforcing positivity in the\ndamped harmonic oscillator. The resulting model has closed-form hazard and\ncumulative hazard functions, facilitating likelihood and Bayesian inference on\nthe parameters. We show that this model can capture a range of hazard shapes,\nsuch as increasing, decreasing, unimodal, bathtub, and oscillatory patterns,\nand characterize the tails of the corresponding survival function. We\nillustrate the use of this model in survival analysis using real data."
                },
                "authors": [
                    {
                        "name": "J. A. Christen"
                    },
                    {
                        "name": "F. J. Rubio"
                    }
                ],
                "author_detail": {
                    "name": "F. J. Rubio"
                },
                "author": "F. J. Rubio",
                "arxiv_comment": "R code and data are available at: https://github.com/FJRubio67/HOH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10260v2",
                "updated": "2024-08-28T17:26:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    26,
                    3,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-11T01:16:10Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    1,
                    16,
                    10,
                    1,
                    163,
                    0
                ],
                "title": "Flextron: Many-in-One Flexible Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flextron: Many-in-One Flexible Large Language Model"
                },
                "summary": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02850v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02850v5",
                "updated": "2024-08-28T17:19:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    19,
                    27,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-05T08:43:07Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    8,
                    43,
                    7,
                    6,
                    126,
                    0
                ],
                "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems"
                },
                "summary": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems characterized by rugged landscapes and high-dimensionality with an\nefficient convergence rate. The study presents a comprehensive comparative\nevaluation of HEO's performance against established optimization algorithms,\nincluding Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial\nFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved\nParticle Swarm Optimization (QPSO). The primary analysis encompasses 14\nbenchmark functions with dimension 30, demonstrating HEO's effectiveness and\nadaptability in navigating general optimization problems and providing valuable\ninsights into its performance. The test of HEO in Pressure Vessel Design and\nTubular Column Design infers its feasibility and potential in real-time\napplications. Further validation in Osmancik-97 and Cammeo Rice Classification\nproves the effectiveness of HEO and achieves a higher accuracy record."
                },
                "authors": [
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Anwar PP Abdul Majeed"
                    },
                    {
                        "name": "Pascal Lefevre"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Lefevre"
                },
                "author": "Pascal Lefevre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02850v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02850v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15954v1",
                "updated": "2024-08-28T17:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    14,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    14,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "InstanSeg: an embedding-based instance segmentation algorithm optimized\n  for accurate, efficient and portable cell segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstanSeg: an embedding-based instance segmentation algorithm optimized\n  for accurate, efficient and portable cell segmentation"
                },
                "summary": "Cell and nucleus segmentation are fundamental tasks for quantitative bioimage\nanalysis. Despite progress in recent years, biologists and other domain experts\nstill require novel algorithms to handle increasingly large and complex\nreal-world datasets. These algorithms must not only achieve state-of-the-art\naccuracy, but also be optimized for efficiency, portability and\nuser-friendliness. Here, we introduce InstanSeg: a novel embedding-based\ninstance segmentation pipeline designed to identify cells and nuclei in\nmicroscopy images. Using six public cell segmentation datasets, we demonstrate\nthat InstanSeg can significantly improve accuracy when compared to the most\nwidely used alternative methods, while reducing the processing time by at least\n60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript\nand supports GPU acceleration on a range of hardware. We provide an open-source\nimplementation of InstanSeg in Python, in addition to a user-friendly,\ninteractive QuPath extension for inference written in Java. Our code and\npre-trained models are available at https://github.com/instanseg/instanseg .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell and nucleus segmentation are fundamental tasks for quantitative bioimage\nanalysis. Despite progress in recent years, biologists and other domain experts\nstill require novel algorithms to handle increasingly large and complex\nreal-world datasets. These algorithms must not only achieve state-of-the-art\naccuracy, but also be optimized for efficiency, portability and\nuser-friendliness. Here, we introduce InstanSeg: a novel embedding-based\ninstance segmentation pipeline designed to identify cells and nuclei in\nmicroscopy images. Using six public cell segmentation datasets, we demonstrate\nthat InstanSeg can significantly improve accuracy when compared to the most\nwidely used alternative methods, while reducing the processing time by at least\n60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript\nand supports GPU acceleration on a range of hardware. We provide an open-source\nimplementation of InstanSeg in Python, in addition to a user-friendly,\ninteractive QuPath extension for inference written in Java. Our code and\npre-trained models are available at https://github.com/instanseg/instanseg ."
                },
                "authors": [
                    {
                        "name": "Thibaut Goldsborough"
                    },
                    {
                        "name": "Ben Philps"
                    },
                    {
                        "name": "Alan O'Callaghan"
                    },
                    {
                        "name": "Fiona Inglis"
                    },
                    {
                        "name": "Leo Leplat"
                    },
                    {
                        "name": "Andrew Filby"
                    },
                    {
                        "name": "Hakan Bilen"
                    },
                    {
                        "name": "Peter Bankhead"
                    }
                ],
                "author_detail": {
                    "name": "Peter Bankhead"
                },
                "author": "Peter Bankhead",
                "arxiv_comment": "12 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15950v1",
                "updated": "2024-08-28T17:08:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:08:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games"
                },
                "summary": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/."
                },
                "authors": [
                    {
                        "name": "Nicholas R. Waytowich"
                    },
                    {
                        "name": "Devin White"
                    },
                    {
                        "name": "MD Sunbeam"
                    },
                    {
                        "name": "Vinicius G. Goecks"
                    }
                ],
                "author_detail": {
                    "name": "Vinicius G. Goecks"
                },
                "author": "Vinicius G. Goecks",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15949v1",
                "updated": "2024-08-28T17:08:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    18,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:08:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Probing Lorentz invariance with a high-energy neutrino flare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Lorentz invariance with a high-energy neutrino flare"
                },
                "summary": "Time-of-flight measurements of high-energy astrophysical neutrinos can be\nused to probe Lorentz invariance, a pillar of modern physics. If\nLorentz-invariance violation (LIV) occurs, it could cause neutrinos to slow\ndown, with the delay scaling linearly or quadratically with their energy. We\nintroduce non-parametric statistical methods designed to detect LIV-induced\ndistortions in the temporal structure of a high-energy neutrino flare as it\ntravels to Earth from a distant astrophysical source, independently of the\nintrinsic timing properties of the source. Our approach, illustrated using the\n2014/2015 TeV-PeV neutrino flare from the blazar TXS 0506+056 detected by\nIceCube, finds that the LIV energy scale must exceed 10^{14} GeV (linear) or\n10^9 GeV (quadratic). Our methods provide a robust means to investigate LIV by\nfocusing solely on a neutrino flare, without relying on electromagnetic\ncounterparts, and account for realistic energy and directional uncertainties.\nFor completeness, we compare our limits inferred from TXS 0506+056 to the\nsensitivity inferred from multi-messenger detection of tentative coincidences\nbetween neutrinos and electromagnetic emission from active galactic nuclei and\ntidal disruption events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-of-flight measurements of high-energy astrophysical neutrinos can be\nused to probe Lorentz invariance, a pillar of modern physics. If\nLorentz-invariance violation (LIV) occurs, it could cause neutrinos to slow\ndown, with the delay scaling linearly or quadratically with their energy. We\nintroduce non-parametric statistical methods designed to detect LIV-induced\ndistortions in the temporal structure of a high-energy neutrino flare as it\ntravels to Earth from a distant astrophysical source, independently of the\nintrinsic timing properties of the source. Our approach, illustrated using the\n2014/2015 TeV-PeV neutrino flare from the blazar TXS 0506+056 detected by\nIceCube, finds that the LIV energy scale must exceed 10^{14} GeV (linear) or\n10^9 GeV (quadratic). Our methods provide a robust means to investigate LIV by\nfocusing solely on a neutrino flare, without relying on electromagnetic\ncounterparts, and account for realistic energy and directional uncertainties.\nFor completeness, we compare our limits inferred from TXS 0506+056 to the\nsensitivity inferred from multi-messenger detection of tentative coincidences\nbetween neutrinos and electromagnetic emission from active galactic nuclei and\ntidal disruption events."
                },
                "authors": [
                    {
                        "name": "Mauricio Bustamante"
                    },
                    {
                        "name": "John Ellis"
                    },
                    {
                        "name": "Rostislav Konoplich"
                    },
                    {
                        "name": "Alexander S. Sakharov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander S. Sakharov"
                },
                "author": "Alexander S. Sakharov",
                "arxiv_comment": "16 pages, 7 figures, plus an appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15918v1",
                "updated": "2024-08-28T16:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    31,
                    34,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:31:34Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    31,
                    34,
                    2,
                    241,
                    0
                ],
                "title": "Halo bias in the peak model. A first-principles non-parametric approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halo bias in the peak model. A first-principles non-parametric approach"
                },
                "summary": "The Press-Schechter (PS) and excursion set (ES) models of structure formation\nfail in reproducing the halo bias found in simulations, while the excursion\nset-peaks (ESP) formalism built in the peak model reproduces it only at high\nmasses and does not address in a fully satisfactory manner peak nesting and the\nmass and time of ellipsoidal collapse of triaxial peaks in the\nGaussian-smoothed density field. Here we apply the CUSP formalism fixing all\nthese issues from first principles and with no free parameters to infer the\nLagrangian local peak bias parameters, which adopt very simple analytic\nexpressions similar to those found in the PS and ES models. The predicted\nEulerian linear halo bias recovers the results of simulations. More\nspecifically, we show that the only small departure observed at intermediate\nand low masses can be due to the spurious halo splitting and grouping caused by\nthe Spherical Overdensity halo-finding algorithm used in simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Press-Schechter (PS) and excursion set (ES) models of structure formation\nfail in reproducing the halo bias found in simulations, while the excursion\nset-peaks (ESP) formalism built in the peak model reproduces it only at high\nmasses and does not address in a fully satisfactory manner peak nesting and the\nmass and time of ellipsoidal collapse of triaxial peaks in the\nGaussian-smoothed density field. Here we apply the CUSP formalism fixing all\nthese issues from first principles and with no free parameters to infer the\nLagrangian local peak bias parameters, which adopt very simple analytic\nexpressions similar to those found in the PS and ES models. The predicted\nEulerian linear halo bias recovers the results of simulations. More\nspecifically, we show that the only small departure observed at intermediate\nand low masses can be due to the spurious halo splitting and grouping caused by\nthe Spherical Overdensity halo-finding algorithm used in simulations."
                },
                "authors": [
                    {
                        "name": "Eduard Salvador-Sol√©"
                    },
                    {
                        "name": "Alberto Manrique"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Manrique"
                },
                "author": "Alberto Manrique",
                "arxiv_comment": "19 pages, 8 figures, accepted for publication in The Astrophysical\n  Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15915v1",
                "updated": "2024-08-28T16:28:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:28:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models"
                },
                "summary": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later."
                },
                "authors": [
                    {
                        "name": "Yuncheng Yang"
                    },
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Yun Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Gu"
                },
                "author": "Yun Gu",
                "arxiv_comment": "28 pages, 12 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11844v3",
                "updated": "2024-08-28T16:26:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    26,
                    16,
                    2,
                    241,
                    0
                ],
                "published": "2023-11-20T15:34:45Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    15,
                    34,
                    45,
                    0,
                    324,
                    0
                ],
                "title": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents"
                },
                "summary": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa."
                },
                "authors": [
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Oscar Magnusson"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Elin Naurin"
                    },
                    {
                        "name": "Lena W√§ngnerud"
                    }
                ],
                "author_detail": {
                    "name": "Lena W√§ngnerud"
                },
                "author": "Lena W√§ngnerud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15911v2",
                "updated": "2024-08-29T11:51:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    51,
                    34,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T16:25:04Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    25,
                    4,
                    2,
                    241,
                    0
                ],
                "title": "Accelerating Image-based Pest Detection on a Heterogeneous Multi-core\n  Microcontroller",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Image-based Pest Detection on a Heterogeneous Multi-core\n  Microcontroller"
                },
                "summary": "The codling moth pest poses a significant threat to global crop production,\nwith potential losses of up to 80% in apple orchards. Special camera-based\nsensor nodes are deployed in the field to record and transmit images of trapped\ninsects to monitor the presence of the pest. This paper investigates the\nembedding of computer vision algorithms in the sensor node using a novel\nState-of-the-Art Microcontroller Unit (MCU), the GreenWaves Technologies' GAP9\nSystem-on-Chip, which combines 10 RISC-V general purposes cores with a\nconvolution hardware accelerator. We compare the performance of a lightweight\nViola-Jones detector algorithm with a Convolutional Neural Network (CNN),\nMobileNetV3-SSDLite, trained for the pest detection task. On two datasets that\ndifferentiate for the distance between the camera sensor and the pest targets,\nthe CNN generalizes better than the other method and achieves a detection\naccuracy between 83% and 72%. Thanks to the GAP9's CNN accelerator, the CNN\ninference task takes only 147 ms to process a 320$\\times$240 image. Compared to\nthe GAP8 MCU, which only relies on general-purpose cores for processing, we\nachieved 9.5$\\times$ faster inference speed. When running on a 1000 mAh battery\nat 3.7 V, the estimated lifetime is approximately 199 days, processing an image\nevery 30 seconds. Our study demonstrates that the novel heterogeneous MCU can\nperform end-to-end CNN inference with an energy consumption of just 4.85 mJ,\nmatching the efficiency of the simpler Viola-Jones algorithm and offering power\nconsumption up to 15$\\times$ lower than previous methods. Code at:\nhttps://github.com/Bomps4/TAFE_Pest_Detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The codling moth pest poses a significant threat to global crop production,\nwith potential losses of up to 80% in apple orchards. Special camera-based\nsensor nodes are deployed in the field to record and transmit images of trapped\ninsects to monitor the presence of the pest. This paper investigates the\nembedding of computer vision algorithms in the sensor node using a novel\nState-of-the-Art Microcontroller Unit (MCU), the GreenWaves Technologies' GAP9\nSystem-on-Chip, which combines 10 RISC-V general purposes cores with a\nconvolution hardware accelerator. We compare the performance of a lightweight\nViola-Jones detector algorithm with a Convolutional Neural Network (CNN),\nMobileNetV3-SSDLite, trained for the pest detection task. On two datasets that\ndifferentiate for the distance between the camera sensor and the pest targets,\nthe CNN generalizes better than the other method and achieves a detection\naccuracy between 83% and 72%. Thanks to the GAP9's CNN accelerator, the CNN\ninference task takes only 147 ms to process a 320$\\times$240 image. Compared to\nthe GAP8 MCU, which only relies on general-purpose cores for processing, we\nachieved 9.5$\\times$ faster inference speed. When running on a 1000 mAh battery\nat 3.7 V, the estimated lifetime is approximately 199 days, processing an image\nevery 30 seconds. Our study demonstrates that the novel heterogeneous MCU can\nperform end-to-end CNN inference with an energy consumption of just 4.85 mJ,\nmatching the efficiency of the simpler Viola-Jones algorithm and offering power\nconsumption up to 15$\\times$ lower than previous methods. Code at:\nhttps://github.com/Bomps4/TAFE_Pest_Detection"
                },
                "authors": [
                    {
                        "name": "Luca Bompani"
                    },
                    {
                        "name": "Luca Crupi"
                    },
                    {
                        "name": "Daniele Palossi"
                    },
                    {
                        "name": "Olmo Baldoni"
                    },
                    {
                        "name": "Davide Brunelli"
                    },
                    {
                        "name": "Francesco Conti"
                    },
                    {
                        "name": "Manuele Rusci"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "11 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.3; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01090v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01090v3",
                "updated": "2024-08-28T16:23:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    23,
                    19,
                    2,
                    241,
                    0
                ],
                "published": "2023-11-02T08:55:11Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    8,
                    55,
                    11,
                    3,
                    306,
                    0
                ],
                "title": "Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion"
                },
                "summary": "Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds."
                },
                "authors": [
                    {
                        "name": "Nicolas Cherel"
                    },
                    {
                        "name": "Andr√©s Almansa"
                    },
                    {
                        "name": "Yann Gousseau"
                    },
                    {
                        "name": "Alasdair Newson"
                    }
                ],
                "author_detail": {
                    "name": "Alasdair Newson"
                },
                "author": "Alasdair Newson",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01090v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15909v1",
                "updated": "2024-08-28T16:22:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    22,
                    53,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:22:53Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    22,
                    53,
                    2,
                    241,
                    0
                ],
                "title": "Measuring $œÉ_8$ using DESI Legacy Imaging Surveys Emission-Line\n  Galaxies and Planck CMB Lensing and the Impact of Dust on Parameter Inferenc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring $œÉ_8$ using DESI Legacy Imaging Surveys Emission-Line\n  Galaxies and Planck CMB Lensing and the Impact of Dust on Parameter Inferenc"
                },
                "summary": "Measuring the growth of structure is a powerful probe for studying the dark\nsector, especially in light of the $\\sigma_8$ tension between primary CMB\nanisotropy and low-redshift surveys. This paper provides a new measurement of\nthe amplitude of the matter power spectrum, $\\sigma_8$, using galaxy-galaxy and\ngalaxy-CMB lensing power spectra of Dark Energy Spectroscopic Instrument Legacy\nImaging Surveys Emission-Line Galaxies and the $\\textit{Planck}$ 2018 CMB\nlensing map. We create an ELG catalog composed of $27$ million galaxies and\nwith a purity of $85\\%$, covering a redshift range $0 < z < 3$, with $z_{\\rm\nmean} = 1.09$. We implement several novel systematic corrections, such as\njointly modeling the contribution of imaging systematics and photometric\nredshift uncertainties to the covariance matrix. We also study the impacts of\nvarious dust maps on cosmological parameter inference. We measure the\ncross-power spectra over $f_{\\rm sky} = 0.25$ with a signal-to-background ratio\nof up to $ 30\\sigma$. We find that the choice of dust maps to account for\nimaging systematics in estimating the ELG overdensity field has a significant\nimpact on the final estimated values of $\\sigma_8$ and $\\Omega_{\\rm M}$, with\nfar-infrared emission-based dust maps preferring $\\sigma_8$ to be as low as\n$0.702 \\pm 0.030$, and stellar-reddening-based dust maps preferring as high as\n$0.719 \\pm 0.030$. The highest preferred value is at $\\sim 3 \\sigma$ tension\nwith the $\\textit{Planck}$ primary anisotropy results. These findings indicate\na need for tomographic analyses at high redshifts and joint modeling of\nsystematics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the growth of structure is a powerful probe for studying the dark\nsector, especially in light of the $\\sigma_8$ tension between primary CMB\nanisotropy and low-redshift surveys. This paper provides a new measurement of\nthe amplitude of the matter power spectrum, $\\sigma_8$, using galaxy-galaxy and\ngalaxy-CMB lensing power spectra of Dark Energy Spectroscopic Instrument Legacy\nImaging Surveys Emission-Line Galaxies and the $\\textit{Planck}$ 2018 CMB\nlensing map. We create an ELG catalog composed of $27$ million galaxies and\nwith a purity of $85\\%$, covering a redshift range $0 < z < 3$, with $z_{\\rm\nmean} = 1.09$. We implement several novel systematic corrections, such as\njointly modeling the contribution of imaging systematics and photometric\nredshift uncertainties to the covariance matrix. We also study the impacts of\nvarious dust maps on cosmological parameter inference. We measure the\ncross-power spectra over $f_{\\rm sky} = 0.25$ with a signal-to-background ratio\nof up to $ 30\\sigma$. We find that the choice of dust maps to account for\nimaging systematics in estimating the ELG overdensity field has a significant\nimpact on the final estimated values of $\\sigma_8$ and $\\Omega_{\\rm M}$, with\nfar-infrared emission-based dust maps preferring $\\sigma_8$ to be as low as\n$0.702 \\pm 0.030$, and stellar-reddening-based dust maps preferring as high as\n$0.719 \\pm 0.030$. The highest preferred value is at $\\sim 3 \\sigma$ tension\nwith the $\\textit{Planck}$ primary anisotropy results. These findings indicate\na need for tomographic analyses at high redshifts and joint modeling of\nsystematics."
                },
                "authors": [
                    {
                        "name": "Tanveer Karim"
                    },
                    {
                        "name": "Sukhdeep Singh"
                    },
                    {
                        "name": "Mehdi Rezaie"
                    },
                    {
                        "name": "Daniel Eisenstein"
                    },
                    {
                        "name": "Boryana Hadzhiyska"
                    },
                    {
                        "name": "Joshua S. Speagle"
                    },
                    {
                        "name": "Jessica Nicole Aguilar"
                    },
                    {
                        "name": "Steven Ahlen"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Todd Claybaugh"
                    },
                    {
                        "name": "Axel de la Macorra"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Jaime E. Forero-Romero"
                    },
                    {
                        "name": "Enrique Gazta√±aga"
                    },
                    {
                        "name": "Satya Gontcho A Gontcho"
                    },
                    {
                        "name": "Gaston Gutierrez"
                    },
                    {
                        "name": "Julien Guy"
                    },
                    {
                        "name": "Klaus Honscheid"
                    },
                    {
                        "name": "Stephanie Juneau"
                    },
                    {
                        "name": "David Kirkby"
                    },
                    {
                        "name": "Alex Krolewski"
                    },
                    {
                        "name": "Andrew Lambert"
                    },
                    {
                        "name": "Martin Landriau"
                    },
                    {
                        "name": "Michael Levi"
                    },
                    {
                        "name": "Aaron Meisner"
                    },
                    {
                        "name": "Ramon Miquel"
                    },
                    {
                        "name": "John Moustakas"
                    },
                    {
                        "name": "Andrea Mu√±oz-Guti√©rrez"
                    },
                    {
                        "name": "Adam Myers"
                    },
                    {
                        "name": "Gustavo Niz"
                    },
                    {
                        "name": "Nathalie Palanque Delabrouille"
                    },
                    {
                        "name": "Will Percival"
                    },
                    {
                        "name": "Francisco Prada"
                    },
                    {
                        "name": "Graziano Rossi"
                    },
                    {
                        "name": "Eusebio Sanchez"
                    },
                    {
                        "name": "Edward Schlafly"
                    },
                    {
                        "name": "David Schlegel"
                    },
                    {
                        "name": "Michael Schubnell"
                    },
                    {
                        "name": "David Sprayberry"
                    },
                    {
                        "name": "Gregory Tarl√©"
                    },
                    {
                        "name": "Benjamin Alan Weaver"
                    },
                    {
                        "name": "Hu Zou"
                    }
                ],
                "author_detail": {
                    "name": "Hu Zou"
                },
                "author": "Hu Zou",
                "arxiv_comment": "50 pages, 24 figures (figure data can be obtained at\n  https://zenodo.org/records/13381499)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15907v1",
                "updated": "2024-08-28T16:20:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:20:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "Decentralized LLM Inference over Edge Networks with Energy Harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized LLM Inference over Edge Networks with Energy Harvesting"
                },
                "summary": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks."
                },
                "authors": [
                    {
                        "name": "Aria Khoshsirat"
                    },
                    {
                        "name": "Giovanni Perin"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08247v2",
                "updated": "2024-08-28T16:20:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-15T16:33:07Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    33,
                    7,
                    3,
                    228,
                    0
                ],
                "title": "Bayesian Inference analysis of jet quenching using inclusive jet and\n  hadron suppression measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference analysis of jet quenching using inclusive jet and\n  hadron suppression measurements"
                },
                "summary": "The JETSCAPE Collaboration reports a new determination of the jet transport\nparameter $\\hat{q}$ in the Quark-Gluon Plasma (QGP) using Bayesian Inference,\nincorporating all available inclusive hadron and jet yield suppression data\nmeasured in heavy-ion collisions at RHIC and the LHC. This multi-observable\nanalysis extends the previously published JETSCAPE Bayesian Inference\ndetermination of $\\hat{q}$, which was based solely on a selection of inclusive\nhadron suppression data. JETSCAPE is a modular framework incorporating detailed\ndynamical models of QGP formation and evolution, and jet propagation and\ninteraction in the QGP. Virtuality-dependent partonic energy loss in the QGP is\nmodeled as a thermalized weakly-coupled plasma, with parameters determined from\nBayesian calibration using soft-sector observables. This Bayesian calibration\nof $\\hat{q}$ utilizes Active Learning, a machine--learning approach, for\nefficient exploitation of computing resources. The experimental data included\nin this analysis span a broad range in collision energy and centrality, and in\ntransverse momentum. In order to explore the systematic dependence of the\nextracted parameter posterior distributions, several different calibrations are\nreported, based on combined jet and hadron data; on jet or hadron data\nseparately; and on restricted kinematic or centrality ranges of the jet and\nhadron data. Tension is observed in comparison of these variations, providing\nnew insights into the physics of jet transport in the QGP and its theoretical\nformulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The JETSCAPE Collaboration reports a new determination of the jet transport\nparameter $\\hat{q}$ in the Quark-Gluon Plasma (QGP) using Bayesian Inference,\nincorporating all available inclusive hadron and jet yield suppression data\nmeasured in heavy-ion collisions at RHIC and the LHC. This multi-observable\nanalysis extends the previously published JETSCAPE Bayesian Inference\ndetermination of $\\hat{q}$, which was based solely on a selection of inclusive\nhadron suppression data. JETSCAPE is a modular framework incorporating detailed\ndynamical models of QGP formation and evolution, and jet propagation and\ninteraction in the QGP. Virtuality-dependent partonic energy loss in the QGP is\nmodeled as a thermalized weakly-coupled plasma, with parameters determined from\nBayesian calibration using soft-sector observables. This Bayesian calibration\nof $\\hat{q}$ utilizes Active Learning, a machine--learning approach, for\nefficient exploitation of computing resources. The experimental data included\nin this analysis span a broad range in collision energy and centrality, and in\ntransverse momentum. In order to explore the systematic dependence of the\nextracted parameter posterior distributions, several different calibrations are\nreported, based on combined jet and hadron data; on jet or hadron data\nseparately; and on restricted kinematic or centrality ranges of the jet and\nhadron data. Tension is observed in comparison of these variations, providing\nnew insights into the physics of jet transport in the QGP and its theoretical\nformulation."
                },
                "authors": [
                    {
                        "name": "R. Ehlers"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "J. Mulligan"
                    },
                    {
                        "name": "Y. Ji"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "S. Mak"
                    },
                    {
                        "name": "P. M. Jacobs"
                    },
                    {
                        "name": "A. Majumder"
                    },
                    {
                        "name": "A. Angerami"
                    },
                    {
                        "name": "R. Arora"
                    },
                    {
                        "name": "S. A. Bass"
                    },
                    {
                        "name": "R. Datta"
                    },
                    {
                        "name": "L. Du"
                    },
                    {
                        "name": "H. Elfner"
                    },
                    {
                        "name": "R. J. Fries"
                    },
                    {
                        "name": "C. Gale"
                    },
                    {
                        "name": "Y. He"
                    },
                    {
                        "name": "B. V. Jacak"
                    },
                    {
                        "name": "S. Jeon"
                    },
                    {
                        "name": "F. Jonas"
                    },
                    {
                        "name": "L. Kasper"
                    },
                    {
                        "name": "M. Kordell II"
                    },
                    {
                        "name": "R. Kunnawalkam-Elayavalli"
                    },
                    {
                        "name": "J. Latessa"
                    },
                    {
                        "name": "Y. -J. Lee"
                    },
                    {
                        "name": "R. Lemmon"
                    },
                    {
                        "name": "M. Luzum"
                    },
                    {
                        "name": "A. Mankolli"
                    },
                    {
                        "name": "C. Martin"
                    },
                    {
                        "name": "H. Mehryar"
                    },
                    {
                        "name": "T. Mengel"
                    },
                    {
                        "name": "C. Nattrass"
                    },
                    {
                        "name": "J. Norman"
                    },
                    {
                        "name": "C. Parker"
                    },
                    {
                        "name": "J. -F. Paquet"
                    },
                    {
                        "name": "J. H. Putschke"
                    },
                    {
                        "name": "H. Roch"
                    },
                    {
                        "name": "G. Roland"
                    },
                    {
                        "name": "B. Schenke"
                    },
                    {
                        "name": "L. Schwiebert"
                    },
                    {
                        "name": "A. Sengupta"
                    },
                    {
                        "name": "C. Shen"
                    },
                    {
                        "name": "M. Singh"
                    },
                    {
                        "name": "C. Sirimanna"
                    },
                    {
                        "name": "D. Soeder"
                    },
                    {
                        "name": "R. A. Soltz"
                    },
                    {
                        "name": "I. Soudi"
                    },
                    {
                        "name": "Y. Tachibana"
                    },
                    {
                        "name": "J. Velkovska"
                    },
                    {
                        "name": "G. Vujanovic"
                    },
                    {
                        "name": "X. -N. Wang"
                    },
                    {
                        "name": "X. Wu"
                    },
                    {
                        "name": "W. Zhao"
                    }
                ],
                "author_detail": {
                    "name": "W. Zhao"
                },
                "arxiv_affiliation": "JETSCAPE Collaboration",
                "author": "W. Zhao",
                "arxiv_comment": "20 pages, 10 figures, 2 tables, submitted to PRC; updated\n  acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15903v1",
                "updated": "2024-08-28T16:15:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:15:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments"
                },
                "summary": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Ishaan Singh Rawal"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15895v1",
                "updated": "2024-08-28T16:05:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:05:20Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "title": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models"
                },
                "summary": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion."
                },
                "authors": [
                    {
                        "name": "Sebastian Vallejo Vera"
                    },
                    {
                        "name": "Hunter Driggers"
                    }
                ],
                "author_detail": {
                    "name": "Hunter Driggers"
                },
                "author": "Hunter Driggers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15894v1",
                "updated": "2024-08-28T16:04:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    4,
                    40,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:04:40Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    4,
                    40,
                    2,
                    241,
                    0
                ],
                "title": "The Role of Fibration Symmetries in Geometric Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Fibration Symmetries in Geometric Deep Learning"
                },
                "summary": "Geometric Deep Learning (GDL) unifies a broad class of machine learning\ntechniques from the perspectives of symmetries, offering a framework for\nintroducing problem-specific inductive biases like Graph Neural Networks\n(GNNs). However, the current formulation of GDL is limited to global symmetries\nthat are not often found in real-world problems. We propose to relax GDL to\nallow for local symmetries, specifically fibration symmetries in graphs, to\nleverage regularities of realistic instances. We show that GNNs apply the\ninductive bias of fibration symmetries and derive a tighter upper bound for\ntheir expressive power. Additionally, by identifying symmetries in networks, we\ncollapse network nodes, thereby increasing their computational efficiency\nduring both inference and training of deep neural networks. The mathematical\nextension introduced here applies beyond graphs to manifolds, bundles, and\ngrids for the development of models with inductive biases induced by local\nsymmetries that can lead to better generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Deep Learning (GDL) unifies a broad class of machine learning\ntechniques from the perspectives of symmetries, offering a framework for\nintroducing problem-specific inductive biases like Graph Neural Networks\n(GNNs). However, the current formulation of GDL is limited to global symmetries\nthat are not often found in real-world problems. We propose to relax GDL to\nallow for local symmetries, specifically fibration symmetries in graphs, to\nleverage regularities of realistic instances. We show that GNNs apply the\ninductive bias of fibration symmetries and derive a tighter upper bound for\ntheir expressive power. Additionally, by identifying symmetries in networks, we\ncollapse network nodes, thereby increasing their computational efficiency\nduring both inference and training of deep neural networks. The mathematical\nextension introduced here applies beyond graphs to manifolds, bundles, and\ngrids for the development of models with inductive biases induced by local\nsymmetries that can lead to better generalization."
                },
                "authors": [
                    {
                        "name": "Osvaldo Velarde"
                    },
                    {
                        "name": "Lucas Parra"
                    },
                    {
                        "name": "Paolo Boldi"
                    },
                    {
                        "name": "Hernan Makse"
                    }
                ],
                "author_detail": {
                    "name": "Hernan Makse"
                },
                "author": "Hernan Makse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15889v1",
                "updated": "2024-08-28T16:02:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    2,
                    0,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:02:00Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    2,
                    0,
                    2,
                    241,
                    0
                ],
                "title": "Strongly Interacting Quark Matter in Massive Quark Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strongly Interacting Quark Matter in Massive Quark Stars"
                },
                "summary": "This paper investigates the properties of strongly coupled matter at high\ndensities in a quark star (QS). The QS is built from the density-dependent\nquark mass model (DDQM), modified to obtain higher maximum gravitational mass\n($\\rm M_{max}$) of the QS using the data from observed pulsars: XMMU\nJ173203.3-344518, PSR J0030+0451, PSR J0740+6620, and PSR J0952-0607 as\nconstraints in Bayesian inference. We observed that the quark matter (QM) that\ncomposes QSs with $\\rm M_{max} > 2M_\\odot$ violates the conformality criteria\ndetermined through conformal field theory. This behavior is interpreted as a\nconsequence of the increase in quark population with $\\rho_B$ and the\nconcomitant formation of colored quark and gluon condensates, which are\ninfluenced by the pressure build-up in the stellar core as $\\rho_B$ increases.\nThis is reflected in the enhanced DDQM model employed, which introduces an\nadditional term relevant at high densities. On the other hand, for $\\rm M_{max}\n< 2M_\\odot$ we observed the desired behavior of the QM as predicted by quantum\nchromodynamics (QCD) at higher densities, where the interaction decreases with\nincreasing $\\rho_B$ and eventually the quarks become deconfined due to the\ndepletion of the DDQM through an additional attractive contribution in this\ncase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the properties of strongly coupled matter at high\ndensities in a quark star (QS). The QS is built from the density-dependent\nquark mass model (DDQM), modified to obtain higher maximum gravitational mass\n($\\rm M_{max}$) of the QS using the data from observed pulsars: XMMU\nJ173203.3-344518, PSR J0030+0451, PSR J0740+6620, and PSR J0952-0607 as\nconstraints in Bayesian inference. We observed that the quark matter (QM) that\ncomposes QSs with $\\rm M_{max} > 2M_\\odot$ violates the conformality criteria\ndetermined through conformal field theory. This behavior is interpreted as a\nconsequence of the increase in quark population with $\\rho_B$ and the\nconcomitant formation of colored quark and gluon condensates, which are\ninfluenced by the pressure build-up in the stellar core as $\\rho_B$ increases.\nThis is reflected in the enhanced DDQM model employed, which introduces an\nadditional term relevant at high densities. On the other hand, for $\\rm M_{max}\n< 2M_\\odot$ we observed the desired behavior of the QM as predicted by quantum\nchromodynamics (QCD) at higher densities, where the interaction decreases with\nincreasing $\\rho_B$ and eventually the quarks become deconfined due to the\ndepletion of the DDQM through an additional attractive contribution in this\ncase."
                },
                "authors": [
                    {
                        "name": "Adamu Issifu"
                    },
                    {
                        "name": "Franciele M. da Silva"
                    },
                    {
                        "name": "Luis C. N. Santos"
                    },
                    {
                        "name": "D√©bora P. Menezes"
                    },
                    {
                        "name": "Tobias Frederico"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Frederico"
                },
                "author": "Tobias Frederico",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15888v1",
                "updated": "2024-08-28T16:01:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    1,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:01:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    1,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "The ESO UVES/FEROS Large Programs of TESS OB pulsators. II. On the\n  physical origin of macroturbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ESO UVES/FEROS Large Programs of TESS OB pulsators. II. On the\n  physical origin of macroturbulence"
                },
                "summary": "Spectral lines of hot massive stars are known to exhibit large excess\nbroadening in addition to rotational broadening. This excess broadening is\noften attributed to macroturbulence whose physical origin is a matter of active\ndebate in the stellar astrophysics community. By looking into the statistical\nproperties of a large sample of O- and B-type stars, both in the Galaxy and\nLMC, we aim to shed light on the physical origin of macroturbulent line\nbroadening. We deliver newly measured macroturbulent velocities for 86 stars\nfrom the Galaxy in a consistent manner with 126 stars from the LMC. A total\nsample of 594 O- and B-type stars with measured macroturbulent velocities was\ncomposed by complementing our sample with archival data. Furthermore, we\ncompute an extensive grid of MESA models to compare, in a statistical manner,\nthe predicted interior properties of stars (such as convection and wave\npropagation) with the inference of macroturbulent velocities from\nhigh-resolution spectroscopic observations. We find the presence of two\nprincipally different regimes where, depending on the initial stellar mass,\ndifferent mechanisms may be responsible for the observed excess line\nbroadening. Stars with initial masses above some 30$M_{\\odot}$ are found to\nhave macroturbulent velocities fully determined by subsurface convective zones\nformed in the iron opacity bump (FeCZ), while some other mechanism is required\nto explain observations for masses below 12$M_{\\odot}$. The latter finding\nleaves the potential for waves generated at the interface of the convective\ncore and radiative envelope of the star to be responsible for the observed\nmacroturbulent broadening. Both mechanisms may co-exist in the intermediate\nregime of stellar masses, between some 12 and 30$M_{\\odot}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral lines of hot massive stars are known to exhibit large excess\nbroadening in addition to rotational broadening. This excess broadening is\noften attributed to macroturbulence whose physical origin is a matter of active\ndebate in the stellar astrophysics community. By looking into the statistical\nproperties of a large sample of O- and B-type stars, both in the Galaxy and\nLMC, we aim to shed light on the physical origin of macroturbulent line\nbroadening. We deliver newly measured macroturbulent velocities for 86 stars\nfrom the Galaxy in a consistent manner with 126 stars from the LMC. A total\nsample of 594 O- and B-type stars with measured macroturbulent velocities was\ncomposed by complementing our sample with archival data. Furthermore, we\ncompute an extensive grid of MESA models to compare, in a statistical manner,\nthe predicted interior properties of stars (such as convection and wave\npropagation) with the inference of macroturbulent velocities from\nhigh-resolution spectroscopic observations. We find the presence of two\nprincipally different regimes where, depending on the initial stellar mass,\ndifferent mechanisms may be responsible for the observed excess line\nbroadening. Stars with initial masses above some 30$M_{\\odot}$ are found to\nhave macroturbulent velocities fully determined by subsurface convective zones\nformed in the iron opacity bump (FeCZ), while some other mechanism is required\nto explain observations for masses below 12$M_{\\odot}$. The latter finding\nleaves the potential for waves generated at the interface of the convective\ncore and radiative envelope of the star to be responsible for the observed\nmacroturbulent broadening. Both mechanisms may co-exist in the intermediate\nregime of stellar masses, between some 12 and 30$M_{\\odot}$."
                },
                "authors": [
                    {
                        "name": "Nadya Serebriakova"
                    },
                    {
                        "name": "Andrew Tkachenko"
                    },
                    {
                        "name": "Conny Aerts"
                    }
                ],
                "author_detail": {
                    "name": "Conny Aerts"
                },
                "author": "Conny Aerts",
                "arxiv_comment": "Submitted to A&A on 19 July 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v2",
                "updated": "2024-08-28T15:53:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    53,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15879v1",
                "updated": "2024-08-28T15:50:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:50:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Persuasion Games using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Games using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase)."
                },
                "authors": [
                    {
                        "name": "Ganesh Prasath Ramani"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Santhosh V"
                    },
                    {
                        "name": "Yash Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Yash Bhatia"
                },
                "author": "Yash Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.06327v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.06327v5",
                "updated": "2024-08-28T15:24:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    24,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2022-09-13T22:20:41Z",
                "published_parsed": [
                    2022,
                    9,
                    13,
                    22,
                    20,
                    41,
                    1,
                    256,
                    0
                ],
                "title": "Reproducibility-Oriented and Privacy-Preserving Genomic Dataset Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducibility-Oriented and Privacy-Preserving Genomic Dataset Sharing"
                },
                "summary": "As genomic research has become increasingly widespread in recent years, few\nstudies have shared datasets due to the privacy concerns about the genomic\nrecords. This hinders the reproduction and validation of research outcomes,\nwhich are crucial for catching errors, e.g., miscalculations, during the\nresearch process. To address the reproducibility issue of genome-wide\nassociation studies (GWAS) outcomes, we propose an innovative method that\ninvolves a differential privacy-based scheme for sharing genomic datasets. The\nproposed scheme involves two stages. In the first stage, we generate a noisy\ncopy of the target dataset by applying an optimized version of a previously\nproposed XOR mechanism on the binarized (encoded) dataset, where the binary\nnoise generation considers biological features. However, the initial step\nintroduces significant noise, making the dataset less suitable for direct GWAS\noutcome validation. Thus, in the second stage, we implement a post-processing\ntechnique that adjusts the Minor Allele Frequency values (MAFs) in the noisy\ndataset to align more closely with public MAF information using optimal\ntransport, and then decode it back to genomic space. We evaluate the proposed\nscheme on three real-life genomic datasets and compare it with a baseline\napproach (local differential privacy) and two synthesis-based solutions with\nregard to GWAS outcome validation, data utility, and resistance against\nmembership inference attacks (MIAs). We show that our proposed scheme\noutperforms all other methods in detecting GWAS outcome errors, achieves better\nutility, and provides higher privacy protection against membership inference\nattacks (MIAs). By utilizing our method, genomic researchers will be inclined\nto share a differentially private, yet of high quality version of their\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As genomic research has become increasingly widespread in recent years, few\nstudies have shared datasets due to the privacy concerns about the genomic\nrecords. This hinders the reproduction and validation of research outcomes,\nwhich are crucial for catching errors, e.g., miscalculations, during the\nresearch process. To address the reproducibility issue of genome-wide\nassociation studies (GWAS) outcomes, we propose an innovative method that\ninvolves a differential privacy-based scheme for sharing genomic datasets. The\nproposed scheme involves two stages. In the first stage, we generate a noisy\ncopy of the target dataset by applying an optimized version of a previously\nproposed XOR mechanism on the binarized (encoded) dataset, where the binary\nnoise generation considers biological features. However, the initial step\nintroduces significant noise, making the dataset less suitable for direct GWAS\noutcome validation. Thus, in the second stage, we implement a post-processing\ntechnique that adjusts the Minor Allele Frequency values (MAFs) in the noisy\ndataset to align more closely with public MAF information using optimal\ntransport, and then decode it back to genomic space. We evaluate the proposed\nscheme on three real-life genomic datasets and compare it with a baseline\napproach (local differential privacy) and two synthesis-based solutions with\nregard to GWAS outcome validation, data utility, and resistance against\nmembership inference attacks (MIAs). We show that our proposed scheme\noutperforms all other methods in detecting GWAS outcome errors, achieves better\nutility, and provides higher privacy protection against membership inference\nattacks (MIAs). By utilizing our method, genomic researchers will be inclined\nto share a differentially private, yet of high quality version of their\ndatasets."
                },
                "authors": [
                    {
                        "name": "Yuzhou Jiang"
                    },
                    {
                        "name": "Tianxi Ji"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Erman Ayday"
                    }
                ],
                "author_detail": {
                    "name": "Erman Ayday"
                },
                "author": "Erman Ayday",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.06327v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.06327v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07839v2",
                "updated": "2024-08-28T15:05:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    5,
                    42,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-11T15:27:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    27,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models"
                },
                "summary": "We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens."
                },
                "authors": [
                    {
                        "name": "Aleksandar Botev"
                    },
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Samuel L Smith"
                    },
                    {
                        "name": "Anushan Fernando"
                    },
                    {
                        "name": "George-Cristian Muraru"
                    },
                    {
                        "name": "Ruba Haroun"
                    },
                    {
                        "name": "Leonard Berrada"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "L√©onard Hussenot"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Laurent Sifre"
                    },
                    {
                        "name": "Morgane Rivi√®re"
                    },
                    {
                        "name": "Mihir Sanjay Kale"
                    },
                    {
                        "name": "Juliette Love"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Srivatsan Srinivasan"
                    },
                    {
                        "name": "Guillaume Desjardins"
                    },
                    {
                        "name": "David Budden"
                    },
                    {
                        "name": "Arnaud Doucet"
                    },
                    {
                        "name": "Sharad Vikram"
                    },
                    {
                        "name": "Adam Paszke"
                    },
                    {
                        "name": "Trevor Gale"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Andy Brock"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Jenny Brennan"
                    },
                    {
                        "name": "Meg Risdal"
                    },
                    {
                        "name": "Raj Gundluru"
                    },
                    {
                        "name": "Nesh Devanathan"
                    },
                    {
                        "name": "Paul Mooney"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Elisa Bandy"
                    },
                    {
                        "name": "David Huntsperger"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Arthur Zucker"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Ludovic Peran"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Cl√©ment Farabet"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Nando de Frietas"
                    }
                ],
                "author_detail": {
                    "name": "Nando de Frietas"
                },
                "author": "Nando de Frietas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01245v2",
                "updated": "2024-08-28T15:01:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    1,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-01T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    3,
                    41,
                    0,
                    92,
                    0
                ],
                "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules"
                },
                "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00612v2",
                "updated": "2024-08-28T14:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-01T14:52:04Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    14,
                    52,
                    4,
                    3,
                    214,
                    0
                ],
                "title": "Downstream bias mitigation is all you need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downstream bias mitigation is all you need"
                },
                "summary": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model."
                },
                "authors": [
                    {
                        "name": "Arkadeep Baksi"
                    },
                    {
                        "name": "Rahul Singh"
                    },
                    {
                        "name": "Tarun Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Tarun Joshi"
                },
                "author": "Tarun Joshi",
                "arxiv_comment": "arXiv admin note: This work has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16696v3",
                "updated": "2024-08-28T14:54:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    54,
                    11,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-26T16:11:03Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    11,
                    3,
                    0,
                    57,
                    0
                ],
                "title": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models"
                },
                "summary": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets."
                },
                "authors": [
                    {
                        "name": "Anchun Gui"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao",
                "arxiv_comment": "20 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15836v1",
                "updated": "2024-08-28T14:48:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:48:37Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature"
                },
                "summary": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available."
                },
                "authors": [
                    {
                        "name": "Uri Katz"
                    },
                    {
                        "name": "Mosh Levy"
                    },
                    {
                        "name": "Yoav Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Goldberg"
                },
                "author": "Yoav Goldberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03741v2",
                "updated": "2024-08-28T14:41:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    41,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2023-10-05T17:59:47Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    17,
                    59,
                    47,
                    3,
                    278,
                    0
                ],
                "title": "EFTofLSS meets simulation-based inference: $œÉ_8$ from biased\n  tracers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFTofLSS meets simulation-based inference: $œÉ_8$ from biased\n  tracers"
                },
                "summary": "Cosmological inferences typically rely on explicit expressions for the\nlikelihood and covariance of the data vector, which normally consists of a set\nof summary statistics. However, in the case of nonlinear large-scale structure,\nexact expressions for either likelihood or covariance are unknown, and even\napproximate expressions can become very cumbersome, depending on the scales and\nsummary statistics considered. Simulation-based inference (SBI), in contrast,\ndoes not require an explicit form for the likelihood but only a prior and a\nsimulator, thereby naturally circumventing these issues. In this paper, we\nexplore how this technique can be used to infer $\\sigma_8$ from a Lagrangian\neffective field theory (EFT) based forward model for biased tracers. The power\nspectrum and bispectrum are used as summary statistics to obtain the posterior\nof the cosmological, bias and noise parameters via neural density estimation.\nWe compare full simulation-based inference with cases where the data vector is\ndrawn from a Gaussian likelihood with sample and analytical covariances. We\nconclude that, for $k_{\\text{max}}=0.1h\\text{Mpc}^{-1}$ and\n$0.2h\\text{Mpc}^{-1}$, the form of the covariance is more important than the\nnon-Gaussianity of the likelihood, although this conclusion is expected to\ndepend on the cosmological parameter inferred, the summary statistics\nconsidered and range of scales probed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological inferences typically rely on explicit expressions for the\nlikelihood and covariance of the data vector, which normally consists of a set\nof summary statistics. However, in the case of nonlinear large-scale structure,\nexact expressions for either likelihood or covariance are unknown, and even\napproximate expressions can become very cumbersome, depending on the scales and\nsummary statistics considered. Simulation-based inference (SBI), in contrast,\ndoes not require an explicit form for the likelihood but only a prior and a\nsimulator, thereby naturally circumventing these issues. In this paper, we\nexplore how this technique can be used to infer $\\sigma_8$ from a Lagrangian\neffective field theory (EFT) based forward model for biased tracers. The power\nspectrum and bispectrum are used as summary statistics to obtain the posterior\nof the cosmological, bias and noise parameters via neural density estimation.\nWe compare full simulation-based inference with cases where the data vector is\ndrawn from a Gaussian likelihood with sample and analytical covariances. We\nconclude that, for $k_{\\text{max}}=0.1h\\text{Mpc}^{-1}$ and\n$0.2h\\text{Mpc}^{-1}$, the form of the covariance is more important than the\nnon-Gaussianity of the likelihood, although this conclusion is expected to\ndepend on the cosmological parameter inferred, the summary statistics\nconsidered and range of scales probed."
                },
                "authors": [
                    {
                        "name": "Beatriz Tucci"
                    },
                    {
                        "name": "Fabian Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schmidt"
                },
                "author": "Fabian Schmidt",
                "arxiv_doi": "10.1088/1475-7516/2024/05/063",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2024/05/063",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.03741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "42 pages, 17 figures",
                "arxiv_journal_ref": "JCAP05(2024)063",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10155v2",
                "updated": "2024-08-28T14:38:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    38,
                    51,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-15T22:02:58Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    2,
                    58,
                    0,
                    106,
                    0
                ],
                "title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Simantika Dristi"
                    },
                    {
                        "name": "Joy Saha"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Joanna C. S. Santos"
                },
                "author": "Joanna C. S. Santos",
                "arxiv_comment": "Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15815v1",
                "updated": "2024-08-28T14:24:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:24:48Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "title": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing"
                },
                "summary": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively."
                },
                "authors": [
                    {
                        "name": "Congying Xu"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Valerio Terragni"
                    },
                    {
                        "name": "Hengcheng Zhu"
                    },
                    {
                        "name": "Jialun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jialun Cao"
                },
                "author": "Jialun Cao",
                "arxiv_comment": "This paper is accepted to ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14511v2",
                "updated": "2024-08-28T14:13:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    13,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T04:07:18Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    7,
                    18,
                    6,
                    238,
                    0
                ],
                "title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods"
                },
                "summary": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xinyang Hu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Zhuoran Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Yang"
                },
                "author": "Zhuoran Yang",
                "arxiv_comment": "150 pages, 18 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15811v1",
                "updated": "2024-08-28T14:11:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    11,
                    59,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:11:59Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    11,
                    59,
                    2,
                    241,
                    0
                ],
                "title": "Identifying Influential and Vulnerable Nodes in Interaction Networks\n  through Estimation of Transfer Entropy Between Univariate and Multivariate\n  Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Influential and Vulnerable Nodes in Interaction Networks\n  through Estimation of Transfer Entropy Between Univariate and Multivariate\n  Time Series"
                },
                "summary": "Transfer entropy (TE) is a powerful tool for measuring causal relationships\nwithin interaction networks. Traditionally, TE and its conditional variants are\napplied pairwise between dynamic variables to infer these causal relationships.\nHowever, identifying the most influential or vulnerable node in a system\nrequires measuring the causal influence of each component on the entire system\nand vice versa. In this paper, I propose using outgoing and incoming transfer\nentropy-where outgoing TE quantifies the influence of a node on the rest of the\nsystem, and incoming TE measures the influence of the rest of the system on the\nnode. The node with the highest outgoing TE is identified as the most\ninfluential, or \"hub\", while the node with the highest incoming TE is the most\nvulnerable, or \"anti-hub\". Since these measures involve transfer entropy\nbetween univariate and multivariate time series, naive estimation methods can\nresult in significant errors, particularly when the number of variables is\ncomparable to or exceeds the number of samples. To address this, I introduce a\nnovel estimation scheme that computes outgoing and incoming TE only between\nsignificantly interacting partners. The feasibility of this approach is\ndemonstrated by using synthetic data, and by applying it to a real data of oral\nmicrobiota. The method successfully identifies the bacterial species known to\nbe key players in the bacterial community, demonstrating the power of the new\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer entropy (TE) is a powerful tool for measuring causal relationships\nwithin interaction networks. Traditionally, TE and its conditional variants are\napplied pairwise between dynamic variables to infer these causal relationships.\nHowever, identifying the most influential or vulnerable node in a system\nrequires measuring the causal influence of each component on the entire system\nand vice versa. In this paper, I propose using outgoing and incoming transfer\nentropy-where outgoing TE quantifies the influence of a node on the rest of the\nsystem, and incoming TE measures the influence of the rest of the system on the\nnode. The node with the highest outgoing TE is identified as the most\ninfluential, or \"hub\", while the node with the highest incoming TE is the most\nvulnerable, or \"anti-hub\". Since these measures involve transfer entropy\nbetween univariate and multivariate time series, naive estimation methods can\nresult in significant errors, particularly when the number of variables is\ncomparable to or exceeds the number of samples. To address this, I introduce a\nnovel estimation scheme that computes outgoing and incoming TE only between\nsignificantly interacting partners. The feasibility of this approach is\ndemonstrated by using synthetic data, and by applying it to a real data of oral\nmicrobiota. The method successfully identifies the bacterial species known to\nbe key players in the bacterial community, demonstrating the power of the new\nmethod."
                },
                "authors": [
                    {
                        "name": "Julian Lee"
                    }
                ],
                "author_detail": {
                    "name": "Julian Lee"
                },
                "author": "Julian Lee",
                "arxiv_comment": "35 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14846v4",
                "updated": "2024-08-28T14:04:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    4,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-19T14:53:01Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    14,
                    53,
                    1,
                    0,
                    50,
                    0
                ],
                "title": "Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models"
                },
                "summary": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Grgur Kovaƒç"
                    },
                    {
                        "name": "R√©my Portelas"
                    },
                    {
                        "name": "Masataka Sawayama"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "arxiv_doi": "10.1371/journal.pone.0309114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1371/journal.pone.0309114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.14846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )",
                "arxiv_journal_ref": "PLOS ONE, August 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15805v1",
                "updated": "2024-08-28T14:02:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    2,
                    10,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:02:10Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    2,
                    10,
                    2,
                    241,
                    0
                ],
                "title": "Investigating Complex HPV Dynamics Using Emulation and History Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Complex HPV Dynamics Using Emulation and History Matching"
                },
                "summary": "The study of transmission and progression of human papillomavirus (HPV) is\ncrucial for understanding the incidence of cervical cancers, and has been\nidentified as a priority worldwide. The complexity of the disease necessitates\na detailed model of HPV transmission and its progression to cancer; to infer\nproperties of the above we require a careful process that can match to\nimperfect or incomplete observational data. In this paper, we describe the\nHPVsim simulator to satisfy the former requirement; to satisfy the latter we\ncouple this stochastic simulator to a process of emulation and history matching\nusing the R package hmer. With these tools, we are able to obtain a\ncomprehensive collection of parameter combinations that could give rise to\nobserved cancer data, and explore the implications of the variability of these\nparameter sets as it relates to future health interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of transmission and progression of human papillomavirus (HPV) is\ncrucial for understanding the incidence of cervical cancers, and has been\nidentified as a priority worldwide. The complexity of the disease necessitates\na detailed model of HPV transmission and its progression to cancer; to infer\nproperties of the above we require a careful process that can match to\nimperfect or incomplete observational data. In this paper, we describe the\nHPVsim simulator to satisfy the former requirement; to satisfy the latter we\ncouple this stochastic simulator to a process of emulation and history matching\nusing the R package hmer. With these tools, we are able to obtain a\ncomprehensive collection of parameter combinations that could give rise to\nobserved cancer data, and explore the implications of the variability of these\nparameter sets as it relates to future health interventions."
                },
                "authors": [
                    {
                        "name": "Andrew Iskauskas"
                    },
                    {
                        "name": "Jamie A. Cohen"
                    },
                    {
                        "name": "Danny Scarponi"
                    },
                    {
                        "name": "Ian Vernon"
                    },
                    {
                        "name": "Michael Goldstein"
                    },
                    {
                        "name": "Daniel Klein"
                    },
                    {
                        "name": "Richard G. White"
                    },
                    {
                        "name": "Nicky McCreesh"
                    }
                ],
                "author_detail": {
                    "name": "Nicky McCreesh"
                },
                "author": "Nicky McCreesh",
                "arxiv_comment": "21 pages, 15 figures; submitted to Epidemics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05165v3",
                "updated": "2024-08-28T13:52:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    57,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-06T19:58:03Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    19,
                    58,
                    3,
                    5,
                    188,
                    0
                ],
                "title": "Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps"
                },
                "summary": "In software development, bug report reproduction is a challenging task. This\npaper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a\nlarge-scale language model (LLM), to automatically reproduce Android bug\nreports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce\n(S2R) entities. Instead, it leverages the entire textual bug report and employs\ninnovative prompts to enhance GPT's contextual reasoning. This approach is more\nflexible and context-aware than the traditional step-by-step entity matching\napproach, resulting in improved accuracy and effectiveness. In addition to\nhandling crash reports, ReBL has the capability of handling non-crash\nfunctional bug reports. Our evaluation of 96 Android bug reports (73 crash and\n23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these\nreports, averaging only 74.98 seconds per bug report. Additionally, ReBL\noutperformed three existing tools in both success rate and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In software development, bug report reproduction is a challenging task. This\npaper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a\nlarge-scale language model (LLM), to automatically reproduce Android bug\nreports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce\n(S2R) entities. Instead, it leverages the entire textual bug report and employs\ninnovative prompts to enhance GPT's contextual reasoning. This approach is more\nflexible and context-aware than the traditional step-by-step entity matching\napproach, resulting in improved accuracy and effectiveness. In addition to\nhandling crash reports, ReBL has the capability of handling non-crash\nfunctional bug reports. Our evaluation of 96 Android bug reports (73 crash and\n23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these\nreports, averaging only 74.98 seconds per bug report. Additionally, ReBL\noutperformed three existing tools in both success rate and speed."
                },
                "authors": [
                    {
                        "name": "Dingbang Wang"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Sidong Feng"
                    },
                    {
                        "name": "Zhaoxu Zhang"
                    },
                    {
                        "name": "William G. J. Halfond"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Xiaoxia Sun"
                    },
                    {
                        "name": "Jiangfan Shi"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "arxiv_doi": "10.1145/3650212.3680341",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3650212.3680341",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.05165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15801v1",
                "updated": "2024-08-28T13:52:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    19,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:52:19Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    19,
                    2,
                    241,
                    0
                ],
                "title": "Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization"
                },
                "summary": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization."
                },
                "authors": [
                    {
                        "name": "L√©o Hemamou"
                    },
                    {
                        "name": "Mehdi Debiane"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Debiane"
                },
                "author": "Mehdi Debiane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15796v1",
                "updated": "2024-08-28T13:42:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:42:28Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models"
                },
                "summary": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility."
                },
                "authors": [
                    {
                        "name": "H√©di Zhegidi"
                    },
                    {
                        "name": "Ludovic Moncla"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Moncla"
                },
                "author": "Ludovic Moncla",
                "arxiv_comment": "Github repo: https://github.com/GEODE-project/ner-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15793v1",
                "updated": "2024-08-28T13:37:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    37,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    37,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough"
                },
                "summary": "We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub."
                },
                "authors": [
                    {
                        "name": "Konstantin Dobler"
                    },
                    {
                        "name": "Gerard de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard de Melo"
                },
                "author": "Gerard de Melo",
                "arxiv_comment": "WANT@ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15792v1",
                "updated": "2024-08-28T13:35:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    35,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    35,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "Efficient LLM Scheduling by Learning to Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Scheduling by Learning to Rank"
                },
                "summary": "In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git"
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Runlong Su"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15789v1",
                "updated": "2024-08-28T13:32:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    32,
                    40,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:32:40Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    32,
                    40,
                    2,
                    241,
                    0
                ],
                "title": "A Stochastic Robust Adaptive Systems Level Approach to Stabilizing\n  Large-Scale Uncertain Markovian Jump Linear Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Robust Adaptive Systems Level Approach to Stabilizing\n  Large-Scale Uncertain Markovian Jump Linear Systems"
                },
                "summary": "We propose a unified framework for robustly and adaptively stabilizing\nlarge-scale networked uncertain Markovian jump linear systems (MJLS) under\nexternal disturbances and mode switches that can change the network's topology.\nAdaptation is achieved by using minimal information on the disturbance to\nidentify modes that are consistent with observable data. Robust control is\nachieved by extending the system level synthesis (SLS) approach, which allows\nus to pose the problem of simultaneously stabilizing multiple plants as a\ntwo-step convex optimization procedure. Our control pipeline computes a\nlikelihood distribution of the system's current mode, uses them as\nprobabilistic weights during simultaneous stabilization, then updates the\nlikelihood via Bayesian inference. Because of this \"softer\" probabilistic\napproach to robust stabilization, our control pipeline does not suffer from\nabrupt destabilization issues due to changes in the system's true mode, which\nwere observed in a previous method. Separability of SLS also lets us compute\nlocalized robust controllers for each subsystem, allowing for network\nscalability; we use several information consensus methods so that mode\nestimation can also be done locally. We apply our algorithms to\ndisturbance-rejection on two sample dynamic power grid networks, a small-scale\nsystem with 7 nodes and a large-scale grid of 25 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unified framework for robustly and adaptively stabilizing\nlarge-scale networked uncertain Markovian jump linear systems (MJLS) under\nexternal disturbances and mode switches that can change the network's topology.\nAdaptation is achieved by using minimal information on the disturbance to\nidentify modes that are consistent with observable data. Robust control is\nachieved by extending the system level synthesis (SLS) approach, which allows\nus to pose the problem of simultaneously stabilizing multiple plants as a\ntwo-step convex optimization procedure. Our control pipeline computes a\nlikelihood distribution of the system's current mode, uses them as\nprobabilistic weights during simultaneous stabilization, then updates the\nlikelihood via Bayesian inference. Because of this \"softer\" probabilistic\napproach to robust stabilization, our control pipeline does not suffer from\nabrupt destabilization issues due to changes in the system's true mode, which\nwere observed in a previous method. Separability of SLS also lets us compute\nlocalized robust controllers for each subsystem, allowing for network\nscalability; we use several information consensus methods so that mode\nestimation can also be done locally. We apply our algorithms to\ndisturbance-rejection on two sample dynamic power grid networks, a small-scale\nsystem with 7 nodes and a large-scale grid of 25 nodes."
                },
                "authors": [
                    {
                        "name": "SooJean Han"
                    },
                    {
                        "name": "Minwoo M. Kim"
                    },
                    {
                        "name": "Ieun Choo"
                    }
                ],
                "author_detail": {
                    "name": "Ieun Choo"
                },
                "author": "Ieun Choo",
                "arxiv_comment": "Full version of accepted paper to 63rd IEEE Conference on Decision\n  and Control (CDC) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.14382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.14382v2",
                "updated": "2024-08-28T13:30:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    30,
                    36,
                    2,
                    241,
                    0
                ],
                "published": "2023-07-25T20:08:41Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    20,
                    8,
                    41,
                    1,
                    206,
                    0
                ],
                "title": "When Multi-Task Learning Meets Partial Supervision: A Computer Vision\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Multi-Task Learning Meets Partial Supervision: A Computer Vision\n  Review"
                },
                "summary": "Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while\nexploiting their mutual relationships. By using shared resources to\nsimultaneously calculate multiple outputs, this learning paradigm has the\npotential to have lower memory requirements and inference times compared to the\ntraditional approach of using separate methods for each task. Previous work in\nMTL has mainly focused on fully-supervised methods, as task relationships can\nnot only be leveraged to lower the level of data-dependency of those methods\nbut they can also improve performance. However, MTL introduces a set of\nchallenges due to a complex optimisation scheme and a higher labeling\nrequirement. This review focuses on how MTL could be utilised under different\npartial supervision settings to address these challenges. First, this review\nanalyses how MTL traditionally uses different parameter sharing techniques to\ntransfer knowledge in between tasks. Second, it presents the different\nchallenges arising from such a multi-objective optimisation scheme. Third, it\nintroduces how task groupings can be achieved by analysing task relationships.\nFourth, it focuses on how partially supervised methods applied to MTL can\ntackle the aforementioned challenges. Lastly, this review presents the\navailable datasets, tools and benchmarking results of such methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while\nexploiting their mutual relationships. By using shared resources to\nsimultaneously calculate multiple outputs, this learning paradigm has the\npotential to have lower memory requirements and inference times compared to the\ntraditional approach of using separate methods for each task. Previous work in\nMTL has mainly focused on fully-supervised methods, as task relationships can\nnot only be leveraged to lower the level of data-dependency of those methods\nbut they can also improve performance. However, MTL introduces a set of\nchallenges due to a complex optimisation scheme and a higher labeling\nrequirement. This review focuses on how MTL could be utilised under different\npartial supervision settings to address these challenges. First, this review\nanalyses how MTL traditionally uses different parameter sharing techniques to\ntransfer knowledge in between tasks. Second, it presents the different\nchallenges arising from such a multi-objective optimisation scheme. Third, it\nintroduces how task groupings can be achieved by analysing task relationships.\nFourth, it focuses on how partially supervised methods applied to MTL can\ntackle the aforementioned challenges. Lastly, this review presents the\navailable datasets, tools and benchmarking results of such methods."
                },
                "authors": [
                    {
                        "name": "Maxime Fontana"
                    },
                    {
                        "name": "Michael Spratling"
                    },
                    {
                        "name": "Miaojing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Miaojing Shi"
                },
                "author": "Miaojing Shi",
                "arxiv_comment": "Accepted by Proceedings of the IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.14382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.14382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15787v1",
                "updated": "2024-08-28T13:29:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    29,
                    59,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:29:59Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    29,
                    59,
                    2,
                    241,
                    0
                ],
                "title": "Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions"
                },
                "summary": "Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health."
                },
                "authors": [
                    {
                        "name": "Huachuan Qiu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v1",
                "updated": "2024-08-28T13:16:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15769v1",
                "updated": "2024-08-28T13:05:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    5,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:05:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    5,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "A Survey on Evaluation of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluation of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs."
                },
                "authors": [
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Jingyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyi Zhang"
                },
                "author": "Jingyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v1",
                "updated": "2024-08-28T12:59:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Harmonized Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonized Speculative Sampling"
                },
                "summary": "Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.01417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.01417v2",
                "updated": "2024-08-28T12:57:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    57,
                    0,
                    2,
                    241,
                    0
                ],
                "published": "2022-10-04T07:07:01Z",
                "published_parsed": [
                    2022,
                    10,
                    4,
                    7,
                    7,
                    1,
                    1,
                    277,
                    0
                ],
                "title": "The impacts of solar wind on the Martian upper atmosphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impacts of solar wind on the Martian upper atmosphere"
                },
                "summary": "Since the first in-situ measurements of the altitude profile of upper\natmospheric density and composition were carried out by the Viking lander\nmissions in 1976, similar data are continuously gathered by MAVEN and MOM\nspacecraft orbiting Mars since their launch in September 2014 with mass\nspectrometers and other related payloads. Using near-simultaneous observations\nby the two orbiters, it is seen that both data sets indicate significant\nday-to-day variations of Argon density profiles in the thermosphere-exosphere,\n150-300 km region, during the period 1-15, June 2018, when the solar EUV\nradiation did not show any appreciable change but the solar wind energetic\nparticle fluxes did so. Extending this study to include the other parent\natmospheric constituents carbon dioxide, helium, nitrogen and their\nphotochemical products atomic oxygen, and carbon monoxide during the same\nperiod it is found that the density profiles of carbon dioxide and atomic\noxygen also show similar variations with carbon dioxide densities showing an\nincreasing trend similar to Argon, but a reversal of this trend for atomic\noxygen densities. Using insitu and near simultaneous measurements of solar EUV\nfluxes and the solar wind plasma velocities and densities near MAVEN periapsis\nit is noted that, unlike the solar EUV radiation, solar wind parameters showed\na decrease by a factor of 2-3. Hence, it is inferred that the energetic and\npenetrating solar wind charged particle impact-driven dissociation, ionisation\nand ion-chemical processes could decrease the carbon dioxide densities leading\nto an increase in atomic oxygen densities. This result is also discussed from\nthe considerations of the proton gyro radius effect, pickup ions, sputtering,\nenergetic neutral atoms driven ionisation and ion losses. Further data and\nmodelling efforts would be necessary to confirm this finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the first in-situ measurements of the altitude profile of upper\natmospheric density and composition were carried out by the Viking lander\nmissions in 1976, similar data are continuously gathered by MAVEN and MOM\nspacecraft orbiting Mars since their launch in September 2014 with mass\nspectrometers and other related payloads. Using near-simultaneous observations\nby the two orbiters, it is seen that both data sets indicate significant\nday-to-day variations of Argon density profiles in the thermosphere-exosphere,\n150-300 km region, during the period 1-15, June 2018, when the solar EUV\nradiation did not show any appreciable change but the solar wind energetic\nparticle fluxes did so. Extending this study to include the other parent\natmospheric constituents carbon dioxide, helium, nitrogen and their\nphotochemical products atomic oxygen, and carbon monoxide during the same\nperiod it is found that the density profiles of carbon dioxide and atomic\noxygen also show similar variations with carbon dioxide densities showing an\nincreasing trend similar to Argon, but a reversal of this trend for atomic\noxygen densities. Using insitu and near simultaneous measurements of solar EUV\nfluxes and the solar wind plasma velocities and densities near MAVEN periapsis\nit is noted that, unlike the solar EUV radiation, solar wind parameters showed\na decrease by a factor of 2-3. Hence, it is inferred that the energetic and\npenetrating solar wind charged particle impact-driven dissociation, ionisation\nand ion-chemical processes could decrease the carbon dioxide densities leading\nto an increase in atomic oxygen densities. This result is also discussed from\nthe considerations of the proton gyro radius effect, pickup ions, sputtering,\nenergetic neutral atoms driven ionisation and ion losses. Further data and\nmodelling efforts would be necessary to confirm this finding."
                },
                "authors": [
                    {
                        "name": "Kamsali Nagaraja"
                    },
                    {
                        "name": "S. C. Chakravarty"
                    }
                ],
                "author_detail": {
                    "name": "S. C. Chakravarty"
                },
                "author": "S. C. Chakravarty",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.01417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.01417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11122v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11122v6",
                "updated": "2024-08-28T12:33:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    33,
                    52,
                    2,
                    241,
                    0
                ],
                "published": "2023-10-17T10:14:10Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    10,
                    14,
                    10,
                    1,
                    290,
                    0
                ],
                "title": "Sensitivity-Aware Amortized Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity-Aware Amortized Bayesian Inference"
                },
                "summary": "Sensitivity analyses reveal the influence of various modeling choices on the\noutcomes of statistical analyses. While theoretically appealing, they are\noverwhelmingly inefficient for complex Bayesian models. In this work, we\npropose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted\napproach to efficiently integrate sensitivity analyses into simulation-based\ninference with neural networks. First, we utilize weight sharing to encode the\nstructural similarities between alternative likelihood and prior specifications\nin the training process with minimal computational overhead. Second, we\nleverage the rapid inference of neural networks to assess sensitivity to data\nperturbations and preprocessing steps. In contrast to most other Bayesian\napproaches, both steps circumvent the costly bottleneck of refitting the model\nfor each choice of likelihood, prior, or data set. Finally, we propose to use\ndeep ensembles to detect sensitivity arising from unreliable approximation\n(e.g., due to model misspecification). We demonstrate the effectiveness of our\nmethod in applied modeling problems, ranging from disease outbreak dynamics and\nglobal warming thresholds to human decision-making. Our results support\nsensitivity-aware inference as a default choice for amortized Bayesian\nworkflows, automatically providing modelers with insights into otherwise hidden\ndimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity analyses reveal the influence of various modeling choices on the\noutcomes of statistical analyses. While theoretically appealing, they are\noverwhelmingly inefficient for complex Bayesian models. In this work, we\npropose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted\napproach to efficiently integrate sensitivity analyses into simulation-based\ninference with neural networks. First, we utilize weight sharing to encode the\nstructural similarities between alternative likelihood and prior specifications\nin the training process with minimal computational overhead. Second, we\nleverage the rapid inference of neural networks to assess sensitivity to data\nperturbations and preprocessing steps. In contrast to most other Bayesian\napproaches, both steps circumvent the costly bottleneck of refitting the model\nfor each choice of likelihood, prior, or data set. Finally, we propose to use\ndeep ensembles to detect sensitivity arising from unreliable approximation\n(e.g., due to model misspecification). We demonstrate the effectiveness of our\nmethod in applied modeling problems, ranging from disease outbreak dynamics and\nglobal warming thresholds to human decision-making. Our results support\nsensitivity-aware inference as a default choice for amortized Bayesian\nworkflows, automatically providing modelers with insights into otherwise hidden\ndimensions."
                },
                "authors": [
                    {
                        "name": "Lasse Elsem√ºller"
                    },
                    {
                        "name": "Hans Olischl√§ger"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Paul-Christian B√ºrkner"
                    },
                    {
                        "name": "Ullrich K√∂the"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "arxiv_comment": "Published in TMLR (2024)",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (08/2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11122v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11122v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14398v2",
                "updated": "2024-08-28T12:03:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "title": "Language-specific Calibration for Pruning Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific Calibration for Pruning Multilingual Language Models"
                },
                "summary": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners."
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15739v1",
                "updated": "2024-08-28T12:03:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    3,
                    25,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T12:03:25Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    3,
                    25,
                    2,
                    241,
                    0
                ],
                "title": "A review of sequential Monte Carlo methods for real-time disease\n  modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A review of sequential Monte Carlo methods for real-time disease\n  modeling"
                },
                "summary": "Sequential Monte Carlo methods are a powerful framework for approximating the\nposterior distribution of a state variable in a sequential manner. They provide\nan attractive way of analyzing dynamic systems in real-time, taking into\naccount the limitations of traditional approaches such as Markov Chain Monte\nCarlo methods, which are not well suited to data that arrives incrementally.\nThis paper reviews and explores the application of Sequential Monte Carlo in\ndynamic disease modeling, highlighting its capacity for online inference and\nreal-time adaptation to evolving disease dynamics. The integration of kernel\ndensity approximation techniques within the stochastic\nSusceptible-Exposed-Infectious-Recovered (SEIR) compartment model is examined,\ndemonstrating the algorithm's effectiveness in monitoring time-varying\nparameters such as the effective reproduction number. Case studies, including\nsimulations with synthetic data and analysis of real-world COVID-19 data from\nIreland, demonstrate the practical applicability of this approach for informing\ntimely public health interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo methods are a powerful framework for approximating the\nposterior distribution of a state variable in a sequential manner. They provide\nan attractive way of analyzing dynamic systems in real-time, taking into\naccount the limitations of traditional approaches such as Markov Chain Monte\nCarlo methods, which are not well suited to data that arrives incrementally.\nThis paper reviews and explores the application of Sequential Monte Carlo in\ndynamic disease modeling, highlighting its capacity for online inference and\nreal-time adaptation to evolving disease dynamics. The integration of kernel\ndensity approximation techniques within the stochastic\nSusceptible-Exposed-Infectious-Recovered (SEIR) compartment model is examined,\ndemonstrating the algorithm's effectiveness in monitoring time-varying\nparameters such as the effective reproduction number. Case studies, including\nsimulations with synthetic data and analysis of real-world COVID-19 data from\nIreland, demonstrate the practical applicability of this approach for informing\ntimely public health interventions."
                },
                "authors": [
                    {
                        "name": "Dhorasso Temfack"
                    },
                    {
                        "name": "Jason Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Jason Wyse"
                },
                "author": "Jason Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17479v3",
                "updated": "2024-08-28T11:27:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    27,
                    25,
                    2,
                    241,
                    0
                ],
                "published": "2023-05-27T13:57:26Z",
                "published_parsed": [
                    2023,
                    5,
                    27,
                    13,
                    57,
                    26,
                    5,
                    147,
                    0
                ],
                "title": "Inferring Individual Direct Causal Effects Under Heterogeneous Peer\n  Influence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Individual Direct Causal Effects Under Heterogeneous Peer\n  Influence"
                },
                "summary": "Causal inference in networks should account for interference, which occurs\nwhen a unit's outcome is influenced by treatments or outcomes of peers.\nHeterogeneous peer influence (HPI) occurs when a unit's outcome is influenced\ndifferently by different peers based on their attributes and relationships, or\nwhen each unit has a different susceptibility to peer influence. Existing\nsolutions to estimating direct causal effects under interference consider\neither homogeneous influence from peers or specific heterogeneous influence\nmechanisms (e.g., based on local neighborhood structure). This paper presents a\nmethodology for estimating individual direct causal effects in the presence of\nHPI where the mechanism of influence is not known a priori. We propose a\nstructural causal model for networks that can capture different possible\nassumptions about network structure, interference conditions, and causal\ndependence and enables reasoning about identifiability in the presence of HPI.\nWe find potential heterogeneous contexts using the causal model and propose a\nnovel graph neural network-based estimator to estimate individual direct causal\neffects. We show that state-of-the-art methods for individual direct effect\nestimation produce biased results in the presence of HPI, and that our proposed\nestimator is robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference in networks should account for interference, which occurs\nwhen a unit's outcome is influenced by treatments or outcomes of peers.\nHeterogeneous peer influence (HPI) occurs when a unit's outcome is influenced\ndifferently by different peers based on their attributes and relationships, or\nwhen each unit has a different susceptibility to peer influence. Existing\nsolutions to estimating direct causal effects under interference consider\neither homogeneous influence from peers or specific heterogeneous influence\nmechanisms (e.g., based on local neighborhood structure). This paper presents a\nmethodology for estimating individual direct causal effects in the presence of\nHPI where the mechanism of influence is not known a priori. We propose a\nstructural causal model for networks that can capture different possible\nassumptions about network structure, interference conditions, and causal\ndependence and enables reasoning about identifiability in the presence of HPI.\nWe find potential heterogeneous contexts using the causal model and propose a\nnovel graph neural network-based estimator to estimate individual direct causal\neffects. We show that state-of-the-art methods for individual direct effect\nestimation produce biased results in the presence of HPI, and that our proposed\nestimator is robust."
                },
                "authors": [
                    {
                        "name": "Shishir Adhikari"
                    },
                    {
                        "name": "Elena Zheleva"
                    }
                ],
                "author_detail": {
                    "name": "Elena Zheleva"
                },
                "author": "Elena Zheleva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.17479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15717v1",
                "updated": "2024-08-28T11:24:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    24,
                    17,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T11:24:17Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    24,
                    17,
                    2,
                    241,
                    0
                ],
                "title": "Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition\n  for Human Robot-Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition\n  for Human Robot-Interaction"
                },
                "summary": "Human pose estimation involves detecting and tracking the positions of\nvarious body parts using input data from sources such as images, videos, or\nmotion and inertial sensors. This paper presents a novel approach to human pose\nestimation using machine learning algorithms to predict human posture and\ntranslate them into robot motion commands using ultra-wideband (UWB) nodes, as\nan alternative to motion sensors. The study utilizes five UWB sensors\nimplemented on the human body to enable the classification of still poses and\nmore robust posture recognition. This approach ensures effective posture\nrecognition across a variety of subjects. These range measurements serve as\ninput features for posture prediction models, which are implemented and\ncompared for accuracy. For this purpose, machine learning algorithms including\nK-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer\nPerceptron (MLP) neural network are employed and compared in predicting\ncorresponding postures. We demonstrate the proposed approach for real-time\ncontrol of different mobile/aerial robots with inference implemented in a ROS 2\nnode. Experimental results demonstrate the efficacy of the approach, showcasing\nsuccessful prediction of human posture and corresponding robot movements with\nhigh accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human pose estimation involves detecting and tracking the positions of\nvarious body parts using input data from sources such as images, videos, or\nmotion and inertial sensors. This paper presents a novel approach to human pose\nestimation using machine learning algorithms to predict human posture and\ntranslate them into robot motion commands using ultra-wideband (UWB) nodes, as\nan alternative to motion sensors. The study utilizes five UWB sensors\nimplemented on the human body to enable the classification of still poses and\nmore robust posture recognition. This approach ensures effective posture\nrecognition across a variety of subjects. These range measurements serve as\ninput features for posture prediction models, which are implemented and\ncompared for accuracy. For this purpose, machine learning algorithms including\nK-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer\nPerceptron (MLP) neural network are employed and compared in predicting\ncorresponding postures. We demonstrate the proposed approach for real-time\ncontrol of different mobile/aerial robots with inference implemented in a ROS 2\nnode. Experimental results demonstrate the efficacy of the approach, showcasing\nsuccessful prediction of human posture and corresponding robot movements with\nhigh accuracy."
                },
                "authors": [
                    {
                        "name": "Salma Salimi"
                    },
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Jorge Pe√±a Queralta"
                    },
                    {
                        "name": "Wallace Moreira Bessa"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15710v2",
                "updated": "2024-08-29T14:47:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    47,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T11:18:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    18,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples"
                },
                "summary": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark"
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11239v2",
                "updated": "2024-08-28T11:10:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    10,
                    59,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-17T06:07:32Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    6,
                    7,
                    32,
                    0,
                    169,
                    0
                ],
                "title": "Evading AI-Generated Content Detectors using Homoglyphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evading AI-Generated Content Detectors using Homoglyphs"
                },
                "summary": "The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks."
                },
                "authors": [
                    {
                        "name": "Aldan Creo"
                    },
                    {
                        "name": "Shushanta Pudasaini"
                    }
                ],
                "author_detail": {
                    "name": "Shushanta Pudasaini"
                },
                "author": "Shushanta Pudasaini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v1",
                "updated": "2024-08-28T10:51:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11537v3",
                "updated": "2024-08-28T10:39:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    39,
                    11,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-18T10:36:05Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    10,
                    36,
                    5,
                    6,
                    49,
                    0
                ],
                "title": "Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning"
                },
                "summary": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted by ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12471v2",
                "updated": "2024-08-28T09:48:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    48,
                    24,
                    2,
                    241,
                    0
                ],
                "published": "2024-01-23T03:45:05Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    3,
                    45,
                    5,
                    1,
                    23,
                    0
                ],
                "title": "Training-Free Action Recognition and Goal Inference with Dynamic Frame\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Action Recognition and Goal Inference with Dynamic Frame\n  Selection"
                },
                "summary": "We introduce VidTFS, a Training-free, open-vocabulary video goal and action\ninference framework that combines the frozen vision foundational model (VFM)\nand large language model (LLM) with a novel dynamic Frame Selection module. Our\nexperiments demonstrate that the proposed frame selection module improves the\nperformance of the framework significantly. We validate the performance of the\nproposed VidTFS on four widely used video datasets, including CrossTask, COIN,\nUCF101, and ActivityNet, covering goal inference and action recognition tasks\nunder open-vocabulary settings without requiring any training or fine-tuning.\nThe results show that VidTFS outperforms pretrained and instruction-tuned\nmultimodal language models that directly stack LLM and VFM for downstream video\ninference tasks. Our VidTFS with its adaptability shows the future potential\nfor generalizing to new training-free video inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VidTFS, a Training-free, open-vocabulary video goal and action\ninference framework that combines the frozen vision foundational model (VFM)\nand large language model (LLM) with a novel dynamic Frame Selection module. Our\nexperiments demonstrate that the proposed frame selection module improves the\nperformance of the framework significantly. We validate the performance of the\nproposed VidTFS on four widely used video datasets, including CrossTask, COIN,\nUCF101, and ActivityNet, covering goal inference and action recognition tasks\nunder open-vocabulary settings without requiring any training or fine-tuning.\nThe results show that VidTFS outperforms pretrained and instruction-tuned\nmultimodal language models that directly stack LLM and VFM for downstream video\ninference tasks. Our VidTFS with its adaptability shows the future potential\nfor generalizing to new training-free video inference tasks."
                },
                "authors": [
                    {
                        "name": "Ee Yeo Keat"
                    },
                    {
                        "name": "Zhang Hao"
                    },
                    {
                        "name": "Alexander Matyasko"
                    },
                    {
                        "name": "Basura Fernando"
                    }
                ],
                "author_detail": {
                    "name": "Basura Fernando"
                },
                "author": "Basura Fernando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15670v1",
                "updated": "2024-08-28T09:46:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    46,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:46:46Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    46,
                    46,
                    2,
                    241,
                    0
                ],
                "title": "Adaptive Weighted Random Isolation (AWRI): a simple design to estimate\n  causal effects under network interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Weighted Random Isolation (AWRI): a simple design to estimate\n  causal effects under network interference"
                },
                "summary": "Recently, causal inference under interference has gained increasing attention\nin the literature. In this paper, we focus on randomized designs for estimating\nthe total treatment effect (TTE), defined as the average difference in\npotential outcomes between fully treated and fully controlled groups. We\npropose a simple design called weighted random isolation (WRI) along with a\nrestricted difference-in-means estimator (RDIM) for TTE estimation.\nAdditionally, we derive a novel mean squared error surrogate for the RDIM\nestimator, supported by a network-adaptive weight selection algorithm. This can\nhelp us determine a fair weight for the WRI design, thereby effectively\nreducing the bias. Our method accommodates directed networks, extending\nprevious frameworks. Extensive simulations demonstrate that the proposed method\noutperforms nine established methods across a wide range of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, causal inference under interference has gained increasing attention\nin the literature. In this paper, we focus on randomized designs for estimating\nthe total treatment effect (TTE), defined as the average difference in\npotential outcomes between fully treated and fully controlled groups. We\npropose a simple design called weighted random isolation (WRI) along with a\nrestricted difference-in-means estimator (RDIM) for TTE estimation.\nAdditionally, we derive a novel mean squared error surrogate for the RDIM\nestimator, supported by a network-adaptive weight selection algorithm. This can\nhelp us determine a fair weight for the WRI design, thereby effectively\nreducing the bias. Our method accommodates directed networks, extending\nprevious frameworks. Extensive simulations demonstrate that the proposed method\noutperforms nine established methods across a wide range of scenarios."
                },
                "authors": [
                    {
                        "name": "Changhao Shi"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Yichen Qin"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "26 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15666v1",
                "updated": "2024-08-28T09:35:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    35,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:35:15Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    35,
                    15,
                    2,
                    241,
                    0
                ],
                "title": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and\n  Perturbation of Style Elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and\n  Perturbation of Style Elements"
                },
                "summary": "Authorship obfuscation, rewriting a text to intentionally obscure the\nidentity of the author, is an important but challenging task. Current methods\nusing large language models (LLMs) lack interpretability and controllability,\noften ignoring author-specific stylistic features, resulting in less robust\nperformance overall.\n  To address this, we develop StyleRemix, an adaptive and interpretable\nobfuscation method that perturbs specific, fine-grained style elements of the\noriginal input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA)\nmodules to rewrite an input specifically along various stylistic axes (e.g.,\nformality and length) while maintaining low computational cost. StyleRemix\noutperforms state-of-the-art baselines and much larger LLMs in a variety of\ndomains as assessed by both automatic and human evaluation.\n  Additionally, we release AuthorMix, a large set of 30K high-quality,\nlong-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a\nparallel corpus of 1,500 texts spanning seven style axes in 16 unique\ndirections",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authorship obfuscation, rewriting a text to intentionally obscure the\nidentity of the author, is an important but challenging task. Current methods\nusing large language models (LLMs) lack interpretability and controllability,\noften ignoring author-specific stylistic features, resulting in less robust\nperformance overall.\n  To address this, we develop StyleRemix, an adaptive and interpretable\nobfuscation method that perturbs specific, fine-grained style elements of the\noriginal input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA)\nmodules to rewrite an input specifically along various stylistic axes (e.g.,\nformality and length) while maintaining low computational cost. StyleRemix\noutperforms state-of-the-art baselines and much larger LLMs in a variety of\ndomains as assessed by both automatic and human evaluation.\n  Additionally, we release AuthorMix, a large set of 30K high-quality,\nlong-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a\nparallel corpus of 1,500 texts spanning seven style axes in 16 unique\ndirections"
                },
                "authors": [
                    {
                        "name": "Jillian Fisher"
                    },
                    {
                        "name": "Skyler Hallinan"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Mitchell Gordon"
                    },
                    {
                        "name": "Zaid Harchaoui"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08835v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08835v3",
                "updated": "2024-08-28T09:30:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    30,
                    26,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-13T05:57:54Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    5,
                    57,
                    54,
                    3,
                    165,
                    0
                ],
                "title": "EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech\n  Recognition Architecture with High Accuracy and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech\n  Recognition Architecture with High Accuracy and Inference Speed"
                },
                "summary": "Non-autoregressive (NAR) automatic speech recognition (ASR) models predict\ntokens independently and simultaneously, bringing high inference speed.\nHowever, there is still a gap in the accuracy of the NAR models compared to the\nautoregressive (AR) models. In this paper, we propose a single-step NAR ASR\narchitecture with high accuracy and inference speed, called EffectiveASR. It\nuses an Index Mapping Vector (IMV) based alignment generator to generate\nalignments during training, and an alignment predictor to learn the alignments\nfor inference. It can be trained end-to-end (E2E) with cross-entropy loss\ncombined with alignment loss. The proposed EffectiveASR achieves competitive\nresults on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the\nleading models. Specifically, it achieves character error rates (CER) of\n4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR\nConformer with about 30x inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-autoregressive (NAR) automatic speech recognition (ASR) models predict\ntokens independently and simultaneously, bringing high inference speed.\nHowever, there is still a gap in the accuracy of the NAR models compared to the\nautoregressive (AR) models. In this paper, we propose a single-step NAR ASR\narchitecture with high accuracy and inference speed, called EffectiveASR. It\nuses an Index Mapping Vector (IMV) based alignment generator to generate\nalignments during training, and an alignment predictor to learn the alignments\nfor inference. It can be trained end-to-end (E2E) with cross-entropy loss\ncombined with alignment loss. The proposed EffectiveASR achieves competitive\nresults on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the\nleading models. Specifically, it achieves character error rates (CER) of\n4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR\nConformer with about 30x inference speedup."
                },
                "authors": [
                    {
                        "name": "Ziyang Zhuang"
                    },
                    {
                        "name": "Chenfeng Miao"
                    },
                    {
                        "name": "Kun Zou"
                    },
                    {
                        "name": "Ming Fang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Ning Cheng"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08835v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08835v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15660v1",
                "updated": "2024-08-28T09:22:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    22,
                    32,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:22:32Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    22,
                    32,
                    2,
                    241,
                    0
                ],
                "title": "Merging and Splitting Diffusion Paths for Semantically Coherent\n  Panoramas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging and Splitting Diffusion Paths for Semantically Coherent\n  Panoramas"
                },
                "summary": "Diffusion models have become the State-of-the-Art for text-to-image\ngeneration, and increasing research effort has been dedicated to adapting the\ninference process of pretrained diffusion models to achieve zero-shot\ncapabilities. An example is the generation of panorama images, which has been\ntackled in recent works by combining independent diffusion paths over\noverlapping latent features, which is referred to as joint diffusion, obtaining\nperceptually aligned panoramas. However, these methods often yield semantically\nincoherent outputs and trade-off diversity for uniformity. To overcome this\nlimitation, we propose the Merge-Attend-Diffuse operator, which can be plugged\ninto different types of pretrained diffusion models used in a joint diffusion\nsetting to improve the perceptual and semantical coherence of the generated\npanorama images. Specifically, we merge the diffusion paths, reprogramming\nself- and cross-attention to operate on the aggregated latent space. Extensive\nquantitative and qualitative experimental analysis, together with a user study,\ndemonstrate that our method maintains compatibility with the input prompt and\nvisual quality of the generated images while increasing their semantic\ncoherence. We release the code at https://github.com/aimagelab/MAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have become the State-of-the-Art for text-to-image\ngeneration, and increasing research effort has been dedicated to adapting the\ninference process of pretrained diffusion models to achieve zero-shot\ncapabilities. An example is the generation of panorama images, which has been\ntackled in recent works by combining independent diffusion paths over\noverlapping latent features, which is referred to as joint diffusion, obtaining\nperceptually aligned panoramas. However, these methods often yield semantically\nincoherent outputs and trade-off diversity for uniformity. To overcome this\nlimitation, we propose the Merge-Attend-Diffuse operator, which can be plugged\ninto different types of pretrained diffusion models used in a joint diffusion\nsetting to improve the perceptual and semantical coherence of the generated\npanorama images. Specifically, we merge the diffusion paths, reprogramming\nself- and cross-attention to operate on the aggregated latent space. Extensive\nquantitative and qualitative experimental analysis, together with a user study,\ndemonstrate that our method maintains compatibility with the input prompt and\nvisual quality of the generated images while increasing their semantic\ncoherence. We release the code at https://github.com/aimagelab/MAD."
                },
                "authors": [
                    {
                        "name": "Fabio Quattrini"
                    },
                    {
                        "name": "Vittorio Pippi"
                    },
                    {
                        "name": "Silvia Cascianelli"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "Accepted at ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15658v1",
                "updated": "2024-08-28T09:19:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    19,
                    9,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:19:09Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    19,
                    9,
                    2,
                    241,
                    0
                ],
                "title": "An Empirical Study on Self-correcting Large Language Models for Data\n  Science Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Self-correcting Large Language Models for Data\n  Science Code Generation"
                },
                "summary": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation."
                },
                "authors": [
                    {
                        "name": "Thai Tang Quoc"
                    },
                    {
                        "name": "Duc Ha Minh"
                    },
                    {
                        "name": "Tho Quan Thanh"
                    },
                    {
                        "name": "Anh Nguyen-Duc"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen-Duc"
                },
                "author": "Anh Nguyen-Duc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13123v2",
                "updated": "2024-08-28T09:18:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    18,
                    0,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-23T14:50:49Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    14,
                    50,
                    49,
                    4,
                    236,
                    0
                ],
                "title": "Evidential Deep Partial Multi-View Classification With Discount Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidential Deep Partial Multi-View Classification With Discount Fusion"
                },
                "summary": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete multi-view data classification poses significant challenges due to\nthe common issue of missing views in real-world scenarios. Despite\nadvancements, existing methods often fail to provide reliable predictions,\nlargely due to the uncertainty of missing views and the inconsistent quality of\nimputed data. To tackle these problems, we propose a novel framework called\nEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we use\nK-means imputation to address missing views, creating a complete set of\nmulti-view data. However, the potential conflicts and uncertainties within this\nimputed data can affect the reliability of downstream inferences. To manage\nthis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), which\ndynamically adjusts based on the reliability of the evidence, ensuring\ntrustworthy discount fusion and producing reliable inference outcomes.\nComprehensive experiments on various benchmark datasets reveal EDP-MVC not only\nmatches but often surpasses the performance of state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Sukumar Letchmunan"
                    },
                    {
                        "name": "Muhammet Deveci"
                    },
                    {
                        "name": "Mingwei Lin"
                    },
                    {
                        "name": "Weizhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weizhong Wang"
                },
                "author": "Weizhong Wang",
                "arxiv_comment": "Ongoing work. 13 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15119v2",
                "updated": "2024-08-28T09:11:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    11,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T14:58:13Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    58,
                    13,
                    1,
                    240,
                    0
                ],
                "title": "Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urdu Digital Text Word Optical Character Recognition Using Permuted Auto\n  Regressive Sequence Modeling"
                },
                "summary": "This research paper presents a novel word-level Optical Character Recognition\n(OCR) model developed specifically for digital Urdu text. The model utilizes\ntransformer-based architectures and attention mechanisms to address the unique\nchallenges of recognizing Urdu script, which includes handling a diverse range\nof text styles, fonts, and variations. Trained on a comprehensive dataset of\napproximately 160,000 Urdu text images, the model incorporates a permuted\nautoregressive sequence (PARSeq) architecture. This design enables\ncontext-aware inference and iterative refinement by leveraging bidirectional\ncontext information, significantly enhancing its ability to accurately\nrecognize Urdu characters. The model achieves a character error rate (CER) of\n0.178, highlighting its effectiveness and precision in real-world applications.\nHowever, the model has some limitations, such as difficulties with blurred\nimages, non-horizontal orientations, and the presence of trailing punctuation\nmarks, which can introduce noise into the recognition process. Addressing these\nchallenges will be a key focus of future work. Future research will aim to\nfurther refine the model through advanced data augmentation techniques,\noptimization of hyperparameters, and the integration of context-aware language\nmodels, ultimately enhancing the model's performance and robustness in Urdu\ntext recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper presents a novel word-level Optical Character Recognition\n(OCR) model developed specifically for digital Urdu text. The model utilizes\ntransformer-based architectures and attention mechanisms to address the unique\nchallenges of recognizing Urdu script, which includes handling a diverse range\nof text styles, fonts, and variations. Trained on a comprehensive dataset of\napproximately 160,000 Urdu text images, the model incorporates a permuted\nautoregressive sequence (PARSeq) architecture. This design enables\ncontext-aware inference and iterative refinement by leveraging bidirectional\ncontext information, significantly enhancing its ability to accurately\nrecognize Urdu characters. The model achieves a character error rate (CER) of\n0.178, highlighting its effectiveness and precision in real-world applications.\nHowever, the model has some limitations, such as difficulties with blurred\nimages, non-horizontal orientations, and the presence of trailing punctuation\nmarks, which can introduce noise into the recognition process. Addressing these\nchallenges will be a key focus of future work. Future research will aim to\nfurther refine the model through advanced data augmentation techniques,\noptimization of hyperparameters, and the integration of context-aware language\nmodels, ultimately enhancing the model's performance and robustness in Urdu\ntext recognition."
                },
                "authors": [
                    {
                        "name": "Ahmed Mustafa"
                    },
                    {
                        "name": "Muhammad Tahir Rafique"
                    },
                    {
                        "name": "Muhammad Ijlal Baig"
                    },
                    {
                        "name": "Hasan Sajid"
                    },
                    {
                        "name": "Muhammad Jawad Khan"
                    },
                    {
                        "name": "Karam Dad Kallu"
                    }
                ],
                "author_detail": {
                    "name": "Karam Dad Kallu"
                },
                "author": "Karam Dad Kallu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15649v1",
                "updated": "2024-08-28T09:04:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    4,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:04:15Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    4,
                    15,
                    2,
                    241,
                    0
                ],
                "title": "Hierarchical Blockmodelling for Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Blockmodelling for Knowledge Graphs"
                },
                "summary": "In this paper, we investigate the use of probabilistic graphical models,\nspecifically stochastic blockmodels, for the purpose of hierarchical entity\nclustering on knowledge graphs. These models, seldom used in the Semantic Web\ncommunity, decompose a graph into a set of probability distributions. The\nparameters of these distributions are then inferred allowing for their\nsubsequent sampling to generate a random graph. In a non-parametric setting,\nthis allows for the induction of hierarchical clusterings without prior\nconstraints on the hierarchy's structure. Specifically, this is achieved by the\nintegration of the Nested Chinese Restaurant Process and the Stick Breaking\nProcess into the generative model. In this regard, we propose a model\nleveraging such integration and derive a collapsed Gibbs sampling scheme for\nits inference. To aid in understanding, we describe the steps in this\nderivation and provide an implementation for the sampler. We evaluate our model\non synthetic and real-world datasets and quantitatively compare against\nbenchmark models. We further evaluate our results qualitatively and find that\nour model is capable of inducing coherent cluster hierarchies in small scale\nsettings. The work presented in this paper provides the first step for the\nfurther application of stochastic blockmodels for knowledge graphs on a larger\nscale. We conclude the paper with potential avenues for future work on more\nscalable inference schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the use of probabilistic graphical models,\nspecifically stochastic blockmodels, for the purpose of hierarchical entity\nclustering on knowledge graphs. These models, seldom used in the Semantic Web\ncommunity, decompose a graph into a set of probability distributions. The\nparameters of these distributions are then inferred allowing for their\nsubsequent sampling to generate a random graph. In a non-parametric setting,\nthis allows for the induction of hierarchical clusterings without prior\nconstraints on the hierarchy's structure. Specifically, this is achieved by the\nintegration of the Nested Chinese Restaurant Process and the Stick Breaking\nProcess into the generative model. In this regard, we propose a model\nleveraging such integration and derive a collapsed Gibbs sampling scheme for\nits inference. To aid in understanding, we describe the steps in this\nderivation and provide an implementation for the sampler. We evaluate our model\non synthetic and real-world datasets and quantitatively compare against\nbenchmark models. We further evaluate our results qualitatively and find that\nour model is capable of inducing coherent cluster hierarchies in small scale\nsettings. The work presented in this paper provides the first step for the\nfurther application of stochastic blockmodels for knowledge graphs on a larger\nscale. We conclude the paper with potential avenues for future work on more\nscalable inference schemes."
                },
                "authors": [
                    {
                        "name": "Marcin Pietrasik"
                    },
                    {
                        "name": "Marek Reformat"
                    },
                    {
                        "name": "Anna Wilbik"
                    }
                ],
                "author_detail": {
                    "name": "Anna Wilbik"
                },
                "author": "Anna Wilbik",
                "arxiv_comment": "31 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02392v4",
                "updated": "2024-08-28T08:49:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    49,
                    57,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-02T16:10:55Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    10,
                    55,
                    1,
                    184,
                    0
                ],
                "title": "TokenPacker: Efficient Visual Projector for Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenPacker: Efficient Visual Projector for Multimodal LLM"
                },
                "summary": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker."
                },
                "authors": [
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Dongqi Tang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jie Qin"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "16 pages, Codes:https://github.com/CircleRadon/TokenPacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20770v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20770v3",
                "updated": "2024-08-28T08:46:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    46,
                    17,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-24T07:23:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    23,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Sentinel: LLM Agent for Adversarial Purification"
                },
                "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."
                },
                "authors": [
                    {
                        "name": "Guang Lin"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20770v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20770v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v2",
                "updated": "2024-08-28T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15630v1",
                "updated": "2024-08-28T08:32:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T08:32:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code\n  Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code\n  Validation"
                },
                "summary": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool."
                },
                "authors": [
                    {
                        "name": "Pooja Aggarwal"
                    },
                    {
                        "name": "Oishik Chatterjee"
                    },
                    {
                        "name": "Ting Dai"
                    },
                    {
                        "name": "Prateeti Mohapatra"
                    },
                    {
                        "name": "Brent Paulovicks"
                    },
                    {
                        "name": "Brad Blancett"
                    },
                    {
                        "name": "Arthur De Magalhaes"
                    }
                ],
                "author_detail": {
                    "name": "Arthur De Magalhaes"
                },
                "author": "Arthur De Magalhaes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15625v1",
                "updated": "2024-08-28T08:25:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T08:25:22Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "title": "CBF-LLM: Safe Control for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBF-LLM: Safe Control for LLM Alignment"
                },
                "summary": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks."
                },
                "authors": [
                    {
                        "name": "Yuya Miyaoka"
                    },
                    {
                        "name": "Masaki Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Masaki Inoue"
                },
                "author": "Masaki Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18312v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18312v4",
                "updated": "2024-08-28T08:07:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    7,
                    49,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-26T12:51:37Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    12,
                    51,
                    37,
                    2,
                    178,
                    0
                ],
                "title": "AI-native Memory: A Pathway from LLMs Towards AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory: A Pathway from LLMs Towards AGI"
                },
                "summary": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions."
                },
                "authors": [
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Zai Zheng"
                    },
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Mindverse Team"
                    }
                ],
                "author_detail": {
                    "name": "Mindverse Team"
                },
                "author": "Mindverse Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18312v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18312v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10389v2",
                "updated": "2024-08-28T07:49:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    49,
                    14,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-15T01:58:54Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    1,
                    58,
                    54,
                    0,
                    197,
                    0
                ],
                "title": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering"
                },
                "summary": "Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Francesco Di Sario"
                    },
                    {
                        "name": "Riccardo Renzulli"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    },
                    {
                        "name": "Marco Grangetto"
                    }
                ],
                "author_detail": {
                    "name": "Marco Grangetto"
                },
                "author": "Marco Grangetto",
                "arxiv_comment": "The paper has been accepted to the ECCV 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13283v2",
                "updated": "2024-08-28T07:36:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    36,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-18T08:36:17Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    8,
                    36,
                    17,
                    3,
                    200,
                    0
                ],
                "title": "Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian\n  Processes"
                },
                "summary": "We make use of Kronecker structure for scaling Gaussian Process models to\nlarge-scale, heterogeneous, clinical data sets. Repeated measures, commonly\nperformed in clinical research, facilitate computational acceleration for\nnonlinear Bayesian nonparametric models and enable exact sampling for\nnon-conjugate inference, when combinations of continuous and discrete endpoints\nare observed. Model inference is performed in Stan, and comparisons are made\nwith brms on simulated data and two real clinical data sets, following a\nradiological image quality theme. Scalable Gaussian Process models compare\nfavourably with parametric models on real data sets with 17,460 observations.\nDifferent GP model specifications are explored, with components analogous to\nrandom effects, and their theoretical properties are described.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We make use of Kronecker structure for scaling Gaussian Process models to\nlarge-scale, heterogeneous, clinical data sets. Repeated measures, commonly\nperformed in clinical research, facilitate computational acceleration for\nnonlinear Bayesian nonparametric models and enable exact sampling for\nnon-conjugate inference, when combinations of continuous and discrete endpoints\nare observed. Model inference is performed in Stan, and comparisons are made\nwith brms on simulated data and two real clinical data sets, following a\nradiological image quality theme. Scalable Gaussian Process models compare\nfavourably with parametric models on real data sets with 17,460 observations.\nDifferent GP model specifications are explored, with components analogous to\nrandom effects, and their theoretical properties are described."
                },
                "authors": [
                    {
                        "name": "Owen Thomas"
                    },
                    {
                        "name": "Leiv R√∏nneberg"
                    }
                ],
                "author_detail": {
                    "name": "Leiv R√∏nneberg"
                },
                "author": "Leiv R√∏nneberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15591v2",
                "updated": "2024-08-29T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    1,
                    56,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T07:31:32Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    31,
                    32,
                    2,
                    241,
                    0
                ],
                "title": "VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification"
                },
                "summary": "Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24"
                },
                "authors": [
                    {
                        "name": "Yungi Cho"
                    },
                    {
                        "name": "Woorim Han"
                    },
                    {
                        "name": "Miseon Yu"
                    },
                    {
                        "name": "Younghan Lee"
                    },
                    {
                        "name": "Ho Bae"
                    },
                    {
                        "name": "Yunheung Paek"
                    }
                ],
                "author_detail": {
                    "name": "Yunheung Paek"
                },
                "author": "Yunheung Paek",
                "arxiv_comment": "Accepted by 29th European Symposium on Research in Computer Security\n  (ESORICS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14668v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14668v4",
                "updated": "2024-08-28T07:28:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    28,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2023-05-24T03:20:09Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    3,
                    20,
                    9,
                    2,
                    144,
                    0
                ],
                "title": "NOVUM: Neural Object Volumes for Robust Object Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVUM: Neural Object Volumes for Robust Object Classification"
                },
                "summary": "Discriminative models for object classification typically learn image-based\nrepresentations that do not capture the compositional and 3D nature of objects.\nIn this work, we show that explicitly integrating 3D compositional object\nrepresentations into deep networks for image classification leads to a largely\nenhanced generalization in out-of-distribution scenarios. In particular, we\nintroduce a novel architecture, referred to as NOVUM, that consists of a\nfeature extractor and a neural object volume for every target object class.\nEach neural object volume is a composition of 3D Gaussians that emit feature\nvectors. This compositional object representation allows for a highly robust\nand fast estimation of the object class by independently matching the features\nof the 3D Gaussians of each category to features extracted from an input image.\nAdditionally, the object pose can be estimated via inverse rendering of the\ncorresponding neural object volume. To enable the classification of objects,\nthe neural features at each 3D Gaussian are trained discriminatively to be\ndistinct from (i) the features of 3D Gaussians in other categories, (ii)\nfeatures of other 3D Gaussians of the same object, and (iii) the background\nfeatures. Our experiments show that NOVUM offers intriguing advantages over\nstandard architectures due to the 3D compositional structure of the object\nrepresentation, namely: (1) An exceptional robustness across a spectrum of\nreal-world and synthetic out-of-distribution shifts and (2) an enhanced human\ninterpretability compared to standard models, all while maintaining real-time\ninference and a competitive accuracy on in-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discriminative models for object classification typically learn image-based\nrepresentations that do not capture the compositional and 3D nature of objects.\nIn this work, we show that explicitly integrating 3D compositional object\nrepresentations into deep networks for image classification leads to a largely\nenhanced generalization in out-of-distribution scenarios. In particular, we\nintroduce a novel architecture, referred to as NOVUM, that consists of a\nfeature extractor and a neural object volume for every target object class.\nEach neural object volume is a composition of 3D Gaussians that emit feature\nvectors. This compositional object representation allows for a highly robust\nand fast estimation of the object class by independently matching the features\nof the 3D Gaussians of each category to features extracted from an input image.\nAdditionally, the object pose can be estimated via inverse rendering of the\ncorresponding neural object volume. To enable the classification of objects,\nthe neural features at each 3D Gaussian are trained discriminatively to be\ndistinct from (i) the features of 3D Gaussians in other categories, (ii)\nfeatures of other 3D Gaussians of the same object, and (iii) the background\nfeatures. Our experiments show that NOVUM offers intriguing advantages over\nstandard architectures due to the 3D compositional structure of the object\nrepresentation, namely: (1) An exceptional robustness across a spectrum of\nreal-world and synthetic out-of-distribution shifts and (2) an enhanced human\ninterpretability compared to standard models, all while maintaining real-time\ninference and a competitive accuracy on in-distribution data."
                },
                "authors": [
                    {
                        "name": "Artur Jesslen"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Angtian Wang"
                    },
                    {
                        "name": "Wufei Ma"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Adam Kortylewski"
                    }
                ],
                "author_detail": {
                    "name": "Adam Kortylewski"
                },
                "author": "Adam Kortylewski",
                "arxiv_comment": "14 pages, 4 figures, accepted at ECCV 2024, code is accessible at\n  https://github.com/GenIntel/NOVUM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14668v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14668v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13893v2",
                "updated": "2024-08-28T07:16:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    16,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T17:07:39Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    17,
                    7,
                    39,
                    6,
                    238,
                    0
                ],
                "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with\n  Flow-based Scalar Latent Transformer Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with\n  Flow-based Scalar Latent Transformer Diffusion Models"
                },
                "summary": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as\nan effective method for improving the diversity and naturalness of synthesized\nspeech. At the high level, previous large-scale TTS models can be categorized\ninto either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or\nNon-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3).\nAlthough these works demonstrate good performance, they still have potential\nweaknesses. For instance, AR-based models are plagued by unstable generation\nquality and slow generation speed; meanwhile, some NAR-based models need\nphoneme-level duration alignment information, thereby increasing the complexity\nof data pre-processing, model design, and loss design. In this work, we build\nupon our previous publication by implementing a simple and efficient\nnon-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2\neffectively combines the strengths of both autoregressive (AR) and\nnon-autoregressive (NAR) methods, offering the following key advantages: (1)\nsimplified data preparation; (2) straightforward model and loss design; and (3)\nstable, high-quality generation performance with fast inference speed. Compared\nto our previous publication, we present ({\\romannumeral1}) a detailed analysis\nof the influence of speech tokenizer and noisy label for TTS performance;\n({\\romannumeral2}) four distinct types of sentence duration predictors;\n({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion\nmodel. With these improvement, we show a significant improvement in generation\nperformance and generation speed compared to our previous work and other\nstate-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that\nSimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on\nmultilingual speech datasets. Demos are available on:\n{https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as\nan effective method for improving the diversity and naturalness of synthesized\nspeech. At the high level, previous large-scale TTS models can be categorized\ninto either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or\nNon-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3).\nAlthough these works demonstrate good performance, they still have potential\nweaknesses. For instance, AR-based models are plagued by unstable generation\nquality and slow generation speed; meanwhile, some NAR-based models need\nphoneme-level duration alignment information, thereby increasing the complexity\nof data pre-processing, model design, and loss design. In this work, we build\nupon our previous publication by implementing a simple and efficient\nnon-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2\neffectively combines the strengths of both autoregressive (AR) and\nnon-autoregressive (NAR) methods, offering the following key advantages: (1)\nsimplified data preparation; (2) straightforward model and loss design; and (3)\nstable, high-quality generation performance with fast inference speed. Compared\nto our previous publication, we present ({\\romannumeral1}) a detailed analysis\nof the influence of speech tokenizer and noisy label for TTS performance;\n({\\romannumeral2}) four distinct types of sentence duration predictors;\n({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion\nmodel. With these improvement, we show a significant improvement in generation\nperformance and generation speed compared to our previous work and other\nstate-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that\nSimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on\nmultilingual speech datasets. Demos are available on:\n{https://dongchaoyang.top/SimpleSpeech2\\_demo/}."
                },
                "authors": [
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Haohan Guo"
                    },
                    {
                        "name": "Dading Chong"
                    },
                    {
                        "name": "Songxiang Liu"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Submit to TASLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15582v1",
                "updated": "2024-08-28T07:08:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    8,
                    9,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T07:08:09Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    7,
                    8,
                    9,
                    2,
                    241,
                    0
                ],
                "title": "Spectral Masking with Explicit Time-Context Windowing for Neural\n  Network-Based Monaural Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Masking with Explicit Time-Context Windowing for Neural\n  Network-Based Monaural Speech Enhancement"
                },
                "summary": "We propose and analyze the use of an explicit time-context window for neural\nnetwork-based spectral masking speech enhancement to leverage signal context\ndependencies between neighboring frames. In particular, we concentrate on soft\nmasking and loss computed on the time-frequency representation of the\nreconstructed speech. We show that the application of a time-context windowing\nfunction at both input and output of the neural network model improves the soft\nmask estimation process by combining multiple estimates taken from different\ncontexts. The proposed approach is only applied as post-optimization in\ninference mode, not requiring additional layers or special training for the\nneural network model. Our results show that the method consistently increases\nboth intelligibility and signal quality of the denoised speech, as demonstrated\nfor two classes of convolutional-based speech enhancement models. Importantly,\nthe proposed method requires only a negligible ($\\leq1\\%$) increase in the\nnumber of model parameters, making it suitable for hardware-constrained\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and analyze the use of an explicit time-context window for neural\nnetwork-based spectral masking speech enhancement to leverage signal context\ndependencies between neighboring frames. In particular, we concentrate on soft\nmasking and loss computed on the time-frequency representation of the\nreconstructed speech. We show that the application of a time-context windowing\nfunction at both input and output of the neural network model improves the soft\nmask estimation process by combining multiple estimates taken from different\ncontexts. The proposed approach is only applied as post-optimization in\ninference mode, not requiring additional layers or special training for the\nneural network model. Our results show that the method consistently increases\nboth intelligibility and signal quality of the denoised speech, as demonstrated\nfor two classes of convolutional-based speech enhancement models. Importantly,\nthe proposed method requires only a negligible ($\\leq1\\%$) increase in the\nnumber of model parameters, making it suitable for hardware-constrained\napplications."
                },
                "authors": [
                    {
                        "name": "Luan Vin√≠cius Fiorio"
                    },
                    {
                        "name": "Boris Karanov"
                    },
                    {
                        "name": "Bruno Defraene"
                    },
                    {
                        "name": "Johan David"
                    },
                    {
                        "name": "Wim van Houtum"
                    },
                    {
                        "name": "Frans Widdershoven"
                    },
                    {
                        "name": "Ronald M. Aarts"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Aarts"
                },
                "author": "Ronald M. Aarts",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15565v1",
                "updated": "2024-08-28T06:33:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    33,
                    3,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T06:33:03Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    33,
                    3,
                    2,
                    241,
                    0
                ],
                "title": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models"
                },
                "summary": "There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm."
                },
                "authors": [
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15562v1",
                "updated": "2024-08-28T06:28:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    28,
                    1,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T06:28:01Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    28,
                    1,
                    2,
                    241,
                    0
                ],
                "title": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation"
                },
                "summary": "Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs."
                },
                "authors": [
                    {
                        "name": "Lujun Gui"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "The work was not submitted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15559v1",
                "updated": "2024-08-28T06:20:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    20,
                    6,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T06:20:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    20,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Regularity for a class of degenerate fully nonlinear nonlocal elliptic\n  equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularity for a class of degenerate fully nonlinear nonlocal elliptic\n  equations"
                },
                "summary": "We consider a wide class of fully nonlinear integro-differential equations\nthat degenerate when the gradient of the solution vanishes. By using\ncompactness and perturbation arguments, we give a complete characterization of\nthe regularity of viscosity solutions according to different diffusion orders.\nMore precisely, when the order of the fractional diffusion is sufficiently\nclose to 2, we obtain H\\\"{o}lder continuity for the gradient of any viscosity\nsolutions and further derive an improved gradient regularity estimate at the\norigin. For the order of the fractional diffusion in the interval $(1, 2)$, we\nprove that there is at least one solution of class $C^{1, \\alpha}_{\\rm loc}$.\nAdditionally, if the order of the fractional diffusion is in the interval\n$(0,1]$, the local H\\\"{o}lder continuity of solutions is inferred.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a wide class of fully nonlinear integro-differential equations\nthat degenerate when the gradient of the solution vanishes. By using\ncompactness and perturbation arguments, we give a complete characterization of\nthe regularity of viscosity solutions according to different diffusion orders.\nMore precisely, when the order of the fractional diffusion is sufficiently\nclose to 2, we obtain H\\\"{o}lder continuity for the gradient of any viscosity\nsolutions and further derive an improved gradient regularity estimate at the\norigin. For the order of the fractional diffusion in the interval $(1, 2)$, we\nprove that there is at least one solution of class $C^{1, \\alpha}_{\\rm loc}$.\nAdditionally, if the order of the fractional diffusion is in the interval\n$(0,1]$, the local H\\\"{o}lder continuity of solutions is inferred."
                },
                "authors": [
                    {
                        "name": "Yuzhou Fang"
                    },
                    {
                        "name": "Vicentiu D. Radulescu"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07626v2",
                "updated": "2024-08-28T06:18:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    18,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-13T10:37:50Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    37,
                    50,
                    0,
                    134,
                    0
                ],
                "title": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models"
                },
                "summary": "Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Di Yao"
                    },
                    {
                        "name": "Lanting Fang"
                    },
                    {
                        "name": "Zhetao Li"
                    },
                    {
                        "name": "Wenbin Li"
                    },
                    {
                        "name": "Kaiyu Feng"
                    },
                    {
                        "name": "XiaoWen Ji"
                    },
                    {
                        "name": "Jingping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Jingping Bi"
                },
                "author": "Jingping Bi",
                "arxiv_comment": "13pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v2",
                "updated": "2024-08-28T06:16:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    16,
                    42,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Krumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15549v1",
                "updated": "2024-08-28T05:53:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:53:46Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    46,
                    2,
                    241,
                    0
                ],
                "title": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback"
                },
                "summary": "As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, and the risk of\nfeedback loops that amplify model biases. To overcome these limitations, we\nintroduce WildFeedback, a novel framework that leverages real-time, in-situ\nuser interactions to create preference datasets that more accurately reflect\nauthentic human values. WildFeedback operates through a three-step process:\nfeedback signal identification, preference data construction, and user-guided\nevaluation. We applied this framework to a large corpus of user-LLM\nconversations, resulting in a rich preference dataset that reflects genuine\nuser preferences. This dataset captures the nuances of user preferences by\nidentifying and classifying feedback signals within natural conversations,\nthereby enabling the construction of more representative and context-sensitive\nalignment data. Our extensive experiments demonstrate that LLMs fine-tuned on\nWildFeedback exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed user-guided\nevaluation. By incorporating real-time feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users. In summary,\nWildFeedback offers a robust, scalable solution for aligning LLMs with true\nhuman values, setting a new standard for the development and evaluation of\nuser-centric language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, and the risk of\nfeedback loops that amplify model biases. To overcome these limitations, we\nintroduce WildFeedback, a novel framework that leverages real-time, in-situ\nuser interactions to create preference datasets that more accurately reflect\nauthentic human values. WildFeedback operates through a three-step process:\nfeedback signal identification, preference data construction, and user-guided\nevaluation. We applied this framework to a large corpus of user-LLM\nconversations, resulting in a rich preference dataset that reflects genuine\nuser preferences. This dataset captures the nuances of user preferences by\nidentifying and classifying feedback signals within natural conversations,\nthereby enabling the construction of more representative and context-sensitive\nalignment data. Our extensive experiments demonstrate that LLMs fine-tuned on\nWildFeedback exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed user-guided\nevaluation. By incorporating real-time feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users. In summary,\nWildFeedback offers a robust, scalable solution for aligning LLMs with true\nhuman values, setting a new standard for the development and evaluation of\nuser-centric language models."
                },
                "authors": [
                    {
                        "name": "Taiwei Shi"
                    },
                    {
                        "name": "Zhuoer Wang"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Ying-Chun Lin"
                    },
                    {
                        "name": "Zexue He"
                    },
                    {
                        "name": "Mengting Wan"
                    },
                    {
                        "name": "Pei Zhou"
                    },
                    {
                        "name": "Sujay Jauhar"
                    },
                    {
                        "name": "Xiaofeng Xu"
                    },
                    {
                        "name": "Xia Song"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15548v1",
                "updated": "2024-08-28T05:53:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    30,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:53:30Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    30,
                    2,
                    241,
                    0
                ],
                "title": "ConsistencyTrack: A Robust Multi-Object Tracker with a Generation\n  Strategy of Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsistencyTrack: A Robust Multi-Object Tracker with a Generation\n  Strategy of Consistency Model"
                },
                "summary": "Multi-object tracking (MOT) is a critical technology in computer vision,\ndesigned to detect multiple targets in video sequences and assign each target a\nunique ID per frame. Existed MOT methods excel at accurately tracking multiple\nobjects in real-time across various scenarios. However, these methods still\nface challenges such as poor noise resistance and frequent ID switches. In this\nresearch, we propose a novel ConsistencyTrack, joint detection and\ntracking(JDT) framework that formulates detection and association as a\ndenoising diffusion process on perturbed bounding boxes. This progressive\ndenoising strategy significantly improves the model's noise resistance. During\nthe training phase, paired object boxes within two adjacent frames are diffused\nfrom ground-truth boxes to a random distribution, and then the model learns to\ndetect and track by reversing this process. In inference, the model refines\nrandomly generated boxes into detection and tracking results through minimal\ndenoising steps. ConsistencyTrack also introduces an innovative target\nassociation strategy to address target occlusion. Experiments on the MOT17 and\nDanceTrack datasets demonstrate that ConsistencyTrack outperforms other\ncompared methods, especially better than DiffusionTrack in inference speed and\nother performance metrics. Our code is available at\nhttps://github.com/Tankowa/ConsistencyTrack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-object tracking (MOT) is a critical technology in computer vision,\ndesigned to detect multiple targets in video sequences and assign each target a\nunique ID per frame. Existed MOT methods excel at accurately tracking multiple\nobjects in real-time across various scenarios. However, these methods still\nface challenges such as poor noise resistance and frequent ID switches. In this\nresearch, we propose a novel ConsistencyTrack, joint detection and\ntracking(JDT) framework that formulates detection and association as a\ndenoising diffusion process on perturbed bounding boxes. This progressive\ndenoising strategy significantly improves the model's noise resistance. During\nthe training phase, paired object boxes within two adjacent frames are diffused\nfrom ground-truth boxes to a random distribution, and then the model learns to\ndetect and track by reversing this process. In inference, the model refines\nrandomly generated boxes into detection and tracking results through minimal\ndenoising steps. ConsistencyTrack also introduces an innovative target\nassociation strategy to address target occlusion. Experiments on the MOT17 and\nDanceTrack datasets demonstrate that ConsistencyTrack outperforms other\ncompared methods, especially better than DiffusionTrack in inference speed and\nother performance metrics. Our code is available at\nhttps://github.com/Tankowa/ConsistencyTrack."
                },
                "authors": [
                    {
                        "name": "Lifan Jiang"
                    },
                    {
                        "name": "Zhihui Wang"
                    },
                    {
                        "name": "Siqi Yin"
                    },
                    {
                        "name": "Guangxiao Ma"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Boxi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Boxi Wu"
                },
                "author": "Boxi Wu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2308.09905 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15547v1",
                "updated": "2024-08-28T05:48:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    48,
                    35,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:48:35Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    48,
                    35,
                    2,
                    241,
                    0
                ],
                "title": "Redshift evolution of the X-ray and UV luminosity relation of quasars:\n  calibrated results from SNe Ia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redshift evolution of the X-ray and UV luminosity relation of quasars:\n  calibrated results from SNe Ia"
                },
                "summary": "Quasars could serve as standard candles if the relation between their\nultraviolet and X-ray luminosities can be accurately calibrated. Previously, we\ndeveloped a model-independent method to calibrate quasar standard candles using\nthe distances-redshift relation reconstructed from Type Ia supernova at z<2\nusing Gaussian process regression. Interestingly, we found that the calibrated\nquasar standard candle dataset preferred a deviation from $\\Lambda$CDM at\nredshifts above z>2. One interpretation of these findings is that the\ncalibration parameters of the quasar UV-X-ray luminosity relationship evolves\nwith redshift. In order to test the redshift dependence of the quasar\ncalibration in a model-independent manner, we divided the quasar sample whose\nredshift overlap with the redshift coverage of Pantheon+ Type Ia supernova\ncompilation into two sub-samples: a low-redshift quasar sub-sample and a\nhigh-redshift quasar sub-sample. Our present results show that there is about a\n4$\\sigma$ inconsistency between the quasar parameters inferred from the\nhigh-redshift quasar sub-sample and from the low-redshift sub-sample if no\nevolution of the quasar relation is considered. This inconsistency suggests the\nnecessity of considering redshift evolution for the relationship between the\nquasars$'$ ultraviolet and X-ray luminosities. We then test an explicit\nparametrization of the redshift evolution of the quasar calibration parameters\nvia $\\gamma(z) = \\gamma_0+\\gamma_1(1+z)$ and $\\beta(z)=\\beta_0+\\beta_1(1+z)$.\nCombining this redshift-dependent calibration relationship with the\ndistance-redshift relationship reconstructed from Pantheon+ supernova\ncompilation, we find the high-redshift sub-sample and low-redshift sub-sample\nbecome consistent at the 1$\\sigma$ level, which means that the parameterized\nform of $\\gamma(z)$ and $\\beta(z)$ works well at describing the evolution of\nthe quasar calibration parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasars could serve as standard candles if the relation between their\nultraviolet and X-ray luminosities can be accurately calibrated. Previously, we\ndeveloped a model-independent method to calibrate quasar standard candles using\nthe distances-redshift relation reconstructed from Type Ia supernova at z<2\nusing Gaussian process regression. Interestingly, we found that the calibrated\nquasar standard candle dataset preferred a deviation from $\\Lambda$CDM at\nredshifts above z>2. One interpretation of these findings is that the\ncalibration parameters of the quasar UV-X-ray luminosity relationship evolves\nwith redshift. In order to test the redshift dependence of the quasar\ncalibration in a model-independent manner, we divided the quasar sample whose\nredshift overlap with the redshift coverage of Pantheon+ Type Ia supernova\ncompilation into two sub-samples: a low-redshift quasar sub-sample and a\nhigh-redshift quasar sub-sample. Our present results show that there is about a\n4$\\sigma$ inconsistency between the quasar parameters inferred from the\nhigh-redshift quasar sub-sample and from the low-redshift sub-sample if no\nevolution of the quasar relation is considered. This inconsistency suggests the\nnecessity of considering redshift evolution for the relationship between the\nquasars$'$ ultraviolet and X-ray luminosities. We then test an explicit\nparametrization of the redshift evolution of the quasar calibration parameters\nvia $\\gamma(z) = \\gamma_0+\\gamma_1(1+z)$ and $\\beta(z)=\\beta_0+\\beta_1(1+z)$.\nCombining this redshift-dependent calibration relationship with the\ndistance-redshift relationship reconstructed from Pantheon+ supernova\ncompilation, we find the high-redshift sub-sample and low-redshift sub-sample\nbecome consistent at the 1$\\sigma$ level, which means that the parameterized\nform of $\\gamma(z)$ and $\\beta(z)$ works well at describing the evolution of\nthe quasar calibration parameters."
                },
                "authors": [
                    {
                        "name": "Xiaolei Li"
                    },
                    {
                        "name": "Ryan E. Keeley"
                    },
                    {
                        "name": "Arman Shafieloo"
                    }
                ],
                "author_detail": {
                    "name": "Arman Shafieloo"
                },
                "author": "Arman Shafieloo",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v1",
                "updated": "2024-08-28T05:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jian Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15542v1",
                "updated": "2024-08-28T05:34:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    34,
                    14,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:34:14Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    34,
                    14,
                    2,
                    241,
                    0
                ],
                "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input"
                },
                "summary": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models."
                },
                "authors": [
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Yibing Wang"
                    },
                    {
                        "name": "Hanghang Ma"
                    },
                    {
                        "name": "Xiaoping Wu"
                    },
                    {
                        "name": "Xiaoqi Ma"
                    },
                    {
                        "name": "Xiaoming Wei"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Enhua Wu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v2",
                "updated": "2024-08-29T08:45:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.06561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.06561v3",
                "updated": "2024-08-28T04:36:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    36,
                    23,
                    2,
                    241,
                    0
                ],
                "published": "2022-08-13T03:25:50Z",
                "published_parsed": [
                    2022,
                    8,
                    13,
                    3,
                    25,
                    50,
                    5,
                    225,
                    0
                ],
                "title": "Drone Referring Localization: An Efficient Heterogeneous Spatial Feature\n  Interaction Method For UAV Self-Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone Referring Localization: An Efficient Heterogeneous Spatial Feature\n  Interaction Method For UAV Self-Localization"
                },
                "summary": "Image retrieval (IR) has emerged as a promising approach for\nself-localization in unmanned aerial vehicles (UAVs). However, IR-based methods\nface several challenges: 1) Pre- and post-processing incur significant\ncomputational and storage overhead; 2) The lack of interaction between\ndual-source features impairs precise spatial perception. In this paper, we\npropose an efficient heterogeneous spatial feature interaction method, termed\nDrone Referring Localization (DRL), which aims to localize UAV-view images\nwithin satellite imagery. Unlike conventional methods that treat different data\nsources in isolation, followed by cosine similarity computations, DRL\nfacilitates the learnable interaction of heterogeneous features. To implement\nthe proposed DRL, we design two transformer-based frameworks, Post-Fusion and\nMix-Fusion, enabling end-to-end training and inference. Furthermore, we\nintroduce random scale cropping and weight balance loss techniques to augment\npaired data and optimize the balance between positive and negative sample\nweights. Additionally, we construct a new dataset, UL14, and establish a\nbenchmark tailored to the DRL framework. Compared to traditional IR methods,\nDRL achieves superior localization accuracy (MA@20 +9.4\\%) while significantly\nreducing computational time (1/7) and storage overhead (1/3). The dataset and\ncode will be made publicly available. The dataset and code are available at\n\\url{https://github.com/Dmmm1997/DRL} .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image retrieval (IR) has emerged as a promising approach for\nself-localization in unmanned aerial vehicles (UAVs). However, IR-based methods\nface several challenges: 1) Pre- and post-processing incur significant\ncomputational and storage overhead; 2) The lack of interaction between\ndual-source features impairs precise spatial perception. In this paper, we\npropose an efficient heterogeneous spatial feature interaction method, termed\nDrone Referring Localization (DRL), which aims to localize UAV-view images\nwithin satellite imagery. Unlike conventional methods that treat different data\nsources in isolation, followed by cosine similarity computations, DRL\nfacilitates the learnable interaction of heterogeneous features. To implement\nthe proposed DRL, we design two transformer-based frameworks, Post-Fusion and\nMix-Fusion, enabling end-to-end training and inference. Furthermore, we\nintroduce random scale cropping and weight balance loss techniques to augment\npaired data and optimize the balance between positive and negative sample\nweights. Additionally, we construct a new dataset, UL14, and establish a\nbenchmark tailored to the DRL framework. Compared to traditional IR methods,\nDRL achieves superior localization accuracy (MA@20 +9.4\\%) while significantly\nreducing computational time (1/7) and storage overhead (1/3). The dataset and\ncode will be made publicly available. The dataset and code are available at\n\\url{https://github.com/Dmmm1997/DRL} ."
                },
                "authors": [
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Enhui Zheng"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Lei Qi"
                    },
                    {
                        "name": "Zhenhua Feng"
                    },
                    {
                        "name": "Wankou Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wankou Yang"
                },
                "author": "Wankou Yang",
                "arxiv_comment": "15 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.06561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.06561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01525v2",
                "updated": "2024-08-28T04:32:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    32,
                    18,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-02T18:35:58Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    35,
                    58,
                    4,
                    215,
                    0
                ],
                "title": "Gravitational-Wave and Gravitational-Wave Memory Signatures of\n  Core-Collapse Supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-Wave and Gravitational-Wave Memory Signatures of\n  Core-Collapse Supernovae"
                },
                "summary": "In this paper, we calculate the energy, signal-to-noise ratio, detection\nrange, and angular anisotropy of the matter, matter memory, and neutrino memory\ngravitational wave (GW) signatures of 21 three-dimensional initially\nnon-rotating core-collapse supernova (CCSN) models carried to late times. We\nfind that inferred energy, signal-to-noise ratio, and detection range are\nangle-dependent quantities, and that the spread of possible energy,\nsignal-to-noise, and detection ranges across all viewing angles generally\nincreases with progenitor mass. When examining the low-frequency matter memory\nand neutrino memory components of the signal, we find that the neutrino memory\nis the most detectable component of a CCSN GW signal, and that DECIGO is\nbest-equipped to detect both matter memory and neutrino memory. Moreover, we\nfind that the polarization angle between the $h_+$ and $h_{\\times}$ strains\nserves as a unique identifier of matter and neutrino memory. Finally, we\ndevelop a galactic density- and stellar mass-weighted formalism to calculate\nthe rate at which we can expect to detect CCSN GW signals with Advanced LIGO.\nWhen considering only the matter component of the signal, the aLIGO detection\nrate is around 65$\\%$ of the total galactic supernova rate, but increases to\n90$\\%$ when incorporating the neutrino memory component. We find that all\nfuture detectors (ET, CE, DECIGO) will be able to detect CCSN GW signals from\nthe entire galaxy, and for the higher-mass progenitors even into the local\ngroup of galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we calculate the energy, signal-to-noise ratio, detection\nrange, and angular anisotropy of the matter, matter memory, and neutrino memory\ngravitational wave (GW) signatures of 21 three-dimensional initially\nnon-rotating core-collapse supernova (CCSN) models carried to late times. We\nfind that inferred energy, signal-to-noise ratio, and detection range are\nangle-dependent quantities, and that the spread of possible energy,\nsignal-to-noise, and detection ranges across all viewing angles generally\nincreases with progenitor mass. When examining the low-frequency matter memory\nand neutrino memory components of the signal, we find that the neutrino memory\nis the most detectable component of a CCSN GW signal, and that DECIGO is\nbest-equipped to detect both matter memory and neutrino memory. Moreover, we\nfind that the polarization angle between the $h_+$ and $h_{\\times}$ strains\nserves as a unique identifier of matter and neutrino memory. Finally, we\ndevelop a galactic density- and stellar mass-weighted formalism to calculate\nthe rate at which we can expect to detect CCSN GW signals with Advanced LIGO.\nWhen considering only the matter component of the signal, the aLIGO detection\nrate is around 65$\\%$ of the total galactic supernova rate, but increases to\n90$\\%$ when incorporating the neutrino memory component. We find that all\nfuture detectors (ET, CE, DECIGO) will be able to detect CCSN GW signals from\nthe entire galaxy, and for the higher-mass progenitors even into the local\ngroup of galaxies."
                },
                "authors": [
                    {
                        "name": "Lyla Choi"
                    },
                    {
                        "name": "Adam Burrows"
                    },
                    {
                        "name": "David Vartanyan"
                    }
                ],
                "author_detail": {
                    "name": "David Vartanyan"
                },
                "author": "David Vartanyan",
                "arxiv_comment": "37 pages, 18 figures. Accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02053v2",
                "updated": "2024-08-28T04:04:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    4,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-03T06:16:28Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    6,
                    16,
                    28,
                    5,
                    34,
                    0
                ],
                "title": "Affordable Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable Generative Agents"
                },
                "summary": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\nhttps://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\nhttps://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents."
                },
                "authors": [
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15040v2",
                "updated": "2024-08-28T03:56:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    56,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:10:05Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    10,
                    5,
                    1,
                    240,
                    0
                ],
                "title": "A Survey of Large Language Models for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for European Languages"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Wazir Ali"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v1",
                "updated": "2024-08-28T03:48:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_comment": "For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07611v2",
                "updated": "2024-08-28T03:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    47,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-14T15:19:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions."
                },
                "authors": [
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Xuefeng Liang"
                    },
                    {
                        "name": "Yuhui Liu"
                    },
                    {
                        "name": "Kaihua Ni"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Zetian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zetian Hu"
                },
                "author": "Zetian Hu",
                "arxiv_comment": "8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12576v2",
                "updated": "2024-08-28T03:15:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    15,
                    10,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-17T14:02:01Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    14,
                    2,
                    1,
                    2,
                    199,
                    0
                ],
                "title": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework\n  Using Open EDA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework\n  Using Open EDA"
                },
                "summary": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Li"
                },
                "author": "Xiaowei Li",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.15998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15998v1",
                "updated": "2024-08-28T17:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:59:31Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders"
                },
                "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle"
                },
                "authors": [
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Shijia Liao"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Yaser Yacoob"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Guilin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Liu"
                },
                "author": "Guilin Liu",
                "arxiv_comment": "Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15978v1",
                "updated": "2024-08-28T17:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:49:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    49,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\n  Execution with Strategic Exploration"
                },
                "summary": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents often fail to execute complex web tasks that\nrequire dynamic interaction due to the inherent uncertainty and complexity of\nthese environments. Existing LLM-based web agents typically rely on rigid,\nexpert-designed policies specific to certain states and actions, which lack the\nflexibility and generalizability needed to adapt to unseen tasks. In contrast,\nhumans excel by exploring unknowns, continuously adapting strategies, and\nresolving ambiguities through exploration. To emulate human-like adaptability,\nweb agents need strategic exploration and complex decision-making. Monte Carlo\nTree Search (MCTS) is well-suited for this, but classical MCTS struggles with\nvast action spaces, unpredictable state transitions, and incomplete information\nin web tasks. In light of this, we develop WebPilot, a multi-agent system with\na dual optimization strategy that improves MCTS to better handle complex web\nenvironments. Specifically, the Global Optimization phase involves generating a\nhigh-level plan by breaking down tasks into manageable subtasks and\ncontinuously refining this plan, thereby focusing the search process and\nmitigating the challenges posed by vast action spaces in classical MCTS.\nSubsequently, the Local Optimization phase executes each subtask using a\ntailored MCTS designed for complex environments, effectively addressing\nuncertainties and managing incomplete information. Experimental results on\nWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on\nWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%\nrelative increase in success rate over the concurrent tree search-based method.\nWebPilot marks a significant advancement in general autonomous agent\ncapabilities, paving the way for more advanced and reliable decision-making in\npractical environments."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Zijian Ma"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15971v1",
                "updated": "2024-08-28T17:43:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:43:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems"
                },
                "summary": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Boyan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15970v1",
                "updated": "2024-08-28T17:43:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:43:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    43,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "Ain't How You Deploy: An Analysis of BGP Security Policies Performance\n  Against Various Attack Scenarios with Differing Deployment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ain't How You Deploy: An Analysis of BGP Security Policies Performance\n  Against Various Attack Scenarios with Differing Deployment Strategies"
                },
                "summary": "This paper investigates the performance of various Border Gateway Protocol\n(BGP) security policies against multiple attack scenarios using different\ndeployment strategies. Through extensive simulations, we evaluate the\neffectiveness of defensive mechanisms such as Root Origin Validation (ROV),\nAutonomous System Provider Authorization (ASPA), and PeerROV across distinct AS\ndeployment types. Our findings reveal critical insights into the strengths and\nlimitations of current BGP security measures, providing guidance for future\npolicy development and implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of various Border Gateway Protocol\n(BGP) security policies against multiple attack scenarios using different\ndeployment strategies. Through extensive simulations, we evaluate the\neffectiveness of defensive mechanisms such as Root Origin Validation (ROV),\nAutonomous System Provider Authorization (ASPA), and PeerROV across distinct AS\ndeployment types. Our findings reveal critical insights into the strengths and\nlimitations of current BGP security measures, providing guidance for future\npolicy development and implementation."
                },
                "authors": [
                    {
                        "name": "Seth Barrett"
                    },
                    {
                        "name": "Calvin Idom"
                    },
                    {
                        "name": "German Zavala Villafuerte"
                    },
                    {
                        "name": "Andrew Byers"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_comment": "8 pages, 1 table, 8 figures, submitted to and accepted by IEEE\n  ISNCC'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v1",
                "updated": "2024-08-28T17:38:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10260v2",
                "updated": "2024-08-28T17:26:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    26,
                    3,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-11T01:16:10Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    1,
                    16,
                    10,
                    1,
                    163,
                    0
                ],
                "title": "Flextron: Many-in-One Flexible Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flextron: Many-in-One Flexible Large Language Model"
                },
                "summary": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training modern LLMs is extremely resource intensive, and customizing them\nfor various deployment scenarios characterized by limited compute and memory\nresources through repeated training is impractical. In this paper, we introduce\nFlextron, a network architecture and post-training model optimization framework\nsupporting flexible model deployment. The Flextron architecture utilizes a\nnested elastic structure to rapidly adapt to specific user-defined latency and\naccuracy targets during inference with no additional fine-tuning required. It\nis also input-adaptive, and can automatically route tokens through its\nsub-networks for improved performance and efficiency. We present a\nsample-efficient training method and associated routing algorithms for\nsystematically transforming an existing trained LLM into a Flextron model. We\nevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate\nsuperior performance over multiple end-to-end trained variants and other\nstate-of-the-art elastic networks, all with a single pretraining run that\nconsumes a mere 7.63% tokens compared to original pretraining."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15950v1",
                "updated": "2024-08-28T17:08:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:08:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    8,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games"
                },
                "summary": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/."
                },
                "authors": [
                    {
                        "name": "Nicholas R. Waytowich"
                    },
                    {
                        "name": "Devin White"
                    },
                    {
                        "name": "MD Sunbeam"
                    },
                    {
                        "name": "Vinicius G. Goecks"
                    }
                ],
                "author_detail": {
                    "name": "Vinicius G. Goecks"
                },
                "author": "Vinicius G. Goecks",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15915v1",
                "updated": "2024-08-28T16:28:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:28:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    28,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models"
                },
                "summary": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later."
                },
                "authors": [
                    {
                        "name": "Yuncheng Yang"
                    },
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Pengcheng Guo"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Yun Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Gu"
                },
                "author": "Yun Gu",
                "arxiv_comment": "28 pages, 12 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11844v3",
                "updated": "2024-08-28T16:26:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    26,
                    16,
                    2,
                    241,
                    0
                ],
                "published": "2023-11-20T15:34:45Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    15,
                    34,
                    45,
                    0,
                    324,
                    0
                ],
                "title": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles\n  in Public Policy Documents"
                },
                "summary": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4\npromise automation with better results and less programming, opening up new\nopportunities for text analysis in political science. In this study, we\nevaluate LLMs on three original coding tasks involving typical complexities\nencountered in political science settings: a non-English language, legal and\npolitical jargon, and complex labels based on abstract constructs. Along the\npaper, we propose a practical workflow to optimize the choice of the model and\nthe prompt. We find that the best prompting strategy consists of providing the\nLLMs with a detailed codebook, as the one provided to human coders. In this\nsetting, an LLM can be as good as or possibly better than a human annotator\nwhile being much faster, considerably cheaper, and much easier to scale to\nlarge amounts of text. We also provide a comparison of GPT and popular\nopen-source LLMs, discussing the trade-offs in the model's choice. Our software\nallows LLMs to be easily used as annotators and is publicly available:\nhttps://github.com/lorelupo/pappa."
                },
                "authors": [
                    {
                        "name": "Lorenzo Lupo"
                    },
                    {
                        "name": "Oscar Magnusson"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Elin Naurin"
                    },
                    {
                        "name": "Lena W√§ngnerud"
                    }
                ],
                "author_detail": {
                    "name": "Lena W√§ngnerud"
                },
                "author": "Lena W√§ngnerud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15907v1",
                "updated": "2024-08-28T16:20:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:20:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    20,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "Decentralized LLM Inference over Edge Networks with Energy Harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized LLM Inference over Edge Networks with Energy Harvesting"
                },
                "summary": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks."
                },
                "authors": [
                    {
                        "name": "Aria Khoshsirat"
                    },
                    {
                        "name": "Giovanni Perin"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15903v1",
                "updated": "2024-08-28T16:15:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:15:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments"
                },
                "summary": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Ishaan Singh Rawal"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15895v1",
                "updated": "2024-08-28T16:05:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T16:05:20Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    5,
                    20,
                    2,
                    241,
                    0
                ],
                "title": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling\n  Decision by Large Language Models"
                },
                "summary": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human coders are biased. We test similar biases in Large Language Models\n(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and\nMeyer (2018), we find evidence that LLMs use political information, and\nspecifically party cues, to judge political statements. Not only do LLMs use\nrelevant information to contextualize whether a statement is positive,\nnegative, or neutral based on the party cue, they also reflect the biases of\nthe human-generated data upon which they have been trained. We also find that\nunlike humans, who are only biased when faced with statements from extreme\nparties, LLMs exhibit significant bias even when prompted with statements from\ncenter-left and center-right parties. The implications of our findings are\ndiscussed in the conclusion."
                },
                "authors": [
                    {
                        "name": "Sebastian Vallejo Vera"
                    },
                    {
                        "name": "Hunter Driggers"
                    }
                ],
                "author_detail": {
                    "name": "Hunter Driggers"
                },
                "author": "Hunter Driggers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v2",
                "updated": "2024-08-28T15:53:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    53,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15879v1",
                "updated": "2024-08-28T15:50:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:50:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    50,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Persuasion Games using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Games using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase)."
                },
                "authors": [
                    {
                        "name": "Ganesh Prasath Ramani"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Santhosh V"
                    },
                    {
                        "name": "Yash Bhatia"
                    }
                ],
                "author_detail": {
                    "name": "Yash Bhatia"
                },
                "author": "Yash Bhatia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15867v1",
                "updated": "2024-08-28T15:35:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:35:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments."
                },
                "authors": [
                    {
                        "name": "Mehdi Monemi"
                    },
                    {
                        "name": "Mehdi Rasti"
                    },
                    {
                        "name": "Arthur S. de Sena"
                    },
                    {
                        "name": "Mohammad Amir Fallah"
                    },
                    {
                        "name": "Matti Latva-Aho"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15857v1",
                "updated": "2024-08-28T15:18:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    18,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T15:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    18,
                    46,
                    2,
                    241,
                    0
                ],
                "title": "What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector"
                },
                "summary": "This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field."
                },
                "authors": [
                    {
                        "name": "Muhammad Yaseen"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Yaseen"
                },
                "author": "Muhammad Yaseen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01245v2",
                "updated": "2024-08-28T15:01:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    1,
                    4,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-01T17:03:41Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    3,
                    41,
                    0,
                    92,
                    0
                ],
                "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules"
                },
                "summary": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Feng Ruan"
                    },
                    {
                        "name": "Huiyuan Wang"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00612v2",
                "updated": "2024-08-28T14:59:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    59,
                    31,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-01T14:52:04Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    14,
                    52,
                    4,
                    3,
                    214,
                    0
                ],
                "title": "Downstream bias mitigation is all you need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downstream bias mitigation is all you need"
                },
                "summary": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model."
                },
                "authors": [
                    {
                        "name": "Arkadeep Baksi"
                    },
                    {
                        "name": "Rahul Singh"
                    },
                    {
                        "name": "Tarun Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Tarun Joshi"
                },
                "author": "Tarun Joshi",
                "arxiv_comment": "arXiv admin note: This work has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16696v3",
                "updated": "2024-08-28T14:54:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    54,
                    11,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-26T16:11:03Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    16,
                    11,
                    3,
                    0,
                    57,
                    0
                ],
                "title": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models"
                },
                "summary": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets."
                },
                "authors": [
                    {
                        "name": "Anchun Gui"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Han Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Han Xiao"
                },
                "author": "Han Xiao",
                "arxiv_comment": "20 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15836v1",
                "updated": "2024-08-28T14:48:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:48:37Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    48,
                    37,
                    2,
                    241,
                    0
                ],
                "title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature"
                },
                "summary": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available."
                },
                "authors": [
                    {
                        "name": "Uri Katz"
                    },
                    {
                        "name": "Mosh Levy"
                    },
                    {
                        "name": "Yoav Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Goldberg"
                },
                "author": "Yoav Goldberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10155v2",
                "updated": "2024-08-28T14:38:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    38,
                    51,
                    2,
                    241,
                    0
                ],
                "published": "2024-04-15T22:02:58Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    22,
                    2,
                    58,
                    0,
                    106,
                    0
                ],
                "title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Simantika Dristi"
                    },
                    {
                        "name": "Joy Saha"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Joanna C. S. Santos"
                },
                "author": "Joanna C. S. Santos",
                "arxiv_comment": "Accepted at the 24th IEEE International Conference on Source Code\n  Analysis and Manipulation(SCAM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15815v1",
                "updated": "2024-08-28T14:24:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T14:24:48Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    24,
                    48,
                    2,
                    241,
                    0
                ],
                "title": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR-Adopt: Automatic Deduction of Input Transformation Function for\n  Metamorphic Testing"
                },
                "summary": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While a recent study reveals that many developer-written test cases can\nencode a reusable Metamorphic Relation (MR), over 70% of them directly\nhard-code the source input and follow-up input in the encoded relation. Such\nencoded MRs, which do not contain an explicit input transformation to transform\nthe source inputs to corresponding follow-up inputs, cannot be reused with new\nsource inputs to enhance test adequacy.\n  In this paper, we propose MR-Adopt (Automatic Deduction Of inPut\nTransformation) to automatically deduce the input transformation from the\nhard-coded source and follow-up inputs, aiming to enable the encoded MRs to be\nreused with new source inputs. With typically only one pair of source and\nfollow-up inputs available in an MR-encoded test case as the example, we\nleveraged LLMs to understand the intention of the test case and generate\nadditional examples of source-followup input pairs. This helps to guide the\ngeneration of input transformations generalizable to multiple source inputs.\nBesides, to mitigate the issue that LLMs generate erroneous code, we refine\nLLM-generated transformations by removing MR- irrelevant code elements with\ndata-flow analysis. Finally, we assess candidate transformations based on\nencoded output relations and select the best transformation as the result.\nEvaluation results show that MR-Adopt can generate input transformations\napplicable to all experimental source inputs for 72.00% of encoded MRs, which\nis 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated\ninput transformations, encoded MR-based test cases can effectively enhance the\ntest adequacy, increasing the line coverage and mutation score by 10.62% and\n18.91%, respectively."
                },
                "authors": [
                    {
                        "name": "Congying Xu"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Valerio Terragni"
                    },
                    {
                        "name": "Hengcheng Zhu"
                    },
                    {
                        "name": "Jialun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jialun Cao"
                },
                "author": "Jialun Cao",
                "arxiv_comment": "This paper is accepted to ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14511v2",
                "updated": "2024-08-28T14:13:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    13,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T04:07:18Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    7,
                    18,
                    6,
                    238,
                    0
                ],
                "title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting\n  Methods"
                },
                "summary": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as\neffective methods for solving multi-step reasoning problems using pretrained\nlarge language models (LLMs). In this work, we analyze CoT prompting from a\nstatistical estimation perspective, providing a comprehensive characterization\nof its sample complexity. To this end, we introduce a multi-step latent\nvariable model that encapsulates the reasoning process, where the latent\nvariable encodes the task information. Under this framework, we demonstrate\nthat when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator\neffectively solves the multi-step reasoning problem by aggregating a posterior\ndistribution inferred from the demonstration examples in the prompt. Moreover,\nwe prove that the statistical error of the CoT estimator can be decomposed into\ntwo main components: (i) a prompting error, which arises from inferring the\ntrue task using CoT prompts, and (ii) the statistical error of the pretrained\nLLM. We establish that, under appropriate assumptions, the prompting error\ndecays exponentially to zero as the number of demonstrations increases.\nAdditionally, we explicitly characterize the approximation and generalization\nerrors of the pretrained LLM. Notably, we construct a transformer model that\napproximates the target distribution of the multi-step reasoning problem with\nan error that decreases exponentially in the number of transformer blocks. Our\nanalysis extends to other variants of CoT, including Self-Consistent CoT,\nTree-of-Thought, and Selection-Inference, offering a broad perspective on the\nefficacy of these methods. We also provide numerical experiments to validate\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xinyang Hu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Zhuoran Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Yang"
                },
                "author": "Zhuoran Yang",
                "arxiv_comment": "150 pages, 18 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.07081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.07081v2",
                "updated": "2024-08-28T14:05:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    5,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2023-04-14T12:09:06Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    12,
                    9,
                    6,
                    4,
                    104,
                    0
                ],
                "title": "Chop Chop: Byzantine Atomic Broadcast to the Network Limit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chop Chop: Byzantine Atomic Broadcast to the Network Limit"
                },
                "summary": "At the heart of state machine replication, the celebrated technique enabling\ndecentralized and secure universal computation, lies Atomic Broadcast, a\nfundamental communication primitive that orders, authenticates, and\ndeduplicates messages. This paper presents Chop Chop, a Byzantine Atomic\nBroadcast system that uses a novel authenticated memory pool to amortize the\ncost of ordering, authenticating and deduplicating messages, achieving \"line\nrate\" (i.e., closely matching the complexity of a protocol that does not ensure\nany ordering, authentication or Byzantine resilience) even when processing\nmessages as small as 8 bytes. Chop Chop attains this performance by means of a\nnew form of batching we call distillation. A distilled batch is a set of\nmessages that are fast to authenticate, deduplicate, and order. Batches are\ndistilled using a novel interactive protocol involving brokers, an untrusted\nlayer of facilitating processes between clients and servers. In a\ngeo-distributed deployment of 64 medium-sized servers, Chop Chop processes\n43,600,000 messages per second with an average latency of 3.6 seconds. Under\nthe same conditions, state-of-the-art alternatives offer two orders of\nmagnitude less throughput for the same latency. We showcase three simple Chop\nChop applications: a Payment system, an Auction house and a \"Pixel war\" game,\nrespectively achieving 32, 2.3 and 35 million operations per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the heart of state machine replication, the celebrated technique enabling\ndecentralized and secure universal computation, lies Atomic Broadcast, a\nfundamental communication primitive that orders, authenticates, and\ndeduplicates messages. This paper presents Chop Chop, a Byzantine Atomic\nBroadcast system that uses a novel authenticated memory pool to amortize the\ncost of ordering, authenticating and deduplicating messages, achieving \"line\nrate\" (i.e., closely matching the complexity of a protocol that does not ensure\nany ordering, authentication or Byzantine resilience) even when processing\nmessages as small as 8 bytes. Chop Chop attains this performance by means of a\nnew form of batching we call distillation. A distilled batch is a set of\nmessages that are fast to authenticate, deduplicate, and order. Batches are\ndistilled using a novel interactive protocol involving brokers, an untrusted\nlayer of facilitating processes between clients and servers. In a\ngeo-distributed deployment of 64 medium-sized servers, Chop Chop processes\n43,600,000 messages per second with an average latency of 3.6 seconds. Under\nthe same conditions, state-of-the-art alternatives offer two orders of\nmagnitude less throughput for the same latency. We showcase three simple Chop\nChop applications: a Payment system, an Auction house and a \"Pixel war\" game,\nrespectively achieving 32, 2.3 and 35 million operations per second."
                },
                "authors": [
                    {
                        "name": "Martina Camaioni"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "Matteo Monti"
                    },
                    {
                        "name": "Pierre-Louis Roman"
                    },
                    {
                        "name": "Manuel Vidigueira"
                    },
                    {
                        "name": "Gauthier Voron"
                    }
                ],
                "author_detail": {
                    "name": "Gauthier Voron"
                },
                "author": "Gauthier Voron",
                "arxiv_comment": "Extended version of the paper appearing at OSDI 2024 with formal\n  definitions, pseudocode, and proofs added in appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.07081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.07081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14846v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14846v4",
                "updated": "2024-08-28T14:04:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    14,
                    4,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-19T14:53:01Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    14,
                    53,
                    1,
                    0,
                    50,
                    0
                ],
                "title": "Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stick to your Role! Stability of Personal Values Expressed in Large\n  Language Models"
                },
                "summary": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard way to study Large Language Models (LLMs) with benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLMs' highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence\n(specifically, value stability) should be studied as a specific property of\nLLMs and used as another dimension of LLM comparison (alongside others such as\ncognitive abilities, knowledge, or model size). We present a case-study on the\nstability of value expression over different contexts (simulated conversations\non different topics) as measured using a standard psychology questionnaire\n(PVQ) and on behavioral downstream tasks. Reusing methods from psychology, we\nstudy Rank-order stability on the population (interpersonal) level, and\nIpsative stability on the individual (intrapersonal) level. We consider two\nsettings (with and without instructing LLMs to simulate particular personas),\ntwo simulated populations, and three downstream tasks. We observe consistent\ntrends in the stability of models and model families - Mixtral, Mistral,\nGPT-3.5 and Qwen families are more stable than LLaMa-2 and Phi. The consistency\nof these trends implies that some models exhibit higher value stability than\nothers, and that stability can be estimated with the set of introduced\nmethodological tools. When instructed to simulate particular personas, LLMs\nexhibit low Rank-order stability, which further diminishes with conversation\nlength. This highlights the need for future research on LLMs that coherently\nsimulate different personas. This paper provides a foundational step in that\ndirection, and, to our knowledge, it is the first study of value stability in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Grgur Kovaƒç"
                    },
                    {
                        "name": "R√©my Portelas"
                    },
                    {
                        "name": "Masataka Sawayama"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "arxiv_doi": "10.1371/journal.pone.0309114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1371/journal.pone.0309114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.14846v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14846v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The project website and code are available at\n  https://sites.google.com/view/llmvaluestability Published in PLOS ONE (\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309114 ),\n  and a shorter version at CogSci 24 (\n  https://escholarship.org/uc/item/7w4823c6 )",
                "arxiv_journal_ref": "PLOS ONE, August 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05165v3",
                "updated": "2024-08-28T13:52:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    57,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-06T19:58:03Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    19,
                    58,
                    3,
                    5,
                    188,
                    0
                ],
                "title": "Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps"
                },
                "summary": "In software development, bug report reproduction is a challenging task. This\npaper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a\nlarge-scale language model (LLM), to automatically reproduce Android bug\nreports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce\n(S2R) entities. Instead, it leverages the entire textual bug report and employs\ninnovative prompts to enhance GPT's contextual reasoning. This approach is more\nflexible and context-aware than the traditional step-by-step entity matching\napproach, resulting in improved accuracy and effectiveness. In addition to\nhandling crash reports, ReBL has the capability of handling non-crash\nfunctional bug reports. Our evaluation of 96 Android bug reports (73 crash and\n23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these\nreports, averaging only 74.98 seconds per bug report. Additionally, ReBL\noutperformed three existing tools in both success rate and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In software development, bug report reproduction is a challenging task. This\npaper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a\nlarge-scale language model (LLM), to automatically reproduce Android bug\nreports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce\n(S2R) entities. Instead, it leverages the entire textual bug report and employs\ninnovative prompts to enhance GPT's contextual reasoning. This approach is more\nflexible and context-aware than the traditional step-by-step entity matching\napproach, resulting in improved accuracy and effectiveness. In addition to\nhandling crash reports, ReBL has the capability of handling non-crash\nfunctional bug reports. Our evaluation of 96 Android bug reports (73 crash and\n23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these\nreports, averaging only 74.98 seconds per bug report. Additionally, ReBL\noutperformed three existing tools in both success rate and speed."
                },
                "authors": [
                    {
                        "name": "Dingbang Wang"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Sidong Feng"
                    },
                    {
                        "name": "Zhaoxu Zhang"
                    },
                    {
                        "name": "William G. J. Halfond"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Xiaoxia Sun"
                    },
                    {
                        "name": "Jiangfan Shi"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "arxiv_doi": "10.1145/3650212.3680341",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3650212.3680341",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.05165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ISSTA 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15801v1",
                "updated": "2024-08-28T13:52:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    19,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:52:19Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    52,
                    19,
                    2,
                    241,
                    0
                ],
                "title": "Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization"
                },
                "summary": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization."
                },
                "authors": [
                    {
                        "name": "L√©o Hemamou"
                    },
                    {
                        "name": "Mehdi Debiane"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Debiane"
                },
                "author": "Mehdi Debiane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15800v1",
                "updated": "2024-08-28T13:51:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    51,
                    52,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:51:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    51,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing"
                },
                "summary": "Achieving personalized intelligence at the edge with real-time learning\ncapabilities holds enormous promise in enhancing our daily experiences and\nhelping decision making, planning, and sensing. However, efficient and reliable\nedge learning remains difficult with current technology due to the lack of\npersonalized data, insufficient hardware capabilities, and inherent challenges\nposed by online learning.\n  Over time and across multiple developmental stages, the brain has evolved to\nefficiently incorporate new knowledge by gradually building on previous\nknowledge. In this work, we emulate the multiple stages of learning with\ndigital neuromorphic technology that simulates the neural and synaptic\nprocesses of the brain using two stages of learning. First, a meta-training\nstage trains the hyperparameters of synaptic plasticity for one-shot learning\nusing a differentiable simulation of the neuromorphic hardware. This\nmeta-training process refines a hardware local three-factor synaptic plasticity\nrule and its associated hyperparameters to align with the trained task domain.\nIn a subsequent deployment stage, these optimized hyperparameters enable fast,\ndata-efficient, and accurate learning of new classes. We demonstrate our\napproach using event-driven vision sensor data and the Intel Loihi neuromorphic\nprocessor with its plasticity dynamics, achieving real-time one-shot learning\nof new classes that is vastly improved over transfer learning. Our methodology\ncan be deployed with arbitrary plasticity models and can be applied to\nsituations demanding quick learning and adaptation at the edge, such as\nnavigating unfamiliar environments or learning unexpected categories of data\nthrough user engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving personalized intelligence at the edge with real-time learning\ncapabilities holds enormous promise in enhancing our daily experiences and\nhelping decision making, planning, and sensing. However, efficient and reliable\nedge learning remains difficult with current technology due to the lack of\npersonalized data, insufficient hardware capabilities, and inherent challenges\nposed by online learning.\n  Over time and across multiple developmental stages, the brain has evolved to\nefficiently incorporate new knowledge by gradually building on previous\nknowledge. In this work, we emulate the multiple stages of learning with\ndigital neuromorphic technology that simulates the neural and synaptic\nprocesses of the brain using two stages of learning. First, a meta-training\nstage trains the hyperparameters of synaptic plasticity for one-shot learning\nusing a differentiable simulation of the neuromorphic hardware. This\nmeta-training process refines a hardware local three-factor synaptic plasticity\nrule and its associated hyperparameters to align with the trained task domain.\nIn a subsequent deployment stage, these optimized hyperparameters enable fast,\ndata-efficient, and accurate learning of new classes. We demonstrate our\napproach using event-driven vision sensor data and the Intel Loihi neuromorphic\nprocessor with its plasticity dynamics, achieving real-time one-shot learning\nof new classes that is vastly improved over transfer learning. Our methodology\ncan be deployed with arbitrary plasticity models and can be applied to\nsituations demanding quick learning and adaptation at the edge, such as\nnavigating unfamiliar environments or learning unexpected categories of data\nthrough user engagement."
                },
                "authors": [
                    {
                        "name": "Kenneth Stewart"
                    },
                    {
                        "name": "Michael Neumeier"
                    },
                    {
                        "name": "Sumit Bam Shrestha"
                    },
                    {
                        "name": "Garrick Orchard"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "17 page journal article. Submitted to IOP NCE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15796v1",
                "updated": "2024-08-28T13:42:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:42:28Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    42,
                    28,
                    2,
                    241,
                    0
                ],
                "title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models"
                },
                "summary": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility."
                },
                "authors": [
                    {
                        "name": "H√©di Zhegidi"
                    },
                    {
                        "name": "Ludovic Moncla"
                    }
                ],
                "author_detail": {
                    "name": "Ludovic Moncla"
                },
                "author": "Ludovic Moncla",
                "arxiv_comment": "Github repo: https://github.com/GEODE-project/ner-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15793v1",
                "updated": "2024-08-28T13:37:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    37,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    37,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough"
                },
                "summary": "We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub."
                },
                "authors": [
                    {
                        "name": "Konstantin Dobler"
                    },
                    {
                        "name": "Gerard de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard de Melo"
                },
                "author": "Gerard de Melo",
                "arxiv_comment": "WANT@ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15792v1",
                "updated": "2024-08-28T13:35:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    35,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    35,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "Efficient LLM Scheduling by Learning to Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Scheduling by Learning to Rank"
                },
                "summary": "In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git"
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Runlong Su"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15787v1",
                "updated": "2024-08-28T13:29:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    29,
                    59,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:29:59Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    29,
                    59,
                    2,
                    241,
                    0
                ],
                "title": "Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions"
                },
                "summary": "Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health."
                },
                "authors": [
                    {
                        "name": "Huachuan Qiu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v1",
                "updated": "2024-08-28T13:16:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15769v1",
                "updated": "2024-08-28T13:05:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    5,
                    55,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T13:05:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    5,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "A Survey on Evaluation of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluation of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs."
                },
                "authors": [
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Jingyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyi Zhang"
                },
                "author": "Jingyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15751v1",
                "updated": "2024-08-28T12:35:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    35,
                    56,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T12:35:56Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    35,
                    56,
                    2,
                    241,
                    0
                ],
                "title": "Adaptive Traffic Signal Control Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Traffic Signal Control Using Reinforcement Learning"
                },
                "summary": "Traffic demand is continuously increasing, leading to significant congestion\nissues in major urban areas. Constructing new infrastructure is a potential\nsolution but presents a substantial financial burden on national economies. An\nalternative approach involves optimizing existing traffic networks through the\ndynamic control of traffic signals at intersections. Recent advancements in\nReinforcement Learning (RL) techniques have demonstrated their capability to\naddress the complexities associated with traffic congestion. In this paper, we\npropose a solution to traffic congestion using reinforcement learning. We\ndefine the state as a scalar representing the queue length, demonstrating that\nthe algorithm can effectively learn from this simplified state representation.\nThis approach can potentially reduce deployment costs by minimizing the number\nof sensors required at intersections. We have developed two RL algorithms: a\nturn-based agent, which prioritizes traffic signals for the intersection side\nwith higher traffic, and a time-based agent, which adheres to a fixed phase\ncycle, adjusting the phase duration based on traffic conditions. To assess the\nperformance of these algorithms, we designed four distinct traffic scenarios\nand computed seven evaluation metrics for each. Simulation results indicate\nthat both algorithms outperform conventional traffic signal control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic demand is continuously increasing, leading to significant congestion\nissues in major urban areas. Constructing new infrastructure is a potential\nsolution but presents a substantial financial burden on national economies. An\nalternative approach involves optimizing existing traffic networks through the\ndynamic control of traffic signals at intersections. Recent advancements in\nReinforcement Learning (RL) techniques have demonstrated their capability to\naddress the complexities associated with traffic congestion. In this paper, we\npropose a solution to traffic congestion using reinforcement learning. We\ndefine the state as a scalar representing the queue length, demonstrating that\nthe algorithm can effectively learn from this simplified state representation.\nThis approach can potentially reduce deployment costs by minimizing the number\nof sensors required at intersections. We have developed two RL algorithms: a\nturn-based agent, which prioritizes traffic signals for the intersection side\nwith higher traffic, and a time-based agent, which adheres to a fixed phase\ncycle, adjusting the phase duration based on traffic conditions. To assess the\nperformance of these algorithms, we designed four distinct traffic scenarios\nand computed seven evaluation metrics for each. Simulation results indicate\nthat both algorithms outperform conventional traffic signal control systems."
                },
                "authors": [
                    {
                        "name": "Muhammad Tahir Rafique"
                    },
                    {
                        "name": "Ahmed Mustafa"
                    },
                    {
                        "name": "Hasan Sajid"
                    }
                ],
                "author_detail": {
                    "name": "Hasan Sajid"
                },
                "author": "Hasan Sajid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15746v1",
                "updated": "2024-08-28T12:24:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    24,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T12:24:54Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    24,
                    54,
                    2,
                    241,
                    0
                ],
                "title": "A Hybrid Approach for Low-Complexity Joint Acoustic Echo and Noise\n  Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Approach for Low-Complexity Joint Acoustic Echo and Noise\n  Reduction"
                },
                "summary": "Deep learning-based methods that jointly perform the task of acoustic echo\nand noise reduction (AENR) often require high memory and computational\nresources, making them unsuitable for real-time deployment on low-resource\nplatforms such as embedded devices. We propose a low-complexity hybrid approach\nfor joint AENR by employing a single model to suppress both residual echo and\nnoise components. Specifically, we integrate the state-of-the-art (SOTA) ULCNet\nmodel, which was originally proposed to achieve ultra-low complexity noise\nsuppression, in a hybrid system and train it for joint AENR. We show that the\nproposed approach achieves better echo reduction and comparable noise reduction\nperformance with much lower computational complexity and memory requirements\nthan all considered SOTA methods, at the cost of slight degradation in speech\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based methods that jointly perform the task of acoustic echo\nand noise reduction (AENR) often require high memory and computational\nresources, making them unsuitable for real-time deployment on low-resource\nplatforms such as embedded devices. We propose a low-complexity hybrid approach\nfor joint AENR by employing a single model to suppress both residual echo and\nnoise components. Specifically, we integrate the state-of-the-art (SOTA) ULCNet\nmodel, which was originally proposed to achieve ultra-low complexity noise\nsuppression, in a hybrid system and train it for joint AENR. We show that the\nproposed approach achieves better echo reduction and comparable noise reduction\nperformance with much lower computational complexity and memory requirements\nthan all considered SOTA methods, at the cost of slight degradation in speech\nquality."
                },
                "authors": [
                    {
                        "name": "Shrishti Saha Shetu"
                    },
                    {
                        "name": "Naveen Kumar Desiraju"
                    },
                    {
                        "name": "Jose Miguel Martinez Aponte"
                    },
                    {
                        "name": "Emanu√´l A. P. Habets"
                    },
                    {
                        "name": "Edwin Mabande"
                    }
                ],
                "author_detail": {
                    "name": "Edwin Mabande"
                },
                "author": "Edwin Mabande",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14398v2",
                "updated": "2024-08-28T12:03:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    3,
                    54,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "title": "Language-specific Calibration for Pruning Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific Calibration for Pruning Multilingual Language Models"
                },
                "summary": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners."
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15710v2",
                "updated": "2024-08-29T14:47:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    14,
                    47,
                    37,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T11:18:06Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    18,
                    6,
                    2,
                    241,
                    0
                ],
                "title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples"
                },
                "summary": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark"
                },
                "authors": [
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Shizhe Chen"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11239v2",
                "updated": "2024-08-28T11:10:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    11,
                    10,
                    59,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-17T06:07:32Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    6,
                    7,
                    32,
                    0,
                    169,
                    0
                ],
                "title": "Evading AI-Generated Content Detectors using Homoglyphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evading AI-Generated Content Detectors using Homoglyphs"
                },
                "summary": "The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks."
                },
                "authors": [
                    {
                        "name": "Aldan Creo"
                    },
                    {
                        "name": "Shushanta Pudasaini"
                    }
                ],
                "author_detail": {
                    "name": "Shushanta Pudasaini"
                },
                "author": "Shushanta Pudasaini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v1",
                "updated": "2024-08-28T10:51:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11537v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11537v3",
                "updated": "2024-08-28T10:39:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    39,
                    11,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-18T10:36:05Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    10,
                    36,
                    5,
                    6,
                    49,
                    0
                ],
                "title": "Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning"
                },
                "summary": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Zhouhao Sun"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted by ACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11537v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15684v1",
                "updated": "2024-08-28T10:18:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    18,
                    29,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T10:18:29Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    18,
                    29,
                    2,
                    241,
                    0
                ],
                "title": "A quasi-ohmic back contact achieved by inserting single-crystal graphene\n  in flexible Kesterite solar cells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quasi-ohmic back contact achieved by inserting single-crystal graphene\n  in flexible Kesterite solar cells"
                },
                "summary": "Flexible photovoltaics with a lightweight and adaptable nature that allows\nfor deployment on curved surfaces and in building facades have always been a\ngoal vigorously pursued by researchers in thin-film solar cell technology. The\nrecent strides made in improving the sunlight-to-electricity conversion\nefficiency of kesterite Cu$_{2}$ZnSn(S, Se)$_{4}$ (CZTSSe) suggest it to be a\nperfect candidate. However, making use of rare Mo foil in CZTSSe solar cells\ncauses severe problems in thermal expansion matching, uneven grain growth, and\nsevere problems at the back contact of the devices. Herein, a strategy\nutilizing single-crystal graphene to modify the back interface of flexible\nCZTSSe solar cells is proposed. It will be shown that the insertion of graphene\nat the Mo foil/CZTSSe interface provides strong physical support for the\nsubsequent deposition of the CZTSSe absorber layer, improving the adhesion\nbetween the absorber layer and the Mo foil substrate. Additionally, the\ngraphene passivates the rough sites on the surface of the Mo foil, enhancing\nthe chemical homogeneity of the substrate, and resulting in a more crystalline\nand homogeneous CZTSSe absorber layer on the Mo foil substrate. The detrimental\nreaction between Mo and CZTSSe has also been eliminated. Through an analysis of\nthe electrical properties, it is found that the introduction of graphene at the\nback interface promotes the formation of a quasi-ohmic contact at the back\ncontact, decreasing the back contact barrier of the solar cell, and leading to\nefficient collection of charges at the back interface. This investigation\ndemonstrates that solution-based CZTSSe photovoltaic devices could form the\nbasis of cheap and flexible solar cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible photovoltaics with a lightweight and adaptable nature that allows\nfor deployment on curved surfaces and in building facades have always been a\ngoal vigorously pursued by researchers in thin-film solar cell technology. The\nrecent strides made in improving the sunlight-to-electricity conversion\nefficiency of kesterite Cu$_{2}$ZnSn(S, Se)$_{4}$ (CZTSSe) suggest it to be a\nperfect candidate. However, making use of rare Mo foil in CZTSSe solar cells\ncauses severe problems in thermal expansion matching, uneven grain growth, and\nsevere problems at the back contact of the devices. Herein, a strategy\nutilizing single-crystal graphene to modify the back interface of flexible\nCZTSSe solar cells is proposed. It will be shown that the insertion of graphene\nat the Mo foil/CZTSSe interface provides strong physical support for the\nsubsequent deposition of the CZTSSe absorber layer, improving the adhesion\nbetween the absorber layer and the Mo foil substrate. Additionally, the\ngraphene passivates the rough sites on the surface of the Mo foil, enhancing\nthe chemical homogeneity of the substrate, and resulting in a more crystalline\nand homogeneous CZTSSe absorber layer on the Mo foil substrate. The detrimental\nreaction between Mo and CZTSSe has also been eliminated. Through an analysis of\nthe electrical properties, it is found that the introduction of graphene at the\nback interface promotes the formation of a quasi-ohmic contact at the back\ncontact, decreasing the back contact barrier of the solar cell, and leading to\nefficient collection of charges at the back interface. This investigation\ndemonstrates that solution-based CZTSSe photovoltaic devices could form the\nbasis of cheap and flexible solar cells."
                },
                "authors": [
                    {
                        "name": "Yixiong Ji"
                    },
                    {
                        "name": "Wentong Yang"
                    },
                    {
                        "name": "Di Yan"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Shi Tang"
                    },
                    {
                        "name": "Jintao Fu"
                    },
                    {
                        "name": "James Bullock"
                    },
                    {
                        "name": "Mei Gao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Zhancheng Li"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Xingzhan Wei"
                    },
                    {
                        "name": "Haofei Shi"
                    },
                    {
                        "name": "Fangyang Liu"
                    },
                    {
                        "name": "Paul Mulvaney"
                    }
                ],
                "author_detail": {
                    "name": "Paul Mulvaney"
                },
                "author": "Paul Mulvaney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12471v2",
                "updated": "2024-08-28T09:48:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    48,
                    24,
                    2,
                    241,
                    0
                ],
                "published": "2024-01-23T03:45:05Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    3,
                    45,
                    5,
                    1,
                    23,
                    0
                ],
                "title": "Training-Free Action Recognition and Goal Inference with Dynamic Frame\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Action Recognition and Goal Inference with Dynamic Frame\n  Selection"
                },
                "summary": "We introduce VidTFS, a Training-free, open-vocabulary video goal and action\ninference framework that combines the frozen vision foundational model (VFM)\nand large language model (LLM) with a novel dynamic Frame Selection module. Our\nexperiments demonstrate that the proposed frame selection module improves the\nperformance of the framework significantly. We validate the performance of the\nproposed VidTFS on four widely used video datasets, including CrossTask, COIN,\nUCF101, and ActivityNet, covering goal inference and action recognition tasks\nunder open-vocabulary settings without requiring any training or fine-tuning.\nThe results show that VidTFS outperforms pretrained and instruction-tuned\nmultimodal language models that directly stack LLM and VFM for downstream video\ninference tasks. Our VidTFS with its adaptability shows the future potential\nfor generalizing to new training-free video inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VidTFS, a Training-free, open-vocabulary video goal and action\ninference framework that combines the frozen vision foundational model (VFM)\nand large language model (LLM) with a novel dynamic Frame Selection module. Our\nexperiments demonstrate that the proposed frame selection module improves the\nperformance of the framework significantly. We validate the performance of the\nproposed VidTFS on four widely used video datasets, including CrossTask, COIN,\nUCF101, and ActivityNet, covering goal inference and action recognition tasks\nunder open-vocabulary settings without requiring any training or fine-tuning.\nThe results show that VidTFS outperforms pretrained and instruction-tuned\nmultimodal language models that directly stack LLM and VFM for downstream video\ninference tasks. Our VidTFS with its adaptability shows the future potential\nfor generalizing to new training-free video inference tasks."
                },
                "authors": [
                    {
                        "name": "Ee Yeo Keat"
                    },
                    {
                        "name": "Zhang Hao"
                    },
                    {
                        "name": "Alexander Matyasko"
                    },
                    {
                        "name": "Basura Fernando"
                    }
                ],
                "author_detail": {
                    "name": "Basura Fernando"
                },
                "author": "Basura Fernando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.12471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15666v1",
                "updated": "2024-08-28T09:35:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    35,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:35:15Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    35,
                    15,
                    2,
                    241,
                    0
                ],
                "title": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and\n  Perturbation of Style Elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleRemix: Interpretable Authorship Obfuscation via Distillation and\n  Perturbation of Style Elements"
                },
                "summary": "Authorship obfuscation, rewriting a text to intentionally obscure the\nidentity of the author, is an important but challenging task. Current methods\nusing large language models (LLMs) lack interpretability and controllability,\noften ignoring author-specific stylistic features, resulting in less robust\nperformance overall.\n  To address this, we develop StyleRemix, an adaptive and interpretable\nobfuscation method that perturbs specific, fine-grained style elements of the\noriginal input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA)\nmodules to rewrite an input specifically along various stylistic axes (e.g.,\nformality and length) while maintaining low computational cost. StyleRemix\noutperforms state-of-the-art baselines and much larger LLMs in a variety of\ndomains as assessed by both automatic and human evaluation.\n  Additionally, we release AuthorMix, a large set of 30K high-quality,\nlong-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a\nparallel corpus of 1,500 texts spanning seven style axes in 16 unique\ndirections",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authorship obfuscation, rewriting a text to intentionally obscure the\nidentity of the author, is an important but challenging task. Current methods\nusing large language models (LLMs) lack interpretability and controllability,\noften ignoring author-specific stylistic features, resulting in less robust\nperformance overall.\n  To address this, we develop StyleRemix, an adaptive and interpretable\nobfuscation method that perturbs specific, fine-grained style elements of the\noriginal input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA)\nmodules to rewrite an input specifically along various stylistic axes (e.g.,\nformality and length) while maintaining low computational cost. StyleRemix\noutperforms state-of-the-art baselines and much larger LLMs in a variety of\ndomains as assessed by both automatic and human evaluation.\n  Additionally, we release AuthorMix, a large set of 30K high-quality,\nlong-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a\nparallel corpus of 1,500 texts spanning seven style axes in 16 unique\ndirections"
                },
                "authors": [
                    {
                        "name": "Jillian Fisher"
                    },
                    {
                        "name": "Skyler Hallinan"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Mitchell Gordon"
                    },
                    {
                        "name": "Zaid Harchaoui"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15658v1",
                "updated": "2024-08-28T09:19:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    19,
                    9,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T09:19:09Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    9,
                    19,
                    9,
                    2,
                    241,
                    0
                ],
                "title": "An Empirical Study on Self-correcting Large Language Models for Data\n  Science Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Self-correcting Large Language Models for Data\n  Science Code Generation"
                },
                "summary": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently advanced many applications on\nsoftware engineering tasks, particularly the potential for code generation.\nAmong contemporary challenges, code generated by LLMs often suffers from\ninaccuracies and hallucinations, requiring external inputs to correct. One\nrecent strategy to fix these issues is to refine the code generated from LLMs\nusing the input from the model itself (self-augmented). In this work, we\nproposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and\nautomatically refines code through a self-correcting process, guided by a chain\nof thought constructed from real-world programming problem feedback. Focusing\non data science code, including Python libraries such as NumPy and Pandas, our\nevaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve\nsignificantly outperforms existing models in solving complex problems. The\nframework shows substantial improvements in both initial code generation and\nsubsequent iterations, with the model's accuracy increasing significantly with\neach additional iteration. This highlights the effectiveness of using\nchain-of-thought prompting to address complexities revealed by program executor\ntraceback error messages. We also discuss how CoT-SelfEvolve can be integrated\ninto continuous software engineering environments, providing a practical\nsolution for improving LLM-based code generation."
                },
                "authors": [
                    {
                        "name": "Thai Tang Quoc"
                    },
                    {
                        "name": "Duc Ha Minh"
                    },
                    {
                        "name": "Tho Quan Thanh"
                    },
                    {
                        "name": "Anh Nguyen-Duc"
                    }
                ],
                "author_detail": {
                    "name": "Anh Nguyen-Duc"
                },
                "author": "Anh Nguyen-Duc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02392v4",
                "updated": "2024-08-28T08:49:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    49,
                    57,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-02T16:10:55Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    16,
                    10,
                    55,
                    1,
                    184,
                    0
                ],
                "title": "TokenPacker: Efficient Visual Projector for Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenPacker: Efficient Visual Projector for Multimodal LLM"
                },
                "summary": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker."
                },
                "authors": [
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Dongqi Tang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jie Qin"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "16 pages, Codes:https://github.com/CircleRadon/TokenPacker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20770v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20770v3",
                "updated": "2024-08-28T08:46:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    46,
                    17,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-24T07:23:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    23,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Sentinel: LLM Agent for Adversarial Purification"
                },
                "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."
                },
                "authors": [
                    {
                        "name": "Guang Lin"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20770v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20770v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v2",
                "updated": "2024-08-28T08:32:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    44,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient and Effective Multi-modal Extension of Large\n  Language Models for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) refers to analyzing and comprehending\ndocuments containing substantial textual content. With the rapid evolution of\nlarge language models (LLMs), they have been widely leveraged for TDU due to\ntheir remarkable versatility and generalization. In this paper, we introduce\nDocLayLLM, an efficient and effective multi-modal extension of LLMs\nspecifically designed for TDU. By integrating visual patch tokens and 2D\npositional tokens into LLMs and encoding the document content using the LLMs\nthemselves, we fully take advantage of the document comprehension capability of\nLLMs and enhance their perception of OCR information. We have also deeply\nconsidered the role of the chain-of-thought (CoT) and innovatively proposed the\ntechniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve\nremarkable performances with lightweight training settings, showcasing its\nefficiency and effectiveness. Experimental results demonstrate that our\nDocLayLLM surpasses existing OCR-dependent methods and also outperforms\nOCR-free competitors."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15630v1",
                "updated": "2024-08-28T08:32:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    21,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T08:32:21Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    32,
                    21,
                    2,
                    241,
                    0
                ],
                "title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code\n  Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code\n  Validation"
                },
                "summary": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has greatly facilitated code\ngeneration, but ensuring the functional correctness of generated code remains a\nchallenge. Traditional validation methods are often time-consuming,\nerror-prone, and impractical for large volumes of code. We introduce CodeSift,\na novel framework that leverages LLMs as the first-line filter of code\nvalidation without the need for execution, reference code, or human feedback,\nthereby reducing the validation effort. We assess the effectiveness of our\nmethod across three diverse datasets encompassing two programming languages.\nOur results indicate that CodeSift outperforms state-of-the-art code evaluation\nmethods. Internal testing conducted with subject matter experts reveals that\nthe output generated by CodeSift is in line with human preference, reinforcing\nits effectiveness as a dependable automated code validation tool."
                },
                "authors": [
                    {
                        "name": "Pooja Aggarwal"
                    },
                    {
                        "name": "Oishik Chatterjee"
                    },
                    {
                        "name": "Ting Dai"
                    },
                    {
                        "name": "Prateeti Mohapatra"
                    },
                    {
                        "name": "Brent Paulovicks"
                    },
                    {
                        "name": "Brad Blancett"
                    },
                    {
                        "name": "Arthur De Magalhaes"
                    }
                ],
                "author_detail": {
                    "name": "Arthur De Magalhaes"
                },
                "author": "Arthur De Magalhaes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15626v1",
                "updated": "2024-08-28T08:25:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    41,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T08:25:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "Can Visual Language Models Replace OCR-Based Visual Question Answering\n  Pipelines in Production? A Case Study in Retail",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Visual Language Models Replace OCR-Based Visual Question Answering\n  Pipelines in Production? A Case Study in Retail"
                },
                "summary": "Most production-level deployments for Visual Question Answering (VQA) tasks\nare still build as processing pipelines of independent steps including image\npre-processing, object- and text detection, Optical Character Recognition (OCR)\nand (mostly supervised) object classification. However, the recent advances in\nvision Foundation Models [25] and Vision Language Models (VLMs) [23] raise the\nquestion if these custom trained, multi-step approaches can be replaced with\npre-trained, single-step VLMs. This paper analyzes the performance and limits\nof various VLMs in the context of VQA and OCR [5, 9, 12] tasks in a\nproduction-level scenario. Using data from the Retail-786k [10] dataset, we\ninvestigate the capabilities of pre-trained VLMs to answer detailed questions\nabout advertised products in images. Our study includes two commercial models,\nGPT-4V [16] and GPT-4o [17], as well as four open-source models: InternVL [5],\nLLaVA 1.5 [12], LLaVA-NeXT [13], and CogAgent [9]. Our initial results show,\nthat there is in general no big performance gap between open-source and\ncommercial models. However, we observe a strong task dependent variance in VLM\nperformance: while most models are able to answer questions regarding the\nproduct brand and price with high accuracy, they completely fail at the same\ntime to correctly identity the specific product name or discount. This\nindicates the problem of VLMs to solve fine-grained classification tasks as\nwell to model the more abstract concept of discounts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most production-level deployments for Visual Question Answering (VQA) tasks\nare still build as processing pipelines of independent steps including image\npre-processing, object- and text detection, Optical Character Recognition (OCR)\nand (mostly supervised) object classification. However, the recent advances in\nvision Foundation Models [25] and Vision Language Models (VLMs) [23] raise the\nquestion if these custom trained, multi-step approaches can be replaced with\npre-trained, single-step VLMs. This paper analyzes the performance and limits\nof various VLMs in the context of VQA and OCR [5, 9, 12] tasks in a\nproduction-level scenario. Using data from the Retail-786k [10] dataset, we\ninvestigate the capabilities of pre-trained VLMs to answer detailed questions\nabout advertised products in images. Our study includes two commercial models,\nGPT-4V [16] and GPT-4o [17], as well as four open-source models: InternVL [5],\nLLaVA 1.5 [12], LLaVA-NeXT [13], and CogAgent [9]. Our initial results show,\nthat there is in general no big performance gap between open-source and\ncommercial models. However, we observe a strong task dependent variance in VLM\nperformance: while most models are able to answer questions regarding the\nproduct brand and price with high accuracy, they completely fail at the same\ntime to correctly identity the specific product name or discount. This\nindicates the problem of VLMs to solve fine-grained classification tasks as\nwell to model the more abstract concept of discounts."
                },
                "authors": [
                    {
                        "name": "Bianca Lamm"
                    },
                    {
                        "name": "Janis Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Janis Keuper"
                },
                "author": "Janis Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15625v1",
                "updated": "2024-08-28T08:25:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T08:25:22Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    25,
                    22,
                    2,
                    241,
                    0
                ],
                "title": "CBF-LLM: Safe Control for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBF-LLM: Safe Control for LLM Alignment"
                },
                "summary": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks."
                },
                "authors": [
                    {
                        "name": "Yuya Miyaoka"
                    },
                    {
                        "name": "Masaki Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Masaki Inoue"
                },
                "author": "Masaki Inoue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18312v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18312v4",
                "updated": "2024-08-28T08:07:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    7,
                    49,
                    2,
                    241,
                    0
                ],
                "published": "2024-06-26T12:51:37Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    12,
                    51,
                    37,
                    2,
                    178,
                    0
                ],
                "title": "AI-native Memory: A Pathway from LLMs Towards AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory: A Pathway from LLMs Towards AGI"
                },
                "summary": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions."
                },
                "authors": [
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Zai Zheng"
                    },
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Mindverse Team"
                    }
                ],
                "author_detail": {
                    "name": "Mindverse Team"
                },
                "author": "Mindverse Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18312v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18312v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15565v1",
                "updated": "2024-08-28T06:33:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    33,
                    3,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T06:33:03Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    33,
                    3,
                    2,
                    241,
                    0
                ],
                "title": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large\n  Language Models"
                },
                "summary": "There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing trend of teaching large language models (LLMs) to solve\nmathematical problems through coding. Existing studies primarily focus on\nprompting powerful, closed-source models to generate seed training data\nfollowed by in-domain data augmentation, equipping LLMs with considerable\ncapabilities for code-aided mathematical reasoning. However, continually\ntraining these models on augmented data derived from a few datasets such as\nGSM8K may impair their generalization abilities and restrict their\neffectiveness to a narrow range of question types. Conversely, the potential of\nimproving such LLMs by leveraging large-scale, expert-written, diverse math\nquestion-answer pairs remains unexplored. To utilize these resources and tackle\nunique challenges such as code response assessment, we propose a novel paradigm\nthat uses a code-based critic model to guide steps including question-code data\nconstruction, quality control, and complementary evaluation. We also explore\ndifferent alignment algorithms with self-generated instruction/preference data\nto foster continuous improvement. Experiments across both in-domain (up to\n+5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate\nthe effectiveness of the proposed paradigm."
                },
                "authors": [
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15562v1",
                "updated": "2024-08-28T06:28:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    28,
                    1,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T06:28:01Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    28,
                    1,
                    2,
                    241,
                    0
                ],
                "title": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Lossless Speculative Decoding via Feature Sampling and Partial\n  Alignment Distillation"
                },
                "summary": "Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless speculative decoding accelerates target large language model (LLM)\ninference by employing a lightweight draft model for generating tree-structured\ncandidates, which are subsequently verified in parallel by the target LLM.\nCurrently, effective approaches leverage feature-level rather than token-level\nautoregression within the draft model to facilitate more straightforward\npredictions and enhanced knowledge distillation. In this paper, we reassess\nthese approaches and propose FSPAD (Feature Sampling and Partial Alignment\nDistillation for Lossless Speculative Decoding), which introduces two\nstraightforward and effective components within the existing framework to boost\nlossless speculative decoding. Firstly, FSPAD utilizes token embeddings to\nsample features of the target LLM in high-dimensional space before feeding them\ninto the draft model, due to the inherent uncertainty of the features\npreventing the draft model from obtaining the specific token output by the\ntarget LLM. Secondly, FSPAD introduces partial alignment distillation to weaken\nthe draft model's connection between features and logits, aiming to reduce the\nconflict between feature alignment and logit confidence during training. Our\nexperiments include both greedy and non-greedy decoding on the largest and\nsmallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in\nmulti-turn conversation, translation, summarization, question answering,\nmathematical reasoning, and retrieval-augmented generation. The results show\nthat FSPAD outperforms the state-of-the-art method across all the\naforementioned tasks and target LLMs."
                },
                "authors": [
                    {
                        "name": "Lujun Gui"
                    },
                    {
                        "name": "Bin Xiao"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "The work was not submitted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07626v2",
                "updated": "2024-08-28T06:18:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    18,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-05-13T10:37:50Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    37,
                    50,
                    0,
                    134,
                    0
                ],
                "title": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models"
                },
                "summary": "Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters."
                },
                "authors": [
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Di Yao"
                    },
                    {
                        "name": "Lanting Fang"
                    },
                    {
                        "name": "Zhetao Li"
                    },
                    {
                        "name": "Wenbin Li"
                    },
                    {
                        "name": "Kaiyu Feng"
                    },
                    {
                        "name": "XiaoWen Ji"
                    },
                    {
                        "name": "Jingping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Jingping Bi"
                },
                "author": "Jingping Bi",
                "arxiv_comment": "13pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13918v2",
                "updated": "2024-08-28T06:16:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    6,
                    16,
                    42,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-25T19:03:46Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    19,
                    3,
                    46,
                    6,
                    238,
                    0
                ],
                "title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with\n  Spatiotemporal Constraints"
                },
                "summary": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating human mobility data is essential for various application domains,\nincluding transportation, urban planning, and epidemic control, since real data\nare often inaccessible to researchers due to expensive costs and privacy\nissues. Several existing deep generative solutions propose learning from real\ntrajectories to generate synthetic ones. Despite the progress, most of them\nsuffer from training stability issues and scale poorly with growing data size.\nMore importantly, they generally lack control mechanisms to steer the generated\ntrajectories based on spatiotemporal constraints such as fixing specific\nvisits. To address such limitations, we formally define the controlled\ntrajectory generation problem with spatiotemporal constraints and propose\nGeo-Llama. This novel LLM-inspired framework enforces explicit visit\nconstraints in a contextually coherent way. It fine-tunes pre-trained LLMs on\ntrajectories with a visit-wise permutation strategy where each visit\ncorresponds to a time and location. This enables the model to capture the\nspatiotemporal patterns regardless of visit orders and allows flexible and\nin-context constraint integration through prompts during generation. Extensive\nexperiments on real-world and synthetic datasets validate the effectiveness of\nGeo-Llama, demonstrating its versatility and robustness in handling a broad\nrange of constraints to generate more realistic trajectories compared to\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Haowen Lin"
                    },
                    {
                        "name": "John Krumm"
                    },
                    {
                        "name": "Cyrus Shahabi"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15549v1",
                "updated": "2024-08-28T05:53:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    46,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:53:46Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    53,
                    46,
                    2,
                    241,
                    0
                ],
                "title": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback"
                },
                "summary": "As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, and the risk of\nfeedback loops that amplify model biases. To overcome these limitations, we\nintroduce WildFeedback, a novel framework that leverages real-time, in-situ\nuser interactions to create preference datasets that more accurately reflect\nauthentic human values. WildFeedback operates through a three-step process:\nfeedback signal identification, preference data construction, and user-guided\nevaluation. We applied this framework to a large corpus of user-LLM\nconversations, resulting in a rich preference dataset that reflects genuine\nuser preferences. This dataset captures the nuances of user preferences by\nidentifying and classifying feedback signals within natural conversations,\nthereby enabling the construction of more representative and context-sensitive\nalignment data. Our extensive experiments demonstrate that LLMs fine-tuned on\nWildFeedback exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed user-guided\nevaluation. By incorporating real-time feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users. In summary,\nWildFeedback offers a robust, scalable solution for aligning LLMs with true\nhuman values, setting a new standard for the development and evaluation of\nuser-centric language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, aligning these models\nwith human preferences has emerged as a critical challenge. Traditional\nalignment methods, relying on human or LLM annotated datasets, are limited by\ntheir resource-intensive nature, inherent subjectivity, and the risk of\nfeedback loops that amplify model biases. To overcome these limitations, we\nintroduce WildFeedback, a novel framework that leverages real-time, in-situ\nuser interactions to create preference datasets that more accurately reflect\nauthentic human values. WildFeedback operates through a three-step process:\nfeedback signal identification, preference data construction, and user-guided\nevaluation. We applied this framework to a large corpus of user-LLM\nconversations, resulting in a rich preference dataset that reflects genuine\nuser preferences. This dataset captures the nuances of user preferences by\nidentifying and classifying feedback signals within natural conversations,\nthereby enabling the construction of more representative and context-sensitive\nalignment data. Our extensive experiments demonstrate that LLMs fine-tuned on\nWildFeedback exhibit significantly improved alignment with user preferences, as\nevidenced by both traditional benchmarks and our proposed user-guided\nevaluation. By incorporating real-time feedback from actual users, WildFeedback\naddresses the scalability, subjectivity, and bias challenges that plague\nexisting approaches, marking a significant step toward developing LLMs that are\nmore responsive to the diverse and evolving needs of their users. In summary,\nWildFeedback offers a robust, scalable solution for aligning LLMs with true\nhuman values, setting a new standard for the development and evaluation of\nuser-centric language models."
                },
                "authors": [
                    {
                        "name": "Taiwei Shi"
                    },
                    {
                        "name": "Zhuoer Wang"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Ying-Chun Lin"
                    },
                    {
                        "name": "Zexue He"
                    },
                    {
                        "name": "Mengting Wan"
                    },
                    {
                        "name": "Pei Zhou"
                    },
                    {
                        "name": "Sujay Jauhar"
                    },
                    {
                        "name": "Xiaofeng Xu"
                    },
                    {
                        "name": "Xia Song"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v1",
                "updated": "2024-08-28T05:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jian Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15542v1",
                "updated": "2024-08-28T05:34:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    34,
                    14,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T05:34:14Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    34,
                    14,
                    2,
                    241,
                    0
                ],
                "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input"
                },
                "summary": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models."
                },
                "authors": [
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Yibing Wang"
                    },
                    {
                        "name": "Hanghang Ma"
                    },
                    {
                        "name": "Xiaoping Wu"
                    },
                    {
                        "name": "Xiaoqi Ma"
                    },
                    {
                        "name": "Xiaoming Wei"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Enhua Wu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15533v2",
                "updated": "2024-08-29T08:45:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    8,
                    45,
                    30,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-28T04:44:43Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    44,
                    43,
                    2,
                    241,
                    0
                ],
                "title": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via\n  Layer-wise Relevance Propagation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a primary technique for\nmitigating hallucinations in large language models (LLMs). However, incomplete\nknowledge extraction and insufficient understanding can still mislead LLMs to\nproduce irrelevant or even contradictory responses, which means hallucinations\npersist in RAG. In this paper, we propose LRP4RAG, a method based on the\nLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations\nin RAG. Specifically, we first utilize LRP to compute the relevance between the\ninput and output of the RAG generator. We then apply further extraction and\nresampling to the relevance matrix. The processed relevance data are input into\nmultiple classifiers to determine whether the output contains hallucinations.\nTo the best of our knowledge, this is the first time that LRP has been used for\ndetecting RAG hallucinations, and extensive experiments demonstrate that\nLRP4RAG outperforms existing baselines."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15519v1",
                "updated": "2024-08-28T04:12:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    12,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T04:12:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    12,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Depth-Weighted Detection of Behaviours of Risk in People with Dementia\n  using Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Weighted Detection of Behaviours of Risk in People with Dementia\n  using Cameras"
                },
                "summary": "The behavioural and psychological symptoms of dementia, such as agitation and\naggression, present a significant health and safety risk in residential care\nsettings. Many care facilities have video cameras in place for digital\nmonitoring of public spaces, which can be leveraged to develop an automated\nbehaviours of risk detection system that can alert the staff to enable timely\nintervention and prevent the situation from escalating. However, one of the\nchallenges in our previous study was the presence of false alarms due to\nobstruction of view by activities happening close to the camera. To address\nthis issue, we proposed a novel depth-weighted loss function to train a\ncustomized convolutional autoencoder to enforce equivalent importance to the\nevents happening both near and far from the cameras; thus, helping to reduce\nfalse alarms and making the method more suitable for real-world deployment. The\nproposed method was trained using data from nine participants with dementia\nacross three cameras situated in a specialized dementia unit and achieved an\narea under the curve of receiver operating characteristic of $0.852$, $0.81$\nand $0.768$ for the three cameras. Ablation analysis was conducted for the\nindividual components of the proposed method and the performance of the\nproposed method was investigated for participant-specific and sex-specific\nbehaviours of risk detection. The proposed method performed reasonably well in\ndetecting behaviours of risk in people with dementia motivating further\nresearch toward the development of a behaviours of risk detection system\nsuitable for deployment in video surveillance systems in care facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavioural and psychological symptoms of dementia, such as agitation and\naggression, present a significant health and safety risk in residential care\nsettings. Many care facilities have video cameras in place for digital\nmonitoring of public spaces, which can be leveraged to develop an automated\nbehaviours of risk detection system that can alert the staff to enable timely\nintervention and prevent the situation from escalating. However, one of the\nchallenges in our previous study was the presence of false alarms due to\nobstruction of view by activities happening close to the camera. To address\nthis issue, we proposed a novel depth-weighted loss function to train a\ncustomized convolutional autoencoder to enforce equivalent importance to the\nevents happening both near and far from the cameras; thus, helping to reduce\nfalse alarms and making the method more suitable for real-world deployment. The\nproposed method was trained using data from nine participants with dementia\nacross three cameras situated in a specialized dementia unit and achieved an\narea under the curve of receiver operating characteristic of $0.852$, $0.81$\nand $0.768$ for the three cameras. Ablation analysis was conducted for the\nindividual components of the proposed method and the performance of the\nproposed method was investigated for participant-specific and sex-specific\nbehaviours of risk detection. The proposed method performed reasonably well in\ndetecting behaviours of risk in people with dementia motivating further\nresearch toward the development of a behaviours of risk detection system\nsuitable for deployment in video surveillance systems in care facilities."
                },
                "authors": [
                    {
                        "name": "Pratik K. Mishra"
                    },
                    {
                        "name": "Irene Ballester"
                    },
                    {
                        "name": "Andrea Iaboni"
                    },
                    {
                        "name": "Bing Ye"
                    },
                    {
                        "name": "Kristine Newman"
                    },
                    {
                        "name": "Alex Mihailidis"
                    },
                    {
                        "name": "Shehroz S. Khan"
                    }
                ],
                "author_detail": {
                    "name": "Shehroz S. Khan"
                },
                "author": "Shehroz S. Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02053v2",
                "updated": "2024-08-28T04:04:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    4,
                    4,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-03T06:16:28Z",
                "published_parsed": [
                    2024,
                    2,
                    3,
                    6,
                    16,
                    28,
                    5,
                    34,
                    0
                ],
                "title": "Affordable Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable Generative Agents"
                },
                "summary": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\nhttps://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly advanced the\nsimulation of believable interactive agents. However, the substantial cost on\nmaintaining the prolonged agent interactions poses challenge over the\ndeployment of believable LLM-based agents. Therefore, in this paper, we develop\nAffordable Generative Agents (AGA), a framework for enabling the generation of\nbelievable and low-cost interactions on both agent-environment and inter-agents\nlevels. Specifically, for agent-environment interactions, we substitute\nrepetitive LLM inferences with learned policies; while for inter-agent\ninteractions, we model the social relationships between agents and compress\nauxiliary dialogue information. Extensive experiments on multiple environments\nshow the effectiveness and efficiency of our proposed framework. Also, we delve\ninto the mechanisms of emergent believable behaviors lying in LLM agents,\ndemonstrating that agents can only generate finite behaviors in fixed\nenvironments, based upon which, we understand ways to facilitate emergent\ninteraction behaviors. Our code is publicly available at:\nhttps://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents."
                },
                "authors": [
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15040v2",
                "updated": "2024-08-28T03:56:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    56,
                    37,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-27T13:10:05Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    10,
                    5,
                    1,
                    240,
                    0
                ],
                "title": "A Survey of Large Language Models for European Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for European Languages"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nhigh performance on a wide range of natural language tasks since the release of\nChatGPT. The LLMs learn to understand and generate language by training\nbillions of model parameters on vast volumes of text data. Despite being a\nrelatively new field, LLM research is rapidly advancing in various directions.\nIn this paper, we present an overview of LLM families, including LLaMA, PaLM,\nGPT, and MoE, and the methods developed to create and enhance LLMs for official\nEuropean Union (EU) languages. We provide a comprehensive summary of common\nmonolingual and multilingual datasets used for pretraining large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Wazir Ali"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v1",
                "updated": "2024-08-28T03:48:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_comment": "For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07611v2",
                "updated": "2024-08-28T03:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    47,
                    28,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-14T15:19:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions."
                },
                "authors": [
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Xuefeng Liang"
                    },
                    {
                        "name": "Yuhui Liu"
                    },
                    {
                        "name": "Kaihua Ni"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Zetian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zetian Hu"
                },
                "author": "Zetian Hu",
                "arxiv_comment": "8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12576v2",
                "updated": "2024-08-28T03:15:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    15,
                    10,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-17T14:02:01Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    14,
                    2,
                    1,
                    2,
                    199,
                    0
                ],
                "title": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework\n  Using Open EDA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework\n  Using Open EDA"
                },
                "summary": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Huawei Li"
                    },
                    {
                        "name": "Xiaowei Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Li"
                },
                "author": "Xiaowei Li",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15501v1",
                "updated": "2024-08-28T03:10:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    10,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T03:10:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    10,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "MODULI: Unlocking Preference Generalization via Diffusion Models for\n  Offline Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODULI: Unlocking Preference Generalization via Diffusion Models for\n  Offline Multi-Objective Reinforcement Learning"
                },
                "summary": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires\nextensive online interactions. Offline MORL provides a promising solution by\ntraining on pre-collected datasets to generalize to any preference upon\ndeployment. However, real-world offline datasets are often conservatively and\nnarrowly distributed, failing to comprehensively cover preferences, leading to\nthe emergence of out-of-distribution (OOD) preference areas. Existing offline\nMORL algorithms exhibit poor generalization to OOD preferences, resulting in\npolicies that do not align with preferences. Leveraging the excellent\nexpressive and generalization capabilities of diffusion models, we propose\nMODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs\na preference-conditioned diffusion model as a planner to generate trajectories\nthat align with various preferences and derive action for decision-making. To\nachieve accurate generation, MODULI introduces two return normalization methods\nunder diverse preferences for refining guidance. To further enhance\ngeneralization to OOD preferences, MODULI proposes a novel sliding guidance\nmechanism, which involves training an additional slider adapter to capture the\ndirection of preference changes. Incorporating the slider, it transitions from\nin-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires\nextensive online interactions. Offline MORL provides a promising solution by\ntraining on pre-collected datasets to generalize to any preference upon\ndeployment. However, real-world offline datasets are often conservatively and\nnarrowly distributed, failing to comprehensively cover preferences, leading to\nthe emergence of out-of-distribution (OOD) preference areas. Existing offline\nMORL algorithms exhibit poor generalization to OOD preferences, resulting in\npolicies that do not align with preferences. Leveraging the excellent\nexpressive and generalization capabilities of diffusion models, we propose\nMODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs\na preference-conditioned diffusion model as a planner to generate trajectories\nthat align with various preferences and derive action for decision-making. To\nachieve accurate generation, MODULI introduces two return normalization methods\nunder diverse preferences for refining guidance. To further enhance\ngeneralization to OOD preferences, MODULI proposes a novel sliding guidance\nmechanism, which involves training an additional slider adapter to capture the\ndirection of preference changes. Incorporating the slider, it transitions from\nin-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences."
                },
                "authors": [
                    {
                        "name": "Yifu Yuan"
                    },
                    {
                        "name": "Zhenrui Zheng"
                    },
                    {
                        "name": "Zibin Dong"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16905v2",
                "updated": "2024-08-28T02:37:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    37,
                    8,
                    2,
                    241,
                    0
                ],
                "published": "2024-02-24T21:36:26Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    21,
                    36,
                    26,
                    5,
                    55,
                    0
                ],
                "title": "Procedural Adherence and Interpretability Through Neuro-Symbolic\n  Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Adherence and Interpretability Through Neuro-Symbolic\n  Generative Agents"
                },
                "summary": "The surge in popularity of large language models (LLMs) has opened doors for\nnew approaches to the creation of interactive agents. However, managing and\ninterpreting the temporal behavior of such agents over the course of a\npotentially infinite interaction remain challenging. The stateful, long-term\nhorizon reasoning required for coherent agent behavior does not fit well into\nthe LLM paradigm. We propose a combination of formal logic-based program\nsynthesis and LLM content generation to bring guarantees of procedural\nadherence and interpretability to generative agent behavior. To illustrate the\nbenefit of procedural adherence and interpretability, we use Temporal Stream\nLogic (TSL) to generate an automaton that enforces an interpretable, high-level\ntemporal structure on an agent. With the automaton tracking the context of the\ninteraction and making decisions to guide the conversation accordingly, we can\ndrive content generation in a way that allows the LLM to focus on a shorter\ncontext window. We evaluated our approach on different tasks involved in\ncreating an interactive agent specialized for generating\nchoose-your-own-adventure games. We found that over all of the tasks, an\nautomaton-enhanced agent with procedural guarantees achieves at least 96%\nadherence to its temporal constraints, whereas a purely LLM-based agent\ndemonstrates as low as 14.67% adherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in popularity of large language models (LLMs) has opened doors for\nnew approaches to the creation of interactive agents. However, managing and\ninterpreting the temporal behavior of such agents over the course of a\npotentially infinite interaction remain challenging. The stateful, long-term\nhorizon reasoning required for coherent agent behavior does not fit well into\nthe LLM paradigm. We propose a combination of formal logic-based program\nsynthesis and LLM content generation to bring guarantees of procedural\nadherence and interpretability to generative agent behavior. To illustrate the\nbenefit of procedural adherence and interpretability, we use Temporal Stream\nLogic (TSL) to generate an automaton that enforces an interpretable, high-level\ntemporal structure on an agent. With the automaton tracking the context of the\ninteraction and making decisions to guide the conversation accordingly, we can\ndrive content generation in a way that allows the LLM to focus on a shorter\ncontext window. We evaluated our approach on different tasks involved in\ncreating an interactive agent specialized for generating\nchoose-your-own-adventure games. We found that over all of the tasks, an\nautomaton-enhanced agent with procedural guarantees achieves at least 96%\nadherence to its temporal constraints, whereas a purely LLM-based agent\ndemonstrates as low as 14.67% adherence."
                },
                "authors": [
                    {
                        "name": "Raven Rothkopf"
                    },
                    {
                        "name": "Hannah Tongxin Zeng"
                    },
                    {
                        "name": "Mark Santolucito"
                    }
                ],
                "author_detail": {
                    "name": "Mark Santolucito"
                },
                "author": "Mark Santolucito",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15491v1",
                "updated": "2024-08-28T02:31:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    31,
                    15,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T02:31:15Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    31,
                    15,
                    2,
                    241,
                    0
                ],
                "title": "Enhancing and Accelerating Large Language Models via Instruction-Aware\n  Contextual Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing and Accelerating Large Language Models via Instruction-Aware\n  Contextual Compression"
                },
                "summary": "Large Language Models (LLMs) have garnered widespread attention due to their\nremarkable performance across various tasks. However, to mitigate the issue of\nhallucinations, LLMs often incorporate retrieval-augmented pipeline to provide\nthem with rich external knowledge and context. Nevertheless, challenges stem\nfrom inaccurate and coarse-grained context retrieved from the retriever.\nSupplying irrelevant context to the LLMs can result in poorer responses,\nincreased inference latency, and higher costs. This paper introduces a method\ncalled Instruction-Aware Contextual Compression, which filters out less\ninformative content, thereby accelerating and enhancing the use of LLMs. The\nexperimental results demonstrate that Instruction-Aware Contextual Compression\nnotably reduces memory consumption and minimizes generation latency while\nmaintaining performance levels comparable to those achieved with the use of the\nfull context. Specifically, we achieved a 50% reduction in context-related\ncosts, resulting in a 5% reduction in inference memory usage and a 2.2-fold\nincrease in inference speed, with only a minor drop of 0.047 in Rouge-1. These\nfindings suggest that our method strikes an effective balance between\nefficiency and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have garnered widespread attention due to their\nremarkable performance across various tasks. However, to mitigate the issue of\nhallucinations, LLMs often incorporate retrieval-augmented pipeline to provide\nthem with rich external knowledge and context. Nevertheless, challenges stem\nfrom inaccurate and coarse-grained context retrieved from the retriever.\nSupplying irrelevant context to the LLMs can result in poorer responses,\nincreased inference latency, and higher costs. This paper introduces a method\ncalled Instruction-Aware Contextual Compression, which filters out less\ninformative content, thereby accelerating and enhancing the use of LLMs. The\nexperimental results demonstrate that Instruction-Aware Contextual Compression\nnotably reduces memory consumption and minimizes generation latency while\nmaintaining performance levels comparable to those achieved with the use of the\nfull context. Specifically, we achieved a 50% reduction in context-related\ncosts, resulting in a 5% reduction in inference memory usage and a 2.2-fold\nincrease in inference speed, with only a minor drop of 0.047 in Rouge-1. These\nfindings suggest that our method strikes an effective balance between\nefficiency and performance."
                },
                "authors": [
                    {
                        "name": "Haowen Hou"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Binwen Bai"
                    },
                    {
                        "name": "Xinxin Zhu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15488v1",
                "updated": "2024-08-28T02:27:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    27,
                    7,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T02:27:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    27,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services"
                },
                "summary": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks."
                },
                "authors": [
                    {
                        "name": "Jialin Wu"
                    },
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Shengyuan Pang"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Jiayang Xu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu",
                "arxiv_comment": "Accepted by ACM Conference on Computer and Communications Security\n  (CCS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05750v3",
                "updated": "2024-08-28T02:04:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    4,
                    24,
                    2,
                    241,
                    0
                ],
                "published": "2024-07-08T09:03:12Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    3,
                    12,
                    0,
                    190,
                    0
                ],
                "title": "Large Language Models Understand Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Understand Layout"
                },
                "summary": "Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems."
                },
                "authors": [
                    {
                        "name": "Weiming Li"
                    },
                    {
                        "name": "Manni Duan"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Yan Shao"
                    }
                ],
                "author_detail": {
                    "name": "Yan Shao"
                },
                "author": "Yan Shao",
                "arxiv_comment": "This paper has been accepted by ECAI-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15460v1",
                "updated": "2024-08-28T00:52:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    0,
                    52,
                    39,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T00:52:39Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    0,
                    52,
                    39,
                    2,
                    241,
                    0
                ],
                "title": "Lagrangian approach to origami vertex analysis: Kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lagrangian approach to origami vertex analysis: Kinematics"
                },
                "summary": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of origami in engineering has significantly expanded in recent years,\nspanning deployable structures across scales, folding robotics, and mechanical\nmetamaterials. However, finding foldable paths can be a formidable task as the\nkinematics are determined by a nonlinear system of equations, often with\nseveral degrees of freedom. In this work, we leverage a Lagrangian approach to\nderive reduced-order compatibility conditions for rigid-facet origami vertices\nwith reflection and rotational symmetries. Then, using the reduced-order\nconditions, we derive exact, multi-degree of freedom solutions for degree 6 and\ndegree 8 vertices with prescribed symmetries. The exact kinematic solutions\nallow us to efficiently investigate the topology of allowable kinematics,\nincluding the consideration of a self-contact constraint, and then visually\ninterpret the role of geometric design parameters on these admissible fold\npaths by monitoring the change in the kinematic topology. We then introduce a\nprocedure to construct lower symmetry kinematic solutions by breaking symmetry\nof higher order kinematic solutions in a systematic way that preserves\ncompatibility. The multi-degree of freedom solutions discovered here should\nassist with building intuition of the kinematic feasibility of higher degree\norigami vertices and also facilitate the development of new algorithmic\nprocedures for origami-engineering design."
                },
                "authors": [
                    {
                        "name": "Matthew Grasinger"
                    },
                    {
                        "name": "Andrew Gillman"
                    },
                    {
                        "name": "Philip Buskohl"
                    }
                ],
                "author_detail": {
                    "name": "Philip Buskohl"
                },
                "author": "Philip Buskohl",
                "arxiv_doi": "10.1098/rsta.2024.0203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rsta.2024.0203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Electronic supplementary information can be found at the published\n  article, https://doi.org/10.1098/rsta.2024.0203",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21209v2",
                "updated": "2024-08-27T23:13:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    23,
                    13,
                    18,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-30T21:44:48Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    44,
                    48,
                    1,
                    212,
                    0
                ],
                "title": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A\n  Study of the Allegheny Housing Assessment Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A\n  Study of the Allegheny Housing Assessment Tool"
                },
                "summary": "The demand for housing assistance across the United States far exceeds the\nsupply, leaving housing providers the task of prioritizing clients for receipt\nof this limited resource. To be eligible for federal funding, local\nhomelessness systems are required to implement assessment tools as part of\ntheir prioritization processes. The Vulnerability Index Service Prioritization\nDecision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool\nnationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial\nbias, which may lead to unwarranted racial disparities in housing provision. In\nresponse to these criticisms, some jurisdictions have developed alternative\ntools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to\nassess clients' risk levels. Drawing on data from its deployment, we conduct\ndescriptive and quantitative analyses to evaluate whether replacing the\nVI-SPDAT with the AHA affects racial disparities in housing allocation. We find\nthat the VI-SPDAT tended to assign higher risk scores to white clients and\nlower risk scores to Black clients, and that white clients were served at a\nhigher rates pre-AHA deployment. While post-deployment service decisions became\nbetter aligned with the AHA score, and the distribution of AHA scores is\nsimilar across racial groups, we do not find evidence of a corresponding\ndecrease in disparities in service rates. We attribute the persistent disparity\nto the use of Alt-AHA, a survey-based tool that is used in cases of low data\nquality, as well as group differences in eligibility-related factors, such as\nchronic homelessness and veteran status. We discuss the implications for\nhousing service systems seeking to reduce racial disparities in their service\ndelivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for housing assistance across the United States far exceeds the\nsupply, leaving housing providers the task of prioritizing clients for receipt\nof this limited resource. To be eligible for federal funding, local\nhomelessness systems are required to implement assessment tools as part of\ntheir prioritization processes. The Vulnerability Index Service Prioritization\nDecision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool\nnationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial\nbias, which may lead to unwarranted racial disparities in housing provision. In\nresponse to these criticisms, some jurisdictions have developed alternative\ntools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to\nassess clients' risk levels. Drawing on data from its deployment, we conduct\ndescriptive and quantitative analyses to evaluate whether replacing the\nVI-SPDAT with the AHA affects racial disparities in housing allocation. We find\nthat the VI-SPDAT tended to assign higher risk scores to white clients and\nlower risk scores to Black clients, and that white clients were served at a\nhigher rates pre-AHA deployment. While post-deployment service decisions became\nbetter aligned with the AHA score, and the distribution of AHA scores is\nsimilar across racial groups, we do not find evidence of a corresponding\ndecrease in disparities in service rates. We attribute the persistent disparity\nto the use of Alt-AHA, a survey-based tool that is used in cases of low data\nquality, as well as group differences in eligibility-related factors, such as\nchronic homelessness and veteran status. We discuss the implications for\nhousing service systems seeking to reduce racial disparities in their service\ndelivery."
                },
                "authors": [
                    {
                        "name": "Lingwei Cheng"
                    },
                    {
                        "name": "Cameron Drayton"
                    },
                    {
                        "name": "Alexandra Chouldechova"
                    },
                    {
                        "name": "Rhema Vaithianathan"
                    }
                ],
                "author_detail": {
                    "name": "Rhema Vaithianathan"
                },
                "author": "Rhema Vaithianathan",
                "arxiv_comment": "18 pages, 11 figures, AAAI/ACM AIES24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15425v1",
                "updated": "2024-08-27T21:57:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    57,
                    16,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T21:57:16Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    57,
                    16,
                    1,
                    240,
                    0
                ],
                "title": "Fast and Modular Autonomy Software for Autonomous Racing Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Modular Autonomy Software for Autonomous Racing Vehicles"
                },
                "summary": "Autonomous motorsports aim to replicate the human racecar driver with\nsoftware and sensors. As in traditional motorsports, Autonomous Racing Vehicles\n(ARVs) are pushed to their handling limits in multi-agent scenarios at\nextremely high ($\\geq 150mph$) speeds. This Operational Design Domain (ODD)\npresents unique challenges across the autonomy stack. The Indy Autonomous\nChallenge (IAC) is an international competition aiming to advance autonomous\nvehicle development through ARV competitions. While far from challenging what a\nhuman racecar driver can do, the IAC is pushing the state of the art by\nfacilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW\nTeam's approach to autonomous racing in the IAC. In this work, we present our\nmodular and fast approach to agent detection, motion planning and controls to\ncreate an autonomy stack. We also provide analysis of the performance of the\nsoftware stack in single and multi-agent scenarios for rapid deployment in a\nfast-paced competition environment. We also cover what did and did not work\nwhen deployed on a physical system the Dallara AV-21 platform and potential\nimprovements to address these shortcomings. Finally, we convey lessons learned\nand discuss limitations and future directions for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous motorsports aim to replicate the human racecar driver with\nsoftware and sensors. As in traditional motorsports, Autonomous Racing Vehicles\n(ARVs) are pushed to their handling limits in multi-agent scenarios at\nextremely high ($\\geq 150mph$) speeds. This Operational Design Domain (ODD)\npresents unique challenges across the autonomy stack. The Indy Autonomous\nChallenge (IAC) is an international competition aiming to advance autonomous\nvehicle development through ARV competitions. While far from challenging what a\nhuman racecar driver can do, the IAC is pushing the state of the art by\nfacilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW\nTeam's approach to autonomous racing in the IAC. In this work, we present our\nmodular and fast approach to agent detection, motion planning and controls to\ncreate an autonomy stack. We also provide analysis of the performance of the\nsoftware stack in single and multi-agent scenarios for rapid deployment in a\nfast-paced competition environment. We also cover what did and did not work\nwhen deployed on a physical system the Dallara AV-21 platform and potential\nimprovements to address these shortcomings. Finally, we convey lessons learned\nand discuss limitations and future directions for improvement."
                },
                "authors": [
                    {
                        "name": "Andrew Saba"
                    },
                    {
                        "name": "Aderotimi Adetunji"
                    },
                    {
                        "name": "Adam Johnson"
                    },
                    {
                        "name": "Aadi Kothari"
                    },
                    {
                        "name": "Matthew Sivaprakasam"
                    },
                    {
                        "name": "Joshua Spisak"
                    },
                    {
                        "name": "Prem Bharatia"
                    },
                    {
                        "name": "Arjun Chauhan"
                    },
                    {
                        "name": "Brendan Duff Jr."
                    },
                    {
                        "name": "Noah Gasparro"
                    },
                    {
                        "name": "Charles King"
                    },
                    {
                        "name": "Ryan Larkin"
                    },
                    {
                        "name": "Brian Mao"
                    },
                    {
                        "name": "Micah Nye"
                    },
                    {
                        "name": "Anjali Parashar"
                    },
                    {
                        "name": "Joseph Attias"
                    },
                    {
                        "name": "Aurimas Balciunas"
                    },
                    {
                        "name": "Austin Brown"
                    },
                    {
                        "name": "Chris Chang"
                    },
                    {
                        "name": "Ming Gao"
                    },
                    {
                        "name": "Cindy Heredia"
                    },
                    {
                        "name": "Andrew Keats"
                    },
                    {
                        "name": "Jose Lavariega"
                    },
                    {
                        "name": "William Muckelroy III"
                    },
                    {
                        "name": "Andre Slavescu"
                    },
                    {
                        "name": "Nickolas Stathas"
                    },
                    {
                        "name": "Nayana Suvarna"
                    },
                    {
                        "name": "Chuan Tian Zhang"
                    },
                    {
                        "name": "Sebastian Scherer"
                    },
                    {
                        "name": "Deva Ramanan"
                    }
                ],
                "author_detail": {
                    "name": "Deva Ramanan"
                },
                "author": "Deva Ramanan",
                "arxiv_doi": "10.55417/fr.2024001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.55417/fr.2024001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Journal of Field Robotics",
                "arxiv_journal_ref": "Field Robotics Volume 4 (2024) 1-45",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15411v1",
                "updated": "2024-08-27T21:21:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    21,
                    13,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T21:21:13Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    21,
                    13,
                    1,
                    240,
                    0
                ],
                "title": "AUTOGENICS: Automated Generation of Context-Aware Inline Comments for\n  Code Snippets on Programming Q&A Sites Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTOGENICS: Automated Generation of Context-Aware Inline Comments for\n  Code Snippets on Programming Q&A Sites Using LLM"
                },
                "summary": "Inline comments in the source code facilitate easy comprehension,\nreusability, and enhanced readability. However, code snippets in answers on Q&A\nsites like Stack Overflow (SO) often lack comments because answerers volunteer\ntheir time and often skip comments or explanations due to time constraints.\nExisting studies show that these online code examples are difficult to read and\nunderstand, making it difficult for developers (especially novices) to use them\ncorrectly and leading to misuse. Given these challenges, we introduced\nAUTOGENICS, a tool designed to integrate with SO to generate effective inline\ncomments for code snippets in SO answers exploiting large language models\n(LLMs). Our contributions are threefold. First, we randomly select 400 answer\ncode snippets from SO and generate inline comments for them using LLMs. We then\nmanually evaluate these comments' effectiveness using four key metrics:\naccuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate\npromising effectiveness in generating inline comments for SO answer code\nsnippets. Second, we surveyed 14 active SO users to perceive the effectiveness\nof these inline comments. The survey results are consistent with our previous\nmanual evaluation. However, according to our evaluation, LLMs-generated\ncomments are less effective for shorter code snippets and sometimes produce\nnoisy comments. Third, to address the gaps, we introduced AUTOGENICS, which\nextracts additional context from question texts and generates context-aware\ninline comments. It also optimizes comments by removing noise (e.g., comments\nin import statements and variable declarations). We evaluate the effectiveness\nof AUTOGENICS-generated comments using the same four metrics that outperform\nthose of standard LLMs. AUTOGENICS might (a) enhance code comprehension, (b)\nsave time, and improve developers' ability to learn and reuse code more\naccurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inline comments in the source code facilitate easy comprehension,\nreusability, and enhanced readability. However, code snippets in answers on Q&A\nsites like Stack Overflow (SO) often lack comments because answerers volunteer\ntheir time and often skip comments or explanations due to time constraints.\nExisting studies show that these online code examples are difficult to read and\nunderstand, making it difficult for developers (especially novices) to use them\ncorrectly and leading to misuse. Given these challenges, we introduced\nAUTOGENICS, a tool designed to integrate with SO to generate effective inline\ncomments for code snippets in SO answers exploiting large language models\n(LLMs). Our contributions are threefold. First, we randomly select 400 answer\ncode snippets from SO and generate inline comments for them using LLMs. We then\nmanually evaluate these comments' effectiveness using four key metrics:\naccuracy, adequacy, conciseness, and usefulness. Overall, LLMs demonstrate\npromising effectiveness in generating inline comments for SO answer code\nsnippets. Second, we surveyed 14 active SO users to perceive the effectiveness\nof these inline comments. The survey results are consistent with our previous\nmanual evaluation. However, according to our evaluation, LLMs-generated\ncomments are less effective for shorter code snippets and sometimes produce\nnoisy comments. Third, to address the gaps, we introduced AUTOGENICS, which\nextracts additional context from question texts and generates context-aware\ninline comments. It also optimizes comments by removing noise (e.g., comments\nin import statements and variable declarations). We evaluate the effectiveness\nof AUTOGENICS-generated comments using the same four metrics that outperform\nthose of standard LLMs. AUTOGENICS might (a) enhance code comprehension, (b)\nsave time, and improve developers' ability to learn and reuse code more\naccurately."
                },
                "authors": [
                    {
                        "name": "Suborno Deb Bappon"
                    },
                    {
                        "name": "Saikat Mondal"
                    },
                    {
                        "name": "Banani Roy"
                    }
                ],
                "author_detail": {
                    "name": "Banani Roy"
                },
                "author": "Banani Roy",
                "arxiv_comment": "Accepted for presentation in the research track at the IEEE\n  International Conference on Source Code Analysis & Manipulation (SCAM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15409v2",
                "updated": "2024-08-29T17:00:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    0,
                    24,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-27T21:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    19,
                    37,
                    1,
                    240,
                    0
                ],
                "title": "Awes, Laws, and Flaws From Today's LLM Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awes, Laws, and Flaws From Today's LLM Research"
                },
                "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works based on criteria typical of what is considered good research\n(e.g. presence of statistical tests and reproducibility) and cross-validate it\nwith arguments that are at the centre of controversy (e.g., claims of emergent\nbehaviour, the use of LLMs as evaluators). We find multiple trends, such as\ndeclines in claims of emergent behaviour and ethics disclaimers; the rise of\nLLMs as evaluators in spite of a lack of consensus from the community about\ntheir useability; and an increase of claims of LLM reasoning abilities,\ntypically without leveraging human evaluation. This paper underscores the need\nfor more scrutiny and rigour by and from this field to live up to the\nfundamentals of a responsible scientific method that is ethical, reproducible,\nsystematic, and open to criticism."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter",
                "arxiv_comment": "Under review -- v1 was an old draft with an unrevised abstract (oops)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03416v3",
                "updated": "2024-08-27T19:10:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    19,
                    10,
                    23,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-06T19:30:49Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    19,
                    30,
                    49,
                    1,
                    219,
                    0
                ],
                "title": "The AI-Native Software Development Lifecycle: A Theoretical and\n  Practical New Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI-Native Software Development Lifecycle: A Theoretical and\n  Practical New Methodology"
                },
                "summary": "As AI continues to advance and impact every phase of the software development\nlifecycle (SDLC), a need for a new way of building software will emerge. By\nanalyzing the factors that influence the current state of the SDLC and how\nthose will change with AI we propose a new model of development. This white\npaper proposes the emergence of a fully AI-native SDLC, where AI is integrated\nseamlessly into every phase of development, from planning to deployment. We\nintroduce the V-Bounce model, an adaptation of the traditional V-model that\nincorporates AI from end to end. The V-Bounce model leverages AI to\ndramatically reduce time spent in implementation phases, shifting emphasis\ntowards requirements gathering, architecture design, and continuous validation.\nThis model redefines the role of humans from primary implementers to primarily\nvalidators and verifiers with AI acting as an implementation engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance and impact every phase of the software development\nlifecycle (SDLC), a need for a new way of building software will emerge. By\nanalyzing the factors that influence the current state of the SDLC and how\nthose will change with AI we propose a new model of development. This white\npaper proposes the emergence of a fully AI-native SDLC, where AI is integrated\nseamlessly into every phase of development, from planning to deployment. We\nintroduce the V-Bounce model, an adaptation of the traditional V-model that\nincorporates AI from end to end. The V-Bounce model leverages AI to\ndramatically reduce time spent in implementation phases, shifting emphasis\ntowards requirements gathering, architecture design, and continuous validation.\nThis model redefines the role of humans from primary implementers to primarily\nvalidators and verifiers with AI acting as an implementation engine."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    }
                ],
                "author_detail": {
                    "name": "Cory Hymel"
                },
                "author": "Cory Hymel",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15339v1",
                "updated": "2024-08-27T18:04:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    4,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T18:04:07Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    4,
                    7,
                    1,
                    240,
                    0
                ],
                "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized\n  Implicit Reward Function"
                },
                "summary": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Bin Bi"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Shiva Kumar Pentyala"
                    },
                    {
                        "name": "Zixu James Zhu"
                    },
                    {
                        "name": "Sitaram Asur"
                    },
                    {
                        "name": "Na Claire Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Na Claire Cheng"
                },
                "author": "Na Claire Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15337v1",
                "updated": "2024-08-27T18:01:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    1,
                    22,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T18:01:22Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    18,
                    1,
                    22,
                    1,
                    240,
                    0
                ],
                "title": "A Multi-Agent Reinforcement Learning Scheme for SFC Placement in Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Reinforcement Learning Scheme for SFC Placement in Edge\n  Computing Networks"
                },
                "summary": "In the 5G era and beyond, it is favorable to deploy latency-sensitive and\nreliability-aware services on edge computing networks in which the computing\nand network resources are more limited compared to cloud and core networks but\ncan respond more promptly. These services can be composed as Service Function\nChains (SFCs) which consist of a sequence of ordered Virtual Network Functions\n(VNFs). To achieve efficient edge resources allocation for SFC requests and\noptimal profit for edge service providers, we formulate the SFC placement\nproblem in an edge environment and propose a multi-agent Reinforcement Learning\n(RL) scheme to address the problem. The proposed scheme employs a set of RL\nagents to collaboratively make SFC placement decisions, such as path selection,\nVNF configuration, and VNF deployment. Simulation results show our model can\nimprove the profit of edge service providers by 12\\% compared with a heuristic\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the 5G era and beyond, it is favorable to deploy latency-sensitive and\nreliability-aware services on edge computing networks in which the computing\nand network resources are more limited compared to cloud and core networks but\ncan respond more promptly. These services can be composed as Service Function\nChains (SFCs) which consist of a sequence of ordered Virtual Network Functions\n(VNFs). To achieve efficient edge resources allocation for SFC requests and\noptimal profit for edge service providers, we formulate the SFC placement\nproblem in an edge environment and propose a multi-agent Reinforcement Learning\n(RL) scheme to address the problem. The proposed scheme employs a set of RL\nagents to collaboratively make SFC placement decisions, such as path selection,\nVNF configuration, and VNF deployment. Simulation results show our model can\nimprove the profit of edge service providers by 12\\% compared with a heuristic\nsolution."
                },
                "authors": [
                    {
                        "name": "Congzhou Li"
                    },
                    {
                        "name": "Zhouxiang Wu"
                    },
                    {
                        "name": "Divya Khanure"
                    },
                    {
                        "name": "Jason P. Jue"
                    }
                ],
                "author_detail": {
                    "name": "Jason P. Jue"
                },
                "author": "Jason P. Jue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15240v1",
                "updated": "2024-08-27T17:57:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:57:45Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    45,
                    1,
                    240,
                    0
                ],
                "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
                },
                "summary": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers or reward models are often used to enhance the reasoning\nperformance of large language models (LLMs). A common approach is the Best-of-N\nmethod, where N candidate solutions generated by the LLM are ranked by a\nverifier, and the best one is selected. While LLM-based verifiers are typically\ntrained as discriminative classifiers to score solutions, they do not utilize\nthe text generation capabilities of pretrained LLMs. To overcome this\nlimitation, we instead propose training verifiers using the ubiquitous\nnext-token prediction objective, jointly on verification and solution\ngeneration. Compared to standard verifiers, such generative verifiers (GenRM)\ncan benefit from several advantages of LLMs: they integrate seamlessly with\ninstruction tuning, enable chain-of-thought reasoning, and can utilize\nadditional inference-time compute via majority voting for better verification.\nWe demonstrate that when using Gemma-based verifiers on algorithmic and\ngrade-school math reasoning tasks, GenRM outperforms discriminative verifiers\nand LLM-as-a-Judge, showing a 16-64% improvement in the percentage of problems\nsolved with Best-of-N. Furthermore, we show that GenRM scales favorably across\ndataset size, model capacity, and inference-time compute."
                },
                "authors": [
                    {
                        "name": "Lunjun Zhang"
                    },
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Aviral Kumar"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16553v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16553v7",
                "updated": "2024-08-27T17:57:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    57,
                    7,
                    1,
                    240,
                    0
                ],
                "published": "2024-01-29T20:44:10Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    20,
                    44,
                    10,
                    0,
                    29,
                    0
                ],
                "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"
                },
                "summary": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning benefits from large and diverse datasets; however,\ncreating such datasets involves a high cost of human labeling. While synthetic\ndatasets generated by large language models (LLMs) have partly solved this\nissue, they often contain low-quality data. One effective solution is\nselectively annotating unlabelled instructions, especially given the relative\nease of acquiring unlabeled instructions or texts from various sources.\nHowever, how to select unlabelled instructions is not well-explored, especially\nin the context of LLMs. Therefore, we introduce SelectLLM, an alternative\nframework that leverages the capabilities of LLMs to select unlabeled\ninstructions more effectively. Specifically, SelectLLM consists of two key\nsteps: Coreset-based clustering of unlabelled instructions for enlarging\ndiversity and prompting of LLM to identify the most beneficial instructions\nwithin each cluster. We evaluate SelectLLM on AlpacaEval2 and MT-Bench,\ndemonstrating its ability to outperform state-of-the-art methods like\nAlpagasus. In addition, we compare the performance and compatibility of\nSelectLLM with various LLMs, such as ChatGPT, LLaMA-3.1-70B, and Gemma-2-27b.\nSelectLLM's adaptability and robustness are further evidenced by its ability to\nmaintain high performance across both human and synthetic datasets. All code\nand data are publicly available (https://github.com/minnesotanlp/select-llm)."
                },
                "authors": [
                    {
                        "name": "Ritik Sachin Parkar"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:\n  Jong Inn Park | PI: Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16553v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16553v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v1",
                "updated": "2024-08-27T17:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "Code is open-sourced at https://github.com/jxiw/MambaInLlama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15231v1",
                "updated": "2024-08-27T17:48:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:48:29Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    48,
                    29,
                    1,
                    240,
                    0
                ],
                "title": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain"
                },
                "summary": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Arjun Roy"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "Under Review; 10 pages content, 3 pages appendix, 4 figures, 8\n  tables; Code TBD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01419v2",
                "updated": "2024-08-27T17:45:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    45,
                    27,
                    1,
                    240,
                    0
                ],
                "published": "2023-11-02T17:33:47Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    17,
                    33,
                    47,
                    3,
                    306,
                    0
                ],
                "title": "C3DM: Constrained-Context Conditional Diffusion Models for Imitation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3DM: Constrained-Context Conditional Diffusion Models for Imitation\n  Learning"
                },
                "summary": "Behavior Cloning (BC) methods are effective at learning complex manipulation\ntasks. However, they are prone to spurious correlation - expressive models may\nfocus on distractors that are irrelevant to action prediction - and are thus\nfragile in real-world deployment. Prior methods have addressed this challenge\nby exploring different model architectures and action representations. However,\nnone were able to balance between sample efficiency and robustness against\ndistractors for solving manipulation tasks with a complex action space. We\npresent \\textbf{C}onstrained-\\textbf{C}ontext \\textbf{C}onditional\n\\textbf{D}iffusion \\textbf{M}odel (C3DM), a diffusion model policy for solving\n6-DoF robotic manipulation tasks with robustness to distractions that can learn\ndeployable robot policies from as little as five demonstrations. A key\ncomponent of C3DM is a fixation step that helps the action denoiser to focus on\ntask-relevant regions around a predicted fixation point while ignoring\ndistractors in the context. We empirically show that C3DM is robust to\nout-of-distribution distractors, and consistently achieves high success rates\non a wide array of tasks, ranging from table-top manipulation to industrial\nkitting that require varying levels of precision and robustness to distractors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Cloning (BC) methods are effective at learning complex manipulation\ntasks. However, they are prone to spurious correlation - expressive models may\nfocus on distractors that are irrelevant to action prediction - and are thus\nfragile in real-world deployment. Prior methods have addressed this challenge\nby exploring different model architectures and action representations. However,\nnone were able to balance between sample efficiency and robustness against\ndistractors for solving manipulation tasks with a complex action space. We\npresent \\textbf{C}onstrained-\\textbf{C}ontext \\textbf{C}onditional\n\\textbf{D}iffusion \\textbf{M}odel (C3DM), a diffusion model policy for solving\n6-DoF robotic manipulation tasks with robustness to distractions that can learn\ndeployable robot policies from as little as five demonstrations. A key\ncomponent of C3DM is a fixation step that helps the action denoiser to focus on\ntask-relevant regions around a predicted fixation point while ignoring\ndistractors in the context. We empirically show that C3DM is robust to\nout-of-distribution distractors, and consistently achieves high success rates\non a wide array of tasks, ranging from table-top manipulation to industrial\nkitting that require varying levels of precision and robustness to distractors."
                },
                "authors": [
                    {
                        "name": "Vaibhav Saxena"
                    },
                    {
                        "name": "Yotto Koga"
                    },
                    {
                        "name": "Danfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danfei Xu"
                },
                "author": "Danfei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15221v1",
                "updated": "2024-08-27T17:33:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:33:30Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    33,
                    30,
                    1,
                    240,
                    0
                ],
                "title": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet"
                },
                "summary": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses."
                },
                "authors": [
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Ian Steneker"
                    },
                    {
                        "name": "Willow Primack"
                    },
                    {
                        "name": "Riley Goodside"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Cristina Menghini"
                    },
                    {
                        "name": "Summer Yue"
                    }
                ],
                "author_detail": {
                    "name": "Summer Yue"
                },
                "author": "Summer Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15219v1",
                "updated": "2024-08-27T17:31:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    26,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:31:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "FRAMER/Miu: Tagged Pointer-based Capability and Fundamental Cost of\n  Memory Safety & Coherence (Position Paper)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAMER/Miu: Tagged Pointer-based Capability and Fundamental Cost of\n  Memory Safety & Coherence (Position Paper)"
                },
                "summary": "Ensuring system correctness, such as memory safety, can eliminate security\nvulnerabilities that attackers could exploit in the first place. However, high\nand unpredictable performance degradation remains a primary challenge.\n  Recognizing that it is extremely difficult to achieve complete system\ncorrectness for production deployment, researchers make trade-offs between\nperformance, detection coverage, interoperability, precision, and detection\ntiming.\n  This research strikes a balance between comprehensive system protection and\nthe costs required to obtain it, identifies the desirable roles of software and\nhardware, and presents a tagged pointer-based capability system as a\nstand-alone software solution and a prototype for future hardware design. This\npaper presents follow-up plans for the FRAMER/Miu generic framework to achieve\nthese goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring system correctness, such as memory safety, can eliminate security\nvulnerabilities that attackers could exploit in the first place. However, high\nand unpredictable performance degradation remains a primary challenge.\n  Recognizing that it is extremely difficult to achieve complete system\ncorrectness for production deployment, researchers make trade-offs between\nperformance, detection coverage, interoperability, precision, and detection\ntiming.\n  This research strikes a balance between comprehensive system protection and\nthe costs required to obtain it, identifies the desirable roles of software and\nhardware, and presents a tagged pointer-based capability system as a\nstand-alone software solution and a prototype for future hardware design. This\npaper presents follow-up plans for the FRAMER/Miu generic framework to achieve\nthese goals."
                },
                "authors": [
                    {
                        "name": "Myoung Jin Nam"
                    }
                ],
                "author_detail": {
                    "name": "Myoung Jin Nam"
                },
                "author": "Myoung Jin Nam",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15313v1",
                "updated": "2024-08-27T17:31:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:31:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during the fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nthe supervised optimization, a labeling function is used to capture global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark including comprehensive discriminative and generative\ntasks for helpfulness and harmlessness. The results indicate that our method\nsignificantly outperforms existing approaches in both safety and helpfulness.\nMoreover, BFPO eliminates the need for human prompting and annotation in LLM\nfine-tuning while achieving the same level of safety as methods that heavily\nrely on human labor, with less than 10% of the computational resources. The\ntraining recipes and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during the fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nthe supervised optimization, a labeling function is used to capture global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark including comprehensive discriminative and generative\ntasks for helpfulness and harmlessness. The results indicate that our method\nsignificantly outperforms existing approaches in both safety and helpfulness.\nMoreover, BFPO eliminates the need for human prompting and annotation in LLM\nfine-tuning while achieving the same level of safety as methods that heavily\nrely on human labor, with less than 10% of the computational resources. The\ntraining recipes and models will be released."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15207v1",
                "updated": "2024-08-27T17:14:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:14:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    14,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Coverage Criteria in Large Language Models: An In-Depth\n  Study Through Jailbreak Attacks"
                },
                "summary": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement of large language models (LLMs) has profoundly shaped\nthe landscape of artificial intelligence; however, their deployment in\nsensitive domains raises grave concerns, particularly due to their\nsusceptibility to malicious exploitation. This situation underscores the\ninsufficiencies in pre-deployment testing, highlighting the urgent need for\nmore rigorous and comprehensive evaluation methods. This study presents a\ncomprehensive empirical analysis assessing the efficacy of conventional\ncoverage criteria in identifying these vulnerabilities, with a particular\nemphasis on the pressing issue of jailbreak attacks. Our investigation begins\nwith a clustering analysis of the hidden states in LLMs, demonstrating that\nintrinsic characteristics of these states can distinctly differentiate between\nvarious types of queries. Subsequently, we assess the performance of these\ncriteria across three critical dimensions: criterion level, layer level, and\ntoken level. Our findings uncover significant disparities in neuron activation\npatterns between the processing of normal and jailbreak queries, thereby\ncorroborating the clustering results. Leveraging these findings, we propose an\ninnovative approach for the real-time detection of jailbreak attacks by\nutilizing neural activation features. Our classifier demonstrates remarkable\naccuracy, averaging 96.33% in identifying jailbreak queries, including those\nthat could lead to adversarial attacks. The importance of our research lies in\nits comprehensive approach to addressing the intricate challenges of LLM\nsecurity. By enabling instantaneous detection from the model's first token\noutput, our method holds promise for future systems integrating LLMs, offering\nrobust real-time detection capabilities. This study advances our understanding\nof LLM security testing, and lays a critical foundation for the development of\nmore resilient AI systems."
                },
                "authors": [
                    {
                        "name": "Shide Zhou"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15204v1",
                "updated": "2024-08-27T17:03:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T17:03:18Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    3,
                    18,
                    1,
                    240,
                    0
                ],
                "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?"
                },
                "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems."
                },
                "authors": [
                    {
                        "name": "Kristina Gligoriƒá"
                    },
                    {
                        "name": "Tijana Zrnic"
                    },
                    {
                        "name": "Cinoo Lee"
                    },
                    {
                        "name": "Emmanuel J. Cand√®s"
                    },
                    {
                        "name": "Dan Jurafsky"
                    }
                ],
                "author_detail": {
                    "name": "Dan Jurafsky"
                },
                "author": "Dan Jurafsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10738v3",
                "updated": "2024-08-27T16:38:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    38,
                    50,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-16T17:11:44Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    11,
                    44,
                    1,
                    107,
                    0
                ],
                "title": "Quantum teleportation coexisting with classical communications in\n  optical fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum teleportation coexisting with classical communications in\n  optical fiber"
                },
                "summary": "The ability for quantum and conventional networks to operate in the same\noptical fibers would aid the deployment of quantum network technology on a\nlarge scale. Quantum teleportation is a fundamental operation in quantum\nnetworking, but has yet to be demonstrated in fibers populated with high-power\nconventional optical signals. Here we report to the best of our knowledge the\nfirst demonstration of quantum teleportation over fibers carrying conventional\ntelecommunications traffic. Quantum state transfer is achieved over a 30.2-km\nfiber carrying 400-Gbps C-band classical traffic with a Bell state measurement\nperformed at the fiber midpoint. To protect quantum fidelity from spontaneous\nRaman scattering noise, we use optimal O-band quantum channels, narrow\nspectro-temporal filtering, and multi-photon coincidence detection. Fidelity is\nshown to be well maintained with an elevated C-band classical power of 18.7\ndBm, which could support multiple classical channels totaling many terabits/s\naggregate data rates. These results show the feasibility of advanced quantum\nand classical network applications operating within a unified fiber\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability for quantum and conventional networks to operate in the same\noptical fibers would aid the deployment of quantum network technology on a\nlarge scale. Quantum teleportation is a fundamental operation in quantum\nnetworking, but has yet to be demonstrated in fibers populated with high-power\nconventional optical signals. Here we report to the best of our knowledge the\nfirst demonstration of quantum teleportation over fibers carrying conventional\ntelecommunications traffic. Quantum state transfer is achieved over a 30.2-km\nfiber carrying 400-Gbps C-band classical traffic with a Bell state measurement\nperformed at the fiber midpoint. To protect quantum fidelity from spontaneous\nRaman scattering noise, we use optimal O-band quantum channels, narrow\nspectro-temporal filtering, and multi-photon coincidence detection. Fidelity is\nshown to be well maintained with an elevated C-band classical power of 18.7\ndBm, which could support multiple classical channels totaling many terabits/s\naggregate data rates. These results show the feasibility of advanced quantum\nand classical network applications operating within a unified fiber\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Jordan M. Thomas"
                    },
                    {
                        "name": "Fei I. Yeh"
                    },
                    {
                        "name": "Jim Hao Chen"
                    },
                    {
                        "name": "Joe J. Mambretti"
                    },
                    {
                        "name": "Scott J. Kohlert"
                    },
                    {
                        "name": "Gregory S. Kanter"
                    },
                    {
                        "name": "Prem Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Prem Kumar"
                },
                "author": "Prem Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15172v1",
                "updated": "2024-08-27T16:10:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:10:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    10,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation"
                },
                "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been\nshown to enhance the effectiveness of enriching item descriptions, thereby\nimproving the accuracy of recommendation systems. However, most existing\napproaches either rely on text-only prompting or employ basic multimodal\nstrategies that do not fully exploit the complementary information available\nfrom both textual and visual modalities. This paper introduces a novel\nframework, Cross-Reflection Prompting, termed X-Reflect, designed to address\nthese limitations by prompting LMMs to explicitly identify and reconcile\nsupportive and conflicting information between text and images. By capturing\nnuanced insights from both modalities, this approach generates more\ncomprehensive and contextually richer item representations. Extensive\nexperiments conducted on two widely used benchmarks demonstrate that our method\noutperforms existing prompting baselines in downstream recommendation accuracy.\nAdditionally, we evaluate the generalizability of our framework across\ndifferent LMM backbones and the robustness of the prompting strategies,\noffering insights for optimization. This work underscores the importance of\nintegrating multimodal information and presents a novel solution for improving\nitem understanding in multimodal recommendation systems."
                },
                "authors": [
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Md Mehrab Tanjim"
                    },
                    {
                        "name": "Stefano Petrangeli"
                    },
                    {
                        "name": "Somdeb Sarkhel"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15171v1",
                "updated": "2024-08-27T16:09:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T16:09:56Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    9,
                    56,
                    1,
                    240,
                    0
                ],
                "title": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring text summarization factuality using atomic facts entailment\n  metrics in the context of retrieval augmented generation"
                },
                "summary": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) has significantly increased since the\nintroduction of ChatGPT in 2022, demonstrating their value across various\napplications. However, a major challenge for enterprise and commercial adoption\nof LLMs is their tendency to generate inaccurate information, a phenomenon\nknown as \"hallucination.\" This project proposes a method for estimating the\nfactuality of a summary generated by LLMs when compared to a source text. Our\napproach utilizes Naive Bayes classification to assess the accuracy of the\ncontent produced."
                },
                "authors": [
                    {
                        "name": "N. E. Kriman"
                    }
                ],
                "author_detail": {
                    "name": "N. E. Kriman"
                },
                "author": "N. E. Kriman",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13993v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13993v3",
                "updated": "2024-08-27T15:56:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    56,
                    33,
                    1,
                    240,
                    0
                ],
                "published": "2024-04-22T08:59:35Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    59,
                    35,
                    0,
                    113,
                    0
                ],
                "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion"
                },
                "summary": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series."
                },
                "authors": [
                    {
                        "name": "Yingxuan Li"
                    },
                    {
                        "name": "Ryota Hinami"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yusuke Matsui"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Matsui"
                },
                "author": "Yusuke Matsui",
                "arxiv_comment": "Accepted to ACM Multimedia 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13993v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13993v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15147v1",
                "updated": "2024-08-27T15:40:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    40,
                    11,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:40:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    40,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "Blackbox optimization for origami-inspired bistable structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blackbox optimization for origami-inspired bistable structures"
                },
                "summary": "Bistable mechanical systems exhibit two stable configurations where the\nelastic energy is locally minimized. To realize such systems, origami\ntechniques have been proposed as a versatile platform to design deployable\nstructures with both compact and functional stable states. Conceptually, a\nbistable origami motif is composed of two-dimensional surfaces connected by\none-dimensional fold lines. This leads to stable configurations exhibiting\nzero-energy local minima. Physically, origami-inspired structures are\nthree-dimensional, comprising facets and hinges fabricated in a distinct stable\nstate where residual stresses are minimized. This leads to the dominance of one\nstable state over the other. To improve mechanical performance, one can solve\nthe constrained optimization problem of maximizing the bistability of origami\nstructures, defined as the amount of elastic energy required to switch between\nstable states, while ensuring materials used for the facets and hinges remain\nwithin their elastic regime. In this study, the Mesh Adaptive Direct Search\n(MADS) algorithm, a blackbox optimization technique, is used to solve the\nconstrained optimization problem. The bistable waterbomb-base origami motif is\nselected as a case-study to present the methodology. The elastic energy of this\norigami pattern under deployment is calculated via Finite Element simulations\nwhich serve as the blackbox in the MADS optimization loop. To validate the\nresults, optimized waterbomb-base geometries are built via Fused Filament\nFabrication and their response under loading is characterized experimentally on\na Uniaxial Test Machine. Ultimately, our method offers a general framework for\noptimizing bistability in mechanical systems, presenting opportunities for\nadvancement across various engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bistable mechanical systems exhibit two stable configurations where the\nelastic energy is locally minimized. To realize such systems, origami\ntechniques have been proposed as a versatile platform to design deployable\nstructures with both compact and functional stable states. Conceptually, a\nbistable origami motif is composed of two-dimensional surfaces connected by\none-dimensional fold lines. This leads to stable configurations exhibiting\nzero-energy local minima. Physically, origami-inspired structures are\nthree-dimensional, comprising facets and hinges fabricated in a distinct stable\nstate where residual stresses are minimized. This leads to the dominance of one\nstable state over the other. To improve mechanical performance, one can solve\nthe constrained optimization problem of maximizing the bistability of origami\nstructures, defined as the amount of elastic energy required to switch between\nstable states, while ensuring materials used for the facets and hinges remain\nwithin their elastic regime. In this study, the Mesh Adaptive Direct Search\n(MADS) algorithm, a blackbox optimization technique, is used to solve the\nconstrained optimization problem. The bistable waterbomb-base origami motif is\nselected as a case-study to present the methodology. The elastic energy of this\norigami pattern under deployment is calculated via Finite Element simulations\nwhich serve as the blackbox in the MADS optimization loop. To validate the\nresults, optimized waterbomb-base geometries are built via Fused Filament\nFabrication and their response under loading is characterized experimentally on\na Uniaxial Test Machine. Ultimately, our method offers a general framework for\noptimizing bistability in mechanical systems, presenting opportunities for\nadvancement across various engineering applications."
                },
                "authors": [
                    {
                        "name": "Luca Boisneault"
                    },
                    {
                        "name": "Charles Audet"
                    },
                    {
                        "name": "David Melancon"
                    }
                ],
                "author_detail": {
                    "name": "David Melancon"
                },
                "author": "David Melancon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v2",
                "updated": "2024-08-27T15:16:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    16,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Large Language Model-based Multi-Agent Clinical\n  Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based\n  Triage and Treatment Planning in Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15133v1",
                "updated": "2024-08-27T15:13:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:13:06Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    13,
                    6,
                    1,
                    240,
                    0
                ],
                "title": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Explaining Sets of Counterfactual Examples to Final Users"
                },
                "summary": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality is vital for understanding true cause-and-effect relationships\nbetween variables within predictive models, rather than relying on mere\ncorrelations, making it highly relevant in the field of Explainable AI. In an\nautomated decision-making scenario, causal inference methods can analyze the\nunderlying data-generation process, enabling explanations of a model's decision\nby manipulating features and creating counterfactual examples. These\ncounterfactuals explore hypothetical scenarios where a minimal number of\nfactors are altered, providing end-users with valuable information on how to\nchange their situation. However, interpreting a set of multiple counterfactuals\ncan be challenging for end-users who are not used to analyzing raw data\nrecords. In our work, we propose a novel multi-step pipeline that uses\ncounterfactuals to generate natural language explanations of actions that will\nlead to a change in outcome in classifiers of tabular data using LLMs. This\npipeline is designed to guide the LLM through smaller tasks that mimic human\nreasoning when explaining a decision based on counterfactual cases. We\nconducted various experiments using a public dataset and proposed a method of\nclosed-loop evaluation to assess the coherence of the final explanation with\nthe counterfactuals, as well as the quality of the content. Results are\npromising, although further experiments with other datasets and human\nevaluations should be carried out."
                },
                "authors": [
                    {
                        "name": "Arturo Fredes"
                    },
                    {
                        "name": "Jordi Vitria"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Vitria"
                },
                "author": "Jordi Vitria",
                "arxiv_comment": "Presented as a poster in the 2nd Workshop on Causal Inference and\n  Machine Learning in Practice at KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.16264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.16264v3",
                "updated": "2024-08-27T15:09:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    9,
                    40,
                    1,
                    240,
                    0
                ],
                "published": "2023-08-30T18:45:21Z",
                "published_parsed": [
                    2023,
                    8,
                    30,
                    18,
                    45,
                    21,
                    2,
                    242,
                    0
                ],
                "title": "Resource Placement for Rate and Fidelity Maximization in Quantum\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Placement for Rate and Fidelity Maximization in Quantum\n  Networks"
                },
                "summary": "Existing classical optical network infrastructure cannot be immediately used\nfor quantum network applications due to photon loss. The first step towards\nenabling quantum networks is the integration of quantum repeaters into optical\nnetworks. However, the expenses and intrinsic noise inherent in quantum\nhardware underscore the need for an efficient deployment strategy that\noptimizes the allocation of quantum repeaters and memories. In this paper, we\npresent a comprehensive framework for network planning, aiming to efficiently\ndistributing quantum repeaters across existing infrastructure, with the\nobjective of maximizing quantum network utility within an entanglement\ndistribution network. We apply our framework to several cases including a\npreliminary illustration of a dumbbell network topology and real-world cases of\nthe SURFnet and ESnet. We explore the effect of quantum memory multiplexing\nwithin quantum repeaters, as well as the influence of memory coherence time on\nquantum network utility. We further examine the effects of different fairness\nassumptions on network planning, uncovering their impacts on real-time network\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing classical optical network infrastructure cannot be immediately used\nfor quantum network applications due to photon loss. The first step towards\nenabling quantum networks is the integration of quantum repeaters into optical\nnetworks. However, the expenses and intrinsic noise inherent in quantum\nhardware underscore the need for an efficient deployment strategy that\noptimizes the allocation of quantum repeaters and memories. In this paper, we\npresent a comprehensive framework for network planning, aiming to efficiently\ndistributing quantum repeaters across existing infrastructure, with the\nobjective of maximizing quantum network utility within an entanglement\ndistribution network. We apply our framework to several cases including a\npreliminary illustration of a dumbbell network topology and real-world cases of\nthe SURFnet and ESnet. We explore the effect of quantum memory multiplexing\nwithin quantum repeaters, as well as the influence of memory coherence time on\nquantum network utility. We further examine the effects of different fairness\nassumptions on network planning, uncovering their impacts on real-time network\nperformance."
                },
                "authors": [
                    {
                        "name": "Shahrooz Pouryousef"
                    },
                    {
                        "name": "Hassan Shapourian"
                    },
                    {
                        "name": "Alireza Shabani"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Don Towsley"
                    }
                ],
                "author_detail": {
                    "name": "Don Towsley"
                },
                "author": "Don Towsley",
                "arxiv_comment": "18 pages, 8 figures, 3 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.16264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.16264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13960v2",
                "updated": "2024-08-27T15:06:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    6,
                    17,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-25T23:48:11Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    23,
                    48,
                    11,
                    6,
                    238,
                    0
                ],
                "title": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Analysis for Education: Methods, Applications, and Future\n  Directions"
                },
                "summary": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the collection and analysis of sequential educational\ndata have brought time series analysis to a pivotal position in educational\nresearch, highlighting its essential role in facilitating data-driven\ndecision-making. However, there is a lack of comprehensive summaries that\nconsolidate these advancements. To the best of our knowledge, this paper is the\nfirst to provide a comprehensive review of time series analysis techniques\nspecifically within the educational context. We begin by exploring the\nlandscape of educational data analytics, categorizing various data sources and\ntypes relevant to education. We then review four prominent time series\nmethods-forecasting, classification, clustering, and anomaly\ndetection-illustrating their specific application points in educational\nsettings. Subsequently, we present a range of educational scenarios and\napplications, focusing on how these methods are employed to address diverse\neducational tasks, which highlights the practical integration of multiple time\nseries methods to solve complex educational problems. Finally, we conclude with\na discussion on future directions, including personalized learning analytics,\nmultimodal data fusion, and the role of large language models (LLMs) in\neducational time series. The contributions of this paper include a detailed\ntaxonomy of educational data, a synthesis of time series techniques with\nspecific educational applications, and a forward-looking perspective on\nemerging trends and future research opportunities in educational analysis. The\nrelated papers and resources are available and regularly updated at the project\npage."
                },
                "authors": [
                    {
                        "name": "Shengzhong Mao"
                    },
                    {
                        "name": "Chaoli Zhang"
                    },
                    {
                        "name": "Yichi Song"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Xiao-Jun Zeng"
                    },
                    {
                        "name": "Zenglin Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "24 pages, 3 figures, 6 tables, project page: see\n  https://github.com/ai-for-edu/time-series-analysis-for-education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15301v1",
                "updated": "2024-08-27T15:03:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    3,
                    1,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T15:03:01Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    3,
                    1,
                    1,
                    240,
                    0
                ],
                "title": "The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical\n  Study"
                },
                "summary": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying\nlarge language models (LLMs) efficiently. Among various bit widths and\nrepresentations for weights and activations, the 8-bit integer weight and 8-bit\ninteger activation (W8A8) configuration is particularly popular due to its\nwidespread hardware support. However, the impact of W8A8 post-training\nquantization on model accuracy remains contentious. While several studies have\nsuggested calibrating either weights or activations to mitigate accuracy\ndegradation, a comprehensive solution has yet to be identified. In this paper,\nwe empirically investigate multiple LLMs featured on an open LLM leaderboard,\ndiscovering that the LLaMA3-70B model series have a unique accuracy degradation\nbehavior with W8A8 per-channel post-training quantization. In contrast, other\nmodel series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and\nFalcon demonstrate robust performance with W8A8, sometimes surpassing their\nFP16 counterparts. Contrary to previous assertions attributing degradation to\nthe large dynamic range of activations, our findings indicate that the weight\ndistribution of the LLaMA3-70B is the primary factor behind the vulnerability.\nBy meticulously analyzing the distinct characteristics of weight distributions\nacross Transformer blocks, we propose a mixed strategy with less than 3% of the\nlayers enabling finer W8A8 quantization granularity, while the remaining 97% of\nlayers retain the per-channel configuration. As a result, the average accuracy\nof LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of\nLLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires\nneither calibration nor fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying\nlarge language models (LLMs) efficiently. Among various bit widths and\nrepresentations for weights and activations, the 8-bit integer weight and 8-bit\ninteger activation (W8A8) configuration is particularly popular due to its\nwidespread hardware support. However, the impact of W8A8 post-training\nquantization on model accuracy remains contentious. While several studies have\nsuggested calibrating either weights or activations to mitigate accuracy\ndegradation, a comprehensive solution has yet to be identified. In this paper,\nwe empirically investigate multiple LLMs featured on an open LLM leaderboard,\ndiscovering that the LLaMA3-70B model series have a unique accuracy degradation\nbehavior with W8A8 per-channel post-training quantization. In contrast, other\nmodel series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and\nFalcon demonstrate robust performance with W8A8, sometimes surpassing their\nFP16 counterparts. Contrary to previous assertions attributing degradation to\nthe large dynamic range of activations, our findings indicate that the weight\ndistribution of the LLaMA3-70B is the primary factor behind the vulnerability.\nBy meticulously analyzing the distinct characteristics of weight distributions\nacross Transformer blocks, we propose a mixed strategy with less than 3% of the\nlayers enabling finer W8A8 quantization granularity, while the remaining 97% of\nlayers retain the per-channel configuration. As a result, the average accuracy\nof LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of\nLLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires\nneither calibration nor fine-tuning."
                },
                "authors": [
                    {
                        "name": "Minghai Qin"
                    }
                ],
                "author_detail": {
                    "name": "Minghai Qin"
                },
                "author": "Minghai Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]